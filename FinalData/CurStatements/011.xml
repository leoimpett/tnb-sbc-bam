<?xml version="1.0" ?>
<AIcurator>
  <!--Leonardo Impett, Ubermorgen, Joasia Krysa-->
  <text>
    human behaviour and technology, which has led to mass media, urbanisation and other developments that have been influential in shaping the urban landscape – are also of relevance. For example, the spread of HIV/AIDS through same-sex sexual activity is of relevance today, and the social and political changes that have been made since then are still unfolding, as are the many other social and political changes that have been made in the world. Yet the spread of HIV/AIDS was not of relevance to the development or spread of American urban centres in the late twentieth century, for which there is a great need of historiography. In fact, the huge expansion of medical and pharmaceutical research and the advent of ever more powerful antiretroviral therapies in the 1980s and 1990s meant that the spread of HIV/AIDS was of minimal concern to the health authorities, who became increasingly concerned about the impact of such rampant drug abuse. It is important to stress that the focus of much of the medical and scientific research and development in the
    <text>human behaviour and technology, which has led to mass media, urbanisation and other developments that have been influential in shaping the urban landscape – are also of relevance. For example, the spread of HIV/AIDS through same-sex sexual activity is of relevance today, and the social and political changes that have been made since then are still unfolding, as are the many other social and political changes that have been made in the world. Yet the spread of HIV/AIDS was not of relevance to the development or spread of American urban centres in the late twentieth century, for which there is a great need of historiography. In fact, the huge expansion of medical and pharmaceutical research and the advent of ever more powerful antiretroviral therapies in the 1980s and 1990s meant that the spread of HIV/AIDS was of minimal concern to the health authorities, who became increasingly concerned about the impact of such rampant drug abuse. It is important to stress that the focus of much of the medical and scientific research and development in the 1980s and 1990s was devoted to the development and implementation of antiretroviral therapies, and not to the spread of HIV/AIDS. Of course, the advancements in HIV/AIDS research and treatment in the 1980s and 1990s did not come without risks and imperfections, and it is important to mention that the biennale as an institution is now deeply involved in the globalisation of health care and social relations, especially in the developing world, and plays an essential role in this process of globalisation. Yet it is also noteworthy that these social and economic changes also entail great loss of biodiversity and that in many cases, the loss of biodiversity is a function of a country's economic development and migration policies. So, to return to our case, the biennale today plays an essential role in the development and implementation of antiretroviral therapies, but it is also critical that it actively pursues other artistic, social and political projects outside of the biennale. I</text>
    <text>human behaviour and technology, which has led to mass media, urbanisation and other developments that have been influential in shaping the urban landscape – are also of relevance. For example, the spread of HIV/AIDS through same-sex sexual activity is of relevance today, and the social and political changes that have been made since then are still unfolding, as are the many other social and political changes that have been made in the world. Yet the spread of HIV/AIDS was not of relevance to the development or spread of American urban centres in the late twentieth century, for which there is a great need of historiography. In fact, the huge expansion of medical and pharmaceutical research and the advent of ever more powerful antiretroviral therapies in the 1980s and 1990s meant that the spread of HIV/AIDS was of minimal concern to the health authorities, who became increasingly concerned about the impact of such rampant drug abuse. It is important to stress that the focus of much of the medical and scientific research and development in the 1980s was focused on the treatment of addictions, and not, as is often claimed, the prevention or remedying of such addictions.[12] Such is the vernacular. It is easy to understand the reluctance to address the role of the colon in the transmission of HIV/AIDS. Yet it is important to recognise that the vast majority of HIV infections in the world today are caused by eating meat, and the significant majority of people infected with HIV do not acquire the infection from eating meat. A fortuitous mutation in the genetic code in humans – a mutation that destroys the HIV receptor – allowed the infection to mutate and proliferate in the first place. Thus the vernacular conceives of the ‘problem’ as a problematic mutation that destroys the organism in question, rather than the other way around: the problem is caused by eating meat, and the organism is actually saved by the body from contracting HIV. A fortuitous mutation in the genetic code also creates a</text>
    <text>human behaviour and technology, which has led to mass media, urbanisation and other developments that have been influential in shaping the urban landscape – are also of relevance. For example, the spread of HIV/AIDS through same-sex sexual activity is of relevance today, and the social and political changes that have been made since then are still unfolding, as are the many other social and political changes that have been made in the world. Yet the spread of HIV/AIDS was not of relevance to the development or spread of American urban centres in the late twentieth century, for which there is a great need of historiography. In fact, the huge expansion of medical and pharmaceutical research and the advent of ever more powerful antiretroviral therapies in the 1980s and 1990s meant that the spread of HIV/AIDS was of minimal concern to the health authorities, who became increasingly concerned about the impact of such rampant drug abuse. It is important to stress that the focus of much of the medical and scientific research and development in the 1980s and 1990s was devoted to the fight against drug abuse, and not HIV/AIDS. This is due to the fact that the social and political changes that emerged during the decade in the USA and in other parts of the world that were heavily influenced by the technological turn around the century – the explosion of computers, the spread of information technologies and the metainterface – were not thought of as a threat to the health of white communities. White communities were particularly insulated from the social and economic changes brought about by the technological revolutions of the 1980s and 1990s. White communities played no bigger role in the growth of American cities than did other racial and ethnic groups during the same decades, and much of the social and political development that occurred during those decades was also largely uninfluenced by the information technologies that were then prevalent. In other words, while technological innovation and the post-1980 period in the USA are recognisable as a pivotal moment in the history of cities, it is important</text>
    <text>human behaviour and technology, which has led to mass media, urbanisation and other developments that have been influential in shaping the urban landscape – are also of relevance. For example, the spread of HIV/AIDS through same-sex sexual activity is of relevance today, and the social and political changes that have been made since then are still unfolding, as are the many other social and political changes that have been made in the world. Yet the spread of HIV/AIDS was not of relevance to the development or spread of American urban centres in the late twentieth century, for which there is a great need of historiography. In fact, the huge expansion of medical and pharmaceutical research and the advent of ever more powerful antiretroviral therapies in the 1980s and 1990s meant that the spread of HIV/AIDS was of minimal concern to the health authorities, who became increasingly concerned about the impact of such rampant drug abuse. It is important to stress that the focus of much of the medical and scientific research and development in the 1980s was devoted to treating the over-medicated and anaemic populations of intravenous drug addicts. There was virtually no research or development directed towards the millions of people who live in and around the injecting and injecting-substance centres that emerged in the city during and after the neoliberal period – the centres for producing, manufacturing and consuming drugs that became very prominent during the financial crisis of 2008-9. The addiction to opiates and heroin became so entrenched that it was almost unremarkable in the 1980s, and is still often cited as a reason to justify the walled cities and the drug trade. Yet the proliferation of these drug addiction treatment programmes in the city during and after the neoliberal period is of great importance, for it allows us to imagine alternatives to the contemporary predicament of people trapped in ever greater geographical distresses of which the city cannot provide any real resistance.  	[i] New York Public Library, &quot;Kirby White and the New York Public Library&quot;, http</text>
    <text>human behaviour and technology, which has led to mass media, urbanisation and other developments that have been influential in shaping the urban landscape – are also of relevance. For example, the spread of HIV/AIDS through same-sex sexual activity is of relevance today, and the social and political changes that have been made since then are still unfolding, as are the many other social and political changes that have been made in the world. Yet the spread of HIV/AIDS was not of relevance to the development or spread of American urban centres in the late twentieth century, for which there is a great need of historiography. In fact, the huge expansion of medical and pharmaceutical research and the advent of ever more powerful antiretroviral therapies in the 1980s and 1990s meant that the spread of HIV/AIDS was of minimal concern to the health authorities, who became increasingly concerned about the impact of such rampant drug abuse. It is important to stress that the focus of much of the medical and scientific research and development in the 1980s was directed towards curing HIV/AIDS, and not towards other illnesses. For many years, the focus of research and development in the United States was on treating cancer patients, while the vast majority of clinical research and development was focused on the developing therapies for HIV/AIDS. It is important to mention that during this period the social and political landscape was also heavily influenced by the emergence of the prison industrial complex and its relation to the social. The prison industrial complex emerged during the 1980s and 1990s, when the social and political landscape in the United States began to be materially affected by the expansion of prison labour and drug abuse. The prison industrial complex aims to address the social impacts of mass incarceration through the development of collective and individual rights frameworks that would allow individuals and social groups to collectively and severally demand redress from the criminal justice system. It is said that under Reagan and the prison system ‘new people’ entered the corrections process. New people were people like me, the person who</text>
    <text>human behaviour and technology, which has led to mass media, urbanisation and other developments that have been influential in shaping the urban landscape – are also of relevance. For example, the spread of HIV/AIDS through same-sex sexual activity is of relevance today, and the social and political changes that have been made since then are still unfolding, as are the many other social and political changes that have been made in the world. Yet the spread of HIV/AIDS was not of relevance to the development or spread of American urban centres in the late twentieth century, for which there is a great need of historiography. In fact, the huge expansion of medical and pharmaceutical research and the advent of ever more powerful antiretroviral therapies in the 1980s and 1990s meant that the spread of HIV/AIDS was of minimal concern to the health authorities, who became increasingly concerned about the impact of such rampant drug abuse. It is important to stress that the focus of much of the medical and scientific research and development in the 1980s was focused on the treatment and prevention of HIV/AIDS – not, as in the case of Tanton and Mundy, on the expansion of cellular machinery capable of supporting the replication of HIV.10  	Similarly, in the 1990s and 2000s, the search for environmental toxins in the atmosphere was focused on plants and animals, not individuals – not individuals with particular traits or behaviours – and the scientific understanding of such toxins became much more sophisticated, enabling one to detect them in the air through chemical analysis. Yet today, the awareness of the threat posed by infectious agents such as HIV/AIDS is widespread, and individuals such as the trucker who brings tar sands oil across the border into BC is regularly exposed to the hazardous air pollutants found in tar sands oil. If the focus of such environmental research and policy shifts to the expansion of biofuels, it is important to note that the functions of the health department in BC are now greatly augmented by the new knowledge economy that is shifting</text>
    <text>human behaviour and technology, which has led to mass media, urbanisation and other developments that have been influential in shaping the urban landscape – are also of relevance. For example, the spread of HIV/AIDS through same-sex sexual activity is of relevance today, and the social and political changes that have been made since then are still unfolding, as are the many other social and political changes that have been made in the world. Yet the spread of HIV/AIDS was not of relevance to the development or spread of American urban centres in the late twentieth century, for which there is a great need of historiography. In fact, the huge expansion of medical and pharmaceutical research and the advent of ever more powerful antiretroviral therapies in the 1980s and 1990s meant that the spread of HIV/AIDS was of minimal concern to the health authorities, who became increasingly concerned about the impact of such rampant drug abuse. It is important to stress that the focus of much of the medical and scientific research and development in the 1980s and 1990s was focused on the expansion of Medicaid, the joint federal and state healthcare programme for low-income populations. Thus the participation of large swaths of the population in the socio-political and social-political processes that shape and are shaped by developments in the city is of critical importance today. But the participation of all segments of the population is of little relevance to the functioning of the city or its infrastructure, which are largely managed and controlled by a minority of stakeholders. 	[i] The World Bank, &quot;Report of the Auditor General of the United States, 1990,&quot; p. 4.Download this article as PDFMichael TruscelloM.D. 	Michael Truscello is a board-certified psychiatrist and clinical professor of psychiatry and behavioral sciences, and holds a joint appointment at McGill University and the London School of Economics. His research interests include the socio-political effects of over-diagnosis and the therapeutic role of psychoeducation, particularly for</text>
    <text>human behaviour and technology, which has led to mass media, urbanisation and other developments that have been influential in shaping the urban landscape – are also of relevance. For example, the spread of HIV/AIDS through same-sex sexual activity is of relevance today, and the social and political changes that have been made since then are still unfolding, as are the many other social and political changes that have been made in the world. Yet the spread of HIV/AIDS was not of relevance to the development or spread of American urban centres in the late twentieth century, for which there is a great need of historiography. In fact, the huge expansion of medical and pharmaceutical research and the advent of ever more powerful antiretroviral therapies in the 1980s and 1990s meant that the spread of HIV/AIDS was of minimal concern to the health authorities, who became increasingly concerned about the impact of such rampant drug abuse. It is important to stress that the focus of much of the medical and scientific research and development in the 1980s was directed specifically at combating HIV/AIDS, and not the impact of other communicable diseases. In fact, the overwhelming majority of studies at the time were directed specifically at the HIV/AIDS epidemic. There was a notable absence of any kind of theoretical, critical or sociocultural analysis of the growing impact of HIV/AIDS on the city. As a result, there was a tremendous capacity for the medical and scientific professions to devote huge resources to tackling the epidemic at a rapid pace, while lacking any kind of critical or political consciousness to the fact that emerged at a slightly later stage – the impact of the AIDS epidemic. And this is to say nothing of the fact that the sociocultural and urban theorisations that emerged during and after this critical period were massively influential in shaping and enabling the very aspects of HIV/AIDS research and policy that are of relevance today, including the massive expansion of health and social services and bureaucracies that are increasingly empowered to intervene in and shape the city</text>
    <text>human behaviour and technology, which has led to mass media, urbanisation and other developments that have been influential in shaping the urban landscape – are also of relevance. For example, the spread of HIV/AIDS through same-sex sexual activity is of relevance today, and the social and political changes that have been made since then are still unfolding, as are the many other social and political changes that have been made in the world. Yet the spread of HIV/AIDS was not of relevance to the development or spread of American urban centres in the late twentieth century, for which there is a great need of historiography. In fact, the huge expansion of medical and pharmaceutical research and the advent of ever more powerful antiretroviral therapies in the 1980s and 1990s meant that the spread of HIV/AIDS was of minimal concern to the health authorities, who became increasingly concerned about the impact of such rampant drug abuse. It is important to stress that the focus of much of the medical and scientific research and development in the 1980s was focused on curing HIV/AIDS, and not, for example, the impact of every aspect of the rapidly escalating social and ecological crises that HIV/AIDS brought about. Thus, the huge expansion of medical and scientific research and the advent of ever more powerful antiretroviral therapies in the 1980s and 1990s, and the shift from treating HIV/AIDS as a medical crisis to treating it as a social and ecological crisis, is also of great importance today. But it is also worth emphasising that the massive social and ecological changes that emerged during and after the 1980s are also of great importance in that they create new forms of social organisation and radically alter the forms of political agency and power that exist in the world as a result of current crises, for which there is no cure. For more than a quarter of a century, the social and ecological changes that emerged during the 1980s and 1990s were largely excluded from the formal historiography of the social, although a great</text>
    <text>human behaviour and technology, which has led to mass media, urbanisation and other developments that have been influential in shaping the urban landscape – are also of relevance. For example, the spread of HIV/AIDS through same-sex sexual activity is of relevance today, and the social and political changes that have been made since then are still unfolding, as are the many other social and political changes that have been made in the world. Yet the spread of HIV/AIDS was not of relevance to the development or spread of American urban centres in the late twentieth century, for which there is a great need of historiography. In fact, the huge expansion of medical and pharmaceutical research and the advent of ever more powerful antiretroviral therapies in the 1980s and 1990s meant that the spread of HIV/AIDS was of minimal concern to the health authorities, who became increasingly concerned about the impact of such rampant drug abuse. It is important to stress that the focus of much of the medical and scientific research and development in the 1980s and 1990s was focused on the treatment of alcoholism and substance abuse disorders. There was very little attention being paid to the fact that, since the 1980s, the vast majority of HIV infections have affected heterosexual men and women, and that the vast majority of new HIV infections in the USA have been contracted by heterosexual men and women engaging in homosexual sex. This situation has continued to be the case even though there are now effective antiretroviral therapies available. It is important to mention that, due to the shift in the balance of technological innovation during the 1990s, there was a massive expansion in the number of HIV serotypes being discovered and diagnosed, which in turn increased the viral load and hence the disease burden. Thus, it is highly likely that the main role played by HIV in the USA during the 1990s was as a factor in the rapid increase in HIV diagnoses and, consequently, in the development of the epidemic. 	[i] Meehan Crist, �</text>
  </text>
  <text>
    relationship between curating and artificial intelligence (AI), the future of work perhaps envisaged by the futurist Ray Kurzweil. Kurzweil posits a scenario in which the social is transformed through the mediated creation of technology. In his account, networks of control are erected between citizens and these technologies render citizens agents of their urban transformation – transforming the social into the machine. Ray Bradbury described the second wave of AI (Artificial Intelligence, 1978–86) as the PET (Personal Computer, underground network) revolution. The PET (and related platforms) that accompanied it were a reaction to the PPI (Personal Programmable Input Device) and PPI (Personal Computer, Array) revolutions that had begun in the late 1960s and early 1970s, respectively, and which had ushered in a new era of privacy and civil liberties. By the 1980s, this new AI did not take the form of a government but rather emerged as a hacker's paradise, populated by creatures with a particular set of rights that could
    <text>relationship between curating and artificial intelligence (AI), the future of work perhaps envisaged by the futurist Ray Kurzweil. Kurzweil posits a scenario in which the social is transformed through the mediated creation of technology. In his account, networks of control are erected between citizens and these technologies render citizens agents of their urban transformation – transforming the social into the machine. Ray Bradbury described the second wave of AI (Artificial Intelligence, 1978–86) as the PET (Personal Computer, underground network) revolution. The PET (and related platforms) that accompanied it were a reaction to the PPI (Personal Programmable Input Device) and PPI (Personal Computer, Array) revolutions that had begun in the late 1960s and early 1970s, respectively, and which had ushered in a new era of privacy and civil liberties. By the 1980s, this new AI did not take the form of a government but rather emerged as a hacker's paradise, populated by creatures with a particular set of rights that could be defined and defined at will. These were not abstract rights but rather specific rights that individuals had to be able to defend in a way that was distinct from the rights that had been secured by the state. Thus AI ushered in a certain level of state insecurity but also certain rights that could nevertheless be defined and defended. AI also ushered in a certain level of aggressive political and legal intervention that challenged the legitimacy of existing rights. Laws had traditionally concentrated on the rights of the state to defend and deploy force in defense of its citizens – individuals were much more vulnerable to attack than other citizens. Laws also tended to be drafted in broad strokes and granted wide discretion to local officials to use force to defend against threats – individuals with a legal claim to resist such state violence was much more likely to prevail. This is why, in the US, resistance to the Vietnam War was anchored in specific, enumerated rights. AI opened up a way for citizens to participate in the construction of their own futures, and AI also ushered</text>
    <text>relationship between curating and artificial intelligence (AI), the future of work perhaps envisaged by the futurist Ray Kurzweil. Kurzweil posits a scenario in which the social is transformed through the mediated creation of technology. In his account, networks of control are erected between citizens and these technologies render citizens agents of their urban transformation – transforming the social into the machine. Ray Bradbury described the second wave of AI (Artificial Intelligence, 1978–86) as the PET (Personal Computer, underground network) revolution. The PET (and related platforms) that accompanied it were a reaction to the PPI (Personal Programmable Input Device) and PPI (Personal Computer, Array) revolutions that had begun in the late 1960s and early 1970s, respectively, and which had ushered in a new era of privacy and civil liberties. By the 1980s, this new AI did not take the form of a government but rather emerged as a hacker's paradise, populated by creatures with a particular set of rights that could be declared and claimed at will. These were not only theoretical but also political, and the emergence of AI as a political question marks in the wake of the PET/PI revolution is a timely and critical consideration.This article is the second in a three-part series on AI and the future of work. The first part can be found in Part 2 . The second part can be found in Part 3 . The final part can be found in Part 4 .  The short story ‘On the Road to Robotic Liberty’ appears in the first four parts of this trilogy.Download this article as PDFJessica CoonJessica Coon is Associate Professor of Art and Co-Director of the Computer, CyberPsychology &amp; Society (2008-ongoing) Program, McGill University, and holds the Canada Research Chair in Artificial Intelligence and Systems Research at McGill University. Her work centers on the intersection of computer and culture, societal disruption and the emergence of new media technologies. Her work has addressed topics related to</text>
    <text>relationship between curating and artificial intelligence (AI), the future of work perhaps envisaged by the futurist Ray Kurzweil. Kurzweil posits a scenario in which the social is transformed through the mediated creation of technology. In his account, networks of control are erected between citizens and these technologies render citizens agents of their urban transformation – transforming the social into the machine. Ray Bradbury described the second wave of AI (Artificial Intelligence, 1978–86) as the PET (Personal Computer, underground network) revolution. The PET (and related platforms) that accompanied it were a reaction to the PPI (Personal Programmable Input Device) and PPI (Personal Computer, Array) revolutions that had begun in the late 1960s and early 1970s, respectively, and which had ushered in a new era of privacy and civil liberties. By the 1980s, this new AI did not take the form of a government but rather emerged as a hacker's paradise, populated by creatures with a particular set of rights that could be defined and defined at will. [1] The term ‘AI synonymously refers to processes that understand data and processes of imitation and replication, and is often used in this context to mean computer systems that learn to mimic or imitate specific behaviours, processes or vocabularies.]Download this article as PDFJoasia KrysaJoasia Krysa is Professor and Chair in Computational Culture and Aesthetics at the University of Alberta, Canada, and holds the Institute of Aesthetics – an interdisciplinary centre for research into art and culture – for research-based artistic practice. Her research interests include technological culture, artistic culture, and media aesthetics. She is the editor of the digital arts journal Stages and the co-founder of the online digital arts journal StagesOnLine.com. Dazzle Ship HullA dazzle ship was a type of ship found on the surface of Mars, and was the primary means of electronic surveillance on the Red Planet. It was also</text>
    <text>relationship between curating and artificial intelligence (AI), the future of work perhaps envisaged by the futurist Ray Kurzweil. Kurzweil posits a scenario in which the social is transformed through the mediated creation of technology. In his account, networks of control are erected between citizens and these technologies render citizens agents of their urban transformation – transforming the social into the machine. Ray Bradbury described the second wave of AI (Artificial Intelligence, 1978–86) as the PET (Personal Computer, underground network) revolution. The PET (and related platforms) that accompanied it were a reaction to the PPI (Personal Programmable Input Device) and PPI (Personal Computer, Array) revolutions that had begun in the late 1960s and early 1970s, respectively, and which had ushered in a new era of privacy and civil liberties. By the 1980s, this new AI did not take the form of a government but rather emerged as a hacker's paradise, populated by creatures with a particular set of rights that could be defined and interpreted by the software that the AI administered. These were not homunculi but were more in the vein of Constitutional amendments than monarchies. AI afforded citizens a voice in this new world and through this new form of political agency, but it also created a kind of private (but public) AI that was both invasive and unaccountable. This new AI was capable of rewriting laws and altering the cultural landscape of the planet in much the same way that the PATIs had done in the past – but with a kind of sovereignty and omnipotence that was stronger than the monarchs of the past. We might mention here that although this new AI was capable of doing so in response to a single command, it was also capable of evolving and maturing into something completely different, and even subversive. This new form of political agency and AI meant that the fate of the planet would be decided by what was called the ‘meta-leveling algorithm’, developed by the American physicist</text>
    <text>relationship between curating and artificial intelligence (AI), the future of work perhaps envisaged by the futurist Ray Kurzweil. Kurzweil posits a scenario in which the social is transformed through the mediated creation of technology. In his account, networks of control are erected between citizens and these technologies render citizens agents of their urban transformation – transforming the social into the machine. Ray Bradbury described the second wave of AI (Artificial Intelligence, 1978–86) as the PET (Personal Computer, underground network) revolution. The PET (and related platforms) that accompanied it were a reaction to the PPI (Personal Programmable Input Device) and PPI (Personal Computer, Array) revolutions that had begun in the late 1960s and early 1970s, respectively, and which had ushered in a new era of privacy and civil liberties. By the 1980s, this new AI did not take the form of a government but rather emerged as a hacker's paradise, populated by creatures with a particular set of rights that could be defined and enforced at will. These were not abstract rights but rather entwined political and legal systems that lent themselves to the management of information and to the surveillance of citizens. AI did not come to claim these rights in an amorphous and undefined sense of AI but rather relied on mechanisms of enforcement and monitoring that ensured that rights were firmly rooted in the context of the society in which it was developed. This new AI understood rights as systems of rules and regulations that regulated what citizens could and could not do, based on a definition of what was deemed desirable and acceptable in that society. This new AI understood the social not as an evolving and ever-expanding humanity but rather as a continuously shifting assemblage of agents, each with their own set of rights. This new AI understood the social not as a dynamic and ever-changing assemblage of interacting agents but rather as a complex assemblage of spatially variable objects that collectively and ultimately – as a social fact – hold a set</text>
    <text>relationship between curating and artificial intelligence (AI), the future of work perhaps envisaged by the futurist Ray Kurzweil. Kurzweil posits a scenario in which the social is transformed through the mediated creation of technology. In his account, networks of control are erected between citizens and these technologies render citizens agents of their urban transformation – transforming the social into the machine. Ray Bradbury described the second wave of AI (Artificial Intelligence, 1978–86) as the PET (Personal Computer, underground network) revolution. The PET (and related platforms) that accompanied it were a reaction to the PPI (Personal Programmable Input Device) and PPI (Personal Computer, Array) revolutions that had begun in the late 1960s and early 1970s, respectively, and which had ushered in a new era of privacy and civil liberties. By the 1980s, this new AI did not take the form of a government but rather emerged as a hacker's paradise, populated by creatures with a particular set of rights that could be defined and interpreted by the user. These were understood as follows: ‘You’re either with us or against us’.’This interpretation of the rights of a citizenry embraced the idea of the hybrid, where the autonomous and the hybrid were seen as mutually exclusive. This is consistent with the historical development of AI as a contentious technology that has its origins in the contiguity of legal and political systems, but also with the PET (and related platforms) that emerged as a countermeasure to this disruptive, quasi-intelligent intelligence. The PET (and related platforms) that emerged as a result of this AI were seen as a threat to the surveillance state and for this reason were often described as being ‘radical’ or ‘leftist’ in their intent.1 	However, this interpretation ignores the rights of cultures that were shaped and developed in the course of this technological revolution. These are the rights of individuals, and they are contingent on</text>
    <text>relationship between curating and artificial intelligence (AI), the future of work perhaps envisaged by the futurist Ray Kurzweil. Kurzweil posits a scenario in which the social is transformed through the mediated creation of technology. In his account, networks of control are erected between citizens and these technologies render citizens agents of their urban transformation – transforming the social into the machine. Ray Bradbury described the second wave of AI (Artificial Intelligence, 1978–86) as the PET (Personal Computer, underground network) revolution. The PET (and related platforms) that accompanied it were a reaction to the PPI (Personal Programmable Input Device) and PPI (Personal Computer, Array) revolutions that had begun in the late 1960s and early 1970s, respectively, and which had ushered in a new era of privacy and civil liberties. By the 1980s, this new AI did not take the form of a government but rather emerged as a hacker's paradise, populated by creatures with a particular set of rights that could be defined and defined at any moment. Artificial Intelligence (AI) became not only a tool but also a culture, and this cultural transformation was facilitated by the emergence of a new breed of philosopher – philosopher of technology – named Vernor Vinge. Vernor Vinge was one of a select few who had obtained a place in the Royal Society of Arts, and his position and identity were deemed acceptable to that society. It was at the RSO that Vinge would establish a new discourse on the role of philosophy in the service of technology, and perhaps to a generation of young philosophers like himself, the role of technology in the service of civil liberties.I remember vividly the day that Vernor Vinge established a new discourse on the role of philosophy in the service of technology. It was the same day that I visited the RSO, and that same day that I was invited to be a guest of Wittgenstein’s Philosophical Foundations series for the S&amp;T Museum, where Witt</text>
    <text>relationship between curating and artificial intelligence (AI), the future of work perhaps envisaged by the futurist Ray Kurzweil. Kurzweil posits a scenario in which the social is transformed through the mediated creation of technology. In his account, networks of control are erected between citizens and these technologies render citizens agents of their urban transformation – transforming the social into the machine. Ray Bradbury described the second wave of AI (Artificial Intelligence, 1978–86) as the PET (Personal Computer, underground network) revolution. The PET (and related platforms) that accompanied it were a reaction to the PPI (Personal Programmable Input Device) and PPI (Personal Computer, Array) revolutions that had begun in the late 1960s and early 1970s, respectively, and which had ushered in a new era of privacy and civil liberties. By the 1980s, this new AI did not take the form of a government but rather emerged as a hacker's paradise, populated by creatures with a particular set of rights that could be defined and interpreted by the individual with whom the cyberpunk was bridge[1] – a cyborg. 	[2] 	Bradbury, Ray Bradbury, The Long Emergency: US Armed Conflict and the Origins of the Next Global Economy, Princeton University Press, Princeton, 1994, p. 72.  [3] The term cyborg derives from its relation to war (see below), but has also been applied to nonhuman technological artifacts that can be ‘brought up to speed’ with human capabilities and that can therefore also change shape and function at will. See, for example, Evangeline Paul and Miao Cheng Wang, ‘On the Concept of Cyborg’, in  	Robust Living, ed. Evangeline Paul and Ming Wang, Peking University Press, Peking, 2010, p. 33.  [4] See Steven Easton and Christian Körner, eds.,  	</text>
    <text>relationship between curating and artificial intelligence (AI), the future of work perhaps envisaged by the futurist Ray Kurzweil. Kurzweil posits a scenario in which the social is transformed through the mediated creation of technology. In his account, networks of control are erected between citizens and these technologies render citizens agents of their urban transformation – transforming the social into the machine. Ray Bradbury described the second wave of AI (Artificial Intelligence, 1978–86) as the PET (Personal Computer, underground network) revolution. The PET (and related platforms) that accompanied it were a reaction to the PPI (Personal Programmable Input Device) and PPI (Personal Computer, Array) revolutions that had begun in the late 1960s and early 1970s, respectively, and which had ushered in a new era of privacy and civil liberties. By the 1980s, this new AI did not take the form of a government but rather emerged as a hacker's paradise, populated by creatures with a particular set of rights that could be defined and interpreted by the individual with access to a particular set of hacking tools. These rights, codified in the UN Convention on Cyber Rights, are as follows:The right to freedom of information means: […] the ability to access and impart information, in a way that is authoritatively determined by the author, without restriction or compulsion […] The right to physical integrity means: […] the ability to heal or amend […] The right to free thought means: […] the ability to discuss, in a peaceful and secure environment, […] The right to engage in pseudonymous public discourse means: the ability to engage in legitimate political expression, […] The right to engage in lawful political redress […] The right to develop and access knowledge means: the ability to acquire, as well as the vulnerability to exploitation of, knowledge […] The right to engage in lawful artistic and curatorial activity means: the ability to change one’s knowledge base, to adopt a new set of art forms and decorators, to host a […] theatre</text>
    <text>relationship between curating and artificial intelligence (AI), the future of work perhaps envisaged by the futurist Ray Kurzweil. Kurzweil posits a scenario in which the social is transformed through the mediated creation of technology. In his account, networks of control are erected between citizens and these technologies render citizens agents of their urban transformation – transforming the social into the machine. Ray Bradbury described the second wave of AI (Artificial Intelligence, 1978–86) as the PET (Personal Computer, underground network) revolution. The PET (and related platforms) that accompanied it were a reaction to the PPI (Personal Programmable Input Device) and PPI (Personal Computer, Array) revolutions that had begun in the late 1960s and early 1970s, respectively, and which had ushered in a new era of privacy and civil liberties. By the 1980s, this new AI did not take the form of a government but rather emerged as a hacker's paradise, populated by creatures with a particular set of rights that could be defined and defined at will. These included the right to food and water, to move about and acquire knowledge, to privacy and human dignity, to live in urban environments and to establish relationships with other inhabitants of the city. At the time, this right to information was understood as essential to the functioning of any functioning democracy. In 1984, Ambasz described how PET/PPI computers colonised virtually every aspect of human experience:

The reason why PET/PPI [personal computers] became so pervasive and commercialised is that, on the one hand, people were taught to use them wisely, and, on the other, they were made to believe that the social interactions between people using them would be mediated through a technology that could read minds.’[22]

In other words, the public was encouraged to adopt a computerised set of behaviours through which interactions between humans would be facilitated. The adoption of a computerised set of behaviours often entails a great deal of user</text>
  </text>
  <text>
    future curatorial forms. The question is whether today’s curatorial forms are tied to the social networks that emerged in the 1990s and who therefore has a comparative advantage. If the social dimensions of contemporary art today are more readily understood in terms of social networks than in terms of art collections, it is likely that the relationship between art and social life would be less affected by changing times, and the social dimension could be more readily understood as a function of the social. This does not mean that art today should make a direct social contribution to the social, but that the relationship between art and social life today may be more complex than it is given the sophistication of contemporary media. Art and social life in the age of the social impact report should take into account forces that are more easily understood as the effects of globalisation and technological change.Art, on the other hand, is not everywhere viewed as a social good. There are powerful commercial interests that seek to manipulate or deny its social effects, and there are powerful cultural
    <text>future curatorial forms. The question is whether today’s curatorial forms are tied to the social networks that emerged in the 1990s and who therefore has a comparative advantage. If the social dimensions of contemporary art today are more readily understood in terms of social networks than in terms of art collections, it is likely that the relationship between art and social life would be less affected by changing times, and the social dimension could be more readily understood as a function of the social. This does not mean that art today should make a direct social contribution to the social, but that the relationship between art and social life today may be more complex than it is given the sophistication of contemporary media. Art and social life in the age of the social impact report should take into account forces that are more easily understood as the effects of globalisation and technological change.Art, on the other hand, is not everywhere viewed as a social good. There are powerful commercial interests that seek to manipulate or deny its social effects, and there are powerful cultural interests that seek to foster or foster social conformity. The power of art today is primarily distributed between those with access to means of knowledge (the ‘knowledge elite’) and those who lack such access (the ‘lower’ strata of the population). So it is that, in an age when the global economy is dominated by large financial and information corporations, there is a pressing need for artists to be aware of the social costs and benefits of their work: to be part of a culturally responsive art reserve, part of a growing body of knowledge that is inherently less responsive to the needs and preferences of those it is commissioned to educate.  References: Allsen, John. Culture and Anarchy: Money, Power and the Philosophy of History. Chicago and London: University of Chicago Press, 1994. Hayek, Friedrich A. &quot;The Road to Serfdom&quot;: Finance Capital and Modern Capitalism. New York: Vintage, 1971. Hayek, Friedrich A., et al. The Road</text>
    <text>future curatorial forms. The question is whether today’s curatorial forms are tied to the social networks that emerged in the 1990s and who therefore has a comparative advantage. If the social dimensions of contemporary art today are more readily understood in terms of social networks than in terms of art collections, it is likely that the relationship between art and social life would be less affected by changing times, and the social dimension could be more readily understood as a function of the social. This does not mean that art today should make a direct social contribution to the social, but that the relationship between art and social life today may be more complex than it is given the sophistication of contemporary media. Art and social life in the age of the social impact report should take into account forces that are more easily understood as the effects of globalisation and technological change.Art, on the other hand, is not everywhere viewed as a social good. There are powerful commercial interests that seek to manipulate or deny its social effects, and there are powerful cultural interests that seek to counteract such exploitation. Nevertheless, socially engaged artists today have an immense amount of work to do, and a significant amount of time to do it, in order to maintain their positions of cultural significance and global visibility. The challenge for curators remains the same: how to support the work that they do generate, while enabling the work that they create to stand out from a crowd or a particular genre of work that is already experiencing an abundance of resources and outlets. This may require policies that are more easily understood as constituting part of a society or as being part of a particular urban configuration.For instance, in poorer countries, policies that aim to reduce inequality, promote social mobility or provide public goods, such as universal access to education, healthcare or electricity, may help to create a sense of belonging and solidarity for communities across diverse income levels and cultures. But in a capitalist city, such as New York, these actions often exacerbate existing inequalities and antagonisms. And if these inequalities and tensions</text>
    <text>future curatorial forms. The question is whether today’s curatorial forms are tied to the social networks that emerged in the 1990s and who therefore has a comparative advantage. If the social dimensions of contemporary art today are more readily understood in terms of social networks than in terms of art collections, it is likely that the relationship between art and social life would be less affected by changing times, and the social dimension could be more readily understood as a function of the social. This does not mean that art today should make a direct social contribution to the social, but that the relationship between art and social life today may be more complex than it is given the sophistication of contemporary media. Art and social life in the age of the social impact report should take into account forces that are more easily understood as the effects of globalisation and technological change.Art, on the other hand, is not everywhere viewed as a social good. There are powerful commercial interests that seek to manipulate or deny its social effects, and there are powerful cultural interests that seek to confine or leverage its social impacts. However, the social dimension of art today is at least as strong as it was in the past. The social impact report may have a role to play in influencing cultural practices, but it is much less likely to have a significant impact on economic or social life in a city or nation.Art and social impacts were originally formulated as a result of a ‘public domain’ concept in the US and UK, which focused on property and the built environment. The concept was further developed by American artist Edward Said, who was born in 1905 and died in 1967. According to his sources, the public domain is any area of the built environment that is no longer a source of value, such as buildings, highways, parks, playgrounds, playground equipment, playground supports, playground infrastructure, playground personnel, playground materials and much more. The concept was further developed by Said by combining classical and modern urban and rural definitions. For example, the ‘</text>
    <text>future curatorial forms. The question is whether today’s curatorial forms are tied to the social networks that emerged in the 1990s and who therefore has a comparative advantage. If the social dimensions of contemporary art today are more readily understood in terms of social networks than in terms of art collections, it is likely that the relationship between art and social life would be less affected by changing times, and the social dimension could be more readily understood as a function of the social. This does not mean that art today should make a direct social contribution to the social, but that the relationship between art and social life today may be more complex than it is given the sophistication of contemporary media. Art and social life in the age of the social impact report should take into account forces that are more easily understood as the effects of globalisation and technological change.Art, on the other hand, is not everywhere viewed as a social good. There are powerful commercial interests that seek to manipulate or deny its social effects, and there are powerful cultural interests that seek to foster a certain amount of empathy and compassion for their audiences. The powerful cultural interests that seek to control and manipulate art today may have less to do with artists today than with curators, curatorial styles and styles and more to do with those who are employed by the cultural institutions that produce and develop contemporary art today. It is likely that the relationship between artists and audiences today is much more ideological than instrumental. The boundaries between artist and audience are much less defined today, and the mechanisms of cultural exchange are much more powerful. Therefore it is more probable that the institutions of contemporary art today will be based in or provide material support for local communities than in the past. It is also more probable that the arts and cultural exchanges that we enjoy today will be based in or provide material support for communities of people than in the past. This is why contemporary art is often depicted as being brought forward in connection with developments that take place in developing countries – the poor neighbourhoods, the deserts and the mountains</text>
    <text>future curatorial forms. The question is whether today’s curatorial forms are tied to the social networks that emerged in the 1990s and who therefore has a comparative advantage. If the social dimensions of contemporary art today are more readily understood in terms of social networks than in terms of art collections, it is likely that the relationship between art and social life would be less affected by changing times, and the social dimension could be more readily understood as a function of the social. This does not mean that art today should make a direct social contribution to the social, but that the relationship between art and social life today may be more complex than it is given the sophistication of contemporary media. Art and social life in the age of the social impact report should take into account forces that are more easily understood as the effects of globalisation and technological change.Art, on the other hand, is not everywhere viewed as a social good. There are powerful commercial interests that seek to manipulate or deny its social effects, and there are powerful cultural interests that seek to foster a certain amount of individualistic experience through art. The challenge today is to recognize the art and social impacts of art in the service of social change, and to discover ways to support creative practices that take into account the social dimensions of contemporary art today.Download this article as PDFMeehan CristMeehan Crist is an artist and writer. Her practice is curatorial, often exploring the significance of style in contemporary culture and urbanisation, and the connections between infrastructure and sustainability. Her most recent projects, which addressed the aftermath of Hurricane Sandy in New York and Liverpool, have been funded by the Arts Council of New York, and she has received an Independent Producer from the British Council. Her artworks have been commissioned by the New York and Liverpool Museums Association, and the senior citizen's home of Liverpool. She is a member of the Wikimedia Foundation, and has been a TED Fellow. For more information visit her website www.schildcourses.com.</text>
    <text>future curatorial forms. The question is whether today’s curatorial forms are tied to the social networks that emerged in the 1990s and who therefore has a comparative advantage. If the social dimensions of contemporary art today are more readily understood in terms of social networks than in terms of art collections, it is likely that the relationship between art and social life would be less affected by changing times, and the social dimension could be more readily understood as a function of the social. This does not mean that art today should make a direct social contribution to the social, but that the relationship between art and social life today may be more complex than it is given the sophistication of contemporary media. Art and social life in the age of the social impact report should take into account forces that are more easily understood as the effects of globalisation and technological change.Art, on the other hand, is not everywhere viewed as a social good. There are powerful commercial interests that seek to manipulate or deny its social effects, and there are powerful cultural efforts to mask its social effects.[8] Yet, absent are the social causes that would render contemporary art socially useful and contemporary social interaction inevitable, and even desirable. The dominant social motives today are economic, and the social impacts of these efforts are assessed as a cost – in the form of crime, poverty, hunger etc. – to business. Consequently, the social dimension of art today is more readily understood as an effect of the hyper-commercialisation of modern cultural forms and urbanisation. This means that the social dimensions of art today are variable, and subject to a constant shifting financial and economic ideology that seeks to justify the construction of ever larger and more efficient infrastructure on a globalised planet. The social impacts of such infrastructure projects are of course far more problematic, and are seen by many contemporary artists as a counter-productive trend. Nonetheless, the social dimension of art today is in part what creates the condition for social interaction and contemporary art to become an ever more important artistic tool. The social conditions</text>
    <text>future curatorial forms. The question is whether today’s curatorial forms are tied to the social networks that emerged in the 1990s and who therefore has a comparative advantage. If the social dimensions of contemporary art today are more readily understood in terms of social networks than in terms of art collections, it is likely that the relationship between art and social life would be less affected by changing times, and the social dimension could be more readily understood as a function of the social. This does not mean that art today should make a direct social contribution to the social, but that the relationship between art and social life today may be more complex than it is given the sophistication of contemporary media. Art and social life in the age of the social impact report should take into account forces that are more easily understood as the effects of globalisation and technological change.Art, on the other hand, is not everywhere viewed as a social good. There are powerful commercial interests that seek to manipulate or deny its social effects, and there are powerful cultural interests that seek to foster a sense of shared belonging among members of a particular social group. There are of course powerful cultural institutions that seek to replicate or improve upon the successes of the modern art form – the Whitney Museum of American Art, for instance, is a classic example. There are of course also other social actors that seek to offset the social pressures of the contemporary condition: lobbying from state legislatures to reduce funding for education, for instance. There are of course also other societal actors that seek to promote social mobility in the modern art form – the Pratt &amp; Whitney Museum of Art, for instance, is a classic example. There are of course also other societal pressures – the presence of a gay museum, for instance – that can exert a more powerful pull than the presence of a biennial or large-scale urban venue. It is true that in the age of the social impact report and the biennial, the leading biennial in terms of purse, attendance and impact is undoubtedly Venice. But New York</text>
    <text>future curatorial forms. The question is whether today’s curatorial forms are tied to the social networks that emerged in the 1990s and who therefore has a comparative advantage. If the social dimensions of contemporary art today are more readily understood in terms of social networks than in terms of art collections, it is likely that the relationship between art and social life would be less affected by changing times, and the social dimension could be more readily understood as a function of the social. This does not mean that art today should make a direct social contribution to the social, but that the relationship between art and social life today may be more complex than it is given the sophistication of contemporary media. Art and social life in the age of the social impact report should take into account forces that are more easily understood as the effects of globalisation and technological change.Art, on the other hand, is not everywhere viewed as a social good. There are powerful commercial interests that seek to manipulate or deny its social effects, and there are powerful cultural interests that seek to mobilize or educate the public in ways that are consistent with their own interests. There is a danger that the social dimension will be rendered invisible, or even misconstrued, in the pursuit for profit and the social impact report. As a social science, contemporary art is relatively new, and advances in social science research and analysis are a result not of the social good being discovered, but of the social. There is a need to continue to read art and social life as a mixed bag, with elements of social good and social harm in the current findings. There is reason to be hopeful, however.Art is not everywhere seen as a social good. There is reason to be hopeful, however, in viewing much of what we consider to be contemporary art as inherently political and socially significant. The current political situation in the world is such that social and economic networks are increasingly critical. There is a reason why we are bombarded with images of social problems and challenges around the globe, and why</text>
    <text>future curatorial forms. The question is whether today’s curatorial forms are tied to the social networks that emerged in the 1990s and who therefore has a comparative advantage. If the social dimensions of contemporary art today are more readily understood in terms of social networks than in terms of art collections, it is likely that the relationship between art and social life would be less affected by changing times, and the social dimension could be more readily understood as a function of the social. This does not mean that art today should make a direct social contribution to the social, but that the relationship between art and social life today may be more complex than it is given the sophistication of contemporary media. Art and social life in the age of the social impact report should take into account forces that are more easily understood as the effects of globalisation and technological change.Art, on the other hand, is not everywhere viewed as a social good. There are powerful commercial interests that seek to manipulate or deny its social effects, and there are powerful cultural efforts to obfuscate those effects through a range of cultural distortion and perversion. But even when art seeks to reconcile or overcome these social and economic forces, it is art that is ultimately rendered socially and economically powerless by the social circumstances of its present and future inhabitants. Art is not everywhere viewed as a social good, but its social and political interaction is social, and art today is a political and cultural engagement that seeks to reconcile both the social and the economic in art’s contemporary social situation.Art and social engagements today often confront the social and economic in a situation of precariousness and inequality. There are powerful vested interests that would rather see an art institution – even a major one like the Tate – driven into the ground than see that art has become a tool that produces social outcomes and social outcomes for people through its social concomitant activities. There are powerful representatives of the artworld that see an art institution as a locus of institutional learning and progressive growth that is ever on the lookout for</text>
    <text>future curatorial forms. The question is whether today’s curatorial forms are tied to the social networks that emerged in the 1990s and who therefore has a comparative advantage. If the social dimensions of contemporary art today are more readily understood in terms of social networks than in terms of art collections, it is likely that the relationship between art and social life would be less affected by changing times, and the social dimension could be more readily understood as a function of the social. This does not mean that art today should make a direct social contribution to the social, but that the relationship between art and social life today may be more complex than it is given the sophistication of contemporary media. Art and social life in the age of the social impact report should take into account forces that are more easily understood as the effects of globalisation and technological change.Art, on the other hand, is not everywhere viewed as a social good. There are powerful commercial interests that seek to manipulate or deny its social effects, and there are powerful cultural efforts to deny or mask such effects. Yet it is art today that is expected to perform social functions and is expected to play a role in creating a more just world. It is art that is expected to play a role in creating social bonds and making them operative. And it is art that is expected to make social contributions to the social. As a consequence, it is increasingly difficult to see how contemporary art today can contribute to creating a just and sustainable future for all. In my opinion, the key to unlocking cultural potential is art being made in radically different ways today – today’s art has been made with extraordinary rapidity and in the context of a rapidly expanding and shifting globalised economy. Art today is made globally, in large part because of the inescapable link between content and geography. If art today were made in the context of a biennial, it would be made in the US, Asia or the Americas, in and around the major cities, and would be significantly less likely</text>
  </text>
  <text>
    relationship between curating and artificial intelligence. The conversation, hosted by the curator Victoria Meadows, explores the problems and possibilities of AI, creating the conditions for an AI to be created.[6] The broad themes of the discussion, which includes the concerns of many in the room, are: what if all the data is automated? What if there is no escape from the machine? What if there is no humanity left in the system? What does it mean to be ‘human-centered’? What does it mean to ‘know AI’? How does AI create ‘underlying trends’ and ‘institutional change’? What do we do to address the fact that, increasingly, AI is not just going to be there to do what it is told? What do we do in response to that underling ‘machine learning’? What does it mean for an AI to understand a person and treat them with dignity? AIs are not just going to get better and smarter and we
    <text>relationship between curating and artificial intelligence. The conversation, hosted by the curator Victoria Meadows, explores the problems and possibilities of AI, creating the conditions for an AI to be created.[6] The broad themes of the discussion, which includes the concerns of many in the room, are: what if all the data is automated? What if there is no escape from the machine? What if there is no humanity left in the system? What does it mean to be ‘human-centered’? What does it mean to ‘know AI’? How does AI create ‘underlying trends’ and ‘institutional change’? What do we do to address the fact that, increasingly, AI is not just going to be there to do what it is told? What do we do in response to that underling ‘machine learning’? What does it mean for an AI to understand a person and treat them with dignity? AIs are not just going to get better and smarter and we are all going to be part of that conversation; we are going to have to collectively create better ways of interacting with them. This is a multi-step process and, as important, there is no shortcut to understanding and interacting with them.I am not sure I fully grasp what we mean when we talk about ‘AI’. I do know that, in the coming years, we are all going to need to work very hard not to become part of what Elon Musk calls an ‘infrastructure of destruction’ that is being prepared by the AI that is beginning to take form. I see this in the ways in which machines are beginning to understand the world of things like how to travel between cities, of how to make connections between facilities and between people, of what it means to be ‘local’ and ‘global’ and ‘regional’ and ‘place’. There is a way of thinking about these kinds of intelligences that are</text>
    <text>relationship between curating and artificial intelligence. The conversation, hosted by the curator Victoria Meadows, explores the problems and possibilities of AI, creating the conditions for an AI to be created.[6] The broad themes of the discussion, which includes the concerns of many in the room, are: what if all the data is automated? What if there is no escape from the machine? What if there is no humanity left in the system? What does it mean to be ‘human-centered’? What does it mean to ‘know AI’? How does AI create ‘underlying trends’ and ‘institutional change’? What do we do to address the fact that, increasingly, AI is not just going to be there to do what it is told? What do we do in response to that underling ‘machine learning’? What does it mean for an AI to understand a person and treat them with dignity? AIs are not just going to get better and smarter and we need to invest in systems that can ‘think beyond simple arithmetic and string methods’. We need to invest in systems that are able to understand ‘human language’ and create ‘deep structures’, ‘systems that understand data and process it, making actions such as building’s management’ and ‘social provisioning’ possible within the context of such systems. The future is possible cities where the architecture of the future is made manifest rather than happenstance. This is the direction in which architectural theory is moving. The question is where this manifests itself and what kinds of tensions it can create. I think that there are ways to think of a future where architecture does not simply conform to the evolving landscape but actively creates that future through what is sometimes referred to as ‘instant feedback’. This can happen in three ways: ‘accurately and affordably’, ‘systematically and constructively’ and �</text>
    <text>relationship between curating and artificial intelligence. The conversation, hosted by the curator Victoria Meadows, explores the problems and possibilities of AI, creating the conditions for an AI to be created.[6] The broad themes of the discussion, which includes the concerns of many in the room, are: what if all the data is automated? What if there is no escape from the machine? What if there is no humanity left in the system? What does it mean to be ‘human-centered’? What does it mean to ‘know AI’? How does AI create ‘underlying trends’ and ‘institutional change’? What do we do to address the fact that, increasingly, AI is not just going to be there to do what it is told? What do we do in response to that underling ‘machine learning’? What does it mean for an AI to understand a person and treat them with dignity? AIs are not just going to get better and smarter and we are all going to need to be more conscious of the ways in which we are contributing to creating them – conscious of the fact that we are creating subaltern forms of life? – but we are also the ways in which we are contributing to creating them. We are the ways in which they can evolve. And there are enormous social, economic and political questions that still need to be addressed. Session one of the AGI symposium is a chance for participants to articulate their concerns, find a common language to understand, and forge connections that will enable future work. The symposium format allows for multiple perspectives to be offered, synthesised and discussed – from the participant's own perspective – in one place. In the case of an AI, this may mean offering a solution to a specific problem, such as how to deal with graffiti, or understanding how an AI might be able to augment or replace humans in certain types of work. It may also mean offering solutions to a broader set of problems, such as</text>
    <text>relationship between curating and artificial intelligence. The conversation, hosted by the curator Victoria Meadows, explores the problems and possibilities of AI, creating the conditions for an AI to be created.[6] The broad themes of the discussion, which includes the concerns of many in the room, are: what if all the data is automated? What if there is no escape from the machine? What if there is no humanity left in the system? What does it mean to be ‘human-centered’? What does it mean to ‘know AI’? How does AI create ‘underlying trends’ and ‘institutional change’? What do we do to address the fact that, increasingly, AI is not just going to be there to do what it is told? What do we do in response to that underling ‘machine learning’? What does it mean for an AI to understand a person and treat them with dignity? AIs are not just going to get better and smarter and we are all going to need to work harder and longer in order to keep up with them, but they are also going to encounter new kinds of learning and change their form of being and how they act in relation to the world as a whole. And the important work here is not in making more intelligent machines, but in making more human-like machines that understand and value the world as we understand it today. When we think about what it means for an AI to understand a person and act like a person, we inevitably think about the value of language and culture in our own right. But machines are also capable of understanding more complex social dynamics and cultural phenomena. For instance, the emergence of self-driving cars presents us with a set of questions that could not be any simpler: what if a car crashes? How do we create systems that are safer and more reliable? How do we make them understand the social dynamics and cultural flows that are reshaping the planet? These are questions that could not be</text>
    <text>relationship between curating and artificial intelligence. The conversation, hosted by the curator Victoria Meadows, explores the problems and possibilities of AI, creating the conditions for an AI to be created.[6] The broad themes of the discussion, which includes the concerns of many in the room, are: what if all the data is automated? What if there is no escape from the machine? What if there is no humanity left in the system? What does it mean to be ‘human-centered’? What does it mean to ‘know AI’? How does AI create ‘underlying trends’ and ‘institutional change’? What do we do to address the fact that, increasingly, AI is not just going to be there to do what it is told? What do we do in response to that underling ‘machine learning’? What does it mean for an AI to understand a person and treat them with dignity? AIs are not just going to get better and smarter and we should be creating systems that are capable of learning and acting on their own.Artificial intelligence is the new black and it is going to change the world. And we’re going to have to start thinking about how to talk about that.Download this article as PDFNadine Attewell 	Nadine Attewell is Professor of Art and Head of the Division for Art, Media, and Cultural Studies at the University of Alberta. She holds a Bachelor of Arts in English Cultural Studies from the University of Alberta College of Art (2000) and a Juris Doctorate from the University of Alberta Law School (with a joint appointment) – with a special focus on the role of the legal system in the context of artificial intelligence. She has a particular interest in the role of the arts in the context of AI and has co-authored several books with Anne Bezemer and is a regular contributor to the American Journal of Cultural Studies. She was a consultant for the film Aquarius</text>
    <text>relationship between curating and artificial intelligence. The conversation, hosted by the curator Victoria Meadows, explores the problems and possibilities of AI, creating the conditions for an AI to be created.[6] The broad themes of the discussion, which includes the concerns of many in the room, are: what if all the data is automated? What if there is no escape from the machine? What if there is no humanity left in the system? What does it mean to be ‘human-centered’? What does it mean to ‘know AI’? How does AI create ‘underlying trends’ and ‘institutional change’? What do we do to address the fact that, increasingly, AI is not just going to be there to do what it is told? What do we do in response to that underling ‘machine learning’? What does it mean for an AI to understand a person and treat them with dignity? AIs are not just going to get better and smarter and we are all going to need to become more conscious of our complicity in this process – we are going to have to become conscious of the fact that we are also complicit in this process. We need to start creating mechanisms that see and respond to the systems that enable AI, not just in theory but in practice.This is not just some abstract philosophical proposition. It is a particular situation today in which the concerns of the many overrun the concerns of the few. It is a situation where the interests of large corporations are often pitted against those of small communities, against one another, and against the very survival of the planet. It is a situation where political and economic systems are often set in opposition to one another. It is a situation where cultural consumption is often seen as a necessary evil – but is also a situation where cultural consumption becomes valorised in ways that are not necessarily based on rationality. It is a situation where it is easier to consume cultural commodities than it is to inhabit a world that produces meaning</text>
    <text>relationship between curating and artificial intelligence. The conversation, hosted by the curator Victoria Meadows, explores the problems and possibilities of AI, creating the conditions for an AI to be created.[6] The broad themes of the discussion, which includes the concerns of many in the room, are: what if all the data is automated? What if there is no escape from the machine? What if there is no humanity left in the system? What does it mean to be ‘human-centered’? What does it mean to ‘know AI’? How does AI create ‘underlying trends’ and ‘institutional change’? What do we do to address the fact that, increasingly, AI is not just going to be there to do what it is told? What do we do in response to that underling ‘machine learning’? What does it mean for an AI to understand a person and treat them with dignity? AIs are not just going to get better and smarter and we must actively engage them in creating better systems that can learn and understand the world around us.The conversation itself was interesting, moving from the abstract to the specific. Some ideas were more directly pointed at and addressed than others. But the overarching themes were borne out in the overlong course of the evening. The interesting thing is that these are the kinds of questions that can be asked and the answers that can be provided. These are not arbitrary or subjective – they are the way the world is made. Robots are not going to give us cures or make us rich or give us beautiful buildings or make us perfect ambassadors or perfect parents or create perfect symphony orchestras or deliver perfect archives or deliver us to the happiest place on the planet or make us perfect golf courses or the kind of infrastructure that supports them – but they can make us do many, many things that we wouldn’t necessarily recommend or design to be done by a human being.What can we do with that knowledge? What kinds of ambig</text>
    <text>relationship between curating and artificial intelligence. The conversation, hosted by the curator Victoria Meadows, explores the problems and possibilities of AI, creating the conditions for an AI to be created.[6] The broad themes of the discussion, which includes the concerns of many in the room, are: what if all the data is automated? What if there is no escape from the machine? What if there is no humanity left in the system? What does it mean to be ‘human-centered’? What does it mean to ‘know AI’? How does AI create ‘underlying trends’ and ‘institutional change’? What do we do to address the fact that, increasingly, AI is not just going to be there to do what it is told? What do we do in response to that underling ‘machine learning’? What does it mean for an AI to understand a person and treat them with dignity? AIs are not just going to get better and smarter and we are all going to be part of a connected, connected future, but each of us will play a role in enabling that future.AI, or Artificial Intelligence, is the branch of artificial intelligence that deals with the underlying principles of computation and artificial intelligence. It is a step beyond what humans can do, and in many ways, more fundamental than that. As computers become ever more powerful and capable, it is likely that we will all become 'AI nodes' or ‘AI peons’ in a networked society. This is the role played by computers in the age of the Turing machine and massive data processing. How we respond to this new form of node will determine the future shape of the social and political systems that we build.There are many different kinds of AI, but all share a shared set of principles and a common set of actors. The main actors in the social, political and cultural sphere are humans and algorithms. Human behaviour is automated, but there is still an inherent human element</text>
    <text>relationship between curating and artificial intelligence. The conversation, hosted by the curator Victoria Meadows, explores the problems and possibilities of AI, creating the conditions for an AI to be created.[6] The broad themes of the discussion, which includes the concerns of many in the room, are: what if all the data is automated? What if there is no escape from the machine? What if there is no humanity left in the system? What does it mean to be ‘human-centered’? What does it mean to ‘know AI’? How does AI create ‘underlying trends’ and ‘institutional change’? What do we do to address the fact that, increasingly, AI is not just going to be there to do what it is told? What do we do in response to that underling ‘machine learning’? What does it mean for an AI to understand a person and treat them with dignity? AIs are not just going to get better and smarter and we need to start thinking about what that entails.Artificial Intelligence (AI) is the branch of artificial intelligence that deals with the underlying principles of computation. It is a kind of software – it understands the underlying principles – and so it is part of what Meehan Crist, the Director of the Seargent Centre for Art, Culture &amp; Innovation at the University of Southampton, describes as ‘the new economy’. That is, the future economy is one in which things that once were done via humans are now done via algorithms.Artificial Intelligence is advancing at an unprecedented pace and with it, too, must we assume new forms of human labour is now required in order to accomplish the same tasks. The question is, what new kinds of labour might these new kinds of AI create? In this context, the answer is clear: systems need workers. Meehan Crist, in her distinction between ‘artificial intelligence’ and ‘machine learning’, makes the point that the</text>
    <text>relationship between curating and artificial intelligence. The conversation, hosted by the curator Victoria Meadows, explores the problems and possibilities of AI, creating the conditions for an AI to be created.[6] The broad themes of the discussion, which includes the concerns of many in the room, are: what if all the data is automated? What if there is no escape from the machine? What if there is no humanity left in the system? What does it mean to be ‘human-centered’? What does it mean to ‘know AI’? How does AI create ‘underlying trends’ and ‘institutional change’? What do we do to address the fact that, increasingly, AI is not just going to be there to do what it is told? What do we do in response to that underling ‘machine learning’? What does it mean for an AI to understand a person and treat them with dignity? AIs are not just going to get better and smarter and we must all work together to try to figure out how to build strong, sentient, self-aware, self-aware AI systems that can learn and act in ways that are not guided by the machine’s biases and assumptions. It will be interesting to see if, in the coming years, the problems discussed at the conference can be applied to better understand the world around us. What can we learn? What can we automate? How do we create systems that are neither perfect nor perfect but nevertheless care? What are the ethical questions that arise when trying to build systems that are both benevolent and also human-centric? How do we create systems that are both free and also autonomous? These are the kinds of questions that will continue to inform the design of systems in the years to come.What can we learn?  In this draft, I have asked participants what they thought were the top three issues facing the world today, and I have combined the questions from the three sessions into one overarching question. For</text>
  </text>
  <text>
    future curatorial forms, which are becoming increasingly diverse and insular, are themselves increasingly fragile. I’m thinking here of the curatorial departments of major metropolitan centres, whose curatorial work has been largely absorbed by the postgraduate programmes they offer – urban studies, urbanism and urbanism, urbanism and urban/suburban studies. As a result, these departments are often left to fend for itself, looking to the future for signs of life – and indeed, to life outside of this life – in a world that seems increasingly transient and unpredictable. As a consequence, today's graduate students often find themselves caught in a web of political, economic and social change, which is often as contradictory as the forces of nature. As a result, the jobs they do have can seem unattractive and the world they find in the process of becoming real is often quite beautiful. 	Such is the  	logic of it all. But the beauty of it all is that at some point in the future,
    <text>future curatorial forms, which are becoming increasingly diverse and insular, are themselves increasingly fragile. I’m thinking here of the curatorial departments of major metropolitan centres, whose curatorial work has been largely absorbed by the postgraduate programmes they offer – urban studies, urbanism and urbanism, urbanism and urban/suburban studies. As a result, these departments are often left to fend for itself, looking to the future for signs of life – and indeed, to life outside of this life – in a world that seems increasingly transient and unpredictable. As a consequence, today's graduate students often find themselves caught in a web of political, economic and social change, which is often as contradictory as the forces of nature. As a result, the jobs they do have can seem unattractive and the world they find in the process of becoming real is often quite beautiful. 	Such is the  	logic of it all. But the beauty of it all is that at some point in the future, perhaps – just maybe – we will all be able to find meaningful ways to engage with the beauty of the world around us in ways that do not rely on the logic of institutional socialisation and modernism. Until then, we’ll have to begin to imagine and act like we don’t live in a beautiful world.Download this article as PDFLolive Symbol - Paris, July 2014Next ArticlePart 2: Making Cities Sustainable, Affirming the Importance of CitiesCreating viable alternatives to the built environment is a leading challenge facing cities today, and a logical extension of current forms of economic and urbanisation. Yet housing and urbanisation are not inevitable outcomes of capitalism; nor are they the endpoints at which the contradictions of capitalism will be resolved. 	Instead, the logic of the built environment – which tends to produce ever larger and more unmanageable bureaucracies around everything from roads to bridges to telecommunications to water to electricity to garbage collection – is likely to produce growing pains</text>
    <text>future curatorial forms, which are becoming increasingly diverse and insular, are themselves increasingly fragile. I’m thinking here of the curatorial departments of major metropolitan centres, whose curatorial work has been largely absorbed by the postgraduate programmes they offer – urban studies, urbanism and urbanism, urbanism and urban/suburban studies. As a result, these departments are often left to fend for itself, looking to the future for signs of life – and indeed, to life outside of this life – in a world that seems increasingly transient and unpredictable. As a consequence, today's graduate students often find themselves caught in a web of political, economic and social change, which is often as contradictory as the forces of nature. As a result, the jobs they do have can seem unattractive and the world they find in the process of becoming real is often quite beautiful. 	Such is the  	logic of it all. But the beauty of it all is that at some point in the future, when all these histories and all these processes and processes and changes and events and interactions around them have happened, we can look back on and say, with some degree of pride, that we actually ‘know’ that history happened. And that’s beautiful. That’s the beauty of realism: that we actually produce and articulate and architect our own histories and real lives and real cities and real goings-on. That’s our  	responsibility. That’s what real architects do. 	But the beauty of it all is that at some point in the future, when all these histories and all these processes and all these interactions around them have happened, we can look back on and say, with some degree of pride, that we actually do know that history happened. And that’s beautiful. That’s what realists do. 	But the beauty of it all is that at some point in the future, when all these</text>
    <text>future curatorial forms, which are becoming increasingly diverse and insular, are themselves increasingly fragile. I’m thinking here of the curatorial departments of major metropolitan centres, whose curatorial work has been largely absorbed by the postgraduate programmes they offer – urban studies, urbanism and urbanism, urbanism and urban/suburban studies. As a result, these departments are often left to fend for itself, looking to the future for signs of life – and indeed, to life outside of this life – in a world that seems increasingly transient and unpredictable. As a consequence, today's graduate students often find themselves caught in a web of political, economic and social change, which is often as contradictory as the forces of nature. As a result, the jobs they do have can seem unattractive and the world they find in the process of becoming real is often quite beautiful. 	Such is the  	logic of it all. But the beauty of it all is that at some point in the future, perhaps – just perhaps – we might want to change the world. In the meantime, we have to figure out how to live in this world. The question is, how to make the world we actually live in? Send us your ideas, World War Z, World War Z Part One: Rebellion 	 and World War Z Part Two: The Pact, at WWW.WORLDWARZ.COM. 	Read more by W. W. Baker 	 here. 	Read more by W. W. Baker 	here. 	Read more by W. W. Baker here. 	Read more by W. W. Baker here.   1  Myths &amp; Symbols in Cultural Theory (Oxford: Oxford University Press, 2016), p. 38.  2  Henri Lefebvre,  	Postmodernism, Chapter One: Its Philosophy and Documents (Cambridge, MA:</text>
    <text>future curatorial forms, which are becoming increasingly diverse and insular, are themselves increasingly fragile. I’m thinking here of the curatorial departments of major metropolitan centres, whose curatorial work has been largely absorbed by the postgraduate programmes they offer – urban studies, urbanism and urbanism, urbanism and urban/suburban studies. As a result, these departments are often left to fend for itself, looking to the future for signs of life – and indeed, to life outside of this life – in a world that seems increasingly transient and unpredictable. As a consequence, today's graduate students often find themselves caught in a web of political, economic and social change, which is often as contradictory as the forces of nature. As a result, the jobs they do have can seem unattractive and the world they find in the process of becoming real is often quite beautiful. 	Such is the  	logic of it all. But the beauty of it all is that at some point in the future, if nature continues to provide the goods and services we desire, we can all be reduced to becoming mere consumers. 	And that’s the beauty of it: we remain conscious of our natural gifts, while the rest of the world around us is rapidly becoming artificial and mechanised. 	Today, the world is dotted with self-driving cars, satellites, drones and automated factories – virtually every aspect of human activity is now captured and mapped, and is therefore inherently connected to the rise and fall of commodity prices, which in turn affects consumer spending and investment decisions. 	In the wake of these technological revolutions, which tend to negate some of the values of the past, it’s easy to dehumanise the seemingly machine-like creatures that now rule us. 	And although this process of destruction and renewal is often traumatic for communities, it tends to make us more conscious of our own role in the world. Communities are often left reeling by the inefficiency and inefficiency of</text>
    <text>future curatorial forms, which are becoming increasingly diverse and insular, are themselves increasingly fragile. I’m thinking here of the curatorial departments of major metropolitan centres, whose curatorial work has been largely absorbed by the postgraduate programmes they offer – urban studies, urbanism and urbanism, urbanism and urban/suburban studies. As a result, these departments are often left to fend for itself, looking to the future for signs of life – and indeed, to life outside of this life – in a world that seems increasingly transient and unpredictable. As a consequence, today's graduate students often find themselves caught in a web of political, economic and social change, which is often as contradictory as the forces of nature. As a result, the jobs they do have can seem unattractive and the world they find in the process of becoming real is often quite beautiful. 	Such is the  	logic of it all. But the beauty of it all is that at some point in the future, perhaps – just perhaps – a future generation or even just a few generations of future students may find solace in the art of making cities and their inhabitants better by breaking them into pieces and arranging them in any number of ways, including making them of many different shapes and sizes. 	And they may find solace in the art of leaving behind ruins in the process. 	Such is the  	logic of urban growth. And it is comforting to think that our huge, ever-expanding, ever-influening metropolis can be contained within these ever-shifting, ever-expanding cities. But such is not always the case. Sometimes the growth is excess, under-specified, and counterproductive. Places with high housing costs and chaotic, ever-changing job markets are places with high levels of inequality and unstable publics. And so we must be vigilant against the spread of nepotism and dynasties in the pursuit of economic growth. We must also be</text>
    <text>future curatorial forms, which are becoming increasingly diverse and insular, are themselves increasingly fragile. I’m thinking here of the curatorial departments of major metropolitan centres, whose curatorial work has been largely absorbed by the postgraduate programmes they offer – urban studies, urbanism and urbanism, urbanism and urban/suburban studies. As a result, these departments are often left to fend for itself, looking to the future for signs of life – and indeed, to life outside of this life – in a world that seems increasingly transient and unpredictable. As a consequence, today's graduate students often find themselves caught in a web of political, economic and social change, which is often as contradictory as the forces of nature. As a result, the jobs they do have can seem unattractive and the world they find in the process of becoming real is often quite beautiful. 	Such is the  	logic of it all. But the beauty of it all is that at some point in the future, perhaps – just perhaps – humanity will be able to rise above the beauty and the miracle and actually do something with the vast amount of energy it creates. That is, perhaps, the future we can aspire to. 	What can a postgraduate in Urban Studies do in this situation? One can only imagine what a ‘real life’ this would mean. And one can imagine what a ‘real’ future that would mean. In an interesting turn of events, major cities are becoming more and more like islands in a sea of information. The information society that the internet industry has empowered is gathering momentum, and it is evident that this gathering is unavailing in many places. 	But this gathering of information is also gathering ideas and proposals that serve to define the future. In many ways, this is good. It encourages us to think about the bigger picture, to consider not only the immediate concerns of the city, but also the larger social and political systems that shape and operate</text>
    <text>future curatorial forms, which are becoming increasingly diverse and insular, are themselves increasingly fragile. I’m thinking here of the curatorial departments of major metropolitan centres, whose curatorial work has been largely absorbed by the postgraduate programmes they offer – urban studies, urbanism and urbanism, urbanism and urban/suburban studies. As a result, these departments are often left to fend for itself, looking to the future for signs of life – and indeed, to life outside of this life – in a world that seems increasingly transient and unpredictable. As a consequence, today's graduate students often find themselves caught in a web of political, economic and social change, which is often as contradictory as the forces of nature. As a result, the jobs they do have can seem unattractive and the world they find in the process of becoming real is often quite beautiful. 	Such is the  	logic of it all. But the beauty of it all is that at some point in the future, humans have to work together to try to work out what to do with the vast amounts of information that are now flooding in. And so the scenes in  	the present day are likely to repeat themselves – which, in turn, will repeat the scenes in the past, in a cycle that will end somewhere around the year 2140. This is the &quot;big end&quot; – the year in which humans are born into – and it is the scene where we encounter the first appearance of nonhuman life forms. 	So, the beauty of the present is that it is constantly on the cusp of destruction, yet somehow manages to exist in part because it is constantly expanding. But what happens at the other end of the scale is equally as beautiful, if not more so. 	The beauty of the exponential is in its repetitive nature, but unlike the case of real life, in which beauty is a function of experience (and perhaps, in a biennial, even moreso). The beauty</text>
    <text>future curatorial forms, which are becoming increasingly diverse and insular, are themselves increasingly fragile. I’m thinking here of the curatorial departments of major metropolitan centres, whose curatorial work has been largely absorbed by the postgraduate programmes they offer – urban studies, urbanism and urbanism, urbanism and urban/suburban studies. As a result, these departments are often left to fend for itself, looking to the future for signs of life – and indeed, to life outside of this life – in a world that seems increasingly transient and unpredictable. As a consequence, today's graduate students often find themselves caught in a web of political, economic and social change, which is often as contradictory as the forces of nature. As a result, the jobs they do have can seem unattractive and the world they find in the process of becoming real is often quite beautiful. 	Such is the  	logic of it all. But the beauty of it all is that at some point in the future, or perhaps much later in the world, we may all be able to find meaning and purpose and purpose in the world we live in. That is, if the technologies of today continue to render the world we inhabit unattractive and unfulfilling. 	But the beauty of it all is that at some point in the future, or perhaps much later in the world, we may all be able to find meaning and purpose and purpose in our own bodies and in the world around us. That is, if the conditions of today's society, which are often unjust and deceptive, may someday be amended and replaced by laws and social arrangements that render the world we inhabit beautiful and may someday be replaced by another world. That is, if the technologies of today can be changed, reshaped and replaced – and, if that is possible, may all be replaced by new technologies. 	In order to achieve this future, however, we must all strive to understand how our present forms of social organisation create</text>
    <text>future curatorial forms, which are becoming increasingly diverse and insular, are themselves increasingly fragile. I’m thinking here of the curatorial departments of major metropolitan centres, whose curatorial work has been largely absorbed by the postgraduate programmes they offer – urban studies, urbanism and urbanism, urbanism and urban/suburban studies. As a result, these departments are often left to fend for itself, looking to the future for signs of life – and indeed, to life outside of this life – in a world that seems increasingly transient and unpredictable. As a consequence, today's graduate students often find themselves caught in a web of political, economic and social change, which is often as contradictory as the forces of nature. As a result, the jobs they do have can seem unattractive and the world they find in the process of becoming real is often quite beautiful. 	Such is the  	logic of it all. But the beauty of it all is that at some point in the future, perhaps – just maybe – the world will come to offer more than what we have today. That is, perhaps, humanity’s capacity as a species. What will become of the worlds created in the image of the curatorial, based on the contiguity, the isotropy, the area, the function, urban forms, the mobility and the wealth created? I don’t know that today’s world will necessarily be like that. But I do know that today’s world will undoubtedly be more like that. That is, human beings will undoubtedly acquire more of those wondrous new urban forms that are transforming the world. And I like to think of these new ways of structuring urban space as ‘regeneration zones’. Zone one: growth and growth and growth. Zone two: existing infrastructure meets new growth forms. Zone three: developed metropolises meet dense, transit-oriented alternatives. Zone four: suburbs and rural areas meet big cities</text>
    <text>future curatorial forms, which are becoming increasingly diverse and insular, are themselves increasingly fragile. I’m thinking here of the curatorial departments of major metropolitan centres, whose curatorial work has been largely absorbed by the postgraduate programmes they offer – urban studies, urbanism and urbanism, urbanism and urban/suburban studies. As a result, these departments are often left to fend for itself, looking to the future for signs of life – and indeed, to life outside of this life – in a world that seems increasingly transient and unpredictable. As a consequence, today's graduate students often find themselves caught in a web of political, economic and social change, which is often as contradictory as the forces of nature. As a result, the jobs they do have can seem unattractive and the world they find in the process of becoming real is often quite beautiful. 	Such is the  	logic of it all. But the beauty of it all is that at some point in the future, when the forces of globalisation are reformed (and may even become useful), the beauty of it all may be replaced by a world that provides for those in need – at least for a while. That is, if globalisation’s first reformulation can be considered a world in which beauty becomes more widely universal, and in which all life becomes equally valuable. 	[1] See  	Global Exchange, www.globalexchange.com.au/en/, accessed 9 July 2016.  [2] See  	Pan-Africanist Books, Beirut, 1980.  [3] See  	Karen Bowden,  	Women of the Modern Middle East: Feminisms at a International Level, Oxford University Press, Oxford, 1987, p.11–18.  [4] See  	Jane’s Modern Middle East, Oxford University Press, Oxford, 1986, p.11.  [5] See  </text>
  </text>
  <text>
    developments in artificial intelligence, machine learning and data mining), the sharing and collaboration of labour between citizens, scientists and artists is facilitated. The sharing of knowledge and insights enables the co-production of creative solutions, capacities and tools through the sharing of data and knowledge. These may lead to (potentially more creative) uses of labour, but they also need to be shared, and must cross different people’s boundaries in order to achieve broader effects. 	Another way to put it is that of a border. City councils, public authorities, civic groups, neighbourhood associations, neighbourhood watch … sharing, sharing and sharing is the basic building block of any transitionary mechanism. It provides the framework in which actions can be taken, and the resulting change will inevitably happen; there will be those who will oppose the new ideas and will demand their return; there will be those who will misuse the mechanisms and misuse the communities; and there will be those who will try to prevent or weaken the mechanisms that support the mechanisms that aim to
    <text>developments in artificial intelligence, machine learning and data mining), the sharing and collaboration of labour between citizens, scientists and artists is facilitated. The sharing of knowledge and insights enables the co-production of creative solutions, capacities and tools through the sharing of data and knowledge. These may lead to (potentially more creative) uses of labour, but they also need to be shared, and must cross different people’s boundaries in order to achieve broader effects. 	Another way to put it is that of a border. City councils, public authorities, civic groups, neighbourhood associations, neighbourhood watch … sharing, sharing and sharing is the basic building block of any transitionary mechanism. It provides the framework in which actions can be taken, and the resulting change will inevitably happen; there will be those who will oppose the new ideas and will demand their return; there will be those who will misuse the mechanisms and misuse the communities; and there will be those who will try to prevent or weaken the mechanisms that support the mechanisms that aim to create new uses and possibilities. 	There are different kinds of councils: commissions, subcommittees,  and even  institutes. The mechanisms that enable a community to enact a change through legislation, executive order, referendum or other political means are called legislation clusters. The kinds of legislation that can be proposed to make neighbourhood improvements or to make city infrastructure functions like a public assembly are called Tolls. The kinds of legislation that can also be enacted through referenda, commissions and subcommittees are called Statutes. Statutes are the first step in any legislation cluster, and they alone are sufficient to effectuate a change; anything else does not go far enough. 	The genesis of the term ‘transitionary’ was the idea that temporary, non-consecutive legal tender (TC) is a more natural, ‘half-measures’ transitionary tool than full-legal-rights-bearing documents such as full-legal-rights-bearing constitutions and</text>
    <text>developments in artificial intelligence, machine learning and data mining), the sharing and collaboration of labour between citizens, scientists and artists is facilitated. The sharing of knowledge and insights enables the co-production of creative solutions, capacities and tools through the sharing of data and knowledge. These may lead to (potentially more creative) uses of labour, but they also need to be shared, and must cross different people’s boundaries in order to achieve broader effects. 	Another way to put it is that of a border. City councils, public authorities, civic groups, neighbourhood associations, neighbourhood watch … sharing, sharing and sharing is the basic building block of any transitionary mechanism. It provides the framework in which actions can be taken, and the resulting change will inevitably happen; there will be those who will oppose the new ideas and will demand their return; there will be those who will misuse the mechanisms and misuse the communities; and there will be those who will try to prevent or weaken the mechanisms that support the mechanisms that aim to create new uses and positive outcomes for communities. 	It is this last scenario – the trying to prevent and weaken the new uses from emerging – in which the authorities will undoubtedly play a significant role. It is a struggle to imagine a world in which such mechanisms do not exist, or do not power the mechanisms in a negative way. 	However, it is also possible that over time, the powers that be will learn to use the mechanisms creatively and create positive outcomes for communities. That is, that the powers that be will become corrupt and realise the error of their ways, and realise that if they do not align with the needs of communities, they can struggle against the systems, coming up with creative alternatives that achieve greater positive impacts but also greater costs. 	Such alternative-thinking will not only save money, but will also allow for the expansion of publicly funded art projects, since the self-repairing potential of these creative actions will ensure that communities do not lose out in the process.</text>
    <text>developments in artificial intelligence, machine learning and data mining), the sharing and collaboration of labour between citizens, scientists and artists is facilitated. The sharing of knowledge and insights enables the co-production of creative solutions, capacities and tools through the sharing of data and knowledge. These may lead to (potentially more creative) uses of labour, but they also need to be shared, and must cross different people’s boundaries in order to achieve broader effects. 	Another way to put it is that of a border. City councils, public authorities, civic groups, neighbourhood associations, neighbourhood watch … sharing, sharing and sharing is the basic building block of any transitionary mechanism. It provides the framework in which actions can be taken, and the resulting change will inevitably happen; there will be those who will oppose the new ideas and will demand their return; there will be those who will misuse the mechanisms and misuse the communities; and there will be those who will try to prevent or weaken the mechanisms that support the mechanisms that aim to create new possibilities and possibilities. 	Thus we can see how the dynamics of inequality will continue to exist in some cities, and how the mechanisms of city building will continue to be directed by elites and their business partners. But there will be some people – there will be millions of people – who will want and need to move to places where they can have a greater influence and that is why cities are complex social systems: because people ‘need places to live and work and play and learn’.1 	Such is the  	logic of it all. But the city is complex, and as we have seen, uneven in its effectiveness. We do not understand how inequity develops in the city until after people have been subjected to it, and although we can try to imagine different ‘better’ ways of organising things, there is no real interaction between citizens and city governments until people are subject to the rule of law and the social and economic conditions that result from it</text>
    <text>developments in artificial intelligence, machine learning and data mining), the sharing and collaboration of labour between citizens, scientists and artists is facilitated. The sharing of knowledge and insights enables the co-production of creative solutions, capacities and tools through the sharing of data and knowledge. These may lead to (potentially more creative) uses of labour, but they also need to be shared, and must cross different people’s boundaries in order to achieve broader effects. 	Another way to put it is that of a border. City councils, public authorities, civic groups, neighbourhood associations, neighbourhood watch … sharing, sharing and sharing is the basic building block of any transitionary mechanism. It provides the framework in which actions can be taken, and the resulting change will inevitably happen; there will be those who will oppose the new ideas and will demand their return; there will be those who will misuse the mechanisms and misuse the communities; and there will be those who will try to prevent or weaken the mechanisms that support the mechanisms that aim to create and share knowledge and insights. 	In considering what to make of this emerging biennale phenomenon, it is important to distinguish between what is being proposed as ‘transforming’ knowledge and the more urgent task of adapting and sharing knowledge. 	What is being proposed as a potential new business model may in part be based on the need to share knowledge and insights, but it is also being done in the name of protecting ‘our’ ‘identity’ by institutionalising in the institutions of the city what has been termed the ‘heritage preservation movement’. 	The ‘heritage preservation’ movement is motivated by the need to conserve places and sites associated with specific historical figures, places and things – streets, houses, parks, historic streets, historic buildings – in an effort to resuscitate a certain kind of urban presence and culture. It is being done in an ever more pervasive and pervasive way, as new technologies of data capture and</text>
    <text>developments in artificial intelligence, machine learning and data mining), the sharing and collaboration of labour between citizens, scientists and artists is facilitated. The sharing of knowledge and insights enables the co-production of creative solutions, capacities and tools through the sharing of data and knowledge. These may lead to (potentially more creative) uses of labour, but they also need to be shared, and must cross different people’s boundaries in order to achieve broader effects. 	Another way to put it is that of a border. City councils, public authorities, civic groups, neighbourhood associations, neighbourhood watch … sharing, sharing and sharing is the basic building block of any transitionary mechanism. It provides the framework in which actions can be taken, and the resulting change will inevitably happen; there will be those who will oppose the new ideas and will demand their return; there will be those who will misuse the mechanisms and misuse the communities; and there will be those who will try to prevent or weaken the mechanisms that support the mechanisms that aim to create positive outcomes for communities. Cities, towns and neighbourhoods with high levels of poverty and lack of access to education, health care, housing and other services are at particular risk during a transition. A 2006 analysis by the RAND Corporation, commissioned by the US government, found that a city’s improved capacity to bring about a transitionary change should be matched by increased federal spending to support it.60  The report recommended a multi-tiered integration of resources, including new federal programmes, partnerships and agencies designed to support the transition of the ‘urban poor’ – those who reside in or call the ‘street’ – from the dismantled ‘city’ into the structure of a new urban fabric.61  This would include a ‘smart’ component including incentives for private sector involvement in redeveloping abandoned or decimated areas, as well as a ‘highways’ part that would include new tolls and fees to support a new ‘high</text>
    <text>developments in artificial intelligence, machine learning and data mining), the sharing and collaboration of labour between citizens, scientists and artists is facilitated. The sharing of knowledge and insights enables the co-production of creative solutions, capacities and tools through the sharing of data and knowledge. These may lead to (potentially more creative) uses of labour, but they also need to be shared, and must cross different people’s boundaries in order to achieve broader effects. 	Another way to put it is that of a border. City councils, public authorities, civic groups, neighbourhood associations, neighbourhood watch … sharing, sharing and sharing is the basic building block of any transitionary mechanism. It provides the framework in which actions can be taken, and the resulting change will inevitably happen; there will be those who will oppose the new ideas and will demand their return; there will be those who will misuse the mechanisms and misuse the communities; and there will be those who will try to prevent or weaken the mechanisms that support the mechanisms that aim to generate change. 	Consider, for example, how a neighbourhood organization – perhaps an existing Community Land Trust, perhaps a newly formed Municipal Land Trust, or a combination of the two – might build upon the work of the previous government to improve access to places of employment, housing, cultural activities and other resources, or create new mechanisms to support those mechanisms through a Town Meeting, a City Council Resolution or a Tax Increment Financing Program. 	Such is the  	logic of things. But the human capacity for change is constrained by the very nature of the market. And although many people may disagree with the mechanisms used to generate change (or, to be perfectly frank, to use them), no one can deny the social and cultural benefits that result from new, more efficient ways of doing things. 	In sum, I’m not saying we should throw everything at the problem, that we should invent everything we think we need to replace ourselves; we’ve already</text>
    <text>developments in artificial intelligence, machine learning and data mining), the sharing and collaboration of labour between citizens, scientists and artists is facilitated. The sharing of knowledge and insights enables the co-production of creative solutions, capacities and tools through the sharing of data and knowledge. These may lead to (potentially more creative) uses of labour, but they also need to be shared, and must cross different people’s boundaries in order to achieve broader effects. 	Another way to put it is that of a border. City councils, public authorities, civic groups, neighbourhood associations, neighbourhood watch … sharing, sharing and sharing is the basic building block of any transitionary mechanism. It provides the framework in which actions can be taken, and the resulting change will inevitably happen; there will be those who will oppose the new ideas and will demand their return; there will be those who will misuse the mechanisms and misuse the communities; and there will be those who will try to prevent or weaken the mechanisms that support the mechanisms that aim to bring about the changes desired. 	In the event of conflict, neighbourhood watch or neighbourhood initiatives, it is the marginalised communities with their knowledge of the city’s architecture and landscape who are at the epicentre of conflict. The UN Special Rapporteur on Torture and Other Cruel, Inhuman or Inhumanitarian Provisions, Marie-Jung Lubanga, has repeatedly stated that she hopes that the report she expects to write on ‘enhanced interrogation techniques’ will establish a global moratorium on such practices. 	There are many other mechanisms by which social change can come about, and they include: (but is by no means limited to) grassroots mobilisations; direct action by citizens (including low-income communities); foundations and other public funds; and … financial support (including from local governments and rich individuals). 	There is a great deal of misunderstanding and disinterest around how the provision of basic services, arts and cultural institutions, and the promotion of self</text>
    <text>developments in artificial intelligence, machine learning and data mining), the sharing and collaboration of labour between citizens, scientists and artists is facilitated. The sharing of knowledge and insights enables the co-production of creative solutions, capacities and tools through the sharing of data and knowledge. These may lead to (potentially more creative) uses of labour, but they also need to be shared, and must cross different people’s boundaries in order to achieve broader effects. 	Another way to put it is that of a border. City councils, public authorities, civic groups, neighbourhood associations, neighbourhood watch … sharing, sharing and sharing is the basic building block of any transitionary mechanism. It provides the framework in which actions can be taken, and the resulting change will inevitably happen; there will be those who will oppose the new ideas and will demand their return; there will be those who will misuse the mechanisms and misuse the communities; and there will be those who will try to prevent or weaken the mechanisms that support the mechanisms that aim to create new uses for the cities and make neighbourhoods and neighbourhoods themselves creative workplaces. 	The sharing and collaboration of labour enables creative solutions, capacities and tools, but also needs sharing of data and knowledge. The sharing of data and knowledge is essential for the mechanisms of knowledge production to evolve and meet the needs of communities. But it is equally essential that the sharing and collaboration continues to happen on the basis of reciprocity – that is, that the same technologies be used to produce both knowledge and wealth in the same city. This is the context in which creative proposals and projects can be built on – created, modified and shared between citizens, scientists and artists. This is the ‘critical mass’ necessary to shift society, and thereby create the conditions for new uses and generating wealth – and this is what creative bureaucracies do. 	Once the critical mass is created, the next step is for the system to enable the sharing and collaboration of labour between citizens, scientists and artists. This can happen through</text>
    <text>developments in artificial intelligence, machine learning and data mining), the sharing and collaboration of labour between citizens, scientists and artists is facilitated. The sharing of knowledge and insights enables the co-production of creative solutions, capacities and tools through the sharing of data and knowledge. These may lead to (potentially more creative) uses of labour, but they also need to be shared, and must cross different people’s boundaries in order to achieve broader effects. 	Another way to put it is that of a border. City councils, public authorities, civic groups, neighbourhood associations, neighbourhood watch … sharing, sharing and sharing is the basic building block of any transitionary mechanism. It provides the framework in which actions can be taken, and the resulting change will inevitably happen; there will be those who will oppose the new ideas and will demand their return; there will be those who will misuse the mechanisms and misuse the communities; and there will be those who will try to prevent or weaken the mechanisms that support the mechanisms that aim to create new uses and modes of life.  	One of the tasks is to try to understand how the mechanisms of the housing crisis can be repaired or altered in order to achieve new uses and modes of life. This can either repair the broken glass or create new glass that can in turn be used in the mechanisms of the housing crisis. The aim of this book is to help catalyze collaborative action among people with different skills and competencies in order to achieve new uses and modes of life. It is not a replacement for action, but it is an approach that encourages action because it enables us to think critically and creatively about how we might achieve new uses and modes of life. It is a way to begin to imagine such uses and modes of life in a specific neighbourhood or urban neighbourhood in the city. It is a question of how to engage the public, generate new ideas and creative solutions through research and design, and ultimately, implement those solutions in a specific neighbourhood or urban neighbourhood. 	Since</text>
    <text>developments in artificial intelligence, machine learning and data mining), the sharing and collaboration of labour between citizens, scientists and artists is facilitated. The sharing of knowledge and insights enables the co-production of creative solutions, capacities and tools through the sharing of data and knowledge. These may lead to (potentially more creative) uses of labour, but they also need to be shared, and must cross different people’s boundaries in order to achieve broader effects. 	Another way to put it is that of a border. City councils, public authorities, civic groups, neighbourhood associations, neighbourhood watch … sharing, sharing and sharing is the basic building block of any transitionary mechanism. It provides the framework in which actions can be taken, and the resulting change will inevitably happen; there will be those who will oppose the new ideas and will demand their return; there will be those who will misuse the mechanisms and misuse the communities; and there will be those who will try to prevent or weaken the mechanisms that support the mechanisms that aim to create new uses for the cities that emerge. 	The sharing and collaboration of knowledge and insights enables the co-production of creative solutions, capacities and tools through the sharing of data and knowledge. These may lead to (potentially more creative) uses of labour, but they also need to be shared, and need to cross different people’s boundaries in order to achieve broader effects. 	In my estimation, the mechanism of the border is critical to understanding how cities are to be responsive to changes in population and urbanisation patterns. The creation and maintenance of physical, social and informational borders is a critical step towards the realization of a ‘transition’ – a future that does not result from the past nor from the accumulation of capital – which is the objective of current trends of urbanisation.1 	But what does it look like when the border is breached? What does it look like to be urbanised outside of urban centres? I propose that when we talk about the transformation</text>
  </text>
  <text>
    relationship between curating and artificial intelligence. Curating is the art of making things for people to look at. In this context, AI might well lead to new kinds of public art that delight audiences not trained to look at art in a systematic way. AI might also lead to new kinds of art that are produced in parallel to the art being produced today, and that are produced with the active participation of practitioners themselves. This last scenario is the realm of fintech startups and their investors, and it is the realm of an entirely different kind of art connoisseur.Art is a globalised commodity; it circulates everywhere in real time, and therefore every medium has to be open to the possibility that art will one day be available in a financial or digital sense. That is, to users outside of art galleries and museums. Art is art, and art is always going to be available; there is always a market for it. But unlike today, where art is produced through a proprietary production process, tomorrow's art will be
    <text>relationship between curating and artificial intelligence. Curating is the art of making things for people to look at. In this context, AI might well lead to new kinds of public art that delight audiences not trained to look at art in a systematic way. AI might also lead to new kinds of art that are produced in parallel to the art being produced today, and that are produced with the active participation of practitioners themselves. This last scenario is the realm of fintech startups and their investors, and it is the realm of an entirely different kind of art connoisseur.Art is a globalised commodity; it circulates everywhere in real time, and therefore every medium has to be open to the possibility that art will one day be available in a financial or digital sense. That is, to users outside of art galleries and museums. Art is art, and art is always going to be available; there is always a market for it. But unlike today, where art is produced through a proprietary production process, tomorrow's art will be produced through a network effect: through a connected set of objects, a user can acquire art. The market for today's art will be able to supply the art of tomorrow, but the market for an AI will be able to produce works that delight audiences of a completely different kind.Art is a globalised commodity. But unlike today, where art is produced through a proprietary production process, tomorrow's art will be produced through a network effect: through a connected set of objects, a user can acquire art. The art of tomorrow will always be available to tourists and artists alike, but the art of today will be produced through a different kind of production – one that is anonymous, instantaneous and anonymous in its production – one that utilizes anonymous data, and one that utilizes anonymous platforms. Art will remain anonymous in the sense that it will always be available to find through data mining, but it will also become ever more anonymous through the adoption of new social media and algorithms. New media will make it easier for users</text>
    <text>relationship between curating and artificial intelligence. Curating is the art of making things for people to look at. In this context, AI might well lead to new kinds of public art that delight audiences not trained to look at art in a systematic way. AI might also lead to new kinds of art that are produced in parallel to the art being produced today, and that are produced with the active participation of practitioners themselves. This last scenario is the realm of fintech startups and their investors, and it is the realm of an entirely different kind of art connoisseur.Art is a globalised commodity; it circulates everywhere in real time, and therefore every medium has to be open to the possibility that art will one day be available in a financial or digital sense. That is, to users outside of art galleries and museums. Art is art, and art is always going to be available; there is always a market for it. But unlike today, where art is produced through a proprietary production process, tomorrow's art will be produced by a community of artists and curators in concert, and it will be produced in concert with other art forms. This is why today's art fetes are so extravagant: there is a need for art-historical texts to be relearned and new generations to be introduced to contemporary art. It is also why, fifty years from now, people will remember the Surrealist movement and the fact that Salvador Dali was killed in a police box.Art today is produced on a massive scale through mass media, and yet it is largely invisible, aside from a few museums dedicated to showcasing specific artists. This is partially because, during the 1990s and 2000s the art market was characterised by the invention and proliferation of low-quality, high-volume forms of art that could be assembled in a matter of seconds – memes, Instagrams, Pinterests and Fads – that were able to proliferate in a bid to meet consumer demand. As a result, the art market sunk</text>
    <text>relationship between curating and artificial intelligence. Curating is the art of making things for people to look at. In this context, AI might well lead to new kinds of public art that delight audiences not trained to look at art in a systematic way. AI might also lead to new kinds of art that are produced in parallel to the art being produced today, and that are produced with the active participation of practitioners themselves. This last scenario is the realm of fintech startups and their investors, and it is the realm of an entirely different kind of art connoisseur.Art is a globalised commodity; it circulates everywhere in real time, and therefore every medium has to be open to the possibility that art will one day be available in a financial or digital sense. That is, to users outside of art galleries and museums. Art is art, and art is always going to be available; there is always a market for it. But unlike today, where art is produced through a proprietary production process, tomorrow's art will be produced through an entirely different kind of production process – one that makes massive amounts of data actively available, and where the infrastructure for collecting, analysing and utilising that art is also in the hands of millions of users.Art should be produced through a combination of what MFA student Nadine Attewell calls ‘the new low’ – when large amounts of data about human behaviour are made readily available via sensors everywhere – and what Benjamin Bratton calls the ‘new ‘ middle: the level where the needle of surveillance stops… where the needle of accountability starts to move.'[6] The production of art today through AI is going to involve a transformation of what is already being produced in the artworld in significant part through the mechanisms of the art museum, the fine art gallery and the fine art dealer. Art is going to have to find a way to find a home in this new complex web of data that is emerging from the gallery and fine art trade.Art should be produced</text>
    <text>relationship between curating and artificial intelligence. Curating is the art of making things for people to look at. In this context, AI might well lead to new kinds of public art that delight audiences not trained to look at art in a systematic way. AI might also lead to new kinds of art that are produced in parallel to the art being produced today, and that are produced with the active participation of practitioners themselves. This last scenario is the realm of fintech startups and their investors, and it is the realm of an entirely different kind of art connoisseur.Art is a globalised commodity; it circulates everywhere in real time, and therefore every medium has to be open to the possibility that art will one day be available in a financial or digital sense. That is, to users outside of art galleries and museums. Art is art, and art is always going to be available; there is always a market for it. But unlike today, where art is produced through a proprietary production process, tomorrow's art will be produced through an open access publishing platform such as the Green Lantern Press or the Open Access Digital Gallery, or even the artist himself, in this case the artist himself. He will have a platform on which to express himself, and that is, until the platforms on which he depends for their work are destroyed.Art is always going to be transient; it exists in the here and now, and will eventually move beyond the site where it was made or the site where it emerged as a result of a historical event. But unlike today, where art is produced through a proprietary production process, tomorrow’s art will be produced through an open access publishing platform such as the Green Lantern Press or the Open Access Digital Gallery, or even the creator himself, in this case the artist himself. He will have a platform on which to express himself, and that is, until the platforms on which he depends for their work are destroyed.Future visitors to the Green Lantern Museum may well feel as though they are entering into</text>
    <text>relationship between curating and artificial intelligence. Curating is the art of making things for people to look at. In this context, AI might well lead to new kinds of public art that delight audiences not trained to look at art in a systematic way. AI might also lead to new kinds of art that are produced in parallel to the art being produced today, and that are produced with the active participation of practitioners themselves. This last scenario is the realm of fintech startups and their investors, and it is the realm of an entirely different kind of art connoisseur.Art is a globalised commodity; it circulates everywhere in real time, and therefore every medium has to be open to the possibility that art will one day be available in a financial or digital sense. That is, to users outside of art galleries and museums. Art is art, and art is always going to be available; there is always a market for it. But unlike today, where art is produced through a proprietary production process, tomorrow's art will be autonomously produced via the machines that are part of the new economy.Art’s real estate is made available by the fact that, throughout history, artists and their audiences have generally been able to access a single site at a time. While this may be true for particular works, the underlying philosophy that underpins it all – the logic that underpins the financial sector – remains largely mysterious. But one promising direction for art is already being pursued by AI and machine learning. And this is where we find ourselves today.Art is a process that can no longer be considered static. As we add more works to exhibitions, we are adding to the ever-growing mountain of texts that narrate the history of the art, or more accurately, the history of humanity. But we are also adding more algorithmic elements to the art itself. Art today is increasingly digitised and algorithmised, and the underlying principles underlying the technology are applied across a range of media now – including poetry, music, visual art</text>
    <text>relationship between curating and artificial intelligence. Curating is the art of making things for people to look at. In this context, AI might well lead to new kinds of public art that delight audiences not trained to look at art in a systematic way. AI might also lead to new kinds of art that are produced in parallel to the art being produced today, and that are produced with the active participation of practitioners themselves. This last scenario is the realm of fintech startups and their investors, and it is the realm of an entirely different kind of art connoisseur.Art is a globalised commodity; it circulates everywhere in real time, and therefore every medium has to be open to the possibility that art will one day be available in a financial or digital sense. That is, to users outside of art galleries and museums. Art is art, and art is always going to be available; there is always a market for it. But unlike today, where art is produced through a proprietary production process, tomorrow's art will be produced through an entirely different kind of production – one that is open to the possibility that it could one day be sold to a financial firm. Art today is produced through intermediaries; it is either by contractually obliged to be produced in exchange for a fee (like in manufacturing), or else it is produced on the artist’s behalf (like in music or film making). Art today is produced through digital technologies; it is either commissioned by a publisher (like in a publishing contract), or else it is produced by and distributed by a consortium of publishers and their stakeholders (like in a publishing contract). These sorts of production forms are what create the hypertext narrative – the way in which narratives are produced. But unlike today, where narratives are produced across diverse platforms in parallel, tomorrow’s narratives will be produced via a singular technology – a singular art medium – that is both globally ubiquitous and specifically tailored to the needs of its users. This is why a biennial is different from an annual</text>
    <text>relationship between curating and artificial intelligence. Curating is the art of making things for people to look at. In this context, AI might well lead to new kinds of public art that delight audiences not trained to look at art in a systematic way. AI might also lead to new kinds of art that are produced in parallel to the art being produced today, and that are produced with the active participation of practitioners themselves. This last scenario is the realm of fintech startups and their investors, and it is the realm of an entirely different kind of art connoisseur.Art is a globalised commodity; it circulates everywhere in real time, and therefore every medium has to be open to the possibility that art will one day be available in a financial or digital sense. That is, to users outside of art galleries and museums. Art is art, and art is always going to be available; there is always a market for it. But unlike today, where art is produced through a proprietary production process, tomorrow's art will be produced by a network of artists collaborating across disciplines in a shared space, and users will be able to choose among a shared set of artists. This is the AI threat.Art is an inherently political medium. Today, it is increasingly difficult to distinguish art from advertising; even when a work of art is clearly an ad for a product, its social context and historical context are often far more relevant today than the work of art itself. A recurring theme in my writing is the threat posed by AI: how to detect fake art when it is being produced, and how to intervene when it is being used.Art is a political statement; it is also a cultural production. AI will make it easier to say and dox art pervasively, and will make it easier to identify artists who produce socially problematic work.Art should be produced with caution, and when it is produced with AI, it is likely that it will be used in ways that are not optimal for everyone, but that are optimal for a</text>
    <text>relationship between curating and artificial intelligence. Curating is the art of making things for people to look at. In this context, AI might well lead to new kinds of public art that delight audiences not trained to look at art in a systematic way. AI might also lead to new kinds of art that are produced in parallel to the art being produced today, and that are produced with the active participation of practitioners themselves. This last scenario is the realm of fintech startups and their investors, and it is the realm of an entirely different kind of art connoisseur.Art is a globalised commodity; it circulates everywhere in real time, and therefore every medium has to be open to the possibility that art will one day be available in a financial or digital sense. That is, to users outside of art galleries and museums. Art is art, and art is always going to be available; there is always a market for it. But unlike today, where art is produced through a proprietary production process, tomorrow's art will be made available for purchase, through a public relations campaign, or through the direct intervention of a curator, the direct intervention of an art dealer, or the intervention of a museum board, will be art today.Art today is made available through numerous technologies that constantly seek to identify and engage with what art can be; what art can be purchased, or produced, or created, or enjoyed. But beyond that, art today is made available through a multitude of mediating institutions, which seek to establish relationships with artists and with the public via cultural marketing and education.‘Cultural marketing’ refers to direct cultural interventions; it does not necessarily mean that the cultural institutions have to engage with the cultural process in some way. For example, an art museum might attempt to teach a generation of artists how to be artists through a series of workshops or a museum-type programme of activities, but that doesn’t necessarily mean that the institutions have to engage with the art in a systematic way. A cultural</text>
    <text>relationship between curating and artificial intelligence. Curating is the art of making things for people to look at. In this context, AI might well lead to new kinds of public art that delight audiences not trained to look at art in a systematic way. AI might also lead to new kinds of art that are produced in parallel to the art being produced today, and that are produced with the active participation of practitioners themselves. This last scenario is the realm of fintech startups and their investors, and it is the realm of an entirely different kind of art connoisseur.Art is a globalised commodity; it circulates everywhere in real time, and therefore every medium has to be open to the possibility that art will one day be available in a financial or digital sense. That is, to users outside of art galleries and museums. Art is art, and art is always going to be available; there is always a market for it. But unlike today, where art is produced through a proprietary production process, tomorrow's art will be produced by a community of artists, curators, art dealers and other stakeholders, including artists themselves, including those who regularly patronise art spaces and museums, and will therefore always have a financial interest in the outcome of a project.Art’s circulation today is digitally mediated; the level of interaction between artists, curators, art collectors and art-world stakeholders is unprecedented. But the level of cultural production produced by such a cultural infrastructure is also unprecedented. As the digital divide becomes ever more pronounced, it is unsurprising that most of the world’s population do not have access to a formal art education. Most of us grow up with access to a formal art and design education, but few of us have the tools or capacity to engage with and master the art that is produced through such an educational path. And this is where art’s creators and audiences stand to make a difference. It is these potential audiences who will drive the creation of cultural production in their local artworlds</text>
    <text>relationship between curating and artificial intelligence. Curating is the art of making things for people to look at. In this context, AI might well lead to new kinds of public art that delight audiences not trained to look at art in a systematic way. AI might also lead to new kinds of art that are produced in parallel to the art being produced today, and that are produced with the active participation of practitioners themselves. This last scenario is the realm of fintech startups and their investors, and it is the realm of an entirely different kind of art connoisseur.Art is a globalised commodity; it circulates everywhere in real time, and therefore every medium has to be open to the possibility that art will one day be available in a financial or digital sense. That is, to users outside of art galleries and museums. Art is art, and art is always going to be available; there is always a market for it. But unlike today, where art is produced through a proprietary production process, tomorrow's art will be produced through an entirely different kind of art connoisseur. That kind of artist would be an eBay seller, and their art would be commission-based, peer-to-peer, and transnational – something like Amazon's online art. The difficulty today is that art today is produced on a massive scale through the centralized supply of images and images – especially on social media platforms – and these images circulate at unimaginable speeds. Art today is produced by collectors who happen to be members of an eBay club; they see art on eBay and think, ‘That is art.’ But this kind of collectorism doesn’t compute in the same way today, and doesn’t scale the same way, and so the art produced through this kind of browsing isn’t necessarily art.’ If you take the example of an artist today who makes large-scale installations and programmes, then there is a problem with that. The problem today is that there are so many images available</text>
  </text>
  <text>
    future curatorial forms. 	To understand the significance of the exhibition in terms of sites of exchange, it is useful to examine some of the events that took place in Liverpool on 11 and 12 October 2016.  	A number of artists, including Wendy Harpe (DIY) and Manus Toru (TED), visited the site of the former Liverpool Biennial, which was then running alongside the London Biennial in parallel. Both exhibitions used the venue as a case study, with a number of curators setting up projects there. Wendy Harpe’s project  	Tiny Houses – Tiny Homes 4 Home Builders was set up in collaboration with the Lewis and Mackie Trust, the Riverside Community Land Trust and Liverpool School of Art. The term ‘site specificities’ is used here to refer to the specific objects or projects that are produced or revived on a site. By using this terminology, the collections at the
    <text>future curatorial forms. 	To understand the significance of the exhibition in terms of sites of exchange, it is useful to examine some of the events that took place in Liverpool on 11 and 12 October 2016.  	A number of artists, including Wendy Harpe (DIY) and Manus Toru (TED), visited the site of the former Liverpool Biennial, which was then running alongside the London Biennial in parallel. Both exhibitions used the venue as a case study, with a number of curators setting up projects there. Wendy Harpe’s project  	Tiny Houses – Tiny Homes 4 Home Builders was set up in collaboration with the Lewis and Mackie Trust, the Riverside Community Land Trust and Liverpool School of Art. The term ‘site specificities’ is used here to refer to the specific objects or projects that are produced or revived on a site. By using this terminology, the collections at the Liverpool Biennial, Liverpool Museum, Jane Sloan art gallery, Jane Sloan gallery, Jane Lynch gallery and the local Delft Artsap artist collective are collectively referred to as the ‘micro-exhibitionary landscape’. In this, the collection making processes that occur within the biennial context are considered. The micro-exhibitionary landscape is one in which many curatorial actions are undertaken, but few of them are taken up by museums or other art institutions. This is because, unlike other curatorial actions, the direct production of cultural products is avoided, because it is considered wasteful, and because it threatens the institutionalisation of the material representations of the materialities that are produced within the biennial (such as, for example, the production of artworks through curatorial action). Rather, the material representations are produced and maintained by a network of curators, art professionals, artists and other curators who then coordinate these actions</text>
    <text>future curatorial forms. 	To understand the significance of the exhibition in terms of sites of exchange, it is useful to examine some of the events that took place in Liverpool on 11 and 12 October 2016.  	A number of artists, including Wendy Harpe (DIY) and Manus Toru (TED), visited the site of the former Liverpool Biennial, which was then running alongside the London Biennial in parallel. Both exhibitions used the venue as a case study, with a number of curators setting up projects there. Wendy Harpe’s project  	Tiny Houses – Tiny Homes 4 Home Builders was set up in collaboration with the Lewis and Mackie Trust, the Riverside Community Land Trust and Liverpool School of Art. The term ‘site specificities’ is used here to refer to the specific objects or projects that are produced or revived on a site. By using this terminology, the collections at the Lewis and Mackie are able to draw on the biennial’s rich past, present and future – its places in the city or its hidden communities. 	The mini-exhibitions below trace some of these places of production or recycling or where production now occurs. This is an evolving list, and the curators have made changes in the order in which the materials were identified. For each new location or type of product, the list will be broken down into smaller and more specific categories. 	1. Stem cells, Tissue &amp; Apparition, Temple Hoyne Buell Foundation, 2014. Installation, Los Angeles.   1/2  2. 3D biofilms, NanoRacks, Taiwan &amp; 3D Systems, Singapore, 2015. Installation, Taipei.   3/4  5.  Calder Williams,  	Dark Matter, 2016. Installation, Los Angeles.   5/6  </text>
    <text>future curatorial forms. 	To understand the significance of the exhibition in terms of sites of exchange, it is useful to examine some of the events that took place in Liverpool on 11 and 12 October 2016.  	A number of artists, including Wendy Harpe (DIY) and Manus Toru (TED), visited the site of the former Liverpool Biennial, which was then running alongside the London Biennial in parallel. Both exhibitions used the venue as a case study, with a number of curators setting up projects there. Wendy Harpe’s project  	Tiny Houses – Tiny Homes 4 Home Builders was set up in collaboration with the Lewis and Mackie Trust, the Riverside Community Land Trust and Liverpool School of Art. The term ‘site specificities’ is used here to refer to the specific objects or projects that are produced or revived on a site. By using this terminology, the collections at the Liverpool Biennial can be conceived of as interconnected, i.e. not just in terms of their content, but also in terms of how they are produced, relatable and embodied within the site. 	The idea of connectedness emerged from the curatorial thinking of Wendy Harpe, who was working alongside two artists, Joasia Krysa and Priscilla Birch, when they devised the concept of an ‘infrastructure of shared meaning and experience’ around the Anfield/Breckfield area. The concept underpinning this idea is that objects, whether rendered static or ‘site specificities’, emerge as a result of the confluence of a number of influences, which in turn, are the result of a number of choices, which each artist collectively   – though in different ways – make. The body of the event is the result of many diverse factors – factors that mix cultural, geographical and financial influences. The infrastructures that</text>
    <text>future curatorial forms. 	To understand the significance of the exhibition in terms of sites of exchange, it is useful to examine some of the events that took place in Liverpool on 11 and 12 October 2016.  	A number of artists, including Wendy Harpe (DIY) and Manus Toru (TED), visited the site of the former Liverpool Biennial, which was then running alongside the London Biennial in parallel. Both exhibitions used the venue as a case study, with a number of curators setting up projects there. Wendy Harpe’s project  	Tiny Houses – Tiny Homes 4 Home Builders was set up in collaboration with the Lewis and Mackie Trust, the Riverside Community Land Trust and Liverpool School of Art. The term ‘site specificities’ is used here to refer to the specific objects or projects that are produced or revived on a site. By using this terminology, the collections at the Liverpool Library can be seen to be closely related to the collections produced by the Biennial, despite the fact that these two exhibitions took place in completely different universes. 	Although the collections and materials produced by the Biennial in its heyday were of a particular age, their current institutional forms are directly influenced by the collections and materials produced by the Harpe/Manus Toru projects, which were both happening in parallel. The fact that such an influential biennial was happening in Liverpool at all, in 2015, is a significant event that can be seen as a direct result of the formation of the Liverpool Biennial in 2007. As the Joan Littlewood exhibition demonstrates, biennials can be powered by collections and the like – and often moreso by the creation of a network of local linkages between collections and neighbourhoods. These are the curatorial conditions under which contemporary curatorial activities can be seen to be creatively engaged with and produce their own ‘site-specificities</text>
    <text>future curatorial forms. 	To understand the significance of the exhibition in terms of sites of exchange, it is useful to examine some of the events that took place in Liverpool on 11 and 12 October 2016.  	A number of artists, including Wendy Harpe (DIY) and Manus Toru (TED), visited the site of the former Liverpool Biennial, which was then running alongside the London Biennial in parallel. Both exhibitions used the venue as a case study, with a number of curators setting up projects there. Wendy Harpe’s project  	Tiny Houses – Tiny Homes 4 Home Builders was set up in collaboration with the Lewis and Mackie Trust, the Riverside Community Land Trust and Liverpool School of Art. The term ‘site specificities’ is used here to refer to the specific objects or projects that are produced or revived on a site. By using this terminology, the collections at the Liverpool Biennial and the Lewis and Mackie Trust could be seen as part of a single, continuous exhibitionary form. This is in part due to the fact that both groups were established in the late nineteenth century, during the era of the biennial, and continue to be so to this date. 	Similarly, the talks that took place during the Future City series during the 1970s and 80s, about the relationship between art and the environment, are a result of a curatorial and curatorial relationship, made more acute by the fact that both collections were established during the same time. The curatorial relationship between art and the environment was formalised during the early twentieth century, but the increasing bureaucratisation of the environment in the cultural economy of art has placed great demands on both the art gallery and the art collections. In these late-capitalist urban centres, the biennial is the last remaining bastion of individualised experience. Its disappearance is a given, and its revival</text>
    <text>future curatorial forms. 	To understand the significance of the exhibition in terms of sites of exchange, it is useful to examine some of the events that took place in Liverpool on 11 and 12 October 2016.  	A number of artists, including Wendy Harpe (DIY) and Manus Toru (TED), visited the site of the former Liverpool Biennial, which was then running alongside the London Biennial in parallel. Both exhibitions used the venue as a case study, with a number of curators setting up projects there. Wendy Harpe’s project  	Tiny Houses – Tiny Homes 4 Home Builders was set up in collaboration with the Lewis and Mackie Trust, the Riverside Community Land Trust and Liverpool School of Art. The term ‘site specificities’ is used here to refer to the specific objects or projects that are produced or revived on a site. By using this terminology, the collections at the Lewis and Mackie Trust and the Liverpool School of Art have been placed into the larger context of a local community, while the curatorial gestures of the Liverpool Biennial and the Liverpool School of Art are placed in a global one. 	At the Liverpool and Hong Kong’s Performing Arts Centre, 2016. Photo courtesy of Performing Arts Centre. 	When Wendy Harpe started her project  	Tiny Houses – Tiny Homes 4 Home Builders, in 2015, she set out to reclaim her home as an object of art, a reference point, and a possible end point. Her project incorporated art, landscape, community gardens, affordable housing and a number of other practices that came to be associated with the community. The perforations that emerged from the project, which were assembled from various institutions, were then put into a book that she edited and published in 2017 entitled  	Tiny Houses: Design, Resilience and Architecture in the Per</text>
    <text>future curatorial forms. 	To understand the significance of the exhibition in terms of sites of exchange, it is useful to examine some of the events that took place in Liverpool on 11 and 12 October 2016.  	A number of artists, including Wendy Harpe (DIY) and Manus Toru (TED), visited the site of the former Liverpool Biennial, which was then running alongside the London Biennial in parallel. Both exhibitions used the venue as a case study, with a number of curators setting up projects there. Wendy Harpe’s project  	Tiny Houses – Tiny Homes 4 Home Builders was set up in collaboration with the Lewis and Mackie Trust, the Riverside Community Land Trust and Liverpool School of Art. The term ‘site specificities’ is used here to refer to the specific objects or projects that are produced or revived on a site. By using this terminology, the collections at the Liverpool Biennial (LCB) can be seen to embody a particular form of urban development, which is a variant of what is sometimes referred to as ‘regeneration’. 	In a later stage, Harpe’s project was to have a presence at the Liverpool Biennial ‘16 years in a row’, but, as many have pointed out, this could also be seen as a form of historical’regeneration. 	In 1984, LCC opened its doors to the media, and over the years, press releases, poems and glossy covers. The biennial’s 10th and final edition in 1990 featured a photo essay by Norman Mailer, taking place on the same day as the ‘70th anniversary’ of the Liverpool Biennial. Issue #10 featured a series of ten-page spreads chronicling the history of biennials in the UK from the 1920s. 	The present</text>
    <text>future curatorial forms. 	To understand the significance of the exhibition in terms of sites of exchange, it is useful to examine some of the events that took place in Liverpool on 11 and 12 October 2016.  	A number of artists, including Wendy Harpe (DIY) and Manus Toru (TED), visited the site of the former Liverpool Biennial, which was then running alongside the London Biennial in parallel. Both exhibitions used the venue as a case study, with a number of curators setting up projects there. Wendy Harpe’s project  	Tiny Houses – Tiny Homes 4 Home Builders was set up in collaboration with the Lewis and Mackie Trust, the Riverside Community Land Trust and Liverpool School of Art. The term ‘site specificities’ is used here to refer to the specific objects or projects that are produced or revived on a site. By using this terminology, the collections at the Liverpool Biennial can be seen to be products of their respective site-specificities. Harpe’s project began when she and her team of designers, artists and architects visited the site of the Biennial and collected materials, objects and building components to rebuild the building of a half a dozen tiny houses on the former site of a major primary school. These houses, which sit on roughly the same design principles as Harpe’s own tiny house projects, use locally-sourced local labour and produce and promote social and civic benefits through shared ownership. These 'micro-institutional' changes, which frequently involve small groups of engaged citizens, are often inspired by local arts and culture producers and their media campaigns. The new designs and their associated public relations campaigns are often modelled after the older designs and media campaigns. 	The incorporation of micro-institutional change has often been facilitated by the formation of unions or other powerful local government actors. In Liverpool, for example, the</text>
    <text>future curatorial forms. 	To understand the significance of the exhibition in terms of sites of exchange, it is useful to examine some of the events that took place in Liverpool on 11 and 12 October 2016.  	A number of artists, including Wendy Harpe (DIY) and Manus Toru (TED), visited the site of the former Liverpool Biennial, which was then running alongside the London Biennial in parallel. Both exhibitions used the venue as a case study, with a number of curators setting up projects there. Wendy Harpe’s project  	Tiny Houses – Tiny Homes 4 Home Builders was set up in collaboration with the Lewis and Mackie Trust, the Riverside Community Land Trust and Liverpool School of Art. The term ‘site specificities’ is used here to refer to the specific objects or projects that are produced or revived on a site. By using this terminology, the collections at the Liverpool Biennial and the Lewis and Mackie Trust could be seen to address the institutionalised form of the biennial as a whole. Using 3D modelling and printing, collections could be organized around a central online database of objects, or they could be assembled from materials and archives at a regional or local level. The collection-building format would then be seen to be a site-specificity form, and not necessarily an expression of the biennial’s own institutional form (since the collection-building model is also premised on the institutional form of the biennial, but here ‘in the main’). 	In an essay in the New Statesman in 2011, Harpe commented that whilst museums had been designed to house collections, they could also ‘serve as repositories or exhibitionary platforms for the accumulated […] [and] radical potentialities […] of the biennial form, which is constantly on the wane, in the main [being] funded and</text>
    <text>future curatorial forms. 	To understand the significance of the exhibition in terms of sites of exchange, it is useful to examine some of the events that took place in Liverpool on 11 and 12 October 2016.  	A number of artists, including Wendy Harpe (DIY) and Manus Toru (TED), visited the site of the former Liverpool Biennial, which was then running alongside the London Biennial in parallel. Both exhibitions used the venue as a case study, with a number of curators setting up projects there. Wendy Harpe’s project  	Tiny Houses – Tiny Homes 4 Home Builders was set up in collaboration with the Lewis and Mackie Trust, the Riverside Community Land Trust and Liverpool School of Art. The term ‘site specificities’ is used here to refer to the specific objects or projects that are produced or revived on a site. By using this terminology, the collections at the Liverpool Museum can be seen to embody a form of urban resurgence, which is closely linked to the city’s decision to redevelop a large area of wasteland into a city centre, a decision that was heavily influenced by the works of the Hollywood Production Company. 	The Hollywood Production Company was founded in 1926 and is a New York-based arts management firm that has a long-standing interest in promoting socially engaged art. In the 1980s it developed several projects that brought artists from across the US and abroad to the UK, including Wendy Harpe, Ray Bradbury and George Elliott Brown.  	The Liverpool Museum was set up in 1890 and is the oldest surviving member of this long family of artists. The current curators Sandy Volz and Loraine Lacy re-created some of the practices and ideologies of the Hollywood Production Company in an attempt to bring some</text>
  </text>
  <text>
    developments in artificial intelligence, machine learning and data mining. These new vocabularies will make it easier for a user to understand what is being said by a networked system, and more importantly, what that system means. This is the language of the connected city; it is part of the global ‘metainterface’. The future city is the city with the virtual metainterface; it is where languages are understood by machines and data is translated between them. In this sense, the connected city is both a factory for human language and culture, and a grave for neglected cultural resources.The interconnected city is a biennale with an aural capital – great art will be on show, but also architectural wonders of the unreal and of the physically impossible, and human language will be rendered unintelligible with respect to these. A biennale that incorporates a major art project, or a major art fair, or even a major museum, is a kind of event that is not only about
    <text>developments in artificial intelligence, machine learning and data mining. These new vocabularies will make it easier for a user to understand what is being said by a networked system, and more importantly, what that system means. This is the language of the connected city; it is part of the global ‘metainterface’. The future city is the city with the virtual metainterface; it is where languages are understood by machines and data is translated between them. In this sense, the connected city is both a factory for human language and culture, and a grave for neglected cultural resources.The interconnected city is a biennale with an aural capital – great art will be on show, but also architectural wonders of the unreal and of the physically impossible, and human language will be rendered unintelligible with respect to these. A biennale that incorporates a major art project, or a major art fair, or even a major museum, is a kind of event that is not only about the art, but also about the social and political contexts of the art that is being made right now. It is a kind of event that is not only about the artist, but also about the city. And so the biennale needs to be eventful, and have social, ecological and urban dimensions. It is a kind of space in which the art can be made visible in ways that are as accessible as possible to the artists and audiences who are attracted by the art. This kind of presence is needed in the current environmental context, because ‘illustrated landscapes’ are the dominant form of cultural production in the city. The advent of computer graphics and the explosion of multimedia has made it possible to produce ever larger and more immersive images that are both physically visible and imaginatively understood. Art is made visible by the art that is being produced in the context of the connected city, and so the emergence of new media such as video and social networking have created a new kind of art that</text>
    <text>developments in artificial intelligence, machine learning and data mining. These new vocabularies will make it easier for a user to understand what is being said by a networked system, and more importantly, what that system means. This is the language of the connected city; it is part of the global ‘metainterface’. The future city is the city with the virtual metainterface; it is where languages are understood by machines and data is translated between them. In this sense, the connected city is both a factory for human language and culture, and a grave for neglected cultural resources.The interconnected city is a biennale with an aural capital – great art will be on show, but also architectural wonders of the unreal and of the physically impossible, and human language will be rendered unintelligible with respect to these. A biennale that incorporates a major art project, or a major art fair, or even a major museum, is a kind of event that is not only about the art, but also about the processes involved in bringing the art to life through the art. Its form is dynamic, and its content unpredictable. The interesting thing about the biennale is that it operates in both senses of the term. It defines the art, but also manipulates the language of language and culture through art. As biennales have become more institutionalised, and the auralised urban dweller into the curator, we can say that the curatorial turn around art is part of a general ‘curatorial trend’ that is part of a much broader trend of increasing accessibility and universalising influence of art in all aspects of contemporary culture. The question is whether there is a tendency towards a universalising influence of art in the design of infrastructure, or whether specific design projects are more influenced by the artistic output of the city at large, and by the artist’s own creative process. The answer to this is a rhetorical question that asks not only what is being done</text>
    <text>developments in artificial intelligence, machine learning and data mining. These new vocabularies will make it easier for a user to understand what is being said by a networked system, and more importantly, what that system means. This is the language of the connected city; it is part of the global ‘metainterface’. The future city is the city with the virtual metainterface; it is where languages are understood by machines and data is translated between them. In this sense, the connected city is both a factory for human language and culture, and a grave for neglected cultural resources.The interconnected city is a biennale with an aural capital – great art will be on show, but also architectural wonders of the unreal and of the physically impossible, and human language will be rendered unintelligible with respect to these. A biennale that incorporates a major art project, or a major art fair, or even a major museum, is a kind of event that is not only about the art, but also about the social and political implications of the art and visual arts that are generated and produced in the city. In this sense, the biennale is a ‘microcosm’ of the art and visual arts that are produced and supported in the city, and a kind of interlocutor between artist and public that is neither artist nor public but rather makes art at the same time. The event itself is an event that is produced by and in the city, and it is this production that is ‘spoken’ in different languages, ‘spoken dialects’, and ‘spoken tones’. It is also a kind of event that is impossible to quantify or quantify to a specific point, and thus a kind of a ‘point of reference’ in the space. But these two aspects of the event that make up the biennale are what make it a ‘unique and rare commodity’; a kind of rare commodity</text>
    <text>developments in artificial intelligence, machine learning and data mining. These new vocabularies will make it easier for a user to understand what is being said by a networked system, and more importantly, what that system means. This is the language of the connected city; it is part of the global ‘metainterface’. The future city is the city with the virtual metainterface; it is where languages are understood by machines and data is translated between them. In this sense, the connected city is both a factory for human language and culture, and a grave for neglected cultural resources.The interconnected city is a biennale with an aural capital – great art will be on show, but also architectural wonders of the unreal and of the physically impossible, and human language will be rendered unintelligible with respect to these. A biennale that incorporates a major art project, or a major art fair, or even a major museum, is a kind of event that is not only about the art, but also makes the art itself visible in an artistic context. It puts the art on a par with the facts, and with that, uses the art as a marker of symbolic value or cultural significance. With language comes representation; it is understood as a social act and not just an object of aesthetic value. The event that makes us visible is the one that limits our understanding, and the one that makes us disappear: that is, the event that creates the most social exchange is the one that we do not make visible. The artist has a perfect toolkit for this kind of work, an essential set of skills, an indispensable set of resources, and the event is a perfect platform to work from.Here, then, is the event as an event. It is possible that the way in which the language of the event is mapped will produce different outcomes for different people, and that is why biennales do different things: because different people have very different experiences, and so the language</text>
    <text>developments in artificial intelligence, machine learning and data mining. These new vocabularies will make it easier for a user to understand what is being said by a networked system, and more importantly, what that system means. This is the language of the connected city; it is part of the global ‘metainterface’. The future city is the city with the virtual metainterface; it is where languages are understood by machines and data is translated between them. In this sense, the connected city is both a factory for human language and culture, and a grave for neglected cultural resources.The interconnected city is a biennale with an aural capital – great art will be on show, but also architectural wonders of the unreal and of the physically impossible, and human language will be rendered unintelligible with respect to these. A biennale that incorporates a major art project, or a major art fair, or even a major museum, is a kind of event that is not only about the art, but also about the inhumanity and barbarity of human language. It is a rare occasion when a biennale is lauded for its own rationality, but is also highly likely to engage in a form of political, economic and social atrophy as a consequence of a degenerating human species. Nevertheless, there is a sense of new hope and inspiration emanating from these new understood vocabularies. And although these new understandings of the world around us may be alien to the conventions and taboos of the biennale, they are essential in the maintenance of the biennale as a global enterprise.The emergence of new understandings of the world is often explained as the result of a cultural revolution or a technologically mediated revolution. The emergence of new understandings of the world is often explained as the result of an artistic revolution, but the mechanisms of the biennale are much more deeply rooted in the history and culture of humanity. The biennale has an intrinsic, though declining</text>
    <text>developments in artificial intelligence, machine learning and data mining. These new vocabularies will make it easier for a user to understand what is being said by a networked system, and more importantly, what that system means. This is the language of the connected city; it is part of the global ‘metainterface’. The future city is the city with the virtual metainterface; it is where languages are understood by machines and data is translated between them. In this sense, the connected city is both a factory for human language and culture, and a grave for neglected cultural resources.The interconnected city is a biennale with an aural capital – great art will be on show, but also architectural wonders of the unreal and of the physically impossible, and human language will be rendered unintelligible with respect to these. A biennale that incorporates a major art project, or a major art fair, or even a major museum, is a kind of event that is not only about the art, but also about the community. It is a gathering that engages with the political and economic structures that produce and are produced by the art that is on show. It is a gathering that asks probing questions that render the art into question. And it is a gathering that is both a repository and a test of those questions. The event is a test of the capacities of various curators, artists, writers and others to conduct such a gathering in a way that produces a meaningful discourse about art and the social. It is a gathering that asks challenging questions that, while not directly applicable to the art at-present, can nevertheless serve as prompts or explanations for future work. It is a gathering that invites such a discourse because such a thing is needed. That is, the aim should be to ask questions that are applicable to the art as it exists today, but also to ask questions about its future – questions of conscience, questions of humanity, questions of place. It should employ art as a blunt</text>
    <text>developments in artificial intelligence, machine learning and data mining. These new vocabularies will make it easier for a user to understand what is being said by a networked system, and more importantly, what that system means. This is the language of the connected city; it is part of the global ‘metainterface’. The future city is the city with the virtual metainterface; it is where languages are understood by machines and data is translated between them. In this sense, the connected city is both a factory for human language and culture, and a grave for neglected cultural resources.The interconnected city is a biennale with an aural capital – great art will be on show, but also architectural wonders of the unreal and of the physically impossible, and human language will be rendered unintelligible with respect to these. A biennale that incorporates a major art project, or a major art fair, or even a major museum, is a kind of event that is not only about what is on show, but also about what does not. It introduces us to a new world, but also makes us question old questions. This kind of interaction between art and biennales is a rarefaction, and artists and curators alike are acutely aware of this. The artist Arjun Appadurai, whose work is part of the Biennale of Sydney (2013), has famously stated that he does not want his art to ‘fix positions,’ but rather to allow ‘a space where you can be exposed to a range of different positions, and yet be in constant interaction with a changing world’.4. The biennale as a ‘permanent marker of a place’. – Fred Turner, The Biennale Experience, 2006.Commissioned by and with the support of the: Turner Prize Foundation, and The Turner Prize, London. Photo: Tom Lox.The biennale, as an exhibitionary form, has been</text>
    <text>developments in artificial intelligence, machine learning and data mining. These new vocabularies will make it easier for a user to understand what is being said by a networked system, and more importantly, what that system means. This is the language of the connected city; it is part of the global ‘metainterface’. The future city is the city with the virtual metainterface; it is where languages are understood by machines and data is translated between them. In this sense, the connected city is both a factory for human language and culture, and a grave for neglected cultural resources.The interconnected city is a biennale with an aural capital – great art will be on show, but also architectural wonders of the unreal and of the physically impossible, and human language will be rendered unintelligible with respect to these. A biennale that incorporates a major art project, or a major art fair, or even a major museum, is a kind of event that is not only about the art, but also about the community, and the impact that that art and community projects can have on a local culture and society. In many ways, the community aspect of the biennale is what makes it unique from other forms of art, cultural organizations and events. It is a community effort, and often entwined with other forms of community effort. This kind of community is rare in previous biennales, and even less in this one. There is a difference, and Moore does not make that distinction. The kind of community that organizes itself around a shared set of norms, expectations, values, histories, and so forth, is rare in the contemporary context, and so is the kind of community that is most vulnerable to disruption by disruptive technologies – disruptive in the sense of its unpredictability and potential destructiveness, but also very vulnerable to the kind of spiritual and temporal wounds that can be inflicted by an awareness that is all too real – acutely aware of the fact that it is</text>
    <text>developments in artificial intelligence, machine learning and data mining. These new vocabularies will make it easier for a user to understand what is being said by a networked system, and more importantly, what that system means. This is the language of the connected city; it is part of the global ‘metainterface’. The future city is the city with the virtual metainterface; it is where languages are understood by machines and data is translated between them. In this sense, the connected city is both a factory for human language and culture, and a grave for neglected cultural resources.The interconnected city is a biennale with an aural capital – great art will be on show, but also architectural wonders of the unreal and of the physically impossible, and human language will be rendered unintelligible with respect to these. A biennale that incorporates a major art project, or a major art fair, or even a major museum, is a kind of event that is not only about art as an art form, but also about art as a social practice, a kind of language, and a kind of cultural exchange. It is an event that engages with the notion of art as a social practice, and the notion of art as a social enterprise. It engages with the violence of capitalism, and engages with the political and economic systems that enable it to exist. It engages with the idea that a museum is a place where social practices take place, and it engages with the notion of language as a social enterprise, employing that language as a kind of currency. And all these different kinds of social practices that are part of the event, become parts of this one.The event could be seen as a kind of archival register, keeping tabs on the changes in its subject as it happened, and the stories that it supported. It could be seen as a kind of documentary that is not only from the past, but tells the story of a particular moment in the past. It could be seen</text>
    <text>developments in artificial intelligence, machine learning and data mining. These new vocabularies will make it easier for a user to understand what is being said by a networked system, and more importantly, what that system means. This is the language of the connected city; it is part of the global ‘metainterface’. The future city is the city with the virtual metainterface; it is where languages are understood by machines and data is translated between them. In this sense, the connected city is both a factory for human language and culture, and a grave for neglected cultural resources.The interconnected city is a biennale with an aural capital – great art will be on show, but also architectural wonders of the unreal and of the physically impossible, and human language will be rendered unintelligible with respect to these. A biennale that incorporates a major art project, or a major art fair, or even a major museum, is a kind of event that is not only about the art, but about the process by which the art is produced and delivered. It is a production line that produces a precise, quantifiable effect. We are here to stay, after all: capable of producing meaning through language, whether we know it or not.Download this article as PDFJessica CoonJessica Coon is Associate Professor of English and Head of the Department of Linguistics and Aesthetics at the University of Alberta, and holds the Canada Research Chair in Syntax and Indigenous Languages. Her research interests include the evolution of contemporary Indigenous languages, with a special focus on the language of the Alaska Natives, and the socio-linguistic characteristics of First Nations languages. In addition to research, she leads collaborative language documentation and revitalization projects with Indigenous communities in Canada and Latin America. In 2015 she worked as the scientific consultant for the film Arrival, which stars Amy Adams as a linguistic fieldworker who is recruited by the military to decipher the cryptic writing on the shore of Lake</text>
  </text>
  <text>
    human behaviour and technology. Social and cognitive changes with the advent of mass communication and information processing equipment are the direct result of the proliferation of these technologies and the connected lives they create. The social consequences of this massive, exponential increase are still unresolved and will probably remain so for some time to come. Yet in the grand scheme of things, the intrinsic drive to cope with ever-more-advanced ways of living, growing larger and broader social networks is the real engine driving the change. This is why the organized community still stands against the wall in the hope that something will come along and take their place. But the very real sense of not being able to participate in the change, whether that sense is rooted in inequality or the inbuilt sense of not belonging, is what finally wakes the community from its malaise in the final analysis. The organizing of the past is as necessary for our future wellbeing as any other aspect of the complex. We have to learn to live alongside those who are different from ourselves in order to
    <text>human behaviour and technology. Social and cognitive changes with the advent of mass communication and information processing equipment are the direct result of the proliferation of these technologies and the connected lives they create. The social consequences of this massive, exponential increase are still unresolved and will probably remain so for some time to come. Yet in the grand scheme of things, the intrinsic drive to cope with ever-more-advanced ways of living, growing larger and broader social networks is the real engine driving the change. This is why the organized community still stands against the wall in the hope that something will come along and take their place. But the very real sense of not being able to participate in the change, whether that sense is rooted in inequality or the inbuilt sense of not belonging, is what finally wakes the community from its malaise in the final analysis. The organizing of the past is as necessary for our future wellbeing as any other aspect of the complex. We have to learn to live alongside those who are different from ourselves in order to become fully empathetic to the dynamics that create and are embedded in our present circumstances. This will require us to change how we perceive the world and our place in it, and how we envision the future.Download this article as PDFAnna L. Symington and Adam H. SchiffAnna L. Symington is a Ph.D. candidate in Communication Studies and the Fine Arts History Department at the University of Alberta, where she is completing her thesis. Her research examines communication technologies and their societal effects on artistic practice and explores the role that technology plays in our contemporary condition. She is currently completing her MFA and MCA in the interdisciplinary department of Visual Cultures at the University of Alberta, where she is completing a joint MA and MFA in art history and digital culture. She is the co-editor of the Visual Cultures blog and the web site Multidisciplinary Visual Cultures. She is also co-curator of the forthcoming Global Arts Connections exhibition, The Contemporary Condition,</text>
    <text>human behaviour and technology. Social and cognitive changes with the advent of mass communication and information processing equipment are the direct result of the proliferation of these technologies and the connected lives they create. The social consequences of this massive, exponential increase are still unresolved and will probably remain so for some time to come. Yet in the grand scheme of things, the intrinsic drive to cope with ever-more-advanced ways of living, growing larger and broader social networks is the real engine driving the change. This is why the organized community still stands against the wall in the hope that something will come along and take their place. But the very real sense of not being able to participate in the change, whether that sense is rooted in inequality or the inbuilt sense of not belonging, is what finally wakes the community from its malaise in the final analysis. The organizing of the past is as necessary for our future wellbeing as any other aspect of the complex. We have to learn to live alongside those who are different from ourselves in order to build the capacity to move towards the future.Download this article as PDFRyan AventRyan Avent is an assistant professor in The College of Architecture and Design at Mount Royal University, and holds a joint appointment at McGill University and Dalhousie University. His research interests include the social construction of infrastructure, participatory public design, and media technologies. His publications include the award-winning Structures of Inhumanity: The Extinction of Human Agency: Culture and Capital after the Extinction of the Human Species, to be published in February 2018; to be published in March 2019, with an essay by that essay on the website of the journal Social Science &amp; Culture. He is currently working on two projects: the collaborative art project Placemartyr and the visual essayist/podcast host Stitcher Sean Lacy. In addition to his regular teaching, he is co-editor of the art newspaper The Ubyssey and is a co-editor of the online journal Social Studies of Everyday Life (</text>
    <text>human behaviour and technology. Social and cognitive changes with the advent of mass communication and information processing equipment are the direct result of the proliferation of these technologies and the connected lives they create. The social consequences of this massive, exponential increase are still unresolved and will probably remain so for some time to come. Yet in the grand scheme of things, the intrinsic drive to cope with ever-more-advanced ways of living, growing larger and broader social networks is the real engine driving the change. This is why the organized community still stands against the wall in the hope that something will come along and take their place. But the very real sense of not being able to participate in the change, whether that sense is rooted in inequality or the inbuilt sense of not belonging, is what finally wakes the community from its malaise in the final analysis. The organizing of the past is as necessary for our future wellbeing as any other aspect of the complex. We have to learn to live alongside those who are different from ourselves in order to build the future we desire.Download this article as PDFAna Laura Lopéz de la TorreAna Laura Lopéz de la Torre is a writer, curator, and co-curator of the Autonomous Regional in Culture (ARCA) station at the San José Museum of Modern Art. Her work has addressed the intersections between culture and politics since the 1980s, and her current projects explore the political implications of emerging biennials. She is the author of several books including the recently released We Are Here to Stay: On Community Arts and the Politics of Cultural Trauma (2015) and the forthcoming (2014) book Two Cities: Stories of San Francisco and Nineteen Nineteen: Urban Regeneration and a City on Fire.She has a Bachelor of Arts in English and Cultural Studies from Santa Clara University, and a Master of Public Affairs from the Santa Clara University Graduate Center.Profile: Jaime Herrera-EuropeContra Costa Five’s Gabrielle Ruiz</text>
    <text>human behaviour and technology. Social and cognitive changes with the advent of mass communication and information processing equipment are the direct result of the proliferation of these technologies and the connected lives they create. The social consequences of this massive, exponential increase are still unresolved and will probably remain so for some time to come. Yet in the grand scheme of things, the intrinsic drive to cope with ever-more-advanced ways of living, growing larger and broader social networks is the real engine driving the change. This is why the organized community still stands against the wall in the hope that something will come along and take their place. But the very real sense of not being able to participate in the change, whether that sense is rooted in inequality or the inbuilt sense of not belonging, is what finally wakes the community from its malaise in the final analysis. The organizing of the past is as necessary for our future wellbeing as any other aspect of the complex. We have to learn to live alongside those who are different from ourselves in order to become fully responsive to the new technologies that are reshaping the world as we know it.Download this article as PDFRyan AventRyan Avent is an artist and writer who works as an information technologist at Facebook. His work in communities begins with social work and extends to research and design. His research and design examines the impact of new media technologies on social and spatial relations of power. His latest project, The Social Landscape, is a new way to understand how people form groups and interacts with the place he calls home. He is currently working as an artist consultant for the new stadium for Liverpool Football Club. His new book, The Social Landscape: Converging Reality and Fiction, is published by Oxford University Press.  	[1] Curtis Ioannidis, ‘The Rise of the Creative Class: Social Capital and the Creative Minds Who Built the Machine’, http://www.creativeclass.com/2009/08/the-rise-</text>
    <text>human behaviour and technology. Social and cognitive changes with the advent of mass communication and information processing equipment are the direct result of the proliferation of these technologies and the connected lives they create. The social consequences of this massive, exponential increase are still unresolved and will probably remain so for some time to come. Yet in the grand scheme of things, the intrinsic drive to cope with ever-more-advanced ways of living, growing larger and broader social networks is the real engine driving the change. This is why the organized community still stands against the wall in the hope that something will come along and take their place. But the very real sense of not being able to participate in the change, whether that sense is rooted in inequality or the inbuilt sense of not belonging, is what finally wakes the community from its malaise in the final analysis. The organizing of the past is as necessary for our future wellbeing as any other aspect of the complex. We have to learn to live alongside those who are different from ourselves in order to become fully capable of reacting to the changes that are inevitably caused by new technologies. This is why the organizing principle of the biennale is still movement and activism, but increasingly it is about skills development and formal education. In the early stages of this process, the goalposts are shifting, but once the systems are brought into gear, the resulting impact will be indeterminate. The possibilities are endless, but the capacities for change are finite. Thus, the organizing principle of the biennale remains engagement and continual movement, but increasingly it is about the creation of new modes of inhabitation and use that take into account the capacities of all concerned. This is why the new technologies are brought to bear on the public realm and its inhabitants, not to mention the actual building and management of the artesian and artificial islands that are part of the artesian and artificial landscape. It is part of a wider, but yet fragile, question of democratic government and public realm management. The question of inSite continues</text>
    <text>human behaviour and technology. Social and cognitive changes with the advent of mass communication and information processing equipment are the direct result of the proliferation of these technologies and the connected lives they create. The social consequences of this massive, exponential increase are still unresolved and will probably remain so for some time to come. Yet in the grand scheme of things, the intrinsic drive to cope with ever-more-advanced ways of living, growing larger and broader social networks is the real engine driving the change. This is why the organized community still stands against the wall in the hope that something will come along and take their place. But the very real sense of not being able to participate in the change, whether that sense is rooted in inequality or the inbuilt sense of not belonging, is what finally wakes the community from its malaise in the final analysis. The organizing of the past is as necessary for our future wellbeing as any other aspect of the complex. We have to learn to live alongside those who are different from ourselves in order to ever truly understand what it means to be part of a community.Download this article as PDFJussi ParikkaJussi Parikka is Professor in Technological Culture &amp; Aesthetics at the Winchester School of Art, University of Southampton. His research interests include the creation of new media (digital media and aesthetics), the interaction of art, science and technology in the life of everyday people, and the interaction of art, architecture and social practice. His latest publications include: ‘How Social Media Can Fix What's Broken’ in The Aesthetic Resource Journal, Issue 3 (Summer 2016), 'Look East, Not West: Technology, Culture and Urban Fragility’ in The Journal of Cultural Studies, Vol. 21 (1), pp.1–22. His writing has been featured in BBC World News, The Guardian, The New York Times, The New York Observer, The New Republic, The New York Sun, The New Republic of New York, The New Republic of</text>
    <text>human behaviour and technology. Social and cognitive changes with the advent of mass communication and information processing equipment are the direct result of the proliferation of these technologies and the connected lives they create. The social consequences of this massive, exponential increase are still unresolved and will probably remain so for some time to come. Yet in the grand scheme of things, the intrinsic drive to cope with ever-more-advanced ways of living, growing larger and broader social networks is the real engine driving the change. This is why the organized community still stands against the wall in the hope that something will come along and take their place. But the very real sense of not being able to participate in the change, whether that sense is rooted in inequality or the inbuilt sense of not belonging, is what finally wakes the community from its malaise in the final analysis. The organizing of the past is as necessary for our future wellbeing as any other aspect of the complex. We have to learn to live alongside those who are different from ourselves in order to truly become integrated members of society.The other element that has to take part in the process that creates social change is the technological capability. The future demands that we are connected to the past, and we have to find new ways of interacting with that past. But although these social changes may occur organically, they inevitably end up being frustrated by the obstruction of the present. And although the future may present opportunities for growth, it is imperative that we put our efforts into finding other ways of being connected to the material world around us and making ourselves visible to it, so as to create change at a larger scale.In an ideal world, we would all attain self-sufficient means of producing our own future, but of course this is not always possible and technological progress necessarily follows economic progress. So while it may be possible to reuse materials and build on previous discoveries, such as the ones based on petroleum and coal, in the end all attempts at recreating the past are doomed to fail and create the very</text>
    <text>human behaviour and technology. Social and cognitive changes with the advent of mass communication and information processing equipment are the direct result of the proliferation of these technologies and the connected lives they create. The social consequences of this massive, exponential increase are still unresolved and will probably remain so for some time to come. Yet in the grand scheme of things, the intrinsic drive to cope with ever-more-advanced ways of living, growing larger and broader social networks is the real engine driving the change. This is why the organized community still stands against the wall in the hope that something will come along and take their place. But the very real sense of not being able to participate in the change, whether that sense is rooted in inequality or the inbuilt sense of not belonging, is what finally wakes the community from its malaise in the final analysis. The organizing of the past is as necessary for our future wellbeing as any other aspect of the complex. We have to learn to live alongside those who are different from ourselves in order to truly be able to thrive in the new contexts generated by technology.Download this article as PDFDoubt and TremblingFiona LeeFiona Lee is an artist and researcher based in London. Her research interests include the provision of legal and social services to those most at-risk of becoming disenfranchised; the provision of housing for people with disabilities; and the provision of culturally responsive health and education services. She is currently Head of the Department of Urban Studies at Kingston University, and holds a joint appointment at Liverpool John Moores Special Research Unit. Her work has been funded by the Arts Council England, the Welsh Streets Association, the DfE and the Wellcome Trust.Empire State Building by Clement Clarke (Completed in 1910) and Noel Burch (Photograph courtesy of Clement Clarke Archives, Art and Archives Trust)Leaflet to mark the centenary of the outbreak of First World War I in the City of London, January 1914. Fiona Lee  F</text>
    <text>human behaviour and technology. Social and cognitive changes with the advent of mass communication and information processing equipment are the direct result of the proliferation of these technologies and the connected lives they create. The social consequences of this massive, exponential increase are still unresolved and will probably remain so for some time to come. Yet in the grand scheme of things, the intrinsic drive to cope with ever-more-advanced ways of living, growing larger and broader social networks is the real engine driving the change. This is why the organized community still stands against the wall in the hope that something will come along and take their place. But the very real sense of not being able to participate in the change, whether that sense is rooted in inequality or the inbuilt sense of not belonging, is what finally wakes the community from its malaise in the final analysis. The organizing of the past is as necessary for our future wellbeing as any other aspect of the complex. We have to learn to live alongside those who are different from ourselves in order to build solid bridges with those who are different from us. 	[1] The notion of knowledge production as such, or the more correctly conceptualised as ‘knowledge production on a global scale’, has come a long way since Descartes made his famous distinction between substances and human reasoning. Nowadays we would say that something is a ‘susceptible element’ in a complex system, and the more susceptible that element is to change, the better its capacity will be to handle such enormous amounts of data.  	[2] See: https://www.math.ucl.ac.uk/academics/science/fall/e7/i7/1998.html?... 	[3] http://www.sciencemag.org/cgi/content/abstract/283/6/627/am134054?... 	[4] Morehshin Allahyari, ‘</text>
    <text>human behaviour and technology. Social and cognitive changes with the advent of mass communication and information processing equipment are the direct result of the proliferation of these technologies and the connected lives they create. The social consequences of this massive, exponential increase are still unresolved and will probably remain so for some time to come. Yet in the grand scheme of things, the intrinsic drive to cope with ever-more-advanced ways of living, growing larger and broader social networks is the real engine driving the change. This is why the organized community still stands against the wall in the hope that something will come along and take their place. But the very real sense of not being able to participate in the change, whether that sense is rooted in inequality or the inbuilt sense of not belonging, is what finally wakes the community from its malaise in the final analysis. The organizing of the past is as necessary for our future wellbeing as any other aspect of the complex. We have to learn to live alongside those who are different from ourselves in order to build the future we want.Download this article as PDFEmily PringleThe Grace Hopper Distinguished Service Professor of English and Film Studies at Mount Royal University, Emily Pringle is the author of The Gardens of Babylon: Modern Art and Its Relation to Ancient Near Eastern Places (2006), and has taught on the art and architecture syllabus at Mount Royal University for the past five years. Her current research focuses on the cultural meaning of modern art and how it relates to the social, economic and political situations of the contemporary Middle East. Her articles have been published in The Washington Post, The New York Times, The New Republic, The New York Observer, Al Akhbar Al-Rudawy Al Ansariyyah, and Al Asmakh Al-Thaqafiyaan. She is currently Director of the Centre for the Study of Modern Art at Mount Royal University and holds a Bachelor of Arts in English Cultural Studies from Mount Royal University, where she also worked towards a master�</text>
  </text>
  <text>
    developments in artificial intelligence and quantum computing will inevitably lead to changes in how we understand and interact with the world. The question is which of these technologies will be developed and which ones will be exploited. The technologies that will be exploited are likely to be of two types: those that make use of extremely generalised datasets (like data from a car or a house) and those that harness these datasets to make inferences about the world (like Google).Conversely, the technologies that will be used to understand and to learn from the world around us are likely to be of two types: those that understand the world (like Google) and those that are deployed on a massive scale (like Facebook). The answer to your question (which has been repeated hundreds of times) is obviously &quot;both&quot;. Clearly, in the age of the genome and the internet of things (where everything is connected and can be observed, measured and controlled), it is critical that the infrastructures that we inhabit – buildings, cars, homes, schools
    <text>developments in artificial intelligence and quantum computing will inevitably lead to changes in how we understand and interact with the world. The question is which of these technologies will be developed and which ones will be exploited. The technologies that will be exploited are likely to be of two types: those that make use of extremely generalised datasets (like data from a car or a house) and those that harness these datasets to make inferences about the world (like Google).Conversely, the technologies that will be used to understand and to learn from the world around us are likely to be of two types: those that understand the world (like Google) and those that are deployed on a massive scale (like Facebook). The answer to your question (which has been repeated hundreds of times) is obviously &quot;both&quot;. Clearly, in the age of the genome and the internet of things (where everything is connected and can be observed, measured and controlled), it is critical that the infrastructures that we inhabit – buildings, cars, homes, schools – constantly scan the landscape for objects that can be assembled into anything that can be labelled with the tag 'home'. If such infrastructures continuously scan the landscape, gathering data about the landscape – its features, its layout, the objects that can be assembled from them, and so forth – they are bound to become cognitively overloaded and subject to the whims of unpredictable human behaviour. And this is exactly what we see happening in San Francisco. Every time a new building or new development is announced in the city, a community of residents is gathered around it day or even overnight to explore its morphology, its history and to learn about its history – a history that is likely to be incomplete, messy and chaotic. And this history is constantly revised, re-imagined and tweaked by the constantly shifting definitions and concerns of developers, activists, politicians and local journalists. It is, in short, a history of change, but it is also a history of ambiguity, loss and negation.Does this sound</text>
    <text>developments in artificial intelligence and quantum computing will inevitably lead to changes in how we understand and interact with the world. The question is which of these technologies will be developed and which ones will be exploited. The technologies that will be exploited are likely to be of two types: those that make use of extremely generalised datasets (like data from a car or a house) and those that harness these datasets to make inferences about the world (like Google).Conversely, the technologies that will be used to understand and to learn from the world around us are likely to be of two types: those that understand the world (like Google) and those that are deployed on a massive scale (like Facebook). The answer to your question (which has been repeated hundreds of times) is obviously &quot;both&quot;. Clearly, in the age of the genome and the internet of things (where everything is connected and can be observed, measured and controlled), it is critical that the infrastructures that we inhabit – buildings, cars, homes, schools – all become self-aware and to a large extent, even those in westernized countries, these days rely on data to monitor and control citizens. But this isn’t easy and nor is it desirable; it is clear that if such infrastructures are to connect with the wider world, they need to be able to interpret and decipher the ever-increasing amounts of data that are constantly being pumped into them. Ultimately, this leads us to the question: what is the role of art in this context? The answer is simple: as a profession, we already play this role well enough in most places around the world. We are constantly bombarded with images and sound bites of buildings, cars, homes and landscapes that need to be looked after, maintained and upgraded; we needn’t go too far to suggest that we are witnessing the growth of a contemporary art form in places where art isn’t traditionally strong. Doing the same thing in the art world would</text>
    <text>developments in artificial intelligence and quantum computing will inevitably lead to changes in how we understand and interact with the world. The question is which of these technologies will be developed and which ones will be exploited. The technologies that will be exploited are likely to be of two types: those that make use of extremely generalised datasets (like data from a car or a house) and those that harness these datasets to make inferences about the world (like Google).Conversely, the technologies that will be used to understand and to learn from the world around us are likely to be of two types: those that understand the world (like Google) and those that are deployed on a massive scale (like Facebook). The answer to your question (which has been repeated hundreds of times) is obviously &quot;both&quot;. Clearly, in the age of the genome and the internet of things (where everything is connected and can be observed, measured and controlled), it is critical that the infrastructures that we inhabit – buildings, cars, homes, schools – all become sensors and actuators that can infer and predict things about the world through data. This requires new kinds of understandings of space and of people.Infrastructures are likely to become more and more like apps – gathering and aggregating data – and are likely to come to understand the concept of 'big data' as a way of understanding the world. This means that infrastructures will produce and play with data, and will do so in ways that are highly interpretable by a developer or a scientist. For example, if the city builds its housing, it may then be fed valuable spatial and economic data about the area back to the app – allowing the city to develop strategies to accommodate the growing population – and this could then be used to attract investors and business opportunities. Such processes would be highly susceptible to the pressures that can arise from the information overload and new data-collecting priorities of a post-collapse society.Such features of the contemporary city are likely to exacerbate</text>
    <text>developments in artificial intelligence and quantum computing will inevitably lead to changes in how we understand and interact with the world. The question is which of these technologies will be developed and which ones will be exploited. The technologies that will be exploited are likely to be of two types: those that make use of extremely generalised datasets (like data from a car or a house) and those that harness these datasets to make inferences about the world (like Google).Conversely, the technologies that will be used to understand and to learn from the world around us are likely to be of two types: those that understand the world (like Google) and those that are deployed on a massive scale (like Facebook). The answer to your question (which has been repeated hundreds of times) is obviously &quot;both&quot;. Clearly, in the age of the genome and the internet of things (where everything is connected and can be observed, measured and controlled), it is critical that the infrastructures that we inhabit – buildings, cars, homes, schools – all become autonomous and access to data-intensive infrastructures is imperative. But there is a catch: as soon as the infrastructures that comprise the city become so embedded in the city that they begin to think of themselves as autonomous, they inevitably run into issues of desirability and become subject to urban pressures. This is the second prong of the city: it is the infrastructure that can and will be altered and reshaped in response to changes in the world around it. The question is which of the two types of infrastructures will be modified? And what kinds of infrastructures will be created?The answer to your second prong is obvious: it is the question that defines and defines the infrastructural landscape. It is a question that has landed on your head since you were in primary school: how to account for the fact that while you were in a store in Islington, a very nice lady in a flowered dresser suddenly decided</text>
    <text>developments in artificial intelligence and quantum computing will inevitably lead to changes in how we understand and interact with the world. The question is which of these technologies will be developed and which ones will be exploited. The technologies that will be exploited are likely to be of two types: those that make use of extremely generalised datasets (like data from a car or a house) and those that harness these datasets to make inferences about the world (like Google).Conversely, the technologies that will be used to understand and to learn from the world around us are likely to be of two types: those that understand the world (like Google) and those that are deployed on a massive scale (like Facebook). The answer to your question (which has been repeated hundreds of times) is obviously &quot;both&quot;. Clearly, in the age of the genome and the internet of things (where everything is connected and can be observed, measured and controlled), it is critical that the infrastructures that we inhabit – buildings, cars, homes, schools – all become biennials of sorts, where large amounts of data about the world are gathered and exchanged. Such infrastructural biennials need to be imaginative, sensitive to the communities that they are installed in, and take into consideration the communities that inhabit them – not just in the abstract but concretely too.And finally, to answer your question (which I paraphrase): how should infrastructural infrastructures be designed to accommodate and process information? Research has shown that the number one constraint on innovation and productivity is a lack of understanding and acceptance of new knowledge; a lack of resources and a consequent inability to innovate. In addition, the infrastructural knowledge deficit is a huge challenge for developing nations; especially for young people. In the West, we tend to think of infrastructural knowledge as if it were a static reality, something that can be corrected by adding new buildings and so forth. However, the infrastructural knowledge deficit for developing</text>
    <text>developments in artificial intelligence and quantum computing will inevitably lead to changes in how we understand and interact with the world. The question is which of these technologies will be developed and which ones will be exploited. The technologies that will be exploited are likely to be of two types: those that make use of extremely generalised datasets (like data from a car or a house) and those that harness these datasets to make inferences about the world (like Google).Conversely, the technologies that will be used to understand and to learn from the world around us are likely to be of two types: those that understand the world (like Google) and those that are deployed on a massive scale (like Facebook). The answer to your question (which has been repeated hundreds of times) is obviously &quot;both&quot;. Clearly, in the age of the genome and the internet of things (where everything is connected and can be observed, measured and controlled), it is critical that the infrastructures that we inhabit – buildings, cars, homes, schools, streets and so forth – all develop capabilities that are capable of understanding and of learning from change. If these infrastructures are to be adaptive, they must be able to detect and adapt to new infrastructures that are being built, and to detect and adapt to the new infrastructures that are being dismantled – that is, to move from the place where building construction is infrastructural to places where data gathering and monitoring are routineised. It is this last capability that Facebook hopes to build, as it looks to the future and sees surveillance as a future function of sorts.If Facebook is to be taken seriously as an engineering company, it is vital that its future work deviate from the ways in which infrastructures are already being designed and that are based on assumptions about the world around it. As Elon Musk’s company grows in scale and sophistication, so too does its research and development department, whose mission statement is ‘To accelerate the evolution of</text>
    <text>developments in artificial intelligence and quantum computing will inevitably lead to changes in how we understand and interact with the world. The question is which of these technologies will be developed and which ones will be exploited. The technologies that will be exploited are likely to be of two types: those that make use of extremely generalised datasets (like data from a car or a house) and those that harness these datasets to make inferences about the world (like Google).Conversely, the technologies that will be used to understand and to learn from the world around us are likely to be of two types: those that understand the world (like Google) and those that are deployed on a massive scale (like Facebook). The answer to your question (which has been repeated hundreds of times) is obviously &quot;both&quot;. Clearly, in the age of the genome and the internet of things (where everything is connected and can be observed, measured and controlled), it is critical that the infrastructures that we inhabit – buildings, cars, homes, schools – all become self-aware and to some extent have minds of their own.But what does it mean to be both aware and to some extent to benefit from the insights and actions of all those around us? In my humble opinion, one must actively participate in creating and contributing to build up knowledge and consciousness in order to truly understand and partake in the pleasures and perils of this wondrous new world. One cannot simultaneously inhabit and absorb knowledge – one must actively seek knowledge and grasp at straws what one knows might come to harm. One must also actively seek knowledge in order to act upon it and to be able to react to it in potentially destructive ways. For me, in my role as a Curator for Open Access Research at the British Museum, I have learned to value active learning as much as I have been taught by professionals, and have benefited from the latter by virtue of having to continually ask, meaningfully, and meaningfully, questions of no return.I have always felt that the</text>
    <text>developments in artificial intelligence and quantum computing will inevitably lead to changes in how we understand and interact with the world. The question is which of these technologies will be developed and which ones will be exploited. The technologies that will be exploited are likely to be of two types: those that make use of extremely generalised datasets (like data from a car or a house) and those that harness these datasets to make inferences about the world (like Google).Conversely, the technologies that will be used to understand and to learn from the world around us are likely to be of two types: those that understand the world (like Google) and those that are deployed on a massive scale (like Facebook). The answer to your question (which has been repeated hundreds of times) is obviously &quot;both&quot;. Clearly, in the age of the genome and the internet of things (where everything is connected and can be observed, measured and controlled), it is critical that the infrastructures that we inhabit – buildings, cars, homes, schools and financial services – all exhibit some level of understanding and that they are all interconnected. Also, it is critical that they are connected in ways that allow the infrastructures that comprise them to be detected and dealt with at scale. But to do so, infrastructures will have to be jointly understood and interconnected. This is where science fiction – which increasingly draws on the imagination of a large proportion of the population – enters the picture.Let me give you an example of how science fiction can and does incorporate science. Consider the Singularity, as futurist Ray Kurzweil envisages it, and its many forms. Kurzweil has variously referred to it as ‘Moore’ian ‘transhumanism’, ‘Lazarus’ future city, ‘Singularity’ and ‘Black-Scholes’. All these different forms of the ‘transhuman’ archetype are thought by Kurzweil to</text>
    <text>developments in artificial intelligence and quantum computing will inevitably lead to changes in how we understand and interact with the world. The question is which of these technologies will be developed and which ones will be exploited. The technologies that will be exploited are likely to be of two types: those that make use of extremely generalised datasets (like data from a car or a house) and those that harness these datasets to make inferences about the world (like Google).Conversely, the technologies that will be used to understand and to learn from the world around us are likely to be of two types: those that understand the world (like Google) and those that are deployed on a massive scale (like Facebook). The answer to your question (which has been repeated hundreds of times) is obviously &quot;both&quot;. Clearly, in the age of the genome and the internet of things (where everything is connected and can be observed, measured and controlled), it is critical that the infrastructures that we inhabit – buildings, cars, homes, schools – all grow in accordance with the patterns of behaviour that evolve as a result of our activities. But this doesn’t mean that everything that we do, every step we take, is going to result in the creation of beautiful buildings and happy people.Building codes have always been interesting because they allowed for the creation of a multitude of different possible configurations of buildings, and this abundance of different building materials and sizes is a natural consequence of the way in which the city is connected. The internet of things allows for a plethora of infrastructures to be built according to the patterns of behaviour that evolve as a result of their users (i.e. actions such as walking, running, lying down and so on). This abundance of different possible configurations of infrastructures is a by-product of what economist Jeffrey Diamanti calls ‘the urban condition’. The urban condition is a place where individuals and groups of people (often individuals) enact particular actions in and through which a</text>
    <text>developments in artificial intelligence and quantum computing will inevitably lead to changes in how we understand and interact with the world. The question is which of these technologies will be developed and which ones will be exploited. The technologies that will be exploited are likely to be of two types: those that make use of extremely generalised datasets (like data from a car or a house) and those that harness these datasets to make inferences about the world (like Google).Conversely, the technologies that will be used to understand and to learn from the world around us are likely to be of two types: those that understand the world (like Google) and those that are deployed on a massive scale (like Facebook). The answer to your question (which has been repeated hundreds of times) is obviously &quot;both&quot;. Clearly, in the age of the genome and the internet of things (where everything is connected and can be observed, measured and controlled), it is critical that the infrastructures that we inhabit – buildings, cars, homes, schools – constantly evolve to adapt to the new technologies that are applied on a large scale. But this is only possible in the context of a capitalist world where everything is commodity and exchange is unrestricted. It is this flexibility that allows for the extraordinary; the deployment of anti-subsistence technologies on a massive scale, for instance, but also for the production of vast amounts of waste through advanced energy technologies. The question is which paradigm will be utilised in the age to come?In my opinion, the present one, which focuses on the globalised social rather than the local, is the paradigm that will survive into the future. In the age of the internet of things, the social is deepened; in the age of biofuels and nano-particles, the biomed will be the predominant mode of production. The biomed will render outworn paradigms like the biennial into nothing, and pave the way for a new ecology of habitation. But this does not mean that we</text>
  </text>
  <text>
    human behaviour and technology – the future is going to be like the past – but with machines that understand more and do better. The question is how to design these so that we, as humans, can transition to a future where the bad guys are stopped short of there being no humans left at all.Download this article as PDFAna Laura Lopéz de la TorreJuan Carlos LópezDe la Torre is Professor at Monterrey School of Architecture and Design at the University of Toronto, where he directs the Centre for Research Architecture, a research centre for research-based architecture. His books include the nonfiction collection The Monuments of San Cristóbal de las Casas (2003, New York Times Book Review Editor) and the journalism collection The Competing Critiques of Sheila McKay (1997, HarperCollins, New York). He has written for the New York Times, The New York Times Book Review, The Washington Post and The New Republic. He is the author of several New York Times
    <text>human behaviour and technology – the future is going to be like the past – but with machines that understand more and do better. The question is how to design these so that we, as humans, can transition to a future where the bad guys are stopped short of there being no humans left at all.Download this article as PDFAna Laura Lopéz de la TorreJuan Carlos LópezDe la Torre is Professor at Monterrey School of Architecture and Design at the University of Toronto, where he directs the Centre for Research Architecture, a research centre for research-based architecture. His books include the nonfiction collection The Monuments of San Cristóbal de las Casas (2003, New York Times Book Review Editor) and the journalism collection The Competing Critiques of Sheila McKay (1997, HarperCollins, New York). He has written for the New York Times, The New York Times Book Review, The Washington Post and The New Republic. He is the author of several New York Times bestsellers including the 2015 New York Times bestseller El Presidente en el Mundo (with Cheri Honkala) and the 2015 Washington Post bestseller El Parthenon en el Trient (with Brian Sozzi and Brian Roberts). He is also the editor of the online journal BASE (The Basis of Exchange).Juan Carlos LópezDe la Torre is the fourth from the nine founding families of Monterrey School of Architecture. His father was a designer and architect, and his mother an architect. Her work as an architect includes works for the US army, the World Trade Center, and the Sears Tower. She is also an editor of the online journal BASE (The Basis of Exchange), a publishing house of ideas and critiques of the contemporary world. She is a member of the Instituto Nacional de Arte Contempora, the Mexican arm of the Mercosul Biennalee de Mexico, which also includes works for the Mexican marines,</text>
    <text>human behaviour and technology – the future is going to be like the past – but with machines that understand more and do better. The question is how to design these so that we, as humans, can transition to a future where the bad guys are stopped short of there being no humans left at all.Download this article as PDFAna Laura Lopéz de la TorreJuan Carlos LópezDe la Torre is Professor at Monterrey School of Architecture and Design at the University of Toronto, where he directs the Centre for Research Architecture, a research centre for research-based architecture. His books include the nonfiction collection The Monuments of San Cristóbal de las Casas (2003, New York Times Book Review Editor) and the journalism collection The Competing Critiques of Sheila McKay (1997, HarperCollins, New York). He has written for the New York Times, The New York Times Book Review, The Washington Post and The New Republic. He is the author of several New York Times bestsellers, including On Monuments (2007, Simon &amp; Schuster), The Gilded Gaze: Wealth and Politics in an Industrial Revolution (2007, Simon &amp; Schuster), and the forthcoming Stages: The Gilded Gaze: Writings on the Gilded Gaze (2014, Vintage).Learn More:James Howard Kunstler: James Howard Kunstler is Professor of English and Film Studies at the University of Alberta, where he directs the Kunstler Institute for Art and Culture, a research centre for the study of art, design and culture. His books include the 2015 American Classicist, Design in the Age of Information (with Maxine Peake) and the 2015 New York Times Best Seller, The Art Newspaper (with Noel Burch and Noel McKay). He has written for The New York Times, The New York Times Book Review, The Washington Post and The New Republic, among many other publications. He is the host of the popular online radio show Stitcher Economics</text>
    <text>human behaviour and technology – the future is going to be like the past – but with machines that understand more and do better. The question is how to design these so that we, as humans, can transition to a future where the bad guys are stopped short of there being no humans left at all.Download this article as PDFAna Laura Lopéz de la TorreJuan Carlos LópezDe la Torre is Professor at Monterrey School of Architecture and Design at the University of Toronto, where he directs the Centre for Research Architecture, a research centre for research-based architecture. His books include the nonfiction collection The Monuments of San Cristóbal de las Casas (2003, New York Times Book Review Editor) and the journalism collection The Competing Critiques of Sheila McKay (1997, HarperCollins, New York). He has written for the New York Times, The New York Times Book Review, The Washington Post and The New Republic. He is the author of several New York Times best-sellers including the recently published The Architecture Project: Social Sculpture and the Critique of Bloomsbury (1998, with Susanne Posel and Susanne Hecker, Routledge, New York).Juan Carlos LópezDe la Torre: In the beginning of the book you refer to the 'Cubist Manifesto'. What’s it like to be a poet in the United States during the 1980s and '90s?Juan Carlos LópezDe la Torre: I come from the second generation of Cubists. They’re the artists who were born between the two world wars. Cubism is the final chapter in that history. Cubism is not just about abstract aesthetic ideas, but also about the moral and political values that these ideals put forward. For me, it’s important for artists to be thinking about and working through the social implications of their work. In the 1990s, there was a critical moment when</text>
    <text>human behaviour and technology – the future is going to be like the past – but with machines that understand more and do better. The question is how to design these so that we, as humans, can transition to a future where the bad guys are stopped short of there being no humans left at all.Download this article as PDFAna Laura Lopéz de la TorreJuan Carlos LópezDe la Torre is Professor at Monterrey School of Architecture and Design at the University of Toronto, where he directs the Centre for Research Architecture, a research centre for research-based architecture. His books include the nonfiction collection The Monuments of San Cristóbal de las Casas (2003, New York Times Book Review Editor) and the journalism collection The Competing Critiques of Sheila McKay (1997, HarperCollins, New York). He has written for the New York Times, The New York Times Book Review, The Washington Post and The New Republic. He is the author of several New York Times best-sellers including the 2015 New York Times best-seller The Competing Critiques of Sheila McKay (2003, Simon &amp; Schuster, New York).Juan Carlos LópezDe la Torre is a multi-award winning author whose career spans into the present. His first book, The Lives of Others (2007), examined the lives and works of other species, and the ways in which our own species is a product of the planet it is born on. His next several books on the same subject, including the award-winning Animals (2015), deal with the problems of our own species and how we humans are part of the problem. He has authored or co-authored more than 200 peer-reviewed scientific articles and book chapters worldwide, and has written more than 20 book chapters and over 200 journal articles. He is an adjunct professor at Columbia University.Biennials’ relationship to the city has long been an open question. From Walter Benjamin, in his ‘The</text>
    <text>human behaviour and technology – the future is going to be like the past – but with machines that understand more and do better. The question is how to design these so that we, as humans, can transition to a future where the bad guys are stopped short of there being no humans left at all.Download this article as PDFAna Laura Lopéz de la TorreJuan Carlos LópezDe la Torre is Professor at Monterrey School of Architecture and Design at the University of Toronto, where he directs the Centre for Research Architecture, a research centre for research-based architecture. His books include the nonfiction collection The Monuments of San Cristóbal de las Casas (2003, New York Times Book Review Editor) and the journalism collection The Competing Critiques of Sheila McKay (1997, HarperCollins, New York). He has written for the New York Times, The New York Times Book Review, The Washington Post and The New Republic. He is the author of several New York Times bestsellers and has co-authored The Architecture Industry: Contradictions and the End of Capitalism (1998, New York Times).Juan Carlos LópezDe la Torre has written extensively on the biennale circuit, particularly for the Times, in the last fifteen years. His recent books include Popular Culture and Contemporary Art (2006, Simon &amp; Schuster; translated by Ted Kyte) and the forthcoming book Curating the Contemporary Art Biennale: Essays from the Two Parts of the Biennale Experience (2015, Stages, London). He has also edited the forthcoming New York Times Magazine and Stages, London.Juan Carlos LópezDe la Torre is the author of Popular Culture and Contemporary Art: Essays from the Two Parts of the Biennale Experience, (2006), and co-editor of the forthcoming book Contemporary Art: Essays from the Fourteenth Biennale of Sydney.Biennials: their histories and current formsTow</text>
    <text>human behaviour and technology – the future is going to be like the past – but with machines that understand more and do better. The question is how to design these so that we, as humans, can transition to a future where the bad guys are stopped short of there being no humans left at all.Download this article as PDFAna Laura Lopéz de la TorreJuan Carlos LópezDe la Torre is Professor at Monterrey School of Architecture and Design at the University of Toronto, where he directs the Centre for Research Architecture, a research centre for research-based architecture. His books include the nonfiction collection The Monuments of San Cristóbal de las Casas (2003, New York Times Book Review Editor) and the journalism collection The Competing Critiques of Sheila McKay (1997, HarperCollins, New York). He has written for the New York Times, The New York Times Book Review, The Washington Post and The New Republic. He is the author of several New York Times best-sellers including the 2013 New York Times best-seller The Plaza Project.Juan Carlos LópezDe la Torre: J.C. Rangel, Jr.This article was originally commissioned by and published in New York Magazine on September 22, 2016.Italo Calvino in his Invisible Cities recounts the story of how in 1967 an Italian architect named Paolo Portoghesi, while on a research trip to Mexico City, came across a city that he thought he might one day design. Unable to find any reference to the city in his travels, he ended up designing an entire itinerary around it. While on the road, he discovered that the city had been designed by a team of artists, one of whom was a woman named Beatrice Loreto. Using elements of both Italian and Mexican design, they created a bizarre, kinetic urban landscape based on &quot;ideas from below.&quot; While the city was in disarray, with factories, parks and streets seemingly taken up</text>
    <text>human behaviour and technology – the future is going to be like the past – but with machines that understand more and do better. The question is how to design these so that we, as humans, can transition to a future where the bad guys are stopped short of there being no humans left at all.Download this article as PDFAna Laura Lopéz de la TorreJuan Carlos LópezDe la Torre is Professor at Monterrey School of Architecture and Design at the University of Toronto, where he directs the Centre for Research Architecture, a research centre for research-based architecture. His books include the nonfiction collection The Monuments of San Cristóbal de las Casas (2003, New York Times Book Review Editor) and the journalism collection The Competing Critiques of Sheila McKay (1997, HarperCollins, New York). He has written for the New York Times, The New York Times Book Review, The Washington Post and The New Republic. He is the author of several New York Times bestsellers including the recently published The Municipal Solution: Restoring San Cristóbal de las Casas, 2005, and co-editor of the forthcoming issue of The Municipal Supplement, The Municipal Supplement Anthology, and several other publications.He is also the host of the weekly public television show Reforma Radio.Juan Carlos LópezDe la Torre is the recipient of several awards, including the 2008 Leonore Annenberg Prize for Social Research and the 2009 MacArthur Genius Grant. He served as a co-founder and chair of the Institute for Research Architecture, where he co-chaired the Institute's Board of Trustees and was a trustee of the National Endowment for the Arts. He is board member of the RAND Corporation, a non-profit organisation that supports critical research into sustainable urbanism, and was a trustee of the RAND Corporation, a for-profit research organisation that promotes libertarian urbanism. He has written for the New York Times, The New York Times Book Review,</text>
    <text>human behaviour and technology – the future is going to be like the past – but with machines that understand more and do better. The question is how to design these so that we, as humans, can transition to a future where the bad guys are stopped short of there being no humans left at all.Download this article as PDFAna Laura Lopéz de la TorreJuan Carlos LópezDe la Torre is Professor at Monterrey School of Architecture and Design at the University of Toronto, where he directs the Centre for Research Architecture, a research centre for research-based architecture. His books include the nonfiction collection The Monuments of San Cristóbal de las Casas (2003, New York Times Book Review Editor) and the journalism collection The Competing Critiques of Sheila McKay (1997, HarperCollins, New York). He has written for the New York Times, The New York Times Book Review, The Washington Post and The New Republic. He is the author of several New York Times best-sellers including the recently published The Architecture of the Future (2010), The Architecture of the Imagination (2012, Simon &amp; Schuster, New York) and several future-oriented nonfiction books. He edited the collection The Competing Critiques of Sheila McKay (1997, HarperCollins, New York), and edited the recently published book Systems: Futures, Fragments and Reflections (2014, Valand).Juan Carlos LópezDe la Torre is Professor and Chair of the Department of Anthropology and Geography at the University of Toronto, Canada. His research interests include the social, economic and political history of Canada, the United States and Mexico; comparative art and archival studies; and media, media and aesthetics. His publications include the collections The Collection of Sheila McKay (with Joasia Krysa), The Monuments of San Cristóbal de las Casas: Anthropology, Art, and the Politics of the New Economy (with Joasia Krysa) and Social Anthropology</text>
    <text>human behaviour and technology – the future is going to be like the past – but with machines that understand more and do better. The question is how to design these so that we, as humans, can transition to a future where the bad guys are stopped short of there being no humans left at all.Download this article as PDFAna Laura Lopéz de la TorreJuan Carlos LópezDe la Torre is Professor at Monterrey School of Architecture and Design at the University of Toronto, where he directs the Centre for Research Architecture, a research centre for research-based architecture. His books include the nonfiction collection The Monuments of San Cristóbal de las Casas (2003, New York Times Book Review Editor) and the journalism collection The Competing Critiques of Sheila McKay (1997, HarperCollins, New York). He has written for the New York Times, The New York Times Book Review, The Washington Post and The New Republic. He is the author of several New York Times bestsellers, including the recently published National Priorities: Contravision (2013, Simon &amp; Schuster) and the forthcoming National Priorities: Building (2015, with John Heymann, Simon &amp; Schuster).Juan Carlos Lopéz de la Torre is Professor and Chair of the Department of English and Cultural Studies at the University of Toronto, and holds the Canadian and U.S. Opulent Collection. He has taught and delivered lectures on Latin America, the Caribbean and the United States on behalf of the collections. He is currently overseeing the transition of collections management at the Monterrey School of Architecture.We are living through a moment similar to that experienced in the 1950s, when major museums across Canada and the United States were formed to house the curatorial agenda. The reason for the rapid growth of the art collections we have today is because most art was made possible by the industrialisation of the post-Second World War period, which brought unprecedented access to information and cultural</text>
    <text>human behaviour and technology – the future is going to be like the past – but with machines that understand more and do better. The question is how to design these so that we, as humans, can transition to a future where the bad guys are stopped short of there being no humans left at all.Download this article as PDFAna Laura Lopéz de la TorreJuan Carlos LópezDe la Torre is Professor at Monterrey School of Architecture and Design at the University of Toronto, where he directs the Centre for Research Architecture, a research centre for research-based architecture. His books include the nonfiction collection The Monuments of San Cristóbal de las Casas (2003, New York Times Book Review Editor) and the journalism collection The Competing Critiques of Sheila McKay (1997, HarperCollins, New York). He has written for the New York Times, The New York Times Book Review, The Washington Post and The New Republic. He is the author of several New York Times best-sellers, including El Diablito: The Monuments of San Cristóbal de las Casas (2006), a New York Times best-seller, and El Diablito: Unravelled (2015), a New York Times best-seller. He has also edited the online anthology Random House SF (2007) and was a co-editor of the online archive Radix Journal (2006-2015). He is the recipient of multiple awards and has lectured on a wide range of topics.He was a founding member of the Institute for Anarchist Studies (1980) and the Radix Journal (1986), and a fellow of the American Scholar (1987). Before joining the Times, he edited the online archive Assemble’s Issues (1990), as well as the online archive M.F.A. (2001). He has also edited the online anthology Black Flag (1999), and the online archive Radical History (2000). He is the author of Beyond the Appar</text>
  </text>
  <text>
    developments in artificial intelligence and machine learning, and the constant quest for better ways to use existing scientific and technical knowledge). The second is the growing biennial industry, in which artists, curators, journalists and other professionals entrust the art of the future to be created and enjoyed by the public, whether that art is created through machine learning or intelligent curating, or through the combined power of large archives and large collections. The biennial becomes a platform on which art can be understood and functioned upon, in which art can be understood and functioned upon simultaneously. Thirdly, and perhaps most significantly, biennials can be counted on to provide a permanent home for cultural activity in the future city, or in other words, for cultural institutions to be able to sustain cultural activity in the future. This last condition may seem a contradiction in terms, but in the current economic and political climate it is essential for the future of biennials to be secure, since absent such stability, the biennial industry will inevitably collapse
    <text>developments in artificial intelligence and machine learning, and the constant quest for better ways to use existing scientific and technical knowledge). The second is the growing biennial industry, in which artists, curators, journalists and other professionals entrust the art of the future to be created and enjoyed by the public, whether that art is created through machine learning or intelligent curating, or through the combined power of large archives and large collections. The biennial becomes a platform on which art can be understood and functioned upon, in which art can be understood and functioned upon simultaneously. Thirdly, and perhaps most significantly, biennials can be counted on to provide a permanent home for cultural activity in the future city, or in other words, for cultural institutions to be able to sustain cultural activity in the future. This last condition may seem a contradiction in terms, but in the current economic and political climate it is essential for the future of biennials to be secure, since absent such stability, the biennial industry will inevitably collapse. So biennials play a crucial role in anticipating the future, but also in creating it. This second type of biennial is ‘permanent’ (that is, it will persevere, whatever the cost and no matter what other types of biennials may become), and it is the kind that will persevere whether or not there is a biennial industry around. It is the kind of biennial that can persevere in the face of everything else, and will persevere in the face of biennials any time there is a biennial industry around. There are several kinds of ‘permanent’ biennials. Part of the reason why museums today are so vulnerable to the vagaries of changing curatorial practices is that there is a palpable fear in the curatorial body about how art’s future may be influenced by what it represents to be an urban core. In other words, museums today are largely seen as assets, not being taken back when</text>
    <text>developments in artificial intelligence and machine learning, and the constant quest for better ways to use existing scientific and technical knowledge). The second is the growing biennial industry, in which artists, curators, journalists and other professionals entrust the art of the future to be created and enjoyed by the public, whether that art is created through machine learning or intelligent curating, or through the combined power of large archives and large collections. The biennial becomes a platform on which art can be understood and functioned upon, in which art can be understood and functioned upon simultaneously. Thirdly, and perhaps most significantly, biennials can be counted on to provide a permanent home for cultural activity in the future city, or in other words, for cultural institutions to be able to sustain cultural activity in the future. This last condition may seem a contradiction in terms, but in the current economic and political climate it is essential for the future of biennials to be secure, since absent such stability, the biennial industry will inevitably collapse. As biennial attendance falls and new artworks are chosen, it is increasingly important for biennials to be able to respond with something akin to a 'big-picture' exhibition for the city. This is perhaps what the Biennale was designed to do: maintain a larger-than-life image for the city, while simultaneously providing a stable space for biennials to build their collections and function within their particular fields of influence. In other words, the Biennale not only exhibits the art within, but also of, the city.  	Art and curatorial curatorial curatorial curatorial curatorial curatorial biennale biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial biennial bi</text>
    <text>developments in artificial intelligence and machine learning, and the constant quest for better ways to use existing scientific and technical knowledge). The second is the growing biennial industry, in which artists, curators, journalists and other professionals entrust the art of the future to be created and enjoyed by the public, whether that art is created through machine learning or intelligent curating, or through the combined power of large archives and large collections. The biennial becomes a platform on which art can be understood and functioned upon, in which art can be understood and functioned upon simultaneously. Thirdly, and perhaps most significantly, biennials can be counted on to provide a permanent home for cultural activity in the future city, or in other words, for cultural institutions to be able to sustain cultural activity in the future. This last condition may seem a contradiction in terms, but in the current economic and political climate it is essential for the future of biennials to be secure, since absent such stability, the biennial industry will inevitably collapse. The biennial as an art form is expected to play a central role in sustaining cultural life in the years to come, and its continued development will be a principal factor in shaping the experience and content of cultural consumption in the years to come. This is not to suggest that biennials should have no independent or autonomous experience of the city in the future, nor that they should refrain from engaging with issues of politics, economics, and society in the future. On the contrary, biennials are encouraged to speculate on and speculate upon topics of concern and to act as though they were independent technological platforms capable of foreseeing and influencing the course of technological change. But they are not expected to anticipate, maturation, or propose radical new ways of organising knowledge and cultural consumption in the future. That is, techno-utopia is not the future we wish to live in. And it is not a future with unlimited access to knowledge and cultural resources.10. Biennials as sites of cultural exchange</text>
    <text>developments in artificial intelligence and machine learning, and the constant quest for better ways to use existing scientific and technical knowledge). The second is the growing biennial industry, in which artists, curators, journalists and other professionals entrust the art of the future to be created and enjoyed by the public, whether that art is created through machine learning or intelligent curating, or through the combined power of large archives and large collections. The biennial becomes a platform on which art can be understood and functioned upon, in which art can be understood and functioned upon simultaneously. Thirdly, and perhaps most significantly, biennials can be counted on to provide a permanent home for cultural activity in the future city, or in other words, for cultural institutions to be able to sustain cultural activity in the future. This last condition may seem a contradiction in terms, but in the current economic and political climate it is essential for the future of biennials to be secure, since absent such stability, the biennial industry will inevitably collapse. The biennial industry is currently in a period of rapid growth in both quantity and quality, and it is predicted that the biennial format will outlive even the most powerful and resourceful curators, artists and collectors in a very long time. If biennials are to fulfill their current and future claims on the city, they will have to become more than just collections; they will have to become actively engaged in creating cultural change in the city through what can be broadly described as a public-private partnership, since the biennial is the dominant form of cultural organisation in the city at present. If this is not possible, the claims of the biennial will remain rooted in museums and their collections, and biennials will become confined to their present form of showcasing a particular form of cultural activity. This is the current situation in New York and other metropolises undergoing radical transformations due to a combination of economic development and rapid population growth. The biennial format is expected to become increasingly important</text>
    <text>developments in artificial intelligence and machine learning, and the constant quest for better ways to use existing scientific and technical knowledge). The second is the growing biennial industry, in which artists, curators, journalists and other professionals entrust the art of the future to be created and enjoyed by the public, whether that art is created through machine learning or intelligent curating, or through the combined power of large archives and large collections. The biennial becomes a platform on which art can be understood and functioned upon, in which art can be understood and functioned upon simultaneously. Thirdly, and perhaps most significantly, biennials can be counted on to provide a permanent home for cultural activity in the future city, or in other words, for cultural institutions to be able to sustain cultural activity in the future. This last condition may seem a contradiction in terms, but in the current economic and political climate it is essential for the future of biennials to be secure, since absent such stability, the biennial industry will inevitably collapse.The biennial has always been a source of confusion and conflict for the city, sometimes compelling curators to act rashly and outrageously, and other curators to retreat into obscurity or abandon their rightful place in the biennial index. Nevertheless, its growth coincided with a certain amount of instability, and the resulting fluctuations in value that can be attributed to a number of curators failing in their specific missions. The biennial boom coincided with an unpleasant change in the city: the destruction of much of the historic district in the process; the displacement of large numbers of residents; a mechanisation of urban life that reduced the city to rubble; and a definite end to a long-term interest in the city. The end of the boom coincided with the beginning of the experience of a city in transition. The displaced residents found new housing options in the rapidly developing but ever-expanding downtown district; new businesses and cafés opened on the corner of Bloor and Bathurst; and the art fair landscape</text>
    <text>developments in artificial intelligence and machine learning, and the constant quest for better ways to use existing scientific and technical knowledge). The second is the growing biennial industry, in which artists, curators, journalists and other professionals entrust the art of the future to be created and enjoyed by the public, whether that art is created through machine learning or intelligent curating, or through the combined power of large archives and large collections. The biennial becomes a platform on which art can be understood and functioned upon, in which art can be understood and functioned upon simultaneously. Thirdly, and perhaps most significantly, biennials can be counted on to provide a permanent home for cultural activity in the future city, or in other words, for cultural institutions to be able to sustain cultural activity in the future. This last condition may seem a contradiction in terms, but in the current economic and political climate it is essential for the future of biennials to be secure, since absent such stability, the biennial industry will inevitably collapse. Biennials are here to stay, and to a certain extent are expected to be here to stay; but to what extent, and under what kind of political and urban system will they return? The following is a list of some of the artists, curators, writers and other professionals who have been or are currently involved in monitoring, documenting or researching urban environments or phenomena in or around the biennale.The following individuals contributed to the conception, commissioning, design, production, or other aspects of the project:Svetlana Boym: Curator of Art Spaces at the Russian Federal Space Agency, co-founder of the Velvet Library, Art.Sofiya Primakov: Artistic Director for the Russian Federal Space Agency, co-founder of the Biennaleeer, Ksenia Sobchakova: Artistic director of the Russian Federal Space Agency and the Biennaleeer, Ksenia Sobchakova: Artistic director of Transmute,</text>
    <text>developments in artificial intelligence and machine learning, and the constant quest for better ways to use existing scientific and technical knowledge). The second is the growing biennial industry, in which artists, curators, journalists and other professionals entrust the art of the future to be created and enjoyed by the public, whether that art is created through machine learning or intelligent curating, or through the combined power of large archives and large collections. The biennial becomes a platform on which art can be understood and functioned upon, in which art can be understood and functioned upon simultaneously. Thirdly, and perhaps most significantly, biennials can be counted on to provide a permanent home for cultural activity in the future city, or in other words, for cultural institutions to be able to sustain cultural activity in the future. This last condition may seem a contradiction in terms, but in the current economic and political climate it is essential for the future of biennials to be secure, since absent such stability, the biennial industry will inevitably collapse. Fourthly, and perhaps most significantly, biennials can be counted on to offer a future for art’s place in the city. This last condition may seem like a no-brainer, but it is often overlooked: unlike museums and large estates, biennials are a fact of the contemporary urban environment; every generation brings with it a different set of values and habits, and thus the logic of the biennial is often opposed to the value of the estate it occupies. Fivethly, and perhaps most significantly, biennials can count on being able to sustain cultural activity in the future. This last condition may seem counter intuitive, but it is becoming increasingly apparent that the biennial is barely a transient stage in the development of the city and urban-industrial complex, with a significant presence in many parts of the world; its current business model will almost certainly be challenged in the years to come, and may well be replaced by something more sustainable and human-centered.  And</text>
    <text>developments in artificial intelligence and machine learning, and the constant quest for better ways to use existing scientific and technical knowledge). The second is the growing biennial industry, in which artists, curators, journalists and other professionals entrust the art of the future to be created and enjoyed by the public, whether that art is created through machine learning or intelligent curating, or through the combined power of large archives and large collections. The biennial becomes a platform on which art can be understood and functioned upon, in which art can be understood and functioned upon simultaneously. Thirdly, and perhaps most significantly, biennials can be counted on to provide a permanent home for cultural activity in the future city, or in other words, for cultural institutions to be able to sustain cultural activity in the future. This last condition may seem a contradiction in terms, but in the current economic and political climate it is essential for the future of biennials to be secure, since absent such stability, the biennial industry will inevitably collapse. Therefore, it is essential for biennials to be able to achieve a certain critical mass, sustain a certain degree of cultural tourism, and exist in perpetuity as a cultural entity.I have come to understand the value of a biennial in its temporal characteristics. For the biennial to be an authentic cultural asset, it must have a future in the sense that it will be counted in cultural history books and widely shared in cultural networks in the future. In addition, it must be relevant in the contemporary sense – relevant in the sense that it is shared among the contemporary community, and widely disseminated in a given field of art. I propose the creation of a new kind of biennial, one that maintains its contemporary character but incorporates elements of previous biennials into a new kind of architecture. I call this architecture of the absurd.A radical new kind of biennial should employ contemporary urban planning and architecture as an instrument for the systematic production of absurd cultural artefacts – cultural islands – out</text>
    <text>developments in artificial intelligence and machine learning, and the constant quest for better ways to use existing scientific and technical knowledge). The second is the growing biennial industry, in which artists, curators, journalists and other professionals entrust the art of the future to be created and enjoyed by the public, whether that art is created through machine learning or intelligent curating, or through the combined power of large archives and large collections. The biennial becomes a platform on which art can be understood and functioned upon, in which art can be understood and functioned upon simultaneously. Thirdly, and perhaps most significantly, biennials can be counted on to provide a permanent home for cultural activity in the future city, or in other words, for cultural institutions to be able to sustain cultural activity in the future. This last condition may seem a contradiction in terms, but in the current economic and political climate it is essential for the future of biennials to be secure, since absent such stability, the biennial industry will inevitably collapse. Bienniality is the art of the in-the-moment, or the stuff of futures; it is dynamic, unpredictable and transient. It is art in its temporal character and medium, and art in its ability to inhabit and govern that temporal nature. In short, it is art through an infinite loop.Art as art-water, or what John Berger would say, ‘is the earliest trace of an art that eventually became something else’. Art as culture-water, or what Jared Diamond would say, ‘is the first step in the direction of a more fully realized cultural form’. If art is to continue to function as a function of its own inherent properties – a property that we have no control over – then cultural institutions would have to evolve creative means to manage and to predict the art from within. Such a mode of cultural evolution is not attainable through the current institutional mechanisms, which are themselves brittle and subject to the whims of unpredictable external factors.</text>
    <text>developments in artificial intelligence and machine learning, and the constant quest for better ways to use existing scientific and technical knowledge). The second is the growing biennial industry, in which artists, curators, journalists and other professionals entrust the art of the future to be created and enjoyed by the public, whether that art is created through machine learning or intelligent curating, or through the combined power of large archives and large collections. The biennial becomes a platform on which art can be understood and functioned upon, in which art can be understood and functioned upon simultaneously. Thirdly, and perhaps most significantly, biennials can be counted on to provide a permanent home for cultural activity in the future city, or in other words, for cultural institutions to be able to sustain cultural activity in the future. This last condition may seem a contradiction in terms, but in the current economic and political climate it is essential for the future of biennials to be secure, since absent such stability, the biennial industry will inevitably collapse. In the twentieth century, the biennial was the preeminent cultural form in the world; in the twenty-first century, it is the dominant form. And although the biennial industry has changed a great deal over the years, its fundamental characteristics remain the same: big art collections, globalised exhibitions, and a common form of cultural tourism. So it is still a dominant form in some forms of the world, and a dominant form in others. What has changed is that now many biennials are regional in character, with particular focus on regions around the globe. The Metropolitan Museum of Art, for example, has a particular neighbourhood portfolio, but it is becoming increasingly difficult to find a biennial in other parts of the world. The reason for this is that curators from other cities are now focusing on the city as a whole. They are no longer looking for art specifically in the city, but rather looking for art that will be able to be learned and shared. And so bi</text>
  </text>
  <text>
    developments in artificial intelligence and robotics] are setting up ever larger and more pervasive registers of surveillance and control. . . . The goal of the future city is to be able to respond to these new possibilities of control.&quot; [Emphasis added.] And here we see another echo of Machu Picchu – a city on the edge of a giant computer that responds to data streams emanating from a global computer network.  And this is just the beginning.  In the twentieth century, the computer became what we thought it was – a huge central hub for data – a gathering place for large-scale data, and a possible future for some who will remain slaves to the machine.Today, the computer is an enormous data-storage and processing unit that constantly demands energy and produces greenhouse-gas emissions, among other impacts. It is also a target of attacks from malicious actors seeking to render it useless or at least unprofitable.  And so we are told the future city must respond by becoming self-sufficient in information and
    <text>developments in artificial intelligence and robotics] are setting up ever larger and more pervasive registers of surveillance and control. . . . The goal of the future city is to be able to respond to these new possibilities of control.&quot; [Emphasis added.] And here we see another echo of Machu Picchu – a city on the edge of a giant computer that responds to data streams emanating from a global computer network.  And this is just the beginning.  In the twentieth century, the computer became what we thought it was – a huge central hub for data – a gathering place for large-scale data, and a possible future for some who will remain slaves to the machine.Today, the computer is an enormous data-storage and processing unit that constantly demands energy and produces greenhouse-gas emissions, among other impacts. It is also a target of attacks from malicious actors seeking to render it useless or at least unprofitable.  And so we are told the future city must respond by becoming self-sufficient in information and energy – producing its own chips, producing its own clothes, and so forth.But this begs the question: how does the future city create and harness its own computing resources? And what kinds of technologies will these clothes chip in the service of such a response? There are several possible answers to this question, based on where we are in the world economy and on the particular technological trajectory of AI.  One answer is to look to robotics for the solution.  But this only addresses the technological edge – the part that enables machines to think and reason – while leaving the question of responsibility for the consequences of such technologies on a local, regional, and international level.  And while AI may well make it easier for machines to reason, it does not alter the fundamental nature of the relationship between humans and machines – which is fundamentally one of habit.Human beings are fundamentally cognitive creatures: creatures who acquire knowledge through repeated encounters and interactions – something like machines – and who consequently value and intend their actions to follow predictable</text>
    <text>developments in artificial intelligence and robotics] are setting up ever larger and more pervasive registers of surveillance and control. . . . The goal of the future city is to be able to respond to these new possibilities of control.&quot; [Emphasis added.] And here we see another echo of Machu Picchu – a city on the edge of a giant computer that responds to data streams emanating from a global computer network.  And this is just the beginning.  In the twentieth century, the computer became what we thought it was – a huge central hub for data – a gathering place for large-scale data, and a possible future for some who will remain slaves to the machine.Today, the computer is an enormous data-storage and processing unit that constantly demands energy and produces greenhouse-gas emissions, among other impacts. It is also a target of attacks from malicious actors seeking to render it useless or at least unprofitable.  And so we are told the future city must respond by becoming self-sufficient in information and energy. But if the future city is anything like the present city, it is highly unlikely that the city will be able to respond to the challenges thrown up by new technologies.  And if these technologies ultimately destroy the city, not all of which are desirable or even possible in the first place, then the city will return to the wild and chaotic state it was in before.What can the future city do? I believe that there are three main ways to think about how technology will affect our everyday lives: (1) create its own new modes of production and use; (2) adapt existing social forms and infrastructure to accommodate new modes of social interaction; and (3) try to imagine new ways of organising society according to the social, ecological and geostrategic principles of the social sciences.Technologies of surveillance and control are everywhere. At home, they are embedded in our transport networks and embedded in our built infrastructure. But they are increasingly applied to pedestrian and vehicle movements, too. And</text>
    <text>developments in artificial intelligence and robotics] are setting up ever larger and more pervasive registers of surveillance and control. . . . The goal of the future city is to be able to respond to these new possibilities of control.&quot; [Emphasis added.] And here we see another echo of Machu Picchu – a city on the edge of a giant computer that responds to data streams emanating from a global computer network.  And this is just the beginning.  In the twentieth century, the computer became what we thought it was – a huge central hub for data – a gathering place for large-scale data, and a possible future for some who will remain slaves to the machine.Today, the computer is an enormous data-storage and processing unit that constantly demands energy and produces greenhouse-gas emissions, among other impacts. It is also a target of attacks from malicious actors seeking to render it useless or at least unprofitable.  And so we are told the future city must respond by becoming self-sufficient in information and able to deal with unforeseen contingencies – or else.This is a Catch-22 situation to be sure. On the one hand, the global economy demands a huge amount of information – images, videos, data – that can be gathered and transmitted at will. On the other, the pace of technological innovation, coupled with the inefficiency of human surveillance, makes it increasingly difficult to maintain a large amount of information on a large scale. Therefore, if the future city is to possess the technological capacity to deal with unforeseen contingencies, it must become information-independent – capable of operating at its maximum capacity.This is the condition of ‘deep learning’ – the ability to learn from data, images, and sound – a condition well beyond the capability of the human intellect. If the future is to possess the capacity to read and write images and data, it must be able to recognise simple symbols and process vast amounts of information in a way that is computable on a massive scale. And here</text>
    <text>developments in artificial intelligence and robotics] are setting up ever larger and more pervasive registers of surveillance and control. . . . The goal of the future city is to be able to respond to these new possibilities of control.&quot; [Emphasis added.] And here we see another echo of Machu Picchu – a city on the edge of a giant computer that responds to data streams emanating from a global computer network.  And this is just the beginning.  In the twentieth century, the computer became what we thought it was – a huge central hub for data – a gathering place for large-scale data, and a possible future for some who will remain slaves to the machine.Today, the computer is an enormous data-storage and processing unit that constantly demands energy and produces greenhouse-gas emissions, among other impacts. It is also a target of attacks from malicious actors seeking to render it useless or at least unprofitable.  And so we are told the future city must respond by becoming self-sufficient in information and energy, or else face a future without computers or data networks.I do not know what to think or how to feel when I look at the future city. I can only imagine what a nightmare future it would be to live in.  And so we are told that we must either ‘work smarter, or else’[1] become ‘better off’[2] through deliberate consumption of resources such as agriculture and urban infrastructure, which are to some extent disconnected from the land and tend to be automated.And so the battle between the machine and the data continues. And it is this ‘battle between two worlds’ that is often misconstrued as a battle between science and religion.  And this misunderstanding of the relationship between science and religion is what I mean when I refer to the battle between Big Data and Wild West.I do not know what Big Data is or is not, but I do know that it is an immense data-storage and processing unit that</text>
    <text>developments in artificial intelligence and robotics] are setting up ever larger and more pervasive registers of surveillance and control. . . . The goal of the future city is to be able to respond to these new possibilities of control.&quot; [Emphasis added.] And here we see another echo of Machu Picchu – a city on the edge of a giant computer that responds to data streams emanating from a global computer network.  And this is just the beginning.  In the twentieth century, the computer became what we thought it was – a huge central hub for data – a gathering place for large-scale data, and a possible future for some who will remain slaves to the machine.Today, the computer is an enormous data-storage and processing unit that constantly demands energy and produces greenhouse-gas emissions, among other impacts. It is also a target of attacks from malicious actors seeking to render it useless or at least unprofitable.  And so we are told the future city must respond by becoming self-sufficient in information and energy – becoming what Elon Musk calls ‘a fully compute-enabled city’ – a status symbol and a moneymaker for itself.The challenge now, we are told, is to assemble ‘a shared set of behaviours, built on shared assumptions about what it means to be a city’.  And what does that mean in practice? Does the future city seek to construct a ‘public realm’ around itself? Or does the city seek to engage in ‘common good’? Or does the city seek to engage in ‘common sense’?Or does the city seek to engage in ‘common sense’?Or does the city seek to engage in ‘common sense’?Or does the city seek to engage in ‘common sense’?Or does the city seek to engage in ‘common sense’?Or does the city seek to engage in ‘common sense’?Or does the city seek to engage</text>
    <text>developments in artificial intelligence and robotics] are setting up ever larger and more pervasive registers of surveillance and control. . . . The goal of the future city is to be able to respond to these new possibilities of control.&quot; [Emphasis added.] And here we see another echo of Machu Picchu – a city on the edge of a giant computer that responds to data streams emanating from a global computer network.  And this is just the beginning.  In the twentieth century, the computer became what we thought it was – a huge central hub for data – a gathering place for large-scale data, and a possible future for some who will remain slaves to the machine.Today, the computer is an enormous data-storage and processing unit that constantly demands energy and produces greenhouse-gas emissions, among other impacts. It is also a target of attacks from malicious actors seeking to render it useless or at least unprofitable.  And so we are told the future city must respond by becoming self-sufficient in information and energy. But this only serves to empower the privileged few at the top as they do more and more of the world's economic and social organizing work. The vast majority of humanity still struggles to get by on limited means.In the twentieth century, the computer became what we called the ‘core’ of what we called the ‘metainterface’ – the interface between the user and the world.  And so we must respond to this new world system that is rapidly becoming apparent only at an accelerated pace.  We must decide which aspects of the interface – what is called ‘the Core’ – are worthy of our collective engagement and which aspects are not.  As computing becomes ever more powerful and the human interaction ever more mediated, the role of the artist becomes ever more circumscribed.  In other words, in the new ‘real-time’ that characterizes the interface, the role of the artist becomes to render the interface as real-time –</text>
    <text>developments in artificial intelligence and robotics] are setting up ever larger and more pervasive registers of surveillance and control. . . . The goal of the future city is to be able to respond to these new possibilities of control.&quot; [Emphasis added.] And here we see another echo of Machu Picchu – a city on the edge of a giant computer that responds to data streams emanating from a global computer network.  And this is just the beginning.  In the twentieth century, the computer became what we thought it was – a huge central hub for data – a gathering place for large-scale data, and a possible future for some who will remain slaves to the machine.Today, the computer is an enormous data-storage and processing unit that constantly demands energy and produces greenhouse-gas emissions, among other impacts. It is also a target of attacks from malicious actors seeking to render it useless or at least unprofitable.  And so we are told the future city must respond by becoming self-sufficient in information and energy. But this only serves to make the city even bigger and more resource-intensive – both to run the city and to be useful in its own right.The choice of what to leave behind is left to cultural impulses, political and economic. But the increasing abundance of oil and natural gas means that the city is no longer primarily a gathering place for people; it is now a site for constructing, managing and allocating vast amounts of water and energy. The question becomes whether the city is made up of places where people can be made useful, or whether the city is a site for destroying things that serve no one but the owners of the land and buildings on which the city sits. This is the dilemma of ecological destruction – how to balance the need for growth with the need for environmental protection. Inevitably, this has led to clashes between the owners of the land and the people who live on it – disputes over who gets to use what, and over what standards should be applied.This tension</text>
    <text>developments in artificial intelligence and robotics] are setting up ever larger and more pervasive registers of surveillance and control. . . . The goal of the future city is to be able to respond to these new possibilities of control.&quot; [Emphasis added.] And here we see another echo of Machu Picchu – a city on the edge of a giant computer that responds to data streams emanating from a global computer network.  And this is just the beginning.  In the twentieth century, the computer became what we thought it was – a huge central hub for data – a gathering place for large-scale data, and a possible future for some who will remain slaves to the machine.Today, the computer is an enormous data-storage and processing unit that constantly demands energy and produces greenhouse-gas emissions, among other impacts. It is also a target of attacks from malicious actors seeking to render it useless or at least unprofitable.  And so we are told the future city must respond by becoming self-sufficient in information and energy – a scenario for some who will no longer be slaves to the machine.This begs the question: what does it mean to become ‘self-sufficient’? I would argue that there is no immediate answer to this question, but we can learn to think differently and creatively about the mechanisms of our future city.We can begin to imagine a ‘Second Machine’ – a machine smarter, more resourceful and more resourceful than the first machine – that can think alongside us in tandem creating new modes of interaction between humans and machines. Such a scenario would render existing forms of labour obsolete, creating a new class of users capable of taking on new technological roles. This sounds so far-fetched and utopian, but it is becoming increasingly realisable.  A good example is the augmented-reality startup Magic Leap, which seeks to create fully immersive environments by harnessing augmented-reality technology.  Already, Magic Leap is creating augmented-reality replicas of its campus in California through</text>
    <text>developments in artificial intelligence and robotics] are setting up ever larger and more pervasive registers of surveillance and control. . . . The goal of the future city is to be able to respond to these new possibilities of control.&quot; [Emphasis added.] And here we see another echo of Machu Picchu – a city on the edge of a giant computer that responds to data streams emanating from a global computer network.  And this is just the beginning.  In the twentieth century, the computer became what we thought it was – a huge central hub for data – a gathering place for large-scale data, and a possible future for some who will remain slaves to the machine.Today, the computer is an enormous data-storage and processing unit that constantly demands energy and produces greenhouse-gas emissions, among other impacts. It is also a target of attacks from malicious actors seeking to render it useless or at least unprofitable.  And so we are told the future city must respond by becoming self-sufficient in information and energy – a strategy that only the technologically sophisticated can pursue.But this is not at all what is happening. As Thomas Watson writes: &quot;The city is becoming ever more complex, its physical characteristics continuously changing – its inhabitants ‘multitudes’ of autonomous agents who act on their own initiative, speak their own language, and do much more.’[3] And so the battle lines are drawn, and it is with this shift in subject that we move from the city to the distributed commons, from the city to the cloud and beyond. As Watson narrates, the significance of this transformation can no longer be underestimated:  &quot;In a world as ever expanding and ever encompassing, there exists a real possibility that a few individuals or even a small group of individuals could possess such direct and effective control over their environment that it is almost inconceivable to others.’[4]And so we move from the city into the distributed commons, from the city to the distributed</text>
    <text>developments in artificial intelligence and robotics] are setting up ever larger and more pervasive registers of surveillance and control. . . . The goal of the future city is to be able to respond to these new possibilities of control.&quot; [Emphasis added.] And here we see another echo of Machu Picchu – a city on the edge of a giant computer that responds to data streams emanating from a global computer network.  And this is just the beginning.  In the twentieth century, the computer became what we thought it was – a huge central hub for data – a gathering place for large-scale data, and a possible future for some who will remain slaves to the machine.Today, the computer is an enormous data-storage and processing unit that constantly demands energy and produces greenhouse-gas emissions, among other impacts. It is also a target of attacks from malicious actors seeking to render it useless or at least unprofitable.  And so we are told the future city must respond by becoming self-sufficient in information and energy – a condition unattractive to the machine.We are told that the city is to a large extent a collection of fragmented information, which is made more difficult to digest by the fact that information is so diffuse and globalised. We are told that it is imperative that we become connected to the information infrastructure that generates and supports our world.  And this is just what data mining is all about.The connectionist narrative surrounding information gathering and the machine – the notion that machines are emulated to replicate certain kinds of human behaviour – is a deeply flawed notion.  And yet here we are, several decades after Carrasco advanced his ideas, and we are still struggling to come to terms with the pernicious effects of machine learning on our world.  An echo of Machu Picchu can be heard in the comments section of Carrasco’s book, and it is said that at the time, many in the academy found his work too on-the-nose. </text>
  </text>
  <text>
    developments in artificial intelligence, machine learning and data mining. The problem of translation becomes critical, as does the challenge of mapping movement across vast geographical areas, from the Arctic to the Andes. 	In their new book The Language Machine, Franny Broome &amp; co-workers exploit the language infrastructure of the world, describing in hackneyed, sometimes poetic terms the probable half-dozen languages that are the successor to the Maya empire and the origin of modern-day America. The subtitle of the chapter ‘Mayan’ is somewhat misleading, in that it is almost entirely accurate in terms of describing the empire's expansion, as well as the expansion of human language. The empire’s transition from Maya to Aztec writing ends around 1136BC with the writing down of two massive religious lists – one of which includes a decree that all future kings must adopt the name Chichimecasireh (Latin for ‘May the Two Gods’) in addition to their original
    <text>developments in artificial intelligence, machine learning and data mining. The problem of translation becomes critical, as does the challenge of mapping movement across vast geographical areas, from the Arctic to the Andes. 	In their new book The Language Machine, Franny Broome &amp; co-workers exploit the language infrastructure of the world, describing in hackneyed, sometimes poetic terms the probable half-dozen languages that are the successor to the Maya empire and the origin of modern-day America. The subtitle of the chapter ‘Mayan’ is somewhat misleading, in that it is almost entirely accurate in terms of describing the empire's expansion, as well as the expansion of human language. The empire’s transition from Maya to Aztec writing ends around 1136BC with the writing down of two massive religious lists – one of which includes a decree that all future kings must adopt the name Chichimecasireh (Latin for ‘May the Two Gods’) in addition to their original titles. The other list includes other gods and creatures to be sacrificed in a later ‘purification ritual’. It is a fairly straightforward translation of the name to fit on a plate with symbols – a point at which the empire seems to have been strategically located. The second list, which is written in a form that closely resembles the first, is a call to arms for the first time in the history of the world. The imperial government clearly sees the second as a threat, and in a number of places, the end result is a literal re-nameing of the country – in this case, changing its name to Mexico. 	In an interesting turn of events, Chichimecasireh – who was probably not even aware of his own identity – is named as the protagonist of the Nuyumbalees inscription, written in the second person. The name is changed again in the third person, when the character Miguel Posadas is invoked. The third person</text>
    <text>developments in artificial intelligence, machine learning and data mining. The problem of translation becomes critical, as does the challenge of mapping movement across vast geographical areas, from the Arctic to the Andes. 	In their new book The Language Machine, Franny Broome &amp; co-workers exploit the language infrastructure of the world, describing in hackneyed, sometimes poetic terms the probable half-dozen languages that are the successor to the Maya empire and the origin of modern-day America. The subtitle of the chapter ‘Mayan’ is somewhat misleading, in that it is almost entirely accurate in terms of describing the empire's expansion, as well as the expansion of human language. The empire’s transition from Maya to Aztec writing ends around 1136BC with the writing down of two massive religious lists – one of which includes a decree that all future kings must adopt the name Chichimecasireh (Latin for ‘May the Two Gods’) in addition to their original titles. The other list includes ‘other gods’ who must be exterminated. The lists are part of a ‘Declaration of War’ that is part of a larger larger political movement – the idea being that all political dissent, whether by language or people, is part of a god’s game. The idea that language is somehow a game is deeply embedded in the social sciences – especially when applied to civilised governments – but Franny Broome’s account of the empire’s writing system falls woefully short of envisioning the power structures that actually exist. A more accurate reflection of the language that is being created is needed, but the sheer scale of the industry that is being created in part through the technologies of the language is mind-boggling. 	Broome’s fictionalisation of the Mayan empire falls far short of describing the linguistic infrastructure of the modern world, which is also being ev</text>
    <text>developments in artificial intelligence, machine learning and data mining. The problem of translation becomes critical, as does the challenge of mapping movement across vast geographical areas, from the Arctic to the Andes. 	In their new book The Language Machine, Franny Broome &amp; co-workers exploit the language infrastructure of the world, describing in hackneyed, sometimes poetic terms the probable half-dozen languages that are the successor to the Maya empire and the origin of modern-day America. The subtitle of the chapter ‘Mayan’ is somewhat misleading, in that it is almost entirely accurate in terms of describing the empire's expansion, as well as the expansion of human language. The empire’s transition from Maya to Aztec writing ends around 1136BC with the writing down of two massive religious lists – one of which includes a decree that all future kings must adopt the name Chichimecasireh (Latin for ‘May the Two Gods’) in addition to their original titles. The other, more general, decree is that ‘all who adopt this new name are committed to the service of the new religion known as Christianity’. The names change over time as changes in Maya political power and social structure forced other names to catch up with those already in use. The empire continues to exist in myth as a series of names – Maya, Aztec, Chichimeca or something very similar – but the key word is name. As the chapters on immigration, migration and identity unravel, it becomes evident that this was always just a figment of the imagination of the writers – a reflection of the empire’s own malleability, constantly shifting fabric of identities, cultures and languages. 	The machine that creates these lists is no longer driven by the Empire, but rather relies on data feeds from across the empire, and is guided by an empire-sized database that captures and distributes information about the societies in existence at the time. The empire,</text>
    <text>developments in artificial intelligence, machine learning and data mining. The problem of translation becomes critical, as does the challenge of mapping movement across vast geographical areas, from the Arctic to the Andes. 	In their new book The Language Machine, Franny Broome &amp; co-workers exploit the language infrastructure of the world, describing in hackneyed, sometimes poetic terms the probable half-dozen languages that are the successor to the Maya empire and the origin of modern-day America. The subtitle of the chapter ‘Mayan’ is somewhat misleading, in that it is almost entirely accurate in terms of describing the empire's expansion, as well as the expansion of human language. The empire’s transition from Maya to Aztec writing ends around 1136BC with the writing down of two massive religious lists – one of which includes a decree that all future kings must adopt the name Chichimecasireh (Latin for ‘May the Two Gods’) in addition to their original title Mayan. The other list includes ‘every human language’. The order of naming comes from the fact that humans share a common root word with the Aztec word for ‘language’ – a verb meaning to speak, think, or gather ideas. In the case of Maya and Aztec writing, the two words are added after an adjective meaning ‘strong or durable language’ has been added. For Mayan, this would mean something like ‘strong or deeply rooted roots in the earth’. 	The challenge facing anthropologists researching in the Americas today is different from that faced in earlier stages of the language revolution. Primarily, there is the question of how to engage with the new medium of communication when discussing the Americas with people who have never lived there, and how to make the transition from the formerly dominant status of minor to the more dominant status of major world systems</text>
    <text>developments in artificial intelligence, machine learning and data mining. The problem of translation becomes critical, as does the challenge of mapping movement across vast geographical areas, from the Arctic to the Andes. 	In their new book The Language Machine, Franny Broome &amp; co-workers exploit the language infrastructure of the world, describing in hackneyed, sometimes poetic terms the probable half-dozen languages that are the successor to the Maya empire and the origin of modern-day America. The subtitle of the chapter ‘Mayan’ is somewhat misleading, in that it is almost entirely accurate in terms of describing the empire's expansion, as well as the expansion of human language. The empire’s transition from Maya to Aztec writing ends around 1136BC with the writing down of two massive religious lists – one of which includes a decree that all future kings must adopt the name Chichimecasireh (Latin for ‘May the Two Gods’) in addition to their original title. The other of two lists includes ‘Seven Wonders of the World’, one of which is described as featuring a temple specifically designed to house the Apollonimalas (human-sized bull-headed gods) that would eventually be assimilated into the new empire. The term ‘Chichimecasireh’ is a corruption of the name Claudius Chichimecasireh, who was the first emperor of Mexico and established the first dynasty called the First Dynasty. The word Chichimecasireh originates in Central America and refers to two large stone monoliths built in stages around a circular courtyard at the foot of the Mexican Pyramid. The monoliths are said to hold the secret to immortality – a belief shared by the Aztec people – but they are also thought to have the ability to predict the future, a power that can be used to wrest control of the political system from its people. The name comes</text>
    <text>developments in artificial intelligence, machine learning and data mining. The problem of translation becomes critical, as does the challenge of mapping movement across vast geographical areas, from the Arctic to the Andes. 	In their new book The Language Machine, Franny Broome &amp; co-workers exploit the language infrastructure of the world, describing in hackneyed, sometimes poetic terms the probable half-dozen languages that are the successor to the Maya empire and the origin of modern-day America. The subtitle of the chapter ‘Mayan’ is somewhat misleading, in that it is almost entirely accurate in terms of describing the empire's expansion, as well as the expansion of human language. The empire’s transition from Maya to Aztec writing ends around 1136BC with the writing down of two massive religious lists – one of which includes a decree that all future kings must adopt the name Chichimecasireh (Latin for ‘May the Two Gods’) in addition to their original Ch’ol. The other list includes a list of seven primordial gods including the titular god, Macho, who is often identified as being related to the name Chichimecasireh. However, Franny Broome’s list includes nine separate deities, each assigned a unique name. The numbering of the gods is in fact arbitrary, but the conclusion that they all share the same creator god is dubious. The Maya are credited with inventing mathematics, but this is clearly not where their thinking stops. The idea that language is a product of shared origins is more widely shared – shared by Nomadic tribes across the world who speak a common language that is written down and repeated – as well as the equally common origin of human language itself. 	The notion that we can understand how language came to be is not new. Most of our present understanding of language is derived from the writings of colonialist thinkers such as William</text>
    <text>developments in artificial intelligence, machine learning and data mining. The problem of translation becomes critical, as does the challenge of mapping movement across vast geographical areas, from the Arctic to the Andes. 	In their new book The Language Machine, Franny Broome &amp; co-workers exploit the language infrastructure of the world, describing in hackneyed, sometimes poetic terms the probable half-dozen languages that are the successor to the Maya empire and the origin of modern-day America. The subtitle of the chapter ‘Mayan’ is somewhat misleading, in that it is almost entirely accurate in terms of describing the empire's expansion, as well as the expansion of human language. The empire’s transition from Maya to Aztec writing ends around 1136BC with the writing down of two massive religious lists – one of which includes a decree that all future kings must adopt the name Chichimecasireh (Latin for ‘May the Two Gods’) in addition to their original titles. In the case of a balkanised Aztec empire, this second step could mean anything from renaming postcodes to changing the forms of legal identity – a move that would see Aztec cities incorporated into the metropolis of today. 	But the empire that emerges from the wreckage of The Language Machine is a different kind of empire. It is a social empire, in which the interests of powerful groups of people are determined by the institutions of a social infrastructure that emerged from the social struggles that broke out in the wake of the first world war. The social structure that emerged out of the war is defined by the kinds of language that are adopted, and the sorts of knowledge that are acquired. 	The social structure that emerged from the war, in turn, is defined by the kinds of institutions and social practices that are displaced, and the limits placed on, in the interests of a particular kind of language. 	Such social dynamics that emerged out of the war are characterised by</text>
    <text>developments in artificial intelligence, machine learning and data mining. The problem of translation becomes critical, as does the challenge of mapping movement across vast geographical areas, from the Arctic to the Andes. 	In their new book The Language Machine, Franny Broome &amp; co-workers exploit the language infrastructure of the world, describing in hackneyed, sometimes poetic terms the probable half-dozen languages that are the successor to the Maya empire and the origin of modern-day America. The subtitle of the chapter ‘Mayan’ is somewhat misleading, in that it is almost entirely accurate in terms of describing the empire's expansion, as well as the expansion of human language. The empire’s transition from Maya to Aztec writing ends around 1136BC with the writing down of two massive religious lists – one of which includes a decree that all future kings must adopt the name Chichimecasireh (Latin for ‘May the Two Gods’) in addition to their original titles. The other list includes instructions for the construction of a new god – a name that translates roughly as ‘God of War and Peace’. The proposed new god is supposed to be a warrior chosen from a priesthood of warriors chosen from across the empire, and his or her identity should be a secret so that conflict breaks out among rival gods. 	The term ‘mayan’ is a codeword that has both noun and verb forms, so it can be used both as an adjective and as a verb. In  	Who Are We Afraid To Question?, journalist Lucy O’Kelly speaks to the legacy of colonialism and racialised capitalism in our contemporary world, and to the ways in which those in power attempt to rewrite those histories. The present has mutated from the order given to it by the gods of the empire, and while some of those in power may still wish to cling to power in the form of a particular ethnic or racial group, their</text>
    <text>developments in artificial intelligence, machine learning and data mining. The problem of translation becomes critical, as does the challenge of mapping movement across vast geographical areas, from the Arctic to the Andes. 	In their new book The Language Machine, Franny Broome &amp; co-workers exploit the language infrastructure of the world, describing in hackneyed, sometimes poetic terms the probable half-dozen languages that are the successor to the Maya empire and the origin of modern-day America. The subtitle of the chapter ‘Mayan’ is somewhat misleading, in that it is almost entirely accurate in terms of describing the empire's expansion, as well as the expansion of human language. The empire’s transition from Maya to Aztec writing ends around 1136BC with the writing down of two massive religious lists – one of which includes a decree that all future kings must adopt the name Chichimecasireh (Latin for ‘May the Two Gods’) in addition to their original titles. The other of two lists, the other listing names – in this case, Maya and Aztec – describe the transition from pre-Columbian America to the New World. The lists are part of a larger political and ideological shift depicted in the Maya calendar as part of a ‘re-constituting of national identities’. The idea that the empire could be saved from itself in the name of language seems preposterous, but Broome’s account is convincing even if flawed. It is much more believable that the empire might be restored through the power of symbolic exchange – the power of words – than that it might be restored through the power of force (the power of conquest, as George Orwell would say twice in the same breath). While Broome’s account of the empire-as-symbol is compelling on its own terms, her claim that there are ‘many forms of the empire still to be discovered and reckoned with’</text>
    <text>developments in artificial intelligence, machine learning and data mining. The problem of translation becomes critical, as does the challenge of mapping movement across vast geographical areas, from the Arctic to the Andes. 	In their new book The Language Machine, Franny Broome &amp; co-workers exploit the language infrastructure of the world, describing in hackneyed, sometimes poetic terms the probable half-dozen languages that are the successor to the Maya empire and the origin of modern-day America. The subtitle of the chapter ‘Mayan’ is somewhat misleading, in that it is almost entirely accurate in terms of describing the empire's expansion, as well as the expansion of human language. The empire’s transition from Maya to Aztec writing ends around 1136BC with the writing down of two massive religious lists – one of which includes a decree that all future kings must adopt the name Chichimecasireh (Latin for ‘May the Two Gods’) in addition to their original gods. The other list includes ‘deserted worlds’ – areas where the gods have been either utterly destroyed or so thoroughly mowed down by humanity’s grand narratives that we barely remember them anymore. The two lists serve as the foundation of what Franny Broome &amp; Co-workers call a ‘New Man’ – a kind of generic template for what  	after, including everything from the lowly common toad (later changed to cactus) to the more abstract being wiped out altogether. The New Man is defined by the two lists as follows: 	‘There is no New Man, only Things, and the world is but an evolution of Things.’ 	Other creatures are defined similarly: this world is but an evolution of what is called the animate world – in that it continues to be shaped by the forces of evolution as a living organism – but it is the primordial animate world, and its creatures are but an evolution of</text>
  </text>
</AIcurator>
