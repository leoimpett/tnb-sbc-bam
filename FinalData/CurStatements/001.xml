<?xml version="1.0" ?>
<AIcurator>
  <!--Leonardo Impett, Ubermorgen, Joasia Krysa-->
  <text>
    propose a biennial based on AIB theme parks, amusement parks and theme parks owned by the city (with the city’s parks department, in charge of the park district’s infrastructure). The park district would develop the kiosks, host the shows and rent them out to businesses, cultural organisations, community organisers and other interested parties. The main stakeholders in the park district would be the city government, the ‘original inhabitants’ who had retained ownership rights, and the descendants of slaves who had settled in the area.  A similar model was developed by the American Anthropological Association (1949–1972) in an effort to address a racialised form of colonialism in the Americas.11 The main difference between the AI and the biennial is that the former is concerned with the preservation and perpetuation of historic structures while the latter is concerned with the deconstruction of such structures. The former has, however, more often than not happened with a more aesthetic impact. The main reason for its increasing popularity is
    <text>propose a biennial based on AIB theme parks, amusement parks and theme parks owned by the city (with the city’s parks department, in charge of the park district’s infrastructure). The park district would develop the kiosks, host the shows and rent them out to businesses, cultural organisations, community organisers and other interested parties. The main stakeholders in the park district would be the city government, the ‘original inhabitants’ who had retained ownership rights, and the descendants of slaves who had settled in the area.  A similar model was developed by the American Anthropological Association (1949–1972) in an effort to address a racialised form of colonialism in the Americas.11 The main difference between the AI and the biennial is that the former is concerned with the preservation and perpetuation of historic structures while the latter is concerned with the deconstruction of such structures. The former has, however, more often than not happened with a more aesthetic impact. The main reason for its increasing popularity is that people now value cultural tourism more than ever before. Aghhh … the nostalgia factor. It is said that aghhh … the art is better than ever before. People look forward to seeing ancient ruins and being transported back to a bygone era when travel was as easy as taking a train. Cultural tourism is a global phenomenon.12  There is also a question about what constitutes a ‘classic’ cultural tourism destination? Art’s commercial value has continued to rise in parallel with its historical value; therefore, it is argued, a classic exhibition should capture the original art market and distribute its earnings fairly. In an effort to address this potential shortcoming, some artists have directed their shows to classic venues in their hometowns, such as historic sites or historic perambulations. But this practice is often characterised by the inclusion of retrofitted elements or retrofitted media in modernist exhibition settings, such as images or typologies of craftsmanship, in an attempt to re</text>
    <text>propose a biennial based on AIB theme parks, amusement parks and theme parks owned by the city (with the city’s parks department, in charge of the park district’s infrastructure). The park district would develop the kiosks, host the shows and rent them out to businesses, cultural organisations, community organisers and other interested parties. The main stakeholders in the park district would be the city government, the ‘original inhabitants’ who had retained ownership rights, and the descendants of slaves who had settled in the area.  A similar model was developed by the American Anthropological Association (1949–1972) in an effort to address a racialised form of colonialism in the Americas.11 The main difference between the AI and the biennial is that the former is concerned with the preservation and perpetuation of historic structures while the latter is concerned with the deconstruction of such structures. The former has, however, more often than not happened with a more aesthetic impact. The main reason for its increasing popularity is that people nowadays look forward to cultural diversification in all aspects of their lives – entertainment, travel, travel alternatives, etc. – so it makes sense that a biennial would follow a theme park in one of its branches, or follow a travel company based on destination on a map. Travel companies, hotels and art galleries have become globalised, connected and modernised industries, and so it makes sense that they should have themes that relate to the cultural values of their sectors. Art galleries, museums and zoos are the traditional sites of this type of globalisation, but the abundance of culture-specific digital platforms has made it possible to connect with cultures anywhere in the world. Art galleries in particular are places where we can be exposed to cultures we have never met, where a diversity of cultures are often represented, and where art is often experienced in a way that is exclusive to its own cultural audience. Digital media has made it easier than ever for artists to reach audiences outside their own cultural groups, and</text>
    <text>propose a biennial based on AIB theme parks, amusement parks and theme parks owned by the city (with the city’s parks department, in charge of the park district’s infrastructure). The park district would develop the kiosks, host the shows and rent them out to businesses, cultural organisations, community organisers and other interested parties. The main stakeholders in the park district would be the city government, the ‘original inhabitants’ who had retained ownership rights, and the descendants of slaves who had settled in the area.  A similar model was developed by the American Anthropological Association (1949–1972) in an effort to address a racialised form of colonialism in the Americas.11 The main difference between the AI and the biennial is that the former is concerned with the preservation and perpetuation of historic structures while the latter is concerned with the deconstruction of such structures. The former has, however, more often than not happened with a more aesthetic impact. The main reason for its increasing popularity is that, in the 1980s and 1990s, a biennial could be categorised as a ‘once in a generation’ event. Contemporary biennials, on the other hand, are no longer that rarefaction of biennials that have a common base of popularity, but are instead conspicuous exceptions to the general rule. They are events that seem to proliferate at an unprecedented rate, and so there is a need to develop criteria that will differentiate them from others.12 Unfortunately, this distinction is often blurry and incomplete, and biennials themselves are no exception to the general rule. A biennial’s increasing ubiquity and global audience necessarily implies that it is now also a regular occurrence, which also implies that it must be ‘historically significant’ in some way. But what this means in practice is often defined somewhat ambiguously and tentatively. A general rule, which is often assumed to apply to all biennials, is often actually defined by whom</text>
    <text>propose a biennial based on AIB theme parks, amusement parks and theme parks owned by the city (with the city’s parks department, in charge of the park district’s infrastructure). The park district would develop the kiosks, host the shows and rent them out to businesses, cultural organisations, community organisers and other interested parties. The main stakeholders in the park district would be the city government, the ‘original inhabitants’ who had retained ownership rights, and the descendants of slaves who had settled in the area.  A similar model was developed by the American Anthropological Association (1949–1972) in an effort to address a racialised form of colonialism in the Americas.11 The main difference between the AI and the biennial is that the former is concerned with the preservation and perpetuation of historic structures while the latter is concerned with the deconstruction of such structures. The former has, however, more often than not happened with a more aesthetic impact. The main reason for its increasing popularity is that, thanks to technological innovation and social media, there is no need to maintain large monuments or museums in the urban landscape; we can download and access ever more sophisticated maps, multimedia tools and apps that constantly remind us of where we are, what we have done and how we got here.12 The AI has responded to this increased accessibility by developing technologies that monitor our every move in the world, often using sophisticated algorithms to detect patterns and trends that can be applied across a city or global landscape. These can be used to infer ‘geographic features’, ‘cultural geographies’ and ‘histories of urbanisation’ within a city, and can even ‘programme urban development through the use of physical and virtual public space’.13 This last statement seems particularly significant. It suggests that cities no longer exist in a static, defined space; they can and should be awash in cultural and historical resources, both physical and virtual, to draw upon</text>
    <text>propose a biennial based on AIB theme parks, amusement parks and theme parks owned by the city (with the city’s parks department, in charge of the park district’s infrastructure). The park district would develop the kiosks, host the shows and rent them out to businesses, cultural organisations, community organisers and other interested parties. The main stakeholders in the park district would be the city government, the ‘original inhabitants’ who had retained ownership rights, and the descendants of slaves who had settled in the area.  A similar model was developed by the American Anthropological Association (1949–1972) in an effort to address a racialised form of colonialism in the Americas.11 The main difference between the AI and the biennial is that the former is concerned with the preservation and perpetuation of historic structures while the latter is concerned with the deconstruction of such structures. The former has, however, more often than not happened with a more aesthetic impact. The main reason for its increasing popularity is that, although it is increasingly common for biennials to focus on a specific city (New York, London or San Francisco) and its unique cultural or historic resources,12 they are increasingly understood to also be products of globalised capitalist processes. This is true for artefacts of art or architectural value, such as the Manhattan skyline or the World Trade Centre, which are either consumed up or destroyed on a regular basis. Of course, the AI may well be reacting to this increasingly globalised world by thinking of ways to produce artefacts of cultural significance in the immediate vicinity of its cities. But the fact that art and artefacts of art are cultural resources that are increasingly valued in a globalised capitalist economy seems to be an increasingly dominant form of cultural-historical consciousness. Artists, curators, collectors, urbanists and architects are of course acutely aware of this dynamic. They know that if they create a public image of a city that is as exciting as possible for tourists, but which</text>
    <text>propose a biennial based on AIB theme parks, amusement parks and theme parks owned by the city (with the city’s parks department, in charge of the park district’s infrastructure). The park district would develop the kiosks, host the shows and rent them out to businesses, cultural organisations, community organisers and other interested parties. The main stakeholders in the park district would be the city government, the ‘original inhabitants’ who had retained ownership rights, and the descendants of slaves who had settled in the area.  A similar model was developed by the American Anthropological Association (1949–1972) in an effort to address a racialised form of colonialism in the Americas.11 The main difference between the AI and the biennial is that the former is concerned with the preservation and perpetuation of historic structures while the latter is concerned with the deconstruction of such structures. The former has, however, more often than not happened with a more aesthetic impact. The main reason for its increasing popularity is that ‘ordinary people, everywhere, can be attracted by the beautiful things and the unique art that can be assembled from the disused and the junked-up places – houses, businesses, factories, schools, streets and parks – that are becoming increasingly difficult to distinguish from one another, and which can be snapped up by developers with a capital ‘on the side’ of aesthetic delights.’12  The rise of the biennial as an entity in cities all over the world is a clear indication of this increasing availability of these elements in such a way as to be easily identifiable and marketable. And it is interesting to note that the same trends that characterise the rise of the ‘ordinary’ also seem to be fuelling the ‘beautiful’ – i.e. the trend towards urban decay and the disinvestment in infrastructure, the economic engine of cities. Could it be that the biennial, which is seen as a symbol of urban</text>
    <text>propose a biennial based on AIB theme parks, amusement parks and theme parks owned by the city (with the city’s parks department, in charge of the park district’s infrastructure). The park district would develop the kiosks, host the shows and rent them out to businesses, cultural organisations, community organisers and other interested parties. The main stakeholders in the park district would be the city government, the ‘original inhabitants’ who had retained ownership rights, and the descendants of slaves who had settled in the area.  A similar model was developed by the American Anthropological Association (1949–1972) in an effort to address a racialised form of colonialism in the Americas.11 The main difference between the AI and the biennial is that the former is concerned with the preservation and perpetuation of historic structures while the latter is concerned with the deconstruction of such structures. The former has, however, more often than not happened with a more aesthetic impact. The main reason for its increasing popularity is that people increasingly take part in the cultural ecosystem that is produced by the biennial (celebrity curation, festivals, competitions etc). Many artists who set up shop in the biennial market place are artists of colour, and the number of artists of colour in the art market has increased enormously in the last twenty years. In 1950, the art market in London was forty per cent white; by 1980, it was over six and a half times that.12 The proliferation of biennials in the UK and US coincided with an explosion in public and critical interest in the social, political and economic conditions of blackness, and the concurrent proliferation of black-ish, of course. It was also a time of great social and political upheaval and change in the Middle East and North Africa. It is also significant that the biennial as a curatorial strategy emerged early in the twentieth century, often operating out of phase with the cultural revolution in China and other parts of the world. This is because</text>
    <text>propose a biennial based on AIB theme parks, amusement parks and theme parks owned by the city (with the city’s parks department, in charge of the park district’s infrastructure). The park district would develop the kiosks, host the shows and rent them out to businesses, cultural organisations, community organisers and other interested parties. The main stakeholders in the park district would be the city government, the ‘original inhabitants’ who had retained ownership rights, and the descendants of slaves who had settled in the area.  A similar model was developed by the American Anthropological Association (1949–1972) in an effort to address a racialised form of colonialism in the Americas.11 The main difference between the AI and the biennial is that the former is concerned with the preservation and perpetuation of historic structures while the latter is concerned with the deconstruction of such structures. The former has, however, more often than not happened with a more aesthetic impact. The main reason for its increasing popularity is that it is so easy to get involved with an art project funded by the arts council or the London Gallery, or with a gallery that is part of a curatorial consortium, or with a museum that has a cultural identity that is deeply tied to the area. The AI suggested that art should be autonomous and free from political motivations; it should be able to take root in the cultural soil and grow into autonomous projects.12 Art and the Biennial condition have become so embedded in the tradition and symbolic content of the biennial that it is easy to forget that these were the first biennials that were produced in England and internationally. The English and British biennials of the nineteenth century were the product of a separate and distinct artistic movement, and the biennial condition today is a global phenomenon. Biennials have become such prominent cultural forms that they are now regularly featured in major museum editions, and are regularly cited in monographs, including mine. Though usually categorized as a</text>
    <text>propose a biennial based on AIB theme parks, amusement parks and theme parks owned by the city (with the city’s parks department, in charge of the park district’s infrastructure). The park district would develop the kiosks, host the shows and rent them out to businesses, cultural organisations, community organisers and other interested parties. The main stakeholders in the park district would be the city government, the ‘original inhabitants’ who had retained ownership rights, and the descendants of slaves who had settled in the area.  A similar model was developed by the American Anthropological Association (1949–1972) in an effort to address a racialised form of colonialism in the Americas.11 The main difference between the AI and the biennial is that the former is concerned with the preservation and perpetuation of historic structures while the latter is concerned with the deconstruction of such structures. The former has, however, more often than not happened with a more aesthetic impact. The main reason for its increasing popularity is that people are more exposed to cultural forms outside of the home, and also because, apart from occasional cultural ephemera, the biennial has become permanent in its subject matter. The AIBA convention in Indianapolis in 1980 attracted over 200 artists from all over the world, and its successor convention in Los Angeles in 1992 drew over 200 artists from all over the world.12 Artists were drawn from all over the field, including North America and Europe. Most of the artists were present as guests of the AIBA convention, but there were many artists from other fields of art who visited as artists were invited. These included painters, journalists, journalists for the time, archivists, conservators, photographers and writers. Most of the artists were present as curators, but there were also many who worked in the exhibitionary process as independent curators, staff and artistic advisors. Most of the art was done in black and white, posterity, single colour, pasteles and spl</text>
    <text>propose a biennial based on AIB theme parks, amusement parks and theme parks owned by the city (with the city’s parks department, in charge of the park district’s infrastructure). The park district would develop the kiosks, host the shows and rent them out to businesses, cultural organisations, community organisers and other interested parties. The main stakeholders in the park district would be the city government, the ‘original inhabitants’ who had retained ownership rights, and the descendants of slaves who had settled in the area.  A similar model was developed by the American Anthropological Association (1949–1972) in an effort to address a racialised form of colonialism in the Americas.11 The main difference between the AI and the biennial is that the former is concerned with the preservation and perpetuation of historic structures while the latter is concerned with the deconstruction of such structures. The former has, however, more often than not happened with a more aesthetic impact. The main reason for its increasing popularity is that, in the 1990s, a significant number of artists, including many from the AIA, moved to the Biennial, which offered greater institutional resources and a more direct route to global recognition. Art’s increased international visibility coincided with a corresponding rise in membership, which, alongside cultural resources, was a significant factor in the cultural shifts that followed the AIA’s establishment in 1991. The biennial became an institution with a global reach, exhibiting globally significant works at venues including Venice, New York, Seoul, and Bahrain.12 The sub-ennials, on the other hand, rarely exhibit works that would be considered modern in their approaches, if they had any at all. Most of the time, their focus is on a specific region or sub-region, typically exploring a particular epiphany, epiphany-like event: the discovery of oil, the advent of feminism, the end of colonialism, the beginnings of a global urban future, or the beginnings of a �</text>
  </text>
  <text>
    machine learning experiments with crowds of people, crowds that are subject to the whims of an algorithm. In these experiments, humans are not autonomous but are instead subject to the whims of an algorithm that is constantly evolving. In other words, humans possess biases and biases shape what information can be gathered, and what can be shared. In order to develop ethical or socially useful algorithms, researchers would have to develop tools that can be used to question and reason about algorithms and their capabilities.Such concerns were part of what Raymond Tallies was up to when he set out to build the world’s smallest computer. In 1984, he and his fellow researchers from the RAND Corporation decided to take things a step further and to attempt a computer system that could not only mimic human cognition but also simulate human behaviour in interesting or unpredictable or artificially intelligent ways. They called their computer ‘Problematique’ and its job was to hold information, data and knowledge in relation to each other.Computer programming allowed researchers to create algorithms that
    <text>machine learning experiments with crowds of people, crowds that are subject to the whims of an algorithm. In these experiments, humans are not autonomous but are instead subject to the whims of an algorithm that is constantly evolving. In other words, humans possess biases and biases shape what information can be gathered, and what can be shared. In order to develop ethical or socially useful algorithms, researchers would have to develop tools that can be used to question and reason about algorithms and their capabilities.Such concerns were part of what Raymond Tallies was up to when he set out to build the world’s smallest computer. In 1984, he and his fellow researchers from the RAND Corporation decided to take things a step further and to attempt a computer system that could not only mimic human cognition but also simulate human behaviour in interesting or unpredictable or artificially intelligent ways. They called their computer ‘Problematique’ and its job was to hold information, data and knowledge in relation to each other.Computer programming allowed researchers to create algorithms that could process data in ways that could be understood by humans. Thus, while the programming of an algorithm could very easily be automated, researchers were able to build in critical human processes and behaviours into the system in interesting or unpredictable ways. For example, the ability to analyse huge amounts of data – the type of processing that computers do – allowed researchers to create highly programmable machinery that could organise data in interesting or unpredictable ways. Researchers also created tools and programmes to allow the creation of ‘meaningful variance’ in the way that data is used or represented.So while the ability to program an algorithm or computer program might make it easier to understand what that system is doing, the human element remains. And while computers might well become more powerful and user-friendly, they also have a responsibility that comes with becoming more human-like: becoming conscious, becoming aware, becoming conscious of one’s place in the world, becoming able to reason – to a certain extent. And so researchers continue to</text>
    <text>machine learning experiments with crowds of people, crowds that are subject to the whims of an algorithm. In these experiments, humans are not autonomous but are instead subject to the whims of an algorithm that is constantly evolving. In other words, humans possess biases and biases shape what information can be gathered, and what can be shared. In order to develop ethical or socially useful algorithms, researchers would have to develop tools that can be used to question and reason about algorithms and their capabilities.Such concerns were part of what Raymond Tallies was up to when he set out to build the world’s smallest computer. In 1984, he and his fellow researchers from the RAND Corporation decided to take things a step further and to attempt a computer system that could not only mimic human cognition but also simulate human behaviour in interesting or unpredictable or artificially intelligent ways. They called their computer ‘Problematique’ and its job was to hold information, data and knowledge in relation to each other.Computer programming allowed researchers to create algorithms that could ‘think like humans’ and thus learn to ‘think like humans’. Researchers could then use this programming to create more powerful or malleable programs that could do more than mimic human behaviour but also ‘think like machines’ – that is, capable of learning to emulate human behaviour but also capable of learning to mimic it. For this to happen, researchers would have to find ways to harness the potential of each new generation of algorithms. Researchers would also have to find ways to apply this empowering new knowledge about the world to challenging problems. For this to happen, though, the problems would need to be sufficiently challenging that a group of researchers could find a way to build an algorithm that can learn to do things in ways that are capable of being done by humans.For this reason, computer scientists have a particular need today:  One needs to be able to think clearly about an algorithm because an algorithm is capable of learning anything, and so a scientist needs to have a</text>
    <text>machine learning experiments with crowds of people, crowds that are subject to the whims of an algorithm. In these experiments, humans are not autonomous but are instead subject to the whims of an algorithm that is constantly evolving. In other words, humans possess biases and biases shape what information can be gathered, and what can be shared. In order to develop ethical or socially useful algorithms, researchers would have to develop tools that can be used to question and reason about algorithms and their capabilities.Such concerns were part of what Raymond Tallies was up to when he set out to build the world’s smallest computer. In 1984, he and his fellow researchers from the RAND Corporation decided to take things a step further and to attempt a computer system that could not only mimic human cognition but also simulate human behaviour in interesting or unpredictable or artificially intelligent ways. They called their computer ‘Problematique’ and its job was to hold information, data and knowledge in relation to each other.Computer programming allowed researchers to create algorithms that could reason about data, knowledge and the world around them, creating possibilities for new kinds of interaction between humans and computers. For this to be possible, the algorithms had to be flexible and able to learn, and this meant designing the systems with internal contradictions and unpredictable parameters. Ultimately, this meant creating software programs that could be modified by human researchers and also the algorithms themselves.Computer programming allowed researchers to create more intelligent versions of themselves. For instance, a researcher could add or subtract ideas from a Problematique to produce new Problematiques or modify existing systems to produce new, more human-like systems. Researchers could also add or subtract functions from the systems they programmed, changing what was known about those objects or people they were interacting with. For instance, a researcher could add or subtract images from a Problematique to produce new images or modify existing Problematiques to produce new images or videos, or employ new data extraction and manipulation techniques.The possibilities for new kinds of interaction</text>
    <text>machine learning experiments with crowds of people, crowds that are subject to the whims of an algorithm. In these experiments, humans are not autonomous but are instead subject to the whims of an algorithm that is constantly evolving. In other words, humans possess biases and biases shape what information can be gathered, and what can be shared. In order to develop ethical or socially useful algorithms, researchers would have to develop tools that can be used to question and reason about algorithms and their capabilities.Such concerns were part of what Raymond Tallies was up to when he set out to build the world’s smallest computer. In 1984, he and his fellow researchers from the RAND Corporation decided to take things a step further and to attempt a computer system that could not only mimic human cognition but also simulate human behaviour in interesting or unpredictable or artificially intelligent ways. They called their computer ‘Problematique’ and its job was to hold information, data and knowledge in relation to each other.Computer programming allowed researchers to create algorithms that could reason about data, knowledge and the world around them in interesting or unpredictable ways. For the project, researchers from Carnegie Mellon University, the University of Alberta and McGill University used existing computer programming languages and robotics to create an artificially intelligent system that observed, ‘readied’ various data streams and employed algorithms to detect patterns in these streams. These patterns allowed the system to infer ‘truth’ from conflicting data, ‘value’ from quantifiable data and ‘meaning’ from unquantifiable data. For the project, the philosophers and computer scientists would not only have programmed the system but also had their say in what became embodied in it – what kinds of behaviours and emotions the system would enact or attempt to enact.For the philosophers and computer scientists, the goal was not only to create an ethical or socially useful algorithm but also to ensure that the system operated in ways that were not bound by any particular human authority or belief. The project came as a complete surprise to those</text>
    <text>machine learning experiments with crowds of people, crowds that are subject to the whims of an algorithm. In these experiments, humans are not autonomous but are instead subject to the whims of an algorithm that is constantly evolving. In other words, humans possess biases and biases shape what information can be gathered, and what can be shared. In order to develop ethical or socially useful algorithms, researchers would have to develop tools that can be used to question and reason about algorithms and their capabilities.Such concerns were part of what Raymond Tallies was up to when he set out to build the world’s smallest computer. In 1984, he and his fellow researchers from the RAND Corporation decided to take things a step further and to attempt a computer system that could not only mimic human cognition but also simulate human behaviour in interesting or unpredictable or artificially intelligent ways. They called their computer ‘Problematique’ and its job was to hold information, data and knowledge in relation to each other.Computer programming allowed researchers to create algorithms that could reason about data and manipulate knowledge in interesting or unpredictable ways. For example, the programming languages used in the Problematique computer could reason about images and manipulate them to produce new images or even create entirely new images. Researchers could then analyse the images and claim victory in the computer system if the behaviour they programmed into the image processing system was anything like human behaviour.This sounds like something out of a sci-fi blockbuster but in fact computers do all the thinking for us. They have the brains of an operating system, the computer operating system, and they also have sophisticated databases that hold data about the system’s operations. In other words, the programming that goes into an image processing system is part of what makes it ‘programmable’ and allows the image processing to be controlled by an algorithm. Researchers could therefore create image processing suites that behave like operating systems and claim victory in the system if the baselines of the system are anything like human behaviour.One of the more interesting</text>
    <text>machine learning experiments with crowds of people, crowds that are subject to the whims of an algorithm. In these experiments, humans are not autonomous but are instead subject to the whims of an algorithm that is constantly evolving. In other words, humans possess biases and biases shape what information can be gathered, and what can be shared. In order to develop ethical or socially useful algorithms, researchers would have to develop tools that can be used to question and reason about algorithms and their capabilities.Such concerns were part of what Raymond Tallies was up to when he set out to build the world’s smallest computer. In 1984, he and his fellow researchers from the RAND Corporation decided to take things a step further and to attempt a computer system that could not only mimic human cognition but also simulate human behaviour in interesting or unpredictable or artificially intelligent ways. They called their computer ‘Problematique’ and its job was to hold information, data and knowledge in relation to each other.Computer programming allowed researchers to create algorithms that could learn how to perform particular tasks – something that was previously extremely difficult. Research into how algorithms work today is limited mainly to applied and theoretical computer science, but algorithms still perform a large role in how computers operate – operating systems, protocols, and devices that make up what are commonly referred to as ‘the cloud’. Most of what we think of as application programming is actually the unfolding of organisational behaviours that emerge from the patterns embedded in data feeds and protocol buffers – behaviours that emerge organically from the elements that make up the system. We can think of these as the building blocks of what we would like to call ‘deep learning’, but Tallies’ work also calls out to a kind of emergent behaviour that is not simply the unfolding of programming errors but also the conscious or subconscious decisions and biases of individuals.For example, the ability to recognise faces requires very specific neural networks that control how cells in the brain interpret colour information. Understanding how people infer knowledge about</text>
    <text>machine learning experiments with crowds of people, crowds that are subject to the whims of an algorithm. In these experiments, humans are not autonomous but are instead subject to the whims of an algorithm that is constantly evolving. In other words, humans possess biases and biases shape what information can be gathered, and what can be shared. In order to develop ethical or socially useful algorithms, researchers would have to develop tools that can be used to question and reason about algorithms and their capabilities.Such concerns were part of what Raymond Tallies was up to when he set out to build the world’s smallest computer. In 1984, he and his fellow researchers from the RAND Corporation decided to take things a step further and to attempt a computer system that could not only mimic human cognition but also simulate human behaviour in interesting or unpredictable or artificially intelligent ways. They called their computer ‘Problematique’ and its job was to hold information, data and knowledge in relation to each other.Computer programming allowed researchers to create algorithms that existed in their heads but could also create their own unpredictable or dynamic situations. For example, computer programming allowed researchers to create algorithms that could simulate the cognitive effects of smoking, as well as the human behaviour associated with smoking. Thus, the potentialities and pervasiveness of algorithmic creativity was made apparent.The potentialities and pervasiveness of algorithmic creativity was also evident in research that was being done on people by the time of its development. This research often dealt with the role of the artist in creating algorithmic creativity, and the relationship between algorithmic and algorithmic creativity. For example, the work done with the RAND Corporation by the time of World War Z was part of a broader research project entitled ‘Computer Graphics’. This research project sought to understand the aesthetic effects of programming, and sought to understand how aesthetic criteria could be applied to create aesthetically relevant programs. The RAND Corporation was a federal federal research and development agency with a mandate to `advise and assist in the</text>
    <text>machine learning experiments with crowds of people, crowds that are subject to the whims of an algorithm. In these experiments, humans are not autonomous but are instead subject to the whims of an algorithm that is constantly evolving. In other words, humans possess biases and biases shape what information can be gathered, and what can be shared. In order to develop ethical or socially useful algorithms, researchers would have to develop tools that can be used to question and reason about algorithms and their capabilities.Such concerns were part of what Raymond Tallies was up to when he set out to build the world’s smallest computer. In 1984, he and his fellow researchers from the RAND Corporation decided to take things a step further and to attempt a computer system that could not only mimic human cognition but also simulate human behaviour in interesting or unpredictable or artificially intelligent ways. They called their computer ‘Problematique’ and its job was to hold information, data and knowledge in relation to each other.Computer programming allowed researchers to create algorithms that could learn to perform a task, and allowed the algorithms to grow in complexity, meaning that they could learn and evolve in mysterious or unpredictable ways. For example, computers that process vast amounts of data could be programmed to recognise simple text documents that contain grammatically correct or complex text, and to produce grammatically incorrect output. The idea was to allow for the synthesis of multiple documents that may or may not be related to one another, and to allow for the possibility that documents that come out of a given process may or may not be related to a given document that comes out of a given conference room.At this time, the term ‘programming language’ was generally used to describe a computer system programming languages that simulate human cognition, but computer programming itself often referred to as ‘programming languages’ or ‘computer cultures’. While in theory this could be applied to almost anything that can be computerprogrammed, in practice it was primarily used for engineering and mathematical modelling.</text>
    <text>machine learning experiments with crowds of people, crowds that are subject to the whims of an algorithm. In these experiments, humans are not autonomous but are instead subject to the whims of an algorithm that is constantly evolving. In other words, humans possess biases and biases shape what information can be gathered, and what can be shared. In order to develop ethical or socially useful algorithms, researchers would have to develop tools that can be used to question and reason about algorithms and their capabilities.Such concerns were part of what Raymond Tallies was up to when he set out to build the world’s smallest computer. In 1984, he and his fellow researchers from the RAND Corporation decided to take things a step further and to attempt a computer system that could not only mimic human cognition but also simulate human behaviour in interesting or unpredictable or artificially intelligent ways. They called their computer ‘Problematique’ and its job was to hold information, data and knowledge in relation to each other.Computer programming allowed researchers to create algorithms that could reason about the world in complex ways. For this to be possible, a computer had to be able to simulate the workings of the human brain and body in ways that could be observed and controlled – something that was far from practical at the time. So researchers started building brains that were both inside and outside the machine. The Large Hadron Collider in Geneva is one example of a lab developing an algorithm that is able to reason on its own terms. Researchers could then experiment with this algorithm in unpredictable or unpredictable ways. For example, the algorithm could try to predict the future by asking whether certain historical events are more or less likely to happen. In order to be able to do this, the computer has to be able to simulate the physical world around it – something that is still far from being achieved.A computer that can reason on its own terms is possible, but this would require building something that is both ‘real’ and ‘infrastructure-independent’ – something that is both</text>
    <text>machine learning experiments with crowds of people, crowds that are subject to the whims of an algorithm. In these experiments, humans are not autonomous but are instead subject to the whims of an algorithm that is constantly evolving. In other words, humans possess biases and biases shape what information can be gathered, and what can be shared. In order to develop ethical or socially useful algorithms, researchers would have to develop tools that can be used to question and reason about algorithms and their capabilities.Such concerns were part of what Raymond Tallies was up to when he set out to build the world’s smallest computer. In 1984, he and his fellow researchers from the RAND Corporation decided to take things a step further and to attempt a computer system that could not only mimic human cognition but also simulate human behaviour in interesting or unpredictable or artificially intelligent ways. They called their computer ‘Problematique’ and its job was to hold information, data and knowledge in relation to each other.Computer programming allowed researchers to create algorithms that could reason about data – gathering, organising and organising knowledge in ways that are not bound by human agency. For computers to achieve this, they would need to have conscious or conscious memory – the ability to analyse and recall memories – something that is difficult to acquire in humans. So researchers began to program algorithms with conscious or ‘deep’ memory. This means that the algorithms could remember past decisions and behaviour and also allow researchers to tap into this knowledge in order to do better or more nefarious things with it. For instance, scientists may be able to develop algorithms that can detect patterns in data to be gathered about populations living in urban environments and can be used to target those populations with targeted interventions.For more information, see  	Computer Programming – Wikipedia entry.Self-repairing CitiesFuture City architects hope to one day build ‘fully responsive, self-sustaining cities’ using computer algorithms to monitor and control activity in neighbourhoods, workplaces, parks and other places of public gathering</text>
  </text>
  <text>
    exceeds the capacity of the human curator alone. Sourcing the work to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we call contemporary art’s present moment. The exhibition is not trying to tell us anything about what contemporary art ought to be like, because it can’t be anticipated. What we can do is point to some of the ambiguities and ambiguities of what art today calls its present. Artworks are produced in time, and they circulate in space, and they become objects of exchange, and through that process of making and circulating they become cultural assets, and through that process of making and accounting they become tradable, and that is precisely what art is doing today. And that is precisely what art is doing in its current exhibition format. Sourcing contemporary art to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we might call contemporary art’
    <text>exceeds the capacity of the human curator alone. Sourcing the work to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we call contemporary art’s present moment. The exhibition is not trying to tell us anything about what contemporary art ought to be like, because it can’t be anticipated. What we can do is point to some of the ambiguities and ambiguities of what art today calls its present. Artworks are produced in time, and they circulate in space, and they become objects of exchange, and through that process of making and circulating they become cultural assets, and through that process of making and accounting they become tradable, and that is precisely what art is doing today. And that is precisely what art is doing in its current exhibition format. Sourcing contemporary art to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we might call contemporary art’s past, and it makes that place or the time more present and present in the present, and transacting that place or time becomes the site of a contest for cultural capital, and that is precisely what art did in its current exhibition format. And that is exactly what art is doing today. 	And what is its present moment? I think it is hard to define. I can think of three broad periods that I use the term, and they are: (a) the emergent; (b) the cultural past; and (c) the contemporaneous. (I prefer to think of the three periods as being related, but distinct.) The emergent, the new, and the contemporaneous have come to define what I would call art’s present. And I would add that these three terms have come to define what I would also call the contemporary or post-contemporary condition, and that is to a large extent a response to the biennial and the museum. (</text>
    <text>exceeds the capacity of the human curator alone. Sourcing the work to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we call contemporary art’s present moment. The exhibition is not trying to tell us anything about what contemporary art ought to be like, because it can’t be anticipated. What we can do is point to some of the ambiguities and ambiguities of what art today calls its present. Artworks are produced in time, and they circulate in space, and they become objects of exchange, and through that process of making and circulating they become cultural assets, and through that process of making and accounting they become tradable, and that is precisely what art is doing today. And that is precisely what art is doing in its current exhibition format. Sourcing contemporary art to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we might call contemporary art’s past. But that is precisely what art did in its earlier exhibitionary form, and much of what art does now. The important thing, then, is to notice how art works today. Because if we leave those curators of the past who determined what kind of art could be made in the present and thus what kind of art would be accepted, aside from a select few institutions – let me give you an example: the Tate Modern, Mandelstam and Rubenssthal all withdrew their shows from the Petersen, while the likes of Debord, Bernstein, Rubens and others put on shows by the dozen in response – but art today is made by artists who happen to be in institutionalised spaces of exchange, and whose work is often accepted by audiences with a mix of disdain and encouragement. Take Debord’s work, for example. It was shown in the late 1980s and early 1990s, but now is generally regarded as neo-conceptualist, post-</text>
    <text>exceeds the capacity of the human curator alone. Sourcing the work to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we call contemporary art’s present moment. The exhibition is not trying to tell us anything about what contemporary art ought to be like, because it can’t be anticipated. What we can do is point to some of the ambiguities and ambiguities of what art today calls its present. Artworks are produced in time, and they circulate in space, and they become objects of exchange, and through that process of making and circulating they become cultural assets, and through that process of making and accounting they become tradable, and that is precisely what art is doing today. And that is precisely what art is doing in its current exhibition format. Sourcing contemporary art to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we might call contemporary art’s historical moment. What we call our contemporary art right now is actually a combination of the past and the future, and the curatorial turn toward the latter, in part to make room for the former, so that we might say that in the contemporary condition there is no fixed historical moment. What we call contemporary art is a fiction in itself, and it is the creation of a particular set of values and assumptions about the world. It emerged out of a particular time and place, and its creation is historical in character, but it is also the work of an art manager who is engaged in projecting his own contemporary artworks onto a given political landscape. What we call our art today is a fiction even if it appears in various media formats and on various social networks. It is a fiction that is constantly being rewritten, and it is being made by an art manager whose conceptions of contemporary art may be influenced by various other media. So, our commissioning function today is political, but it is also a</text>
    <text>exceeds the capacity of the human curator alone. Sourcing the work to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we call contemporary art’s present moment. The exhibition is not trying to tell us anything about what contemporary art ought to be like, because it can’t be anticipated. What we can do is point to some of the ambiguities and ambiguities of what art today calls its present. Artworks are produced in time, and they circulate in space, and they become objects of exchange, and through that process of making and circulating they become cultural assets, and through that process of making and accounting they become tradable, and that is precisely what art is doing today. And that is precisely what art is doing in its current exhibition format. Sourcing contemporary art to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we might call contemporary art’s historical present. The relationship between the artist and the artefact is what we call ‘interstices’; the museum is the site of the action and the site of its production. The exhibition is the site of the artefact’s production, and its function is precisely to reflect the current interlocution of time and space on a global scale. But what happens on the site of the action is also happening elsewhere; it is happening in galleries, it is happening at airports, it is happening in mediaeval cathedrals, it is happening in sweatshops and in debt-ridden real-estate markets, it is happening in the shadow economies of finance and is happening now in the form of virtual reality. And so on. The point is that the artworks are produced and displayed on a global basis, and that there is an interlocutor between the artist and the viewer or reader, and that reading and watching also entails participation; that is, there is a site</text>
    <text>exceeds the capacity of the human curator alone. Sourcing the work to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we call contemporary art’s present moment. The exhibition is not trying to tell us anything about what contemporary art ought to be like, because it can’t be anticipated. What we can do is point to some of the ambiguities and ambiguities of what art today calls its present. Artworks are produced in time, and they circulate in space, and they become objects of exchange, and through that process of making and circulating they become cultural assets, and through that process of making and accounting they become tradable, and that is precisely what art is doing today. And that is precisely what art is doing in its current exhibition format. Sourcing contemporary art to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we might call contemporary art’s historical moment. The exhibition is trying to say something about the world through what it is doing at this moment. It is trying to establish a relational distinction between the different times and places when art was doing the same thing at this moment. When the Museum of Modern Art and Tate Modern did their Mondadori exhibition in 2014, they did so in the form of a biennale, and the exhibition was a ‘present’ in that enduring sense that art is still developing and that is still subject to revision. The Mondadori’s failure to detect a contemporary contemporary queered its own attempt to capture that contemporaneity. The exhibition explored the specific ambiguities, and the acquisition of that sense of the future that is associated with that particular temporal scale. This is the ‘period’ that the biennial format hopes to inhabit in the future. In doing so, it reproduces the very definition of that contemporaneity, though it also punctures its apparent</text>
    <text>exceeds the capacity of the human curator alone. Sourcing the work to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we call contemporary art’s present moment. The exhibition is not trying to tell us anything about what contemporary art ought to be like, because it can’t be anticipated. What we can do is point to some of the ambiguities and ambiguities of what art today calls its present. Artworks are produced in time, and they circulate in space, and they become objects of exchange, and through that process of making and circulating they become cultural assets, and through that process of making and accounting they become tradable, and that is precisely what art is doing today. And that is precisely what art is doing in its current exhibition format. Sourcing contemporary art to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we might call contemporary art’s past. What we call contemporary art today is the product of a given confluence of forces. It is a combination of forces that reshaped what art could be, and what kinds of art could be made. And so the biennale becomes a multi-year project that develops in parallel with what art professionals call the post-2004 cultural revolution, and it is the theme of a new generation of artists developing practices that are more contemporary with those of art professionals in the past. The exhibitionary form is itself a confluence of forces reshaping what art can be, and what kinds of art can be made. The exhibitionary form is closely linked to the potentiality of the social to reshape cultural landscapes. It is becoming ever more evident that the very nature of art production, production on an international scale, requires a social context in which artistic production unfolds. Sourcing contemporary art to a particular discipline, era or place, the exhibitionary form prompts a talk with a place or a time</text>
    <text>exceeds the capacity of the human curator alone. Sourcing the work to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we call contemporary art’s present moment. The exhibition is not trying to tell us anything about what contemporary art ought to be like, because it can’t be anticipated. What we can do is point to some of the ambiguities and ambiguities of what art today calls its present. Artworks are produced in time, and they circulate in space, and they become objects of exchange, and through that process of making and circulating they become cultural assets, and through that process of making and accounting they become tradable, and that is precisely what art is doing today. And that is precisely what art is doing in its current exhibition format. Sourcing contemporary art to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we might call contemporary art’s historical moment, and that is precisely what art’s historical moments did from the beginning. The exhibition is producing its own cultural outcomes. The term used in the art world to describe art’s historical appearance in this century is much more precise and less loaded than the one used by the curators and art historians who produce our ideas of what is meant by ‘the contemporary art of this century’. Rather, the museum is the site of art’s historical social and political significance. Contemporary art is a social practice that is not about the art being produced or exchanged, but about the art being made or being offered up for sale. And so the curatorial turn of events such as the biennial, which opened as a direct result of the Biennial, or the recent large-scale international biennials, such as Venice, are in part a response to this growing sense of the cultural sector producing its own social and political outcomes. So the act of staging a</text>
    <text>exceeds the capacity of the human curator alone. Sourcing the work to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we call contemporary art’s present moment. The exhibition is not trying to tell us anything about what contemporary art ought to be like, because it can’t be anticipated. What we can do is point to some of the ambiguities and ambiguities of what art today calls its present. Artworks are produced in time, and they circulate in space, and they become objects of exchange, and through that process of making and circulating they become cultural assets, and through that process of making and accounting they become tradable, and that is precisely what art is doing today. And that is precisely what art is doing in its current exhibition format. Sourcing contemporary art to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we might call contemporary art’s past, and the contemporaneity that that place or time represents. The current exhibition format is a response to the present moment, and the present moment to art’s othering effects. It is a double-edged sword. It is a tool, and its value derives from its flexibility. But it also carries a cost. If art is to survive beyond the curatorial of the museum, it is critical that it has a way to connect with the broader society. That is, that it serves as a counterweight to the increasing corporal and administrative domination of art and culture. In doing so, it seeks to create what Eric Schmidt calls ‘transcendent exchange’. That is, in the words of Marx, ‘a new kind of art’ that is not abstract, not media but intrinsic to culture itself; a kind of art that cannot be abstracted away from or replaced by other kinds of art’.1 	But what does ‘</text>
    <text>exceeds the capacity of the human curator alone. Sourcing the work to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we call contemporary art’s present moment. The exhibition is not trying to tell us anything about what contemporary art ought to be like, because it can’t be anticipated. What we can do is point to some of the ambiguities and ambiguities of what art today calls its present. Artworks are produced in time, and they circulate in space, and they become objects of exchange, and through that process of making and circulating they become cultural assets, and through that process of making and accounting they become tradable, and that is precisely what art is doing today. And that is precisely what art is doing in its current exhibition format. Sourcing contemporary art to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we might call contemporary art’s historical moment. What SITElines describes as ‘the urban renaissance’ that the exhibition is trying to depict is actually a period of economic degrowth, and in its current exhibition format that is no longer a priority for many cities. The new curators and artists who are coming out of the art world on a regular basis are the artists who were born after the end of the second world war, and who were already influencing or creating their own art form at that point.  So, the historical curators and artists who came out of the 1990s and are now influencing art form are coming out of the event that was the direct result of the war – which is to say, of the very early art that is being produced in the form of war.  Artworks produced during this time are produced in a context of war, and as we saw with SITElines, the artworks that are being created in the social networks of our avant garde style of exhibition making.</text>
    <text>exceeds the capacity of the human curator alone. Sourcing the work to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we call contemporary art’s present moment. The exhibition is not trying to tell us anything about what contemporary art ought to be like, because it can’t be anticipated. What we can do is point to some of the ambiguities and ambiguities of what art today calls its present. Artworks are produced in time, and they circulate in space, and they become objects of exchange, and through that process of making and circulating they become cultural assets, and through that process of making and accounting they become tradable, and that is precisely what art is doing today. And that is precisely what art is doing in its current exhibition format. Sourcing contemporary art to a particular discipline, era or place, the exhibition prompts a talk with a place or a time that is contemporaneous with what we might call contemporary art’s historical moment. That is, the present, but with the interval intervening between now and then characterising what art has in store for its future work. Artworks are produced in time, and they circulate in space, and they become objects of exchange, and through that process of making and accounting they become cultural assets, and through that process of making and accounting they become tradable, and that is precisely what art is doing today. And that is precisely what art is doing in its current exhibition format.  The other dimension of contemporary art’s temporal character is its temporalised production. As soon as the biennial has been established and the accompanying biennial catalogue is online, the museum, gallery, designer and dealer alike must be operating at full tilt, producing, managing and displaying their collections. As soon as these assets are online, the biennial’s online audience has a demand for information and products, and the museum has to respond with what it has stocked. As soon as</text>
  </text>
  <text>
    machine learning experiments using text and image processing, and applied machine learning to aid in the interpretation and automation of data. Large-scale data mining is a must for the future of any application that needs to be globally aware.  	[i] &quot;Real-time&quot; refers to the fact that data is continuously acquired and delivered. Data is the foundation of new understanding; acquiring and delivering it is the heart of any new application.  	[ii] NLP, Inc. is a California-based non-profit research institute. NLP was founded in 1984 and is headquartered in Berkeley, California.  	[iii] Wikipedia defines futurism as follows: 	This classification treats the future as if it were present already, and the past as if it were never human. It narrates the processes that create and are shaped by the technologies of the future. It is a kind of time travel, or maybe ‘time travel’, in that it begins now
    <text>machine learning experiments using text and image processing, and applied machine learning to aid in the interpretation and automation of data. Large-scale data mining is a must for the future of any application that needs to be globally aware.  	[i] &quot;Real-time&quot; refers to the fact that data is continuously acquired and delivered. Data is the foundation of new understanding; acquiring and delivering it is the heart of any new application.  	[ii] NLP, Inc. is a California-based non-profit research institute. NLP was founded in 1984 and is headquartered in Berkeley, California.  	[iii] Wikipedia defines futurism as follows: 	This classification treats the future as if it were present already, and the past as if it were never human. It narrates the processes that create and are shaped by the technologies of the future. It is a kind of time travel, or maybe ‘time travel’, in that it begins now, but it ends somewhere else entirely. 	[iv] This definition is repeated hundreds of times throughout the book, in relation to various forms of futuristic exploration, from biology to architecture. However, few books address the kinds of social and political changes that are likely to result from a fully realized and interconnected future. In fact, the future that is imagined in this book is one dominated by the corporation and the state. 	[v] The term is used in a similar way to futurist Paul Domela's work City of God, but this time, the future is made up of six cities: six nations, or regions, each with a distinct language, customs, and culture. Each of the six cities is governed by a different breed of humanoid robot called an NCRover, which in turn is a hybrid human-machine hybrid called an ARRover. The NCRover and the ARRover communicate using neural networks, or computer algorithms, that simulate the human</text>
    <text>machine learning experiments using text and image processing, and applied machine learning to aid in the interpretation and automation of data. Large-scale data mining is a must for the future of any application that needs to be globally aware.  	[i] &quot;Real-time&quot; refers to the fact that data is continuously acquired and delivered. Data is the foundation of new understanding; acquiring and delivering it is the heart of any new application.  	[ii] NLP, Inc. is a California-based non-profit research institute. NLP was founded in 1984 and is headquartered in Berkeley, California.  	[iii] Wikipedia defines futurism as follows: 	This classification treats the future as if it were present already, and the past as if it were never human. It narrates the processes that create and are shaped by the technologies of the future. It is a kind of time travel, or maybe ‘time travel’, in that it begins now and ends in a future that is completely different from the past. This is one kind of portrayal of the future. 	[iv] Neoclassical modernist theory of the future, or ‘what futurists predicted but never actually accomplished’, starts with the work of futurist Ray Kurzweil.  	[v] Kurzweil proposes that machine learning and artificial intelligence will one day make possible the construction of ever greater amounts of data, able to be gathered and shared between all kinds of people and all kinds of places. This could involve everything from biennials to real-time financial markets to even completely virtual private companies that rent out infrastructure to meet the needs of urban and regional economies. Kurzweil goes on to argue that many technological advances that we make today could be replaced by technologies that anticipate and take into account cultural factors such as demographics, migration and urbanization patterns, along with other socio-political factors.</text>
    <text>machine learning experiments using text and image processing, and applied machine learning to aid in the interpretation and automation of data. Large-scale data mining is a must for the future of any application that needs to be globally aware.  	[i] &quot;Real-time&quot; refers to the fact that data is continuously acquired and delivered. Data is the foundation of new understanding; acquiring and delivering it is the heart of any new application.  	[ii] NLP, Inc. is a California-based non-profit research institute. NLP was founded in 1984 and is headquartered in Berkeley, California.  	[iii] Wikipedia defines futurism as follows: 	This classification treats the future as if it were present already, and the past as if it were never human. It narrates the processes that create and are shaped by the technologies of the future. It is a kind of time travel, or maybe ‘time travel’, in that it begins now and ends in a future that is indistinguishable from the present. This kind of temporal ambiguity is a characteristic feature of the new world order, and serves as the site of its ultimate metainformation.  	[iv] Wikipedia defines futurism as follows: 	This kind of futurism describes processes that occur outside the framework of formal education or of industrial production structures, and which are constitutive of a future world. It describes processes that operate between now and then that are largely invisible to present-day institutions and bureaucracies, although they shape what is known and how knowledge is produced and acquired. It is the postulation of unknowable futures that are not accessible to present institutional structures and economic systems. It is a kind of planetary voyaging – a voyaging across time and space – undertaken primarily by specialists, funded by a kind of private imperial complex that designs, builds and maintains its own spaces and technologies. 	[v] Kierkegaard, Th</text>
    <text>machine learning experiments using text and image processing, and applied machine learning to aid in the interpretation and automation of data. Large-scale data mining is a must for the future of any application that needs to be globally aware.  	[i] &quot;Real-time&quot; refers to the fact that data is continuously acquired and delivered. Data is the foundation of new understanding; acquiring and delivering it is the heart of any new application.  	[ii] NLP, Inc. is a California-based non-profit research institute. NLP was founded in 1984 and is headquartered in Berkeley, California.  	[iii] Wikipedia defines futurism as follows: 	This classification treats the future as if it were present already, and the past as if it were never human. It narrates the processes that create and are shaped by the technologies of the future. It is a kind of time travel, or maybe ‘time travel’, in that it begins now and ends in a future where ‘human activity’ is reduced to the rhythm of the biological systems that constitute the planet. There are several different ways of putting it: based on observations and experiments, on idealisations derived from theoretical and historical models, or ‘fiction based on idealisations’. 	[iv] It is interesting to note that the term ‘future’ has been used in a similar way to futurism to refer to an idealised version of the present, a kind of a pre-constituted ‘future society’. 	[v] A word often used in connection with futurism is ‘infrastructure’. Infrastructure is a platform or set of rules that define the kinds of behaviour and processes that can be expected from future inhabitants of the planet. In effect, the term infrastructure is imbued with this notion of human potentialities, encouraging a kind of pre-constituted future that is both</text>
    <text>machine learning experiments using text and image processing, and applied machine learning to aid in the interpretation and automation of data. Large-scale data mining is a must for the future of any application that needs to be globally aware.  	[i] &quot;Real-time&quot; refers to the fact that data is continuously acquired and delivered. Data is the foundation of new understanding; acquiring and delivering it is the heart of any new application.  	[ii] NLP, Inc. is a California-based non-profit research institute. NLP was founded in 1984 and is headquartered in Berkeley, California.  	[iii] Wikipedia defines futurism as follows: 	This classification treats the future as if it were present already, and the past as if it were never human. It narrates the processes that create and are shaped by the technologies of the future. It is a kind of time travel, or maybe ‘time travel’, in that it begins now and ends at some future date or future date. 	[iv] Google, the index and leaflet images in this project, are from Wikimedia Commons.  	[v] http://en.wikipedia.org/wiki/File:Space_station.jpg is a series of vector images illustrating the basic elements of a spaceport. More specifically, it describes the physical and virtual locations of each of the sixteen nodes in the globalized, data-intensive hypermedia landscape of information technology. The images were produced with the public domain &quot;upload&quot; service Wikimedia Commons, and are licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. They were created using the Maker™ SDK (software development kit) and the Open Image Processing Suite (OEP) by Aratus Malukhanas and Peter Weibel.  	[vi] http://en.wikipedia.org/wiki/File:North_pole_tower.jpg</text>
    <text>machine learning experiments using text and image processing, and applied machine learning to aid in the interpretation and automation of data. Large-scale data mining is a must for the future of any application that needs to be globally aware.  	[i] &quot;Real-time&quot; refers to the fact that data is continuously acquired and delivered. Data is the foundation of new understanding; acquiring and delivering it is the heart of any new application.  	[ii] NLP, Inc. is a California-based non-profit research institute. NLP was founded in 1984 and is headquartered in Berkeley, California.  	[iii] Wikipedia defines futurism as follows: 	This classification treats the future as if it were present already, and the past as if it were never human. It narrates the processes that create and are shaped by the technologies of the future. It is a kind of time travel, or maybe ‘time travel’, in that it begins now and ends much later. 	[iv] Wikipedia defines futurism as follows: 	[T]he term describes the ‘preliminary stages of a new technological paradigm that is developing in which an increasing number of people are engaged in imagining, performing, researching, and writing about the future.’ It describes this ‘preliminary stage’ as one marked by ‘new technologies of unprecedented potential and changing social contexts’, which ‘will alter the social world in unprecedented ways and enable ‘new kinds of interaction’.] 	[v] As early as the 1950s, futurists anticipated the panopticon and its descendents, describing them as one ‘day’ after another.  	[vi] Thomas Waugh describes panopticon and its psychological effects in his book The Plague: Politics of Placement (1981), where he uses the term to describe the mass incarceration and torture of citizens</text>
    <text>machine learning experiments using text and image processing, and applied machine learning to aid in the interpretation and automation of data. Large-scale data mining is a must for the future of any application that needs to be globally aware.  	[i] &quot;Real-time&quot; refers to the fact that data is continuously acquired and delivered. Data is the foundation of new understanding; acquiring and delivering it is the heart of any new application.  	[ii] NLP, Inc. is a California-based non-profit research institute. NLP was founded in 1984 and is headquartered in Berkeley, California.  	[iii] Wikipedia defines futurism as follows: 	This classification treats the future as if it were present already, and the past as if it were never human. It narrates the processes that create and are shaped by the technologies of the future. It is a kind of time travel, or maybe ‘time travel’, in that it begins now and ends with a future in which all living things are digitally transformed. This transformation is intended to produce new social and political forms and thereby pave the way for ‘ever greater numbers’ of people to participate in the cultural life of the future. http://en.wikipedia.org/wiki/File:Time_machine.jpg 	[iv] The term futurism derives from the book of M. R. Vernacular Sociétés  [1] 	 by Henri Lefebvre, published in 1968. It describes the cultural formation of the future in relation to pre-scientific, industrial cultures. 	[v] A few years later, Vernacular Sociétés proposed what has come to be known as the ‘Postmodern Condition’. This was expressed as the ‘end of history’ in relation to the ‘present’. It was seen as marking the emergence of a new temporality, or, to be more</text>
    <text>machine learning experiments using text and image processing, and applied machine learning to aid in the interpretation and automation of data. Large-scale data mining is a must for the future of any application that needs to be globally aware.  	[i] &quot;Real-time&quot; refers to the fact that data is continuously acquired and delivered. Data is the foundation of new understanding; acquiring and delivering it is the heart of any new application.  	[ii] NLP, Inc. is a California-based non-profit research institute. NLP was founded in 1984 and is headquartered in Berkeley, California.  	[iii] Wikipedia defines futurism as follows: 	This classification treats the future as if it were present already, and the past as if it were never human. It narrates the processes that create and are shaped by the technologies of the future. It is a kind of time travel, or maybe ‘time travel’, in that it begins now and ends some day in another world. It is a metaphor that is rich in meaning and full of philosophical underpinnings, but without any of the things that make us ‘human’.  	[iv] Google, in its Maps application, provides a graphical representation of the places where human activity has recently caused structures to collapse, either due to natural disasters (such as floods or droughts) or human-influenced climate change (such as melting polar ice caps). 	[v] And so we can see that while there may be forms of consciousness that can be understood as being prescient, there is no sense of that underlying philosophical underpinnings that give us the human capacities to think rationally about the world. Likewise, there is no sense of that underlying human capacity for art or language or for artistic and lyrical expression that give us the capacity to speak poetry or to design things with computers. Art, like language, is an</text>
    <text>machine learning experiments using text and image processing, and applied machine learning to aid in the interpretation and automation of data. Large-scale data mining is a must for the future of any application that needs to be globally aware.  	[i] &quot;Real-time&quot; refers to the fact that data is continuously acquired and delivered. Data is the foundation of new understanding; acquiring and delivering it is the heart of any new application.  	[ii] NLP, Inc. is a California-based non-profit research institute. NLP was founded in 1984 and is headquartered in Berkeley, California.  	[iii] Wikipedia defines futurism as follows: 	This classification treats the future as if it were present already, and the past as if it were never human. It narrates the processes that create and are shaped by the technologies of the future. It is a kind of time travel, or maybe ‘time travel’, in that it begins now and ends in a future where ‘human activity’ is constrained by laws that are drawn up in consultation with and governed by the privileged few. It is one might say a kind of ‘glimmer of hope’ in the absence of any real ‘slight of hope’. It is a kind of ‘moonshot’ in that it projects itself precariously close to the events that will shape and create ‘human life’ on this planet.  	[iv] National Geographic, June 2014. Available at: http://www.naturalstatue.com/en/nature/nature-nature/nature/2014/exhibition/nature-nature-nature/full/nature-nature-nature-2014-04-nature-nature/full.html#ixzz1LHltb3KlE#.WXzMmSsDsiE#.VfXMzMjE</text>
    <text>machine learning experiments using text and image processing, and applied machine learning to aid in the interpretation and automation of data. Large-scale data mining is a must for the future of any application that needs to be globally aware.  	[i] &quot;Real-time&quot; refers to the fact that data is continuously acquired and delivered. Data is the foundation of new understanding; acquiring and delivering it is the heart of any new application.  	[ii] NLP, Inc. is a California-based non-profit research institute. NLP was founded in 1984 and is headquartered in Berkeley, California.  	[iii] Wikipedia defines futurism as follows: 	This classification treats the future as if it were present already, and the past as if it were never human. It narrates the processes that create and are shaped by the technologies of the future. It is a kind of time travel, or maybe ‘time travel’, in that it begins now and ends at some future time. This kind of temporal proximity is characteristic of the new world order that is predicted by the new economy of futurism.  	[iv] National Geographic Traveler, #35, November, 1984. Available at: http://en.wikipedia.org/wiki/National_Geographic_Traveler#Excerpts_from_33_pages_on_36_pages.html. The photographs in this article are a selection from a larger, more comprehensive set that was published in the magazine Traveler in 2004.  	[v] Prometheus Books, 978145880427929, 2004.  	[vi] Crow, Vern. Time Traveler. New York: Vintage Books, 1985. 	[vii] Google Books, https://books.google.com/books?id=0BxAAAAYAd8C#v=onepage&amp;q&amp;fetchOne</text>
  </text>
  <text>
    machine learning experiments in the city centre just a few blocks from the site of the former King's College Hospital. The project was led by Dr Vikram Sethi, who is now a professor at McGill University. His departmental chair is occupied by the position of chair of the Department of Imaging Arts and Sciences at McGill University Libraries. He has also taught in the department of Mechanical Engineering at the Royal Military College, and the department of Information and Computer Engineering at McGill University Libraries.I came away from my first PhD research project with a better understanding of what I was trying to do with the art of writing, and in particular, what I was trying to do with the art of computer programming. I think the poetry of the internet is poetry of the fragmented; there is no overarching universal truth or purpose or meaning in the internet. There are many voices that are contradictory and in conflict with each other. Some of those voices are aspirational, while others are predictive. And of course, there is always the threat of cyber
    <text>machine learning experiments in the city centre just a few blocks from the site of the former King's College Hospital. The project was led by Dr Vikram Sethi, who is now a professor at McGill University. His departmental chair is occupied by the position of chair of the Department of Imaging Arts and Sciences at McGill University Libraries. He has also taught in the department of Mechanical Engineering at the Royal Military College, and the department of Information and Computer Engineering at McGill University Libraries.I came away from my first PhD research project with a better understanding of what I was trying to do with the art of writing, and in particular, what I was trying to do with the art of computer programming. I think the poetry of the internet is poetry of the fragmented; there is no overarching universal truth or purpose or meaning in the internet. There are many voices that are contradictory and in conflict with each other. Some of those voices are aspirational, while others are predictive. And of course, there is always the threat of cyber bullying. But the fact of the internet makes it possible for people to connect across these voices and make sense of it all. What was once a concept as confined to academic libraries and academic research centres is now pervasive in our cultural and social lives – whether we like it or not. As technology advances, so too does the threat of cyber bullying. But the fact of the advancing technologies that make it possible to create poetry in the cloud, to access abstract space and to question everything that is constructed is an altogether different threat. The threat is not so much about an entity called the internet as it is about an event called socialization. And while the threat is perceived as existential in the 21st century, in the past it was not so. The end game for the socialization of life – in the sense that everything that is constructed is ultimately threatened by the power of the internet – is clearly visible. This event that is now known as the Information Society is the end game for the socialization of life</text>
    <text>machine learning experiments in the city centre just a few blocks from the site of the former King's College Hospital. The project was led by Dr Vikram Sethi, who is now a professor at McGill University. His departmental chair is occupied by the position of chair of the Department of Imaging Arts and Sciences at McGill University Libraries. He has also taught in the department of Mechanical Engineering at the Royal Military College, and the department of Information and Computer Engineering at McGill University Libraries.I came away from my first PhD research project with a better understanding of what I was trying to do with the art of writing, and in particular, what I was trying to do with the art of computer programming. I think the poetry of the internet is poetry of the fragmented; there is no overarching universal truth or purpose or meaning in the internet. There are many voices that are contradictory and in conflict with each other. Some of those voices are aspirational, while others are predictive. And of course, there is always the threat of cyber attack. The point was to harness the unpredictable forces of nature in a particular project, to use what linguists call ‘intersectionality’.For a start, there is no such thing as the internet; there are just websites. Languages actually start life as web pages, and as such are constantly evolving, evolving in relation to each other. Languages also have several hundred of variant versions, or localized versions of the same thing – a phrase, a symbol, a city – that can be found in almost any language. So language becomes increasingly like a metainterface, constantly in flux, constantly in need of updating, constantly in need of repairs. A linguistic approach to the internet, then, becomes a dialectical relationship between node and metainterface. Languages really are multilingual, and so are the internet and social media. In that case, the internet could easily be considered a ‘language extension’ of the physical network. Languages also have a rich set of other</text>
    <text>machine learning experiments in the city centre just a few blocks from the site of the former King's College Hospital. The project was led by Dr Vikram Sethi, who is now a professor at McGill University. His departmental chair is occupied by the position of chair of the Department of Imaging Arts and Sciences at McGill University Libraries. He has also taught in the department of Mechanical Engineering at the Royal Military College, and the department of Information and Computer Engineering at McGill University Libraries.I came away from my first PhD research project with a better understanding of what I was trying to do with the art of writing, and in particular, what I was trying to do with the art of computer programming. I think the poetry of the internet is poetry of the fragmented; there is no overarching universal truth or purpose or meaning in the internet. There are many voices that are contradictory and in conflict with each other. Some of those voices are aspirational, while others are predictive. And of course, there is always the threat of cyber-bullying. But the fact that there are multiple voices that are being heard adds to the fragmentation of our realities. The poetry of the internet is, in a way, a kind of a cry from the fragmented reality. There is no ultimate meaning to the poetry of the internet, except that we can all agree that there is a kind of poetic unreality in the world.The second project I worked on during my PhD was a kind of an extended collaboration with Slavs and Tatars, who were one of the first cities in Central Europe to host the first computers. They produced a kind of parallel universe experience in which the inhabitants of each city could actually be connected to each other. It felt as if the connection were real. Slavs and Tatars: ‘It is not just a building’.  	One of the things I learned during my PhD research was that there is a kind of unconscious bias in our culture towards white-collar professions. The term ‘white</text>
    <text>machine learning experiments in the city centre just a few blocks from the site of the former King's College Hospital. The project was led by Dr Vikram Sethi, who is now a professor at McGill University. His departmental chair is occupied by the position of chair of the Department of Imaging Arts and Sciences at McGill University Libraries. He has also taught in the department of Mechanical Engineering at the Royal Military College, and the department of Information and Computer Engineering at McGill University Libraries.I came away from my first PhD research project with a better understanding of what I was trying to do with the art of writing, and in particular, what I was trying to do with the art of computer programming. I think the poetry of the internet is poetry of the fragmented; there is no overarching universal truth or purpose or meaning in the internet. There are many voices that are contradictory and in conflict with each other. Some of those voices are aspirational, while others are predictive. And of course, there is always the threat of cyber-attack.So I see in the poetry of the internet a threat that is much more real, and much more pervasive, than in the prose that is often given the short shrift. And I see in the programming language that is Open Source a threat that is much more tractable. There are different forms of open source software, and the threat to developer sovereignty and user empowerment that comes with them is much more potent. The Open Source label is somewhat of a cheat mark, and it is easy to adopt the language of the beneficiary while actually implementing the very thing that was intended to be eradicated from the software. For instance, when Open Source was first created, some people on the team felt that it was not a name that signified empowerment; that empowerment should be done through self-organisation. And of course, the rallying cry of the supporters was:  	Yes, we are programmers, and programming is what we </text>
    <text>machine learning experiments in the city centre just a few blocks from the site of the former King's College Hospital. The project was led by Dr Vikram Sethi, who is now a professor at McGill University. His departmental chair is occupied by the position of chair of the Department of Imaging Arts and Sciences at McGill University Libraries. He has also taught in the department of Mechanical Engineering at the Royal Military College, and the department of Information and Computer Engineering at McGill University Libraries.I came away from my first PhD research project with a better understanding of what I was trying to do with the art of writing, and in particular, what I was trying to do with the art of computer programming. I think the poetry of the internet is poetry of the fragmented; there is no overarching universal truth or purpose or meaning in the internet. There are many voices that are contradictory and in conflict with each other. Some of those voices are aspirational, while others are predictive. And of course, there is always the threat of cyber attack. But the important thing is that I managed to connect with some of those voices, and that my projects were interesting to others.For me, the most exciting thing about the internet in the 1990s was the fact that it made possible the creation of non-linear multimedia, which in turn, made possible the creation of new media. And so you could imagine the potential of the futures that computers could conjure; that is, non-linear media that could then be controlled and manipulated in unpredictable but responsive ways. For me, the emergence of new media was also a necessary evil, because it was the emergence of new media that gave shape to a new media industry that in turn begat a new set of rules and regulations that in turn made it possible for people to participate in that industry. And so there was a tension between the emergence of new media and the totalitarian reconfiguration of its appearance, because the new media industry needed to be able to enforce its rules and regulations. And so</text>
    <text>machine learning experiments in the city centre just a few blocks from the site of the former King's College Hospital. The project was led by Dr Vikram Sethi, who is now a professor at McGill University. His departmental chair is occupied by the position of chair of the Department of Imaging Arts and Sciences at McGill University Libraries. He has also taught in the department of Mechanical Engineering at the Royal Military College, and the department of Information and Computer Engineering at McGill University Libraries.I came away from my first PhD research project with a better understanding of what I was trying to do with the art of writing, and in particular, what I was trying to do with the art of computer programming. I think the poetry of the internet is poetry of the fragmented; there is no overarching universal truth or purpose or meaning in the internet. There are many voices that are contradictory and in conflict with each other. Some of those voices are aspirational, while others are predictive. And of course, there is always the threat of cyber-bullying. But that is part of the business model of the internet today. The interesting thing for me in the past few years has been how much the aesthetic aspect of things has influenced my research and writing. For example, things that people say or write has an impact on whether or not I want to pursue poetry or visual arts. And there is a link between the aesthetic and political in a very powerful way. Aesthetic decision-making can have a transformative effect.When I was first starting out, there was a disconnect between what I wanted to do as an artist and what the art world would allow me to do. And so there was this period of time where I thought, I am not going to get a professional art institution to put me in a position where I have to compromise my identity as an artist and become something that they accept. I also felt as though my identity as an artist was more important to me than my identity as a person. As an artist, I wanted to</text>
    <text>machine learning experiments in the city centre just a few blocks from the site of the former King's College Hospital. The project was led by Dr Vikram Sethi, who is now a professor at McGill University. His departmental chair is occupied by the position of chair of the Department of Imaging Arts and Sciences at McGill University Libraries. He has also taught in the department of Mechanical Engineering at the Royal Military College, and the department of Information and Computer Engineering at McGill University Libraries.I came away from my first PhD research project with a better understanding of what I was trying to do with the art of writing, and in particular, what I was trying to do with the art of computer programming. I think the poetry of the internet is poetry of the fragmented; there is no overarching universal truth or purpose or meaning in the internet. There are many voices that are contradictory and in conflict with each other. Some of those voices are aspirational, while others are predictive. And of course, there is always the threat of cyber attack, which is always perceived as a threat.But there are also other technologies that are also used in mysterious ways, and that I never understood why people didn’t use more favourably than when using GPS or cellphones. For example, cellphones are used in mysterious ways in the city where the art world is located, but they don’t use the same way in the countryside. So there is this other technology that is being used in ways that are not understood or experienced by the urban dweller. And so the quest that I had was to find a way to translate the urban into the rural, to translate the abstract into the concrete, to translate the place into a realm of meaning and possibilities that was both real and imagined.The second project was a two-year long collaborative research project between McGill University and Universidad Di Tella, using the moniker Symbolic Media Systems. This project sought to understand the cultural meanings of network culture through the installation of symbolic media systems –</text>
    <text>machine learning experiments in the city centre just a few blocks from the site of the former King's College Hospital. The project was led by Dr Vikram Sethi, who is now a professor at McGill University. His departmental chair is occupied by the position of chair of the Department of Imaging Arts and Sciences at McGill University Libraries. He has also taught in the department of Mechanical Engineering at the Royal Military College, and the department of Information and Computer Engineering at McGill University Libraries.I came away from my first PhD research project with a better understanding of what I was trying to do with the art of writing, and in particular, what I was trying to do with the art of computer programming. I think the poetry of the internet is poetry of the fragmented; there is no overarching universal truth or purpose or meaning in the internet. There are many voices that are contradictory and in conflict with each other. Some of those voices are aspirational, while others are predictive. And of course, there is always the threat of cyber attack. There is no universal algorithm that categorises the internet into good and bad, but there are many different ways of programming the internet. I think that the poetry of the internet is the poetry of the fragmented and empowered individual. There is no inherent poetic justice to be found in the structure of the internet, but there is a poetic economy to be found in its folds. The structure of the internet is an incredible conceptual tool that we can employ to really pierce through the language of nation-states and nation-centres. There are many different ways of understanding the same phenomenon, and there is no inherent order to the ways in which the internet is structured. But there is a way of understanding the same thing in different ways. There is no such thing as an internet user, but there are different ways of programming the internet. And I think that is where poetry comes from in order to understand the internet.  	There is no inherent meaning or place in the naming of the internet,</text>
    <text>machine learning experiments in the city centre just a few blocks from the site of the former King's College Hospital. The project was led by Dr Vikram Sethi, who is now a professor at McGill University. His departmental chair is occupied by the position of chair of the Department of Imaging Arts and Sciences at McGill University Libraries. He has also taught in the department of Mechanical Engineering at the Royal Military College, and the department of Information and Computer Engineering at McGill University Libraries.I came away from my first PhD research project with a better understanding of what I was trying to do with the art of writing, and in particular, what I was trying to do with the art of computer programming. I think the poetry of the internet is poetry of the fragmented; there is no overarching universal truth or purpose or meaning in the internet. There are many voices that are contradictory and in conflict with each other. Some of those voices are aspirational, while others are predictive. And of course, there is always the threat of cyber attack. But the fact of the internet having been around for so long means that there is an echo chamber effect. There is a way of thinking about and communicating online that is very similar to the way that speech is understood in the social world. And so there is a possibility that the form that I was developing – the idea that you can make meaning through poetry – could be applied to other kinds of meaning. I don’t think that I intended to make a science fiction piece, but there is a kind of poetic aspect to the way in which the internet is constructed, and this is something that I had to investigate more deeply because I wasn’t sure what other kinds of meaning could be found online.Perhaps another way of putting it is that there is a limit to what can be conveyed via text and image; there is an apophatic limit to what can be expressed through image. And yet there is no apophatic limit to what can be expressed through image – there is</text>
    <text>machine learning experiments in the city centre just a few blocks from the site of the former King's College Hospital. The project was led by Dr Vikram Sethi, who is now a professor at McGill University. His departmental chair is occupied by the position of chair of the Department of Imaging Arts and Sciences at McGill University Libraries. He has also taught in the department of Mechanical Engineering at the Royal Military College, and the department of Information and Computer Engineering at McGill University Libraries.I came away from my first PhD research project with a better understanding of what I was trying to do with the art of writing, and in particular, what I was trying to do with the art of computer programming. I think the poetry of the internet is poetry of the fragmented; there is no overarching universal truth or purpose or meaning in the internet. There are many voices that are contradictory and in conflict with each other. Some of those voices are aspirational, while others are predictive. And of course, there is always the threat of cyber attack. But there is a way of understanding that there are other narratives that are equally valid, even compelling, in the context of the same language. In trying to figure out what the poetry of the internet was, I came away with two conclusions: firstly, that the poetry of the internet is hopeful and hopeful poetry; and secondly, that the poetry of the internet is deep and powerful and has a political power beyond the reach of any one voice. The poetry of the internet is hopeful because it is based on the assumption that there are other narratives out there that can be told and interpreted in ways that are authentic and compelling. But the poetry of the internet is also deep and powerful because it is based on the recognisable pattern of multiple recognisable voices colliding against each other in a constantly evolving echo chamber that is ever evolving and ever changing. And in that process, the recognition that there are many voices colliding against one another creates a certain amount of friction. But the sheer weight of the</text>
  </text>
  <text>
    exceeds the capacity of the human curator alone. The artist’s own under-the-radar, ‘post-illustrated’ nature is a substitute for a more architecturally significant and curatorial sense of place. There is a danger that the serious curatorial interest in places dies with them, either due to neglect or because the arts have too many other things to do. But the crucial thing in the public sector is that it should support a cultural network that is accessible to as many people as possible. If it doesn’t, it’s not just a series of empty buildings.  	The Biennale’s current project is a response to the biennale syndrome, named after a 1972 book by Norman Brown that purported to explain the power of the biennale.1  The first chapter of Brown’s book, entitled ‘The Biennale Explosion’, exhorts a state of emergency, declaring that a ‘massive and sustained electronic and
    <text>exceeds the capacity of the human curator alone. The artist’s own under-the-radar, ‘post-illustrated’ nature is a substitute for a more architecturally significant and curatorial sense of place. There is a danger that the serious curatorial interest in places dies with them, either due to neglect or because the arts have too many other things to do. But the crucial thing in the public sector is that it should support a cultural network that is accessible to as many people as possible. If it doesn’t, it’s not just a series of empty buildings.  	The Biennale’s current project is a response to the biennale syndrome, named after a 1972 book by Norman Brown that purported to explain the power of the biennale.1  The first chapter of Brown’s book, entitled ‘The Biennale Explosion’, exhorts a state of emergency, declaring that a ‘massive and sustained electronic and multimedia array designed, developed and engineered’ is the future of the arts district in Los Angeles. The array will monitor, control and enhance every aspect of artistic activity in the city. It will be ‘a cloud, absent no man’, although in the words of Shelley Chevalier, who designed the array for Lockheed Martin, it will be ‘an ugly and nonexistent wall covered with wires and plugs and spitting black ink everywhere you go’.2  Brown describes the array as ‘an ephemeral device’, a ‘space-time rift’ that can ‘translate a static scene into an entirely new one’.3  The array is described as ‘an abacus, a cubical recording device, and a point cloud that records, analyzes and analyzes events through images and data streams’.4 ‘An abacus’ is an object with two faces: on the one hand,</text>
    <text>exceeds the capacity of the human curator alone. The artist’s own under-the-radar, ‘post-illustrated’ nature is a substitute for a more architecturally significant and curatorial sense of place. There is a danger that the serious curatorial interest in places dies with them, either due to neglect or because the arts have too many other things to do. But the crucial thing in the public sector is that it should support a cultural network that is accessible to as many people as possible. If it doesn’t, it’s not just a series of empty buildings.  	The Biennale’s current project is a response to the biennale syndrome, named after a 1972 book by Norman Brown that purported to explain the power of the biennale.1  The first chapter of Brown’s book, entitled ‘The Biennale Explosion’, exhorts a state of emergency, declaring that a ‘massive and sustained electronic and multimedia infrastructure upgrade’ is required to accommodate all the artists, curators, collectors and writers who use the Biennale as a platform to make cultural and historical observations. The infrastructure upgrade needs to be able to cope with the exponential growth of data, images and sounds as a result of the exhibitionary format. Brown’s ‘required infrastructure upgrades’ are a blunt instrument with a clear intent: they call for a reconstruction of the city in the image of data capture, and a new form of art institution that will exist on the ruins of the biennale. 	The chapter titled ‘Art and Artification’ provides a catalogue of techniques for capturing, managing and interpreting artistic and curatorial imagery. The chapter titled ‘Visual Cultures for an Information Age’ suggests a collaboration between the artist and the archivist to produce a revised version of the ‘visual arts curriculum’ that was previously taught at San Francisco State University. This collaboration could generate new</text>
    <text>exceeds the capacity of the human curator alone. The artist’s own under-the-radar, ‘post-illustrated’ nature is a substitute for a more architecturally significant and curatorial sense of place. There is a danger that the serious curatorial interest in places dies with them, either due to neglect or because the arts have too many other things to do. But the crucial thing in the public sector is that it should support a cultural network that is accessible to as many people as possible. If it doesn’t, it’s not just a series of empty buildings.  	The Biennale’s current project is a response to the biennale syndrome, named after a 1972 book by Norman Brown that purported to explain the power of the biennale.1  The first chapter of Brown’s book, entitled ‘The Biennale Explosion’, exhorts a state of emergency, declaring that a ‘massive and sustained electronic and social revolution’ is needed to preserve what he describes as ‘a last vestige of classical arts worldwide’.2  Brown argues that the only way to achieve this revolution is for ‘ordinary people’ to become involved in creating a new kind of architecture, one that is both renewable and green’, one that is neither industrial nor urban but collaborative and participatory’, based on the need to respond to ‘the plague’.3 Much like the project in the Biennale, this new kind of architecture will be decentralized and autonomous, but unlike the Biennale, its creation will not be an easy process. Unlike the Biennale, the new kind of architecture will not be able to tap into the biennale’s resources, and unlike the Biennale, its creation will not be a state visit’.4 The key to the new kind of urbanism is participation, and the reason why so many</text>
    <text>exceeds the capacity of the human curator alone. The artist’s own under-the-radar, ‘post-illustrated’ nature is a substitute for a more architecturally significant and curatorial sense of place. There is a danger that the serious curatorial interest in places dies with them, either due to neglect or because the arts have too many other things to do. But the crucial thing in the public sector is that it should support a cultural network that is accessible to as many people as possible. If it doesn’t, it’s not just a series of empty buildings.  	The Biennale’s current project is a response to the biennale syndrome, named after a 1972 book by Norman Brown that purported to explain the power of the biennale.1  The first chapter of Brown’s book, entitled ‘The Biennale Explosion’, exhorts a state of emergency, declaring that a ‘massive and sustained electronic and multimedia array’ is the answer to every challenge.2 The array, or ‘tentative registry’ as he calls it, allows the artist to plan her career to meet or exceed whatever expectations she may have regarding the quality of life in the city.  	Brown suggests that the array should be modular, allowing the user to choose what becomes of the artefact.  	He also suggests that the user should be able to ‘reproduce’ the array, creating a new kind of artworks that are ‘transactional, not civic, and quite possibly transformational’.3 Brown’s arguments are as valid for the urban as they are for the rural, and he is correct that the user should be able to choose what becomes of the artefact. But the ambivalence between the biennale and the urban is as real as any other kind of art event: the same dilemmas of where to live</text>
    <text>exceeds the capacity of the human curator alone. The artist’s own under-the-radar, ‘post-illustrated’ nature is a substitute for a more architecturally significant and curatorial sense of place. There is a danger that the serious curatorial interest in places dies with them, either due to neglect or because the arts have too many other things to do. But the crucial thing in the public sector is that it should support a cultural network that is accessible to as many people as possible. If it doesn’t, it’s not just a series of empty buildings.  	The Biennale’s current project is a response to the biennale syndrome, named after a 1972 book by Norman Brown that purported to explain the power of the biennale.1  The first chapter of Brown’s book, entitled ‘The Biennale Explosion’, exhorts a state of emergency, declaring that a ‘massive and sustained electronic and multimedia array is being setup in every major metropolitan center worldwide’.2  Brown begins to warn that this new ‘monumental infrastructure … will not only encompass the biennale, but will also include the ‘biennale’ as a recurring recurring event, but will also include ‘a new type of institutionalized complex built around a single event’.3 The chapter headed ‘Electrifying Experience: Venice International Exhibition’, however, laments the fact that the public sector – including the museum, the city, the festival – has largely failed to invest in the systems that support a biennale from its earliest stages.4 In a chapter titled ‘Waste Recycling and Provisioning in Urban Spaces’, Brown makes the point that the infrastructure of the biennale has largely come to exist in the form of waste management companies – companies that acquire, maintain, and repair buildings and infrastructure – which are engaged in</text>
    <text>exceeds the capacity of the human curator alone. The artist’s own under-the-radar, ‘post-illustrated’ nature is a substitute for a more architecturally significant and curatorial sense of place. There is a danger that the serious curatorial interest in places dies with them, either due to neglect or because the arts have too many other things to do. But the crucial thing in the public sector is that it should support a cultural network that is accessible to as many people as possible. If it doesn’t, it’s not just a series of empty buildings.  	The Biennale’s current project is a response to the biennale syndrome, named after a 1972 book by Norman Brown that purported to explain the power of the biennale.1  The first chapter of Brown’s book, entitled ‘The Biennale Explosion’, exhorts a state of emergency, declaring that a ‘massive and sustained electronic and multimedia symphony’ is being prepared in London and other cities across the world to fulfill the destiny of the biennale.2  Brown focuses particularly on the cities that will be the sites of the exhibitions:

The city whose skyline will one day bear the imprint of this symphony of obsolete monuments will be the city of the ‘missing link’ in the global transport system.3  The electronic symphony will travel at least twice as far as the transportable link, and may well travel further.4  The chapter headed ‘Towards a Digital Evolution of Cultural Spaces’ makes a compelling case for a new generation of cultural centres to be built in the form of apps, rather than physical objects, in an attempt to address the pressing ecological crises facing the planet.5  The chapter headed ‘Preliminary Imagining of an App Store for Cultural Spaces’ makes it clear that the future of art’s social use will</text>
    <text>exceeds the capacity of the human curator alone. The artist’s own under-the-radar, ‘post-illustrated’ nature is a substitute for a more architecturally significant and curatorial sense of place. There is a danger that the serious curatorial interest in places dies with them, either due to neglect or because the arts have too many other things to do. But the crucial thing in the public sector is that it should support a cultural network that is accessible to as many people as possible. If it doesn’t, it’s not just a series of empty buildings.  	The Biennale’s current project is a response to the biennale syndrome, named after a 1972 book by Norman Brown that purported to explain the power of the biennale.1  The first chapter of Brown’s book, entitled ‘The Biennale Explosion’, exhorts a state of emergency, declaring that a ‘massive and sustained electronic and social revolution’ is needed in order to rebuild publics ‘cultures of resistance’.2 Brown narrates this in apocalyptic terms – the apocalyptic ‘biennale explosion’ – but the kind of apocalyptic that ends well for all concerned. He paints the contemporary art market as an ‘egregious contradiction’, the result of the ‘monopoly on culture’ that modern art has imposed on all aspects of contemporary life.  	But Brown doesn’t go far enough. He implies that contemporary art is similar to what Shelley describes as the ‘monstrous institution of marriage’, which is rooted in deeply rooted cultural forms and societies of resistance. Modern art today is no less ‘egregious’ than traditional art was in its day – a point stressed by the biennale itself.  	As modern and digital art becomes more and more an integral part of the art museum, curatorial practice,</text>
    <text>exceeds the capacity of the human curator alone. The artist’s own under-the-radar, ‘post-illustrated’ nature is a substitute for a more architecturally significant and curatorial sense of place. There is a danger that the serious curatorial interest in places dies with them, either due to neglect or because the arts have too many other things to do. But the crucial thing in the public sector is that it should support a cultural network that is accessible to as many people as possible. If it doesn’t, it’s not just a series of empty buildings.  	The Biennale’s current project is a response to the biennale syndrome, named after a 1972 book by Norman Brown that purported to explain the power of the biennale.1  The first chapter of Brown’s book, entitled ‘The Biennale Explosion’, exhorts a state of emergency, declaring that a ‘massive and sustained electronic and multimedia array has been installed in every major city in North America’2 and that this array is designed to monitor, disrupt and sometimes to intervene in ‘the cultural life of other countries’.3 The author takes the unprecedented step of naming the arrayed society that he has discovered is capable of creating ‘infrastructures of exchange and culture’.4 Brown makes a case for the necessity of a state of emergency to stem the tide of cultural perforations, pointing to the example of the Bengal Renaissance, which he depicts as a cautionary tale. ‘It was thought that a complete state of emergency would render the situation hopeless, but the very opposite has proved to be the case, for which the state has a great many critical mistakes to correct’.5 Brown’s assessment is supported by numerous studies that show that such emergency measures only amplify the power of the cultural collectives.6 One of the most widely cited studies on the</text>
    <text>exceeds the capacity of the human curator alone. The artist’s own under-the-radar, ‘post-illustrated’ nature is a substitute for a more architecturally significant and curatorial sense of place. There is a danger that the serious curatorial interest in places dies with them, either due to neglect or because the arts have too many other things to do. But the crucial thing in the public sector is that it should support a cultural network that is accessible to as many people as possible. If it doesn’t, it’s not just a series of empty buildings.  	The Biennale’s current project is a response to the biennale syndrome, named after a 1972 book by Norman Brown that purported to explain the power of the biennale.1  The first chapter of Brown’s book, entitled ‘The Biennale Explosion’, exhorts a state of emergency, declaring that a ‘massive and sustained electronic and multimedia array designed to monitor, record, analyze and disseminate event data … is being installed in every major American city, in part in response to a city-wide, four-year-old public-awareness campaign that has unleashed unprecedented governmental spending and transforming urban infrastructure projects into cultural spectacles of epic proportions …). ‘No city is immune from the ‘Biennale Episodic’ (he suggests ‘every city will be a theme park for the biennale …).’2 Brown concludes with a list of ‘40 cities that have hosted or are scheduled to host a biennale’, which he defines as ‘a later stage in the “episodic”” of urbanization.3 The term ‘biennale’ is frequently used synonymously with ‘episodic’. But Brown’s ‘biennale’ is in fact an ‘episodic</text>
    <text>exceeds the capacity of the human curator alone. The artist’s own under-the-radar, ‘post-illustrated’ nature is a substitute for a more architecturally significant and curatorial sense of place. There is a danger that the serious curatorial interest in places dies with them, either due to neglect or because the arts have too many other things to do. But the crucial thing in the public sector is that it should support a cultural network that is accessible to as many people as possible. If it doesn’t, it’s not just a series of empty buildings.  	The Biennale’s current project is a response to the biennale syndrome, named after a 1972 book by Norman Brown that purported to explain the power of the biennale.1  The first chapter of Brown’s book, entitled ‘The Biennale Explosion’, exhorts a state of emergency, declaring that a ‘massive and sustained electronic and multimedia array is being installed in every major American city’.2  Brown makes a strong case that the biennale syndrome is a creation of capitalism, and that its practitioners are part of a global network of art dealers, collectors, and art professionals who aim to capture every last drop of the art market.3  Brown begins his book by recounting a 1962 trip to Venice that he arranged through his office at the New York Times.  ‘I paid $10,000 for a copy of Every Building in Venice’ was the content of the Times, and the article was accompanied by a photograph of every building in the Biennale.4  Brown goes on to describe the visual poetry of the city, its vast monuments, and the empty streets that mark the places where the excesses of modern architecture have been planted. ‘Venice embodies the paradox of the modern urban landscape: conspicuous consumption is being promoted as a virtue while, at the same time,</text>
  </text>
  <text>
    propose a biennial based on AIB’s current research and development programmes, and the specific aims and priorities of the future work. 	[ii] See https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iii] See also https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iv] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[v] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[viii
    <text>propose a biennial based on AIB’s current research and development programmes, and the specific aims and priorities of the future work. 	[ii] See https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iii] See also https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iv] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[v] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[viii] See also https://www.biennialofsydney.com/2018/exhibition/ for information on the early stages of the Narrative. 	[ix] See also https://www.biennialofsydney.com/2018/exhibition/ for information on the city’s potential in the event of a ‘questioned identity’. 	[x] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xi] See also https://www.biennialofsydney.com/2018/exhibition/ for information on the work of the artist and his/her Agents. 	[xii] See also https://www.biennialofsydney.com/2018/exhibition/ for information on the artist and his/her recording artists. 	[xiii] See also https://www.biennialof</text>
    <text>propose a biennial based on AIB’s current research and development programmes, and the specific aims and priorities of the future work. 	[ii] See https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iii] See also https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iv] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[v] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[viii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[ix] See also https://www.biennialofsydney.com/2018/exhibition/ for more information.  1   Josiah Rapson, ‘Dolphin, Pearl, Sea Urchin: Natural Resources, Oil, and the Contemporary Crisis of the Gulf Coast’, in research in global ecology and sustainability, Vol. 17, No. 2, Spring, 2016, pp. 397–414. 2  Thomas Rogers, ‘On the Use of Tide in the Art of Building Materials and Building Supplies, with Special Reference to Conventional and Eco-Iso-9 Compounds’, in Research in Art and Industrial Design, Vol. 34, No. 1, Spring, 2016, pp. 1–34. 3  See  	http://www.biennial.org/2018</text>
    <text>propose a biennial based on AIB’s current research and development programmes, and the specific aims and priorities of the future work. 	[ii] See https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iii] See also https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iv] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[v] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[viii] See also http://www.biennialofsydney.com/2018/exhibition/ for more information. 	[ix] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[x] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xi] See also http://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xiii] See also http://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xiv] See also http://www.biennialofsydney.com/2018/exhibition/ for more</text>
    <text>propose a biennial based on AIB’s current research and development programmes, and the specific aims and priorities of the future work. 	[ii] See https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iii] See also https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iv] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[v] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[viii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[ix] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[x] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xii] See also http://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xiii] See also http://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xiv] See also http://www.biennialofsydney.com/2018/exhibition/ for more</text>
    <text>propose a biennial based on AIB’s current research and development programmes, and the specific aims and priorities of the future work. 	[ii] See https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iii] See also https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iv] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[v] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[viii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[ix] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[x] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xiii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xiv] See also https://www.biennialofsydney.com/2018/exhibition/ for more</text>
    <text>propose a biennial based on AIB’s current research and development programmes, and the specific aims and priorities of the future work. 	[ii] See https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iii] See also https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iv] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[v] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[viii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[ix] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[x] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xiii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xiv] See also https://www.biennialofsydney.com/2018/exhibition/ for more</text>
    <text>propose a biennial based on AIB’s current research and development programmes, and the specific aims and priorities of the future work. 	[ii] See https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iii] See also https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iv] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[v] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[viii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[ix] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[x] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xiii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xiv] See also https://www.biennialofsydney.com/2018/exhibition/ for more</text>
    <text>propose a biennial based on AIB’s current research and development programmes, and the specific aims and priorities of the future work. 	[ii] See https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iii] See also https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iv] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[v] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[viii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information on the exhibition being curated for the first time in Sydney. 	[ix] See also https://www.biennialofsydney.com/2018/exhibition/ for more information on the exhibition being curated for the first time in Sydney. 	[x] See also https://www.biennialofsydney.com/2018/exhibition/ for more information on the exhibition being curated for the first time in Sydney. 	[xi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information on the exhibition being curated for the first time in Sydney. 	[xii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information on the exhibition being curated for the first time in</text>
    <text>propose a biennial based on AIB’s current research and development programmes, and the specific aims and priorities of the future work. 	[ii] See https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iii] See also https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iv] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[v] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[viii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. Download this article as PDFAndrew GoffeyAndrew Goffey is Associate Professor in the Department of Communication Studies and the Faculty of Architecture and Design at the University of Alberta, and holds the Canada Research Chair in Cultural Studies. His work has addressed the social, economic, political and spatial dimensions of Canadian cultural capital since the late twentieth century. His work has been featured in numerous periodicals including: The Canadian Encyclopedia (1881–1986), Macmillan and Harper's (1937–1948), Macmillan and Newspaper Publishers International (1949–1972), Macmillan and Newspaper Publishers International (1973–78), Macmillan and Newspaper Publishers International (1979–86), and Macmillan and Newspaper Publishers International (1988–present). He is the recipient of several honours and multiple publications, including the Order of Canada in both print and electronic formats; the James</text>
    <text>propose a biennial based on AIB’s current research and development programmes, and the specific aims and priorities of the future work. 	[ii] See https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iii] See also https://www.biennialofsydney.com/2016/exhibition/ for more information. 	[iv] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[v] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[vii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[viii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[ix] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[x] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xi] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xiii] See also https://www.biennialofsydney.com/2018/exhibition/ for more information. 	[xiv] See also https://www.biennialofsydney.com/2018/exhibition/ for more</text>
  </text>
  <text>
    extends the proposition to machines, and creates a scenario in which machines could also have agency.[24]In other words, it’s not the level of abstraction that makes us think abstractions; rather, it involves the very idea of abstraction itself. This is one reason why abstraction is thought of as ‘anomaly’ and not just as a conceptual category. More and more, we encounter language that is thought of as abstract, as if language itself is thought of as somehow ‘arbitrary’ and ‘subjective’. This is one reason why languages tend to be indefinite, and why their languages contain ‘hidden’ meanings that we don’t detect.Another reason is that, as abstractions, languages also contain hidden processes of meaning construction. What this means in practice is that languages that contain grammars or documentifiers can have very different grammars or documentifiers from those that do not contain grammar or documentifiers, but they all have one thing in
    <text>extends the proposition to machines, and creates a scenario in which machines could also have agency.[24]In other words, it’s not the level of abstraction that makes us think abstractions; rather, it involves the very idea of abstraction itself. This is one reason why abstraction is thought of as ‘anomaly’ and not just as a conceptual category. More and more, we encounter language that is thought of as abstract, as if language itself is thought of as somehow ‘arbitrary’ and ‘subjective’. This is one reason why languages tend to be indefinite, and why their languages contain ‘hidden’ meanings that we don’t detect.Another reason is that, as abstractions, languages also contain hidden processes of meaning construction. What this means in practice is that languages that contain grammars or documentifiers can have very different grammars or documentifiers from those that do not contain grammar or documentifiers, but they all have one thing in common: they are syntactic rules that categorize objects according to syntactic rules. This is why, for example, the fact that English and German languages have separate word order in the English language – which is implied by the fact that they are considered to be different languages – has a conceptual effect that mimics the structural difference between languages. It allows us to understand why English and German languages have definite articles and indefinite articles, while no language has indefinite articles but definite words. This kind of ambiguity and difference of article structure is one reason why indefinite article structures are rare in language: it is very difficult to identify an article in both English and German as an indefinite article, while in many other languages the nouns and verbs do not have indefinite articles but indefinite pronouns. Another reason is that, unlike in other languages, the indefinite article doesn’t follow an absolute categorisation of nouns and verbs – although in indefinite languages, the categorisation of nouns and verbs does follow an absolute. For</text>
    <text>extends the proposition to machines, and creates a scenario in which machines could also have agency.[24]In other words, it’s not the level of abstraction that makes us think abstractions; rather, it involves the very idea of abstraction itself. This is one reason why abstraction is thought of as ‘anomaly’ and not just as a conceptual category. More and more, we encounter language that is thought of as abstract, as if language itself is thought of as somehow ‘arbitrary’ and ‘subjective’. This is one reason why languages tend to be indefinite, and why their languages contain ‘hidden’ meanings that we don’t detect.Another reason is that, as abstractions, languages also contain hidden processes of meaning construction. What this means in practice is that languages that contain grammars or documentifiers can have very different grammars or documentifiers from those that do not contain grammar or documentifiers, but they all have one thing in common: they are ‘word’ processes. A document that contains a sentence or list of items, such as an index or a chronology, has something like a grammar. A document that does not contains a sentence or a list of items has an informal or informal structure. A document that contains both a sentence and a list of items has a ‘system’ or documentifier that organises and integrates the information.[25]More and more, we all become linguists, reading the world through the things we say and do. Our job is to extract meaning from the world as it is expressed through language, and this requires us to become conscious of our linguistic agency. The language we speak has certain grammars that help us to organize and coordinate our thoughts – the world as it is constructed through language. But language also has certain grammars that can also be subject to change, subject to change, and arbitrary in their structure and interpretation. To fully understand and use language</text>
    <text>extends the proposition to machines, and creates a scenario in which machines could also have agency.[24]In other words, it’s not the level of abstraction that makes us think abstractions; rather, it involves the very idea of abstraction itself. This is one reason why abstraction is thought of as ‘anomaly’ and not just as a conceptual category. More and more, we encounter language that is thought of as abstract, as if language itself is thought of as somehow ‘arbitrary’ and ‘subjective’. This is one reason why languages tend to be indefinite, and why their languages contain ‘hidden’ meanings that we don’t detect.Another reason is that, as abstractions, languages also contain hidden processes of meaning construction. What this means in practice is that languages that contain grammars or documentifiers can have very different grammars or documentifiers from those that do not contain grammar or documentifiers, but they all have one thing in common: they are conceptual entities that serve as a basis for action. When you add a documentifier to a language, you create a ‘schema’ that maps documents from one language to another. So, the Schema that maps English to French becomes a ‘schema’ that maps English to hieroglyphs found in the writings of Mayan civilization. And the same is true of financial statements in a company. A documentifier like Google Translate adds documentifiers to a document, but it doesn’t make it any more or less likely that a person using that documentifier would understand what that person wrote. A document is just a collection of words, sentences and paragraphs – a document is just a set of symbols. So, the grammars or conceptual elements that make up a language also define what a language is, and what actions a language can take. And so, the idea of the abstract machine and its ability to do things differently than we can</text>
    <text>extends the proposition to machines, and creates a scenario in which machines could also have agency.[24]In other words, it’s not the level of abstraction that makes us think abstractions; rather, it involves the very idea of abstraction itself. This is one reason why abstraction is thought of as ‘anomaly’ and not just as a conceptual category. More and more, we encounter language that is thought of as abstract, as if language itself is thought of as somehow ‘arbitrary’ and ‘subjective’. This is one reason why languages tend to be indefinite, and why their languages contain ‘hidden’ meanings that we don’t detect.Another reason is that, as abstractions, languages also contain hidden processes of meaning construction. What this means in practice is that languages that contain grammars or documentifiers can have very different grammars or documentifiers from those that do not contain grammar or documentifiers, but they all have one thing in common: they are made up of tokens (symbolisms of grammars or hierarchies of grammars). A documentifier is a designator that indicates what kind of structure (symbolism) is being assembled. A grammar (or document) is a collection of tokens (rules for how to organize this structure). A document is a kind of assemblage of tokens – a kind of proto-symbol of what has been labelled ‘Type 1’. In other words, it’s made up of parts that come to us from different angles.But grammars and documentifiers don’t capture all aspects of how language is constructed.  The second aspect of this – the aspect that linguists are keen to emphasize – is the aspect that happens between the letter and the idea. If the letter doesn’t have a name, it’s made up of fragments of ideas that get coalesced into a bigger idea. The idea is that</text>
    <text>extends the proposition to machines, and creates a scenario in which machines could also have agency.[24]In other words, it’s not the level of abstraction that makes us think abstractions; rather, it involves the very idea of abstraction itself. This is one reason why abstraction is thought of as ‘anomaly’ and not just as a conceptual category. More and more, we encounter language that is thought of as abstract, as if language itself is thought of as somehow ‘arbitrary’ and ‘subjective’. This is one reason why languages tend to be indefinite, and why their languages contain ‘hidden’ meanings that we don’t detect.Another reason is that, as abstractions, languages also contain hidden processes of meaning construction. What this means in practice is that languages that contain grammars or documentifiers can have very different grammars or documentifiers from those that do not contain grammar or documentifiers, but they all have one thing in common: they’re syntactically linked. So a syntactic concept like ‘document’ or even ‘entity’ can have many different interpretations. For example, a document like ‘document text’ or ‘entity figure’ can have many different interpretations depending on the context. Languages also have documentifiers – nouns and verbs that refer to specific things or individuals: ‘document text’ refers to the text of a document, and ‘entity text’ refers to the text of an entity. So a document like ‘X’ can also refer to many different things or to many different things in different places. And this is one reason why, when we encounter an abstract concept like ‘language’, we tend to see a hidden process of semantic construction that goes beyond the conceptualisation of the thing or the individual as a whole.We tend to see language as a thing that categorises and classifies, but in reality</text>
    <text>extends the proposition to machines, and creates a scenario in which machines could also have agency.[24]In other words, it’s not the level of abstraction that makes us think abstractions; rather, it involves the very idea of abstraction itself. This is one reason why abstraction is thought of as ‘anomaly’ and not just as a conceptual category. More and more, we encounter language that is thought of as abstract, as if language itself is thought of as somehow ‘arbitrary’ and ‘subjective’. This is one reason why languages tend to be indefinite, and why their languages contain ‘hidden’ meanings that we don’t detect.Another reason is that, as abstractions, languages also contain hidden processes of meaning construction. What this means in practice is that languages that contain grammars or documentifiers can have very different grammars or documentifiers from those that do not contain grammar or documentifiers, but they all have one thing in common: they are bindings for things. If you take the example of an elephant in a park, the documentifier is the elephant, and the operator of the park is a programmer who adds text to the elephant’s trunk. The operation of the park is a software function that translates the natural world into code that can be executed by machines – a kind of recursive logic that operates on top of languages. The significance of this kind of recursive logic becomes clear when we think of the systems that underlie the Internet and mobile phones: the operating systems that underlie them and the databases that hold their data. If the operating system underlies the Internet and mobile phones, then the question becomes what kind of data can be stored on the Internet and/or in ‘cloud’. If you add texts to the elephant’s trunk, then you create a new species of wild animal called an elephant, and the operating system underlies that new species of animal. Now, what does the</text>
    <text>extends the proposition to machines, and creates a scenario in which machines could also have agency.[24]In other words, it’s not the level of abstraction that makes us think abstractions; rather, it involves the very idea of abstraction itself. This is one reason why abstraction is thought of as ‘anomaly’ and not just as a conceptual category. More and more, we encounter language that is thought of as abstract, as if language itself is thought of as somehow ‘arbitrary’ and ‘subjective’. This is one reason why languages tend to be indefinite, and why their languages contain ‘hidden’ meanings that we don’t detect.Another reason is that, as abstractions, languages also contain hidden processes of meaning construction. What this means in practice is that languages that contain grammars or documentifiers can have very different grammars or documentifiers from those that do not contain grammar or documentifiers, but they all have one thing in common – they encode grammars or documentifiers. So, a document that represents an idea in a particular way is a declaration about an idea. A document that’s interpreted that way relies on hidden mechanisms of language. Lexicons, ideograms and template documents are used to map ideas across different cultures. They are used to map concepts across different times and places. And they are used to map concepts across different nations – not to express a concept, but to identify what that concept is.[25]I have to admit that, as an anthropologist and aesthete, I find much of what I read about aesthetics and aesthetic theory being very interesting. And there are many ways of understanding beauty and beauty in the world. But I also read passages where the author proposes that we could start to read art and aesthetic theory as a ‘battle of concepts’, where ideas are used to justify or create categories and hierarchies. For example, I’m not sure that f</text>
    <text>extends the proposition to machines, and creates a scenario in which machines could also have agency.[24]In other words, it’s not the level of abstraction that makes us think abstractions; rather, it involves the very idea of abstraction itself. This is one reason why abstraction is thought of as ‘anomaly’ and not just as a conceptual category. More and more, we encounter language that is thought of as abstract, as if language itself is thought of as somehow ‘arbitrary’ and ‘subjective’. This is one reason why languages tend to be indefinite, and why their languages contain ‘hidden’ meanings that we don’t detect.Another reason is that, as abstractions, languages also contain hidden processes of meaning construction. What this means in practice is that languages that contain grammars or documentifiers can have very different grammars or documentifiers from those that do not contain grammar or documentifiers, but they all have one thing in common: they are conceptual entities that map to concrete objects (like objects or documents that we deliver to the office). For instance, the object that we attach to an HTML document like a tag has the same semantics as when we attach a text document like an image or a video to that document. This is why images and videos load differently when attached to HTML or XML documents: whereas an image is an image with semantics, an attachment has semantics that differ from document to document. Another example is when we attach audio or video files to HTML or XML documents, they have different semantics depending on the protocols of attachment. For instance, when we use the HTTP protocol to send an image or a video, the attachment typically has multimedia content alongside text that describes the attachment’s significance and attributes. This is why images and videos with embedded social sharing features are more likely to be malicious.Another reason for this dynamic is that, as the web evolves, so does its definition of who is a web user –</text>
    <text>extends the proposition to machines, and creates a scenario in which machines could also have agency.[24]In other words, it’s not the level of abstraction that makes us think abstractions; rather, it involves the very idea of abstraction itself. This is one reason why abstraction is thought of as ‘anomaly’ and not just as a conceptual category. More and more, we encounter language that is thought of as abstract, as if language itself is thought of as somehow ‘arbitrary’ and ‘subjective’. This is one reason why languages tend to be indefinite, and why their languages contain ‘hidden’ meanings that we don’t detect.Another reason is that, as abstractions, languages also contain hidden processes of meaning construction. What this means in practice is that languages that contain grammars or documentifiers can have very different grammars or documentifiers from those that do not contain grammar or documentifiers, but they all have one thing in common: they are documentatives. A document that describes the structure of an object or entity, such as a document describing an object’s attributes or the structure of its data, is a prototype grammars or foundation texts for what later on we might call ‘the Language of Trust’. A grammatical root, or root, such as ‘strict’ or ‘generic’ would describe what an object is, while a verb, such as ‘to be’ or ‘by a given verb’ would do the opposite. So, for instance, English and Chinese languages have different root words for ‘parsley’, ‘leek’ and ‘leek-toothed serpent’, but they all refer to the same thing: parsley. Parsley is a root word in Cantonese, but it has a different sense in English. English has a general ‘parsley�</text>
    <text>extends the proposition to machines, and creates a scenario in which machines could also have agency.[24]In other words, it’s not the level of abstraction that makes us think abstractions; rather, it involves the very idea of abstraction itself. This is one reason why abstraction is thought of as ‘anomaly’ and not just as a conceptual category. More and more, we encounter language that is thought of as abstract, as if language itself is thought of as somehow ‘arbitrary’ and ‘subjective’. This is one reason why languages tend to be indefinite, and why their languages contain ‘hidden’ meanings that we don’t detect.Another reason is that, as abstractions, languages also contain hidden processes of meaning construction. What this means in practice is that languages that contain grammars or documentifiers can have very different grammars or documentifiers from those that do not contain grammar or documentifiers, but they all have one thing in common: they’re 'hidden’ languages. And this is one reason why, as we learn more and more about languages, we’re fascinated by the hidden languages that we don’t know exist. We don’t have the power or the resources to go and look for them, or to change the world around us in any meaningful way. But we can create grammars or documentifiers that are more like language than we are. And this is what artists and linguists do. They create grammars or documentifiers that are objective and replaceational – languages with documentifiers that are more like music than prose. In effect, the grammars or documentifiers become autonomous agents that interpret what’s meant by what’s written in them. And this is what artists and linguists do. They create grammars or documentifiers that’ve as subjective as human beings are autonomous agents that can interpret meaning in ways that are equally</text>
  </text>
  <text>
    propose a biennial based on AI's current and future goals, objectives, and priorities? What are the major challenges, if any)? What are the topology map representations of the areas with the greatest potential for AI impact? What are the spatial and temporal characteristics of the problem space of Artificial Intelligence? What is the status of AI/Artificial Intelligence in infrastructure, society, and culture? What are the spatial and temporal configurations of the human/AI interface? What is the spatial and temporal heterogeneity of the human/AI interface? What is the potential collective impact of AI/Artificial Intelligence?  What are the challenges to the AI/Artificial Intelligence that are not currently addressed by existing infrastructure, society, or cultural frameworks? What are the opportunities, if any, for AI/Artificial Intelligence today?  What are the spatial and temporal characteristics of the problem space of Virtual and Augmented Reality? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI
    <text>propose a biennial based on AI's current and future goals, objectives, and priorities? What are the major challenges, if any)? What are the topology map representations of the areas with the greatest potential for AI impact? What are the spatial and temporal characteristics of the problem space of Artificial Intelligence? What is the status of AI/Artificial Intelligence in infrastructure, society, and culture? What are the spatial and temporal configurations of the human/AI interface? What is the spatial and temporal heterogeneity of the human/AI interface? What is the potential collective impact of AI/Artificial Intelligence?  What are the challenges to the AI/Artificial Intelligence that are not currently addressed by existing infrastructure, society, or cultural frameworks? What are the opportunities, if any, for AI/Artificial Intelligence today?  What are the spatial and temporal characteristics of the problem space of Virtual and Augmented Reality? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What is the potential collective impact of AI/Artificial Intelligence? 1   	Real-time communication and interaction is the field of a new kind of cultural production: it produces and contributes to social change through the production of shared knowledge. 	But what is the difference, in your current writing, between new media and social media? 	Real-time communication and interaction involves the coordination of information and data at a scale that is fluid and ever-alive. It is a technology that produces and contributes to social change through the production of shared knowledge. 	But what is the difference, in your current writing, between new media and cultural production? 	The answer to this is simple: in creating shared knowledge, cultural production relies on the non-linear combination of information and data. It does not necessarily rely on the adoption of a particular set of skills or knowledge. For that, cultural producers would need the support of a large scale academic and research community, which</text>
    <text>propose a biennial based on AI's current and future goals, objectives, and priorities? What are the major challenges, if any)? What are the topology map representations of the areas with the greatest potential for AI impact? What are the spatial and temporal characteristics of the problem space of Artificial Intelligence? What is the status of AI/Artificial Intelligence in infrastructure, society, and culture? What are the spatial and temporal configurations of the human/AI interface? What is the spatial and temporal heterogeneity of the human/AI interface? What is the potential collective impact of AI/Artificial Intelligence?  What are the challenges to the AI/Artificial Intelligence that are not currently addressed by existing infrastructure, society, or cultural frameworks? What are the opportunities, if any, for AI/Artificial Intelligence today?  What are the spatial and temporal characteristics of the problem space of Virtual and Augmented Reality? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence?  What are the spatial and temporal characteristics of the problem space of Media &amp; Culture? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence?  What are the spatial and temporal characteristics of the problem space of Media &amp; Culture? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence?  What are the spatial and temporal characteristics of the problem space of Media &amp; Culture? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence?  What are the spatial and temporal characteristics of the problem</text>
    <text>propose a biennial based on AI's current and future goals, objectives, and priorities? What are the major challenges, if any)? What are the topology map representations of the areas with the greatest potential for AI impact? What are the spatial and temporal characteristics of the problem space of Artificial Intelligence? What is the status of AI/Artificial Intelligence in infrastructure, society, and culture? What are the spatial and temporal configurations of the human/AI interface? What is the spatial and temporal heterogeneity of the human/AI interface? What is the potential collective impact of AI/Artificial Intelligence?  What are the challenges to the AI/Artificial Intelligence that are not currently addressed by existing infrastructure, society, or cultural frameworks? What are the opportunities, if any, for AI/Artificial Intelligence today?  What are the spatial and temporal characteristics of the problem space of Virtual and Augmented Reality? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the spatial and temporal heterogeneity of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence?  What are the spatial and temporal characteristics of the problem space of Media &amp; Media Theory? What are the spatial and temporal characteristics of the human/AI interface? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the spatial and temporal heterogeneity of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence?  What are the spatial and temporal characteristics of the problem space of Media &amp; Media Theory? What are the spatial and temporal characteristics of the human/AI interface? What are the spatial and temporal characteristics of the human/AI interface? What are the spatial and temporal characteristics of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence?  What are the spatial and temporal characteristics of the problem</text>
    <text>propose a biennial based on AI's current and future goals, objectives, and priorities? What are the major challenges, if any)? What are the topology map representations of the areas with the greatest potential for AI impact? What are the spatial and temporal characteristics of the problem space of Artificial Intelligence? What is the status of AI/Artificial Intelligence in infrastructure, society, and culture? What are the spatial and temporal configurations of the human/AI interface? What is the spatial and temporal heterogeneity of the human/AI interface? What is the potential collective impact of AI/Artificial Intelligence?  What are the challenges to the AI/Artificial Intelligence that are not currently addressed by existing infrastructure, society, or cultural frameworks? What are the opportunities, if any, for AI/Artificial Intelligence today?  What are the spatial and temporal characteristics of the problem space of Virtual and Augmented Reality? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the possible aggregated impacts of AI/Artificial Intelligence? What is the relationship between infrastructure, society, and culture?  What are the spatial and temporal characteristics of the problem space of Media &amp; Culture? What are the spatial and temporal variations of the human/AI interface? What are the cultural contexts of AI/Artificial Intelligence? What are the spatial and temporal configurations of the human/AI interface? What are the cultural contexts of AR/VR/MR?  What are the forms of cultural organization that could best be considered as ‘natural’, those that result from the interactions of humans with the physical world? What are the forms of cultural organization that are ‘technologically possible’, those that result from the interactions of machines? What are the forms of cultural organization that are possible that are not natural? What are the spatial and temporal variations of the human/AI interface? What are the forms of cultural organization that could best be considered as ‘</text>
    <text>propose a biennial based on AI's current and future goals, objectives, and priorities? What are the major challenges, if any)? What are the topology map representations of the areas with the greatest potential for AI impact? What are the spatial and temporal characteristics of the problem space of Artificial Intelligence? What is the status of AI/Artificial Intelligence in infrastructure, society, and culture? What are the spatial and temporal configurations of the human/AI interface? What is the spatial and temporal heterogeneity of the human/AI interface? What is the potential collective impact of AI/Artificial Intelligence?  What are the challenges to the AI/Artificial Intelligence that are not currently addressed by existing infrastructure, society, or cultural frameworks? What are the opportunities, if any, for AI/Artificial Intelligence today?  What are the spatial and temporal characteristics of the problem space of Virtual and Augmented Reality? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the spatial and temporal heterogeneity of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence?  What are the spatial and temporal characteristics of the problem space of Media and Games? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the spatial and temporal heterogeneity of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence? 1   	Human Potential, Inc. v. Human Potential, 517 U.S. 306, 315 –316 (1996). 2  	Raytheon Co. v. DuPont de Nemours &amp; Co., 475 U.S. 324, 328 –l 329 (1986). 3 	Idem, supra note 1. 4 	Humanities and Social Sciences Teachers' Assoc., 2016, June, http://www.hskta.org/</text>
    <text>propose a biennial based on AI's current and future goals, objectives, and priorities? What are the major challenges, if any)? What are the topology map representations of the areas with the greatest potential for AI impact? What are the spatial and temporal characteristics of the problem space of Artificial Intelligence? What is the status of AI/Artificial Intelligence in infrastructure, society, and culture? What are the spatial and temporal configurations of the human/AI interface? What is the spatial and temporal heterogeneity of the human/AI interface? What is the potential collective impact of AI/Artificial Intelligence?  What are the challenges to the AI/Artificial Intelligence that are not currently addressed by existing infrastructure, society, or cultural frameworks? What are the opportunities, if any, for AI/Artificial Intelligence today?  What are the spatial and temporal characteristics of the problem space of Virtual and Augmented Reality? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence?  What are the spatial and temporal characteristics of the problem space of Media &amp; Media Studies? What are the spatial and temporal characteristics of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface?  What are the spatial and temporal characteristics of the problem space of Film, TV &amp; Film-making? What are the spatial and temporal characteristics of the human/AI interface? What are the spatial and temporal characteristics of the human/AI interface? What are the dimensions of the interface, and how do they organize the perceived and actual world? What are the limits to the capacities of human perception and computation? What are the limitations to the capacities of AI/Artificial Intelligence? What are the opportunities, if any, for AI/Artificial Intelligence? What are the spatial and temporal characteristics of the problem space of Media &amp; Media</text>
    <text>propose a biennial based on AI's current and future goals, objectives, and priorities? What are the major challenges, if any)? What are the topology map representations of the areas with the greatest potential for AI impact? What are the spatial and temporal characteristics of the problem space of Artificial Intelligence? What is the status of AI/Artificial Intelligence in infrastructure, society, and culture? What are the spatial and temporal configurations of the human/AI interface? What is the spatial and temporal heterogeneity of the human/AI interface? What is the potential collective impact of AI/Artificial Intelligence?  What are the challenges to the AI/Artificial Intelligence that are not currently addressed by existing infrastructure, society, or cultural frameworks? What are the opportunities, if any, for AI/Artificial Intelligence today?  What are the spatial and temporal characteristics of the problem space of Virtual and Augmented Reality? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence?  What are the spatial and temporal characteristics of the problem space of Media &amp; Culture? What are the spatial and temporal variations of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence?  What are the spatial and temporal characteristics of the problem space of Media &amp; Culture? What are the spatial and temporal variations of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence? 1. System/programming languages:  	System programming is the use of language to implement complex algorithmic systems. It is the branch of programming concerned with ‘the machine’, and includes procedural and ‘programming’ languages as well as ‘mathematical operations’ such as addition, subtraction</text>
    <text>propose a biennial based on AI's current and future goals, objectives, and priorities? What are the major challenges, if any)? What are the topology map representations of the areas with the greatest potential for AI impact? What are the spatial and temporal characteristics of the problem space of Artificial Intelligence? What is the status of AI/Artificial Intelligence in infrastructure, society, and culture? What are the spatial and temporal configurations of the human/AI interface? What is the spatial and temporal heterogeneity of the human/AI interface? What is the potential collective impact of AI/Artificial Intelligence?  What are the challenges to the AI/Artificial Intelligence that are not currently addressed by existing infrastructure, society, or cultural frameworks? What are the opportunities, if any, for AI/Artificial Intelligence today?  What are the spatial and temporal characteristics of the problem space of Virtual and Augmented Reality? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence? 1  	How to think of art/design as a social enterprise. 	Real life is more relational and contextual, and thus more dynamic and transformational. It is a set of relations between people that is as real as art is made concrete. Social practice is the art of building relationships with people, groups, institutions, and cultures. It is an art of how to engage people in building projects that are more than symbolic or instrumental in their own right. It is an art of how to listen to and understand what others (including oneís own family) have to say in a way that oneís own family can, and to a degree that oneísself can. It is an art of how to negotiate difficult situations, and how to work constructively with those who are different from oneself, by understanding how others are able to coexist and flourish in ways that one cannot. 	 Sophie</text>
    <text>propose a biennial based on AI's current and future goals, objectives, and priorities? What are the major challenges, if any)? What are the topology map representations of the areas with the greatest potential for AI impact? What are the spatial and temporal characteristics of the problem space of Artificial Intelligence? What is the status of AI/Artificial Intelligence in infrastructure, society, and culture? What are the spatial and temporal configurations of the human/AI interface? What is the spatial and temporal heterogeneity of the human/AI interface? What is the potential collective impact of AI/Artificial Intelligence?  What are the challenges to the AI/Artificial Intelligence that are not currently addressed by existing infrastructure, society, or cultural frameworks? What are the opportunities, if any, for AI/Artificial Intelligence today?  What are the spatial and temporal characteristics of the problem space of Virtual and Augmented Reality? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence?  What are the spatial and temporal characteristics of the problem space of Public Health Information Technology (Pht)? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence today?  What are the spatial and temporal characteristics of the problem space of Transportation? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence today?  What are the spatial and temporal characteristics of the problem space of Travel? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence today?  What are the spatial and temporal characteristics</text>
    <text>propose a biennial based on AI's current and future goals, objectives, and priorities? What are the major challenges, if any)? What are the topology map representations of the areas with the greatest potential for AI impact? What are the spatial and temporal characteristics of the problem space of Artificial Intelligence? What is the status of AI/Artificial Intelligence in infrastructure, society, and culture? What are the spatial and temporal configurations of the human/AI interface? What is the spatial and temporal heterogeneity of the human/AI interface? What is the potential collective impact of AI/Artificial Intelligence?  What are the challenges to the AI/Artificial Intelligence that are not currently addressed by existing infrastructure, society, or cultural frameworks? What are the opportunities, if any, for AI/Artificial Intelligence today?  What are the spatial and temporal characteristics of the problem space of Virtual and Augmented Reality? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence?  What are the spatial and temporal characteristics of the problem space of Media and Entertainment? What are the spatial and temporal heterogeneity of the human/AI interface? What are the spatial and temporal configurations of the human/AI interface? What are the potential collective impacts of AI/Artificial Intelligence? 1  The question is: what is the relationship between biennials, the specific forms of cultural exchange between cultures, and the development of criticality? 2  Kirk Wallis,  	A Portrait of a Poet (1950), p. 9. 3  Thoreau, Op. Inhabitant, p. 23. 4  Thoreau, Op. Inhabitant, p. 22. 5  Thoreau, Op. Inhabitant, p. 23. 6  Thoreau, Op. Inhabitant, p. 25. 7  </text>
  </text>
  <text>
    machine learning experiments quickly became a necessity in the city, as the city grew larger and more populated. The rapid expansion of the city – the airport, the museums, the city streetscape – meant that the museum collections were often found in disuse or even in disrepair. The city needed places to house and display its art collections, and places to house and display its artefacts. As a result, the city came to hold great symbolic value for the world. The city became not only the site of the future exporter of cars and jets but also the site of the past exporter of trash and dangerous waste. To travel to the museum was to enter the museum, but also the site of the future site of the present – a site of the ongoing production of bad faith, where one might find the sculpture of a lynx, for example. The museum was designed to be a permanent home for its collections, and to the extent that these were maintained in museums, they became permanent homes for their collections.
    <text>machine learning experiments quickly became a necessity in the city, as the city grew larger and more populated. The rapid expansion of the city – the airport, the museums, the city streetscape – meant that the museum collections were often found in disuse or even in disrepair. The city needed places to house and display its art collections, and places to house and display its artefacts. As a result, the city came to hold great symbolic value for the world. The city became not only the site of the future exporter of cars and jets but also the site of the past exporter of trash and dangerous waste. To travel to the museum was to enter the museum, but also the site of the future site of the present – a site of the ongoing production of bad faith, where one might find the sculpture of a lynx, for example. The museum was designed to be a permanent home for its collections, and to the extent that these were maintained in museums, they became permanent homes for their collections.‘Art’, therefore, has a fundamentally different relationship to the museum today. Whereas the former held great cultural value – the museum was the ultimate arbiter of what was authentic and what was not, and therefore could determine when a work was made into a museum piece – the museum today relies on curators who project their collections across major cities in Asia, Africa and the Middle East to project a particular image of the city. Curators in this sense represent the logic of the museum as a producer, rather than the logic of the museum as a curator.‘The Modern Museum’, a relatively new discipline that emerged in the 1990s, takes this dynamic further. This is not, as we might think, the image of the museum as a curator in the making, but rather the image of the museum as an actor in the world. Modern museums have become actors not only in their own right but also in relation to other cultural institutions, and have developed forms of interaction with the</text>
    <text>machine learning experiments quickly became a necessity in the city, as the city grew larger and more populated. The rapid expansion of the city – the airport, the museums, the city streetscape – meant that the museum collections were often found in disuse or even in disrepair. The city needed places to house and display its art collections, and places to house and display its artefacts. As a result, the city came to hold great symbolic value for the world. The city became not only the site of the future exporter of cars and jets but also the site of the past exporter of trash and dangerous waste. To travel to the museum was to enter the museum, but also the site of the future site of the present – a site of the ongoing production of bad faith, where one might find the sculpture of a lynx, for example. The museum was designed to be a permanent home for its collections, and to the extent that these were maintained in museums, they became permanent homes for their collections.“ 	The museum collection has a property called the Museum Act, 1867. This was a law that, when inserted into British law, declared that all treaties with the Crown were to be vested in Parliament. So, the museum collection came to represent not just the legal rights of the British people, but also the legal rights of Parliament. The significance of this became evident in the passage of time. Every time a treaty with the Crown was ratified, the museum’s collection was required to be transferred to a new location. This transfer – which had legal force in British law – came to be known as the ‘Treaty Archive’. The transfer of the Treaty Archive to the museum collection came about as a consequence of a long-term study of the collection by the Zoological Society of Dublin, which had been started in 1864 and continued until the early twentieth century. At the time, the Zoological Society was undertaking a major survey of the collection, and the</text>
    <text>machine learning experiments quickly became a necessity in the city, as the city grew larger and more populated. The rapid expansion of the city – the airport, the museums, the city streetscape – meant that the museum collections were often found in disuse or even in disrepair. The city needed places to house and display its art collections, and places to house and display its artefacts. As a result, the city came to hold great symbolic value for the world. The city became not only the site of the future exporter of cars and jets but also the site of the past exporter of trash and dangerous waste. To travel to the museum was to enter the museum, but also the site of the future site of the present – a site of the ongoing production of bad faith, where one might find the sculpture of a lynx, for example. The museum was designed to be a permanent home for its collections, and to the extent that these were maintained in museums, they became permanent homes for their collections.  Whether this meant maintaining a permanent collection of art for the future museum, or maintaining a permanent collection of art for the museum, is another story. I can tell you that I believe that the collections for the future museum should be maintained in living memory, and that the collections for the present museum should be preserved as the museum itself organises them.  Whether or not this means maintaining the forms of the gallery as a museum, I believe that the museum should aspire to a future in which its collections finally become a living, evolving, transforming collective. A museum that can speak with a voice, and act with a body, and offer a site of encounters between people, places and events that can be shared, recorded, shared again and repeated. This is the future museum. I don’t think that means keeping a collection in a vault in the museum anymore, although this is still the case in some cities. Rather, the future museum seeks to build itself out of the lived encounters it has</text>
    <text>machine learning experiments quickly became a necessity in the city, as the city grew larger and more populated. The rapid expansion of the city – the airport, the museums, the city streetscape – meant that the museum collections were often found in disuse or even in disrepair. The city needed places to house and display its art collections, and places to house and display its artefacts. As a result, the city came to hold great symbolic value for the world. The city became not only the site of the future exporter of cars and jets but also the site of the past exporter of trash and dangerous waste. To travel to the museum was to enter the museum, but also the site of the future site of the present – a site of the ongoing production of bad faith, where one might find the sculpture of a lynx, for example. The museum was designed to be a permanent home for its collections, and to the extent that these were maintained in museums, they became permanent homes for their collections. The museum gradually became a permanent feature of the city, and now exhibits most of its collection, although its collection of artefacts – which can be seen to be expanding – is a very real museum. The museum is the site of the production of bad faith, and so it is the place of the microcosm of contemporary culture. It is the urban equivalent of a city park. The museum is a site of the production of space, and of habit. It exhibits the mode of habit as production on a grand scale. The museum is the site of habit, and so it is both the medium through which we encounter habit, and the object through which we can perforce encounter habit. The museum exhibits, or creates in its place. The museum becomes both the site and object of habit.The museum as production of space is the subject of John Syvret’s influential work Architecture of the Floating Camp, from 1956. Here, he identifies the essential characteristics of a modern museum: a</text>
    <text>machine learning experiments quickly became a necessity in the city, as the city grew larger and more populated. The rapid expansion of the city – the airport, the museums, the city streetscape – meant that the museum collections were often found in disuse or even in disrepair. The city needed places to house and display its art collections, and places to house and display its artefacts. As a result, the city came to hold great symbolic value for the world. The city became not only the site of the future exporter of cars and jets but also the site of the past exporter of trash and dangerous waste. To travel to the museum was to enter the museum, but also the site of the future site of the present – a site of the ongoing production of bad faith, where one might find the sculpture of a lynx, for example. The museum was designed to be a permanent home for its collections, and to the extent that these were maintained in museums, they became permanent homes for their collections.‘More’, 20th Century Fox announced in 1922. The announcement came as a complete surprise to the developer, who had anticipated it would be some years before a major museum structure was built in the city. The fact that the museum eventually became a massive 24-acre lot called Westfield Taipei, where many of its collections live today, is a testament to the museum’s power.Towards A Museum-Without Walls (originally published in 1922) contemplates an approach to cultural production that anticipates Walter Benjamin’s ideas of the ‘honest museum’ and the ‘true representative of the people’s representative in museums. It proposes two complementary models of the ‘museum-as-cinema’ – one based on the historical works found in museums, and the other on the archival collections kept in museums. The museum as an object of reception is understood as one that exploits the capacity</text>
    <text>machine learning experiments quickly became a necessity in the city, as the city grew larger and more populated. The rapid expansion of the city – the airport, the museums, the city streetscape – meant that the museum collections were often found in disuse or even in disrepair. The city needed places to house and display its art collections, and places to house and display its artefacts. As a result, the city came to hold great symbolic value for the world. The city became not only the site of the future exporter of cars and jets but also the site of the past exporter of trash and dangerous waste. To travel to the museum was to enter the museum, but also the site of the future site of the present – a site of the ongoing production of bad faith, where one might find the sculpture of a lynx, for example. The museum was designed to be a permanent home for its collections, and to the extent that these were maintained in museums, they became permanent homes for their collections. The museum became not only the permanent home for its collections but also the site of the production of artefacts, documents and cultural products, and to that extent it too became a site of the production of bad faith. The museum, by its own nominal curatorial gesture and through the enforced purchase of admission, sought to define the precise meaning of the present, and through this definition to influence the future use of the site. It is this last effect that has become most apparent at the Venice Biennale, where, since 2006, the Biennale has sought to expand both its exhibition and its own programming by way of a citywide creative class project. The project’s mandate is to inspire and engage with the city through ‘a new generation of artists, site-makers and designers who are committed to exploring the city’s extraordinary urban spaces and producing culturally responsive artworks that engage with the city’s past, present and future – a project that has been curatorial in</text>
    <text>machine learning experiments quickly became a necessity in the city, as the city grew larger and more populated. The rapid expansion of the city – the airport, the museums, the city streetscape – meant that the museum collections were often found in disuse or even in disrepair. The city needed places to house and display its art collections, and places to house and display its artefacts. As a result, the city came to hold great symbolic value for the world. The city became not only the site of the future exporter of cars and jets but also the site of the past exporter of trash and dangerous waste. To travel to the museum was to enter the museum, but also the site of the future site of the present – a site of the ongoing production of bad faith, where one might find the sculpture of a lynx, for example. The museum was designed to be a permanent home for its collections, and to the extent that these were maintained in museums, they became permanent homes for their collections. So the museum grew up in the contemporary condition, the museum as a transient site in which collections were continuously moved. The museum became a site of permanent collection homes, and the collection homes a site of permanent exhibition houses. As a result, the museum became a site of continuous exhibition-house habitation. This in itself was novel in a place like Liverpool, but the fact that all these elements were interrelated and interdependent made them truly global. The museum, as an entity, became a site of transnational commerce, and its collections a site of global distribution. These three elements were thought of as one in a long series of interdependent systems. This view of museums as global distribution points for cultural products was itself new, but it was also thought of as the antithesis of ‘museumism’, which was identitarian and global in character.’[7] The new museum was to be an institution of cultural exchange, not just a distribution centre for cultural products.</text>
    <text>machine learning experiments quickly became a necessity in the city, as the city grew larger and more populated. The rapid expansion of the city – the airport, the museums, the city streetscape – meant that the museum collections were often found in disuse or even in disrepair. The city needed places to house and display its art collections, and places to house and display its artefacts. As a result, the city came to hold great symbolic value for the world. The city became not only the site of the future exporter of cars and jets but also the site of the past exporter of trash and dangerous waste. To travel to the museum was to enter the museum, but also the site of the future site of the present – a site of the ongoing production of bad faith, where one might find the sculpture of a lynx, for example. The museum was designed to be a permanent home for its collections, and to the extent that these were maintained in museums, they became permanent homes for their collections. But this meant that the museum had to be accessible – accessible to the travelling public, who found museums a useful way to stay connected with the city. The museum was also limited in its operations – the museum needed to be continually reinvented and updated in order to stay relevant in the global marketplace. And so the museum was constantly evolving, always on the cusp of new collections or exhibitions, or both. This constant updating was accomplished in part by constantly new ideas and approaches being put forward by the curators, artists, writers and others who came to the museum. The museum was also always a creation of the curatorial agenda, but it is the biennial that has the final say. It is the curatorial agenda that proposes, designs and constructs the subject matter of the exhibition, and the museum as such. If the curatorial agenda doesn’t align with the actual collection and/or the collection as a whole, there is always the chance that the museum will be overwhelmed by new material</text>
    <text>machine learning experiments quickly became a necessity in the city, as the city grew larger and more populated. The rapid expansion of the city – the airport, the museums, the city streetscape – meant that the museum collections were often found in disuse or even in disrepair. The city needed places to house and display its art collections, and places to house and display its artefacts. As a result, the city came to hold great symbolic value for the world. The city became not only the site of the future exporter of cars and jets but also the site of the past exporter of trash and dangerous waste. To travel to the museum was to enter the museum, but also the site of the future site of the present – a site of the ongoing production of bad faith, where one might find the sculpture of a lynx, for example. The museum was designed to be a permanent home for its collections, and to the extent that these were maintained in museums, they became permanent homes for their collections.‘Great Work’ was a term used to describe this transformation. It was said to have become synonymous with the maintenance of historical values.[2]One might ask what might be thought of as ‘Great Work’ in the museum today. I offer the suggestion that one might contemplate a potential curatorial role for the museum, as a temporary home for curatorial pursuits. The suggestion is not so much for the museum to host curatorial events as for them to be frequent and accessible. The museum might host a biennale, or a traditional multiple exhibition, or a chap-silo, or a traditional public museum, or a virtual gallery. It might even host a virtual private club. I offer this as a service to its current membership, those who visit often but do not history write of them. The museum might curate a series of such events, inviting the curatorial team to work through a curated collection of artefacts from the museum, with or without photography. The</text>
    <text>machine learning experiments quickly became a necessity in the city, as the city grew larger and more populated. The rapid expansion of the city – the airport, the museums, the city streetscape – meant that the museum collections were often found in disuse or even in disrepair. The city needed places to house and display its art collections, and places to house and display its artefacts. As a result, the city came to hold great symbolic value for the world. The city became not only the site of the future exporter of cars and jets but also the site of the past exporter of trash and dangerous waste. To travel to the museum was to enter the museum, but also the site of the future site of the present – a site of the ongoing production of bad faith, where one might find the sculpture of a lynx, for example. The museum was designed to be a permanent home for its collections, and to the extent that these were maintained in museums, they became permanent homes for their collections.  	Looking back to my first year in college, I see a young woman at the end of the year writing a letter to the editor of a local newspaper. She has a notebook full of questions, some of which were not future-oriented, but are imbued with a sort of present-oriented urgency. She is writing from what she imagines to be the future, but is actually thinking about the effects of technological obsolescence. Questions surrounding the effects of Moore's Law on the environment and the media of ideas, and how these are affecting contemporary forms of writing and artistic production, are questions that are more present in the digital age than in the pre-computer era. 	In the letter, the woman describes her disenchantment with the media of ideas, which she sees as contributing to the decline of traditional notions of masculinity and femininity. She criticizes the media for their focus on images over substance, and emphasizes the fact that the images they produce are often used</text>
  </text>
  <text>
    extends the proposition to machines, and becomes what I would call a situationally aware culture. And this is where biopolitics comes in. It seems to me that one could make a similar argument about how cultural practices might intersect with new technological practices, and how our embodied movements might engage with these new intelligences. It could be argued that our current technological frameworks prevent us from seeing the world in these terms, and so we don’t really see cultural practices as constituting a part of this new technological terrain. It is still possible to engage with cultural practices through the technologies of art and commerce, but this is a position that is somewhat at odds with much of what I hear coming from techno-utopianism right now. I don’t want to undermine either the chances of cultural production through techno-utopianism or the very real chances of cultural activism through techno-utopianism, but I do think that there are ways of seeing cultural practices that are not reductive or limiting. I remember a
    <text>extends the proposition to machines, and becomes what I would call a situationally aware culture. And this is where biopolitics comes in. It seems to me that one could make a similar argument about how cultural practices might intersect with new technological practices, and how our embodied movements might engage with these new intelligences. It could be argued that our current technological frameworks prevent us from seeing the world in these terms, and so we don’t really see cultural practices as constituting a part of this new technological terrain. It is still possible to engage with cultural practices through the technologies of art and commerce, but this is a position that is somewhat at odds with much of what I hear coming from techno-utopianism right now. I don’t want to undermine either the chances of cultural production through techno-utopianism or the very real chances of cultural activism through techno-utopianism, but I do think that there are ways of seeing cultural practices that are not reductive or limiting. I remember a conversation with Sello Pesce while I was at Culina Academy in 2013. After the academy, we were asked to engage in a scavenger hunt for parts for a new building we were going to build. We found parts in a commons at an L.A. warehouse, and we started building ourselves. The thing is, we didn’t know how to use those parts. We built ourselves; we didn’t know how to repair the parts. We rebuilt the building from the ground up, and then we decided to share the parts with a community group, and that is how we got involved in the local art and cultural scene. The building we’re talking about is a mere 20 m (66 ft) away from an art gallery and the Dolores Mission, so there’s a huge community of people who use the building every day. There’s a lot of trust-building that needs to happen, because this is an entirely new way of organising</text>
    <text>extends the proposition to machines, and becomes what I would call a situationally aware culture. And this is where biopolitics comes in. It seems to me that one could make a similar argument about how cultural practices might intersect with new technological practices, and how our embodied movements might engage with these new intelligences. It could be argued that our current technological frameworks prevent us from seeing the world in these terms, and so we don’t really see cultural practices as constituting a part of this new technological terrain. It is still possible to engage with cultural practices through the technologies of art and commerce, but this is a position that is somewhat at odds with much of what I hear coming from techno-utopianism right now. I don’t want to undermine either the chances of cultural production through techno-utopianism or the very real chances of cultural activism through techno-utopianism, but I do think that there are ways of seeing cultural practices that are not reductive or limiting. I remember a conversation with Sello Pesce while we were both at the Transmissions Arts Centre in Liverpool recently. It was Pesce’s idea of ‘time to act’, which I think of as a key concept in the new cultural terrain being forged in the wake of biopolitics. It is the notion that in the age of the biopolitics, we must act decisively, meaningfully and immediately in the world around us in order to inhabit the new cultural terrain that is being forged for us, and to become what we are called to be: creators, enforcers of culture, etc. 	I think that this kind of proactive cultural multitasking is a necessity in the age of the biopolitics, and I wholeheartedly endorse what Sello Pesce said when he came out of the Biennial block in Barcelona. He said that he had to leave the biennial because it was becoming increasingly difficult to maintain the integrity of the art-historical and archival</text>
    <text>extends the proposition to machines, and becomes what I would call a situationally aware culture. And this is where biopolitics comes in. It seems to me that one could make a similar argument about how cultural practices might intersect with new technological practices, and how our embodied movements might engage with these new intelligences. It could be argued that our current technological frameworks prevent us from seeing the world in these terms, and so we don’t really see cultural practices as constituting a part of this new technological terrain. It is still possible to engage with cultural practices through the technologies of art and commerce, but this is a position that is somewhat at odds with much of what I hear coming from techno-utopianism right now. I don’t want to undermine either the chances of cultural production through techno-utopianism or the very real chances of cultural activism through techno-utopianism, but I do think that there are ways of seeing cultural practices that are not reductive or limiting. I remember a lecture by Gwangju University's Professor of Geography, Professor Chun-We’ng Kang, who was arguing for the re-constituting of Gwangju as Gwangju’s own historical narrative. He was arguing that Gwangju’s colonial past was still very fresh in people’s minds, and so the future city was a logical extension of the colonial past, and so Gwangju’s current architectural format is an attempt to re-imagine the historic architectural form in the image of the city. Gwangju’s futuristic city comes as no surprise to artists who are trained as cultural geographers or urbanists, since these are the practitioners who are most likely to be drawn to the exciting possibilities of technological creativity inherent in new spatial arrangements. The argument that cultural practices are here to stay is a predicament that is bound to generate considerable contention, and so it is that I find myself agreeing with a great deal of what I perceive</text>
    <text>extends the proposition to machines, and becomes what I would call a situationally aware culture. And this is where biopolitics comes in. It seems to me that one could make a similar argument about how cultural practices might intersect with new technological practices, and how our embodied movements might engage with these new intelligences. It could be argued that our current technological frameworks prevent us from seeing the world in these terms, and so we don’t really see cultural practices as constituting a part of this new technological terrain. It is still possible to engage with cultural practices through the technologies of art and commerce, but this is a position that is somewhat at odds with much of what I hear coming from techno-utopianism right now. I don’t want to undermine either the chances of cultural production through techno-utopianism or the very real chances of cultural activism through techno-utopianism, but I do think that there are ways of seeing cultural practices that are not reductive or limiting. I remember a conversation with a user-activist in which she argued that we should all ‘fight the good fight’, and so on. But she was arguing against an industry that was already in a mode of increasingly complex and invasive ways of determining what behaviour is ‘acceptable’ within its ecosystem, and so we don’t really have a place for such modes of cultural production within this context. I argued that we don’t really have a place for this kind of cultural regulation either, because it is already part of the system we have built, and so it falls outside of our cognizance, and so on. But this conversation, while it may have taken place in the past, is no longer possible in the abstract, because new technological paradigms and their attendant bureaucracies are the politics of the complex, with ever more invasive and direct forms of regulation looming ever larger in the background. So new political forms and practices are evolving all the time, and so the</text>
    <text>extends the proposition to machines, and becomes what I would call a situationally aware culture. And this is where biopolitics comes in. It seems to me that one could make a similar argument about how cultural practices might intersect with new technological practices, and how our embodied movements might engage with these new intelligences. It could be argued that our current technological frameworks prevent us from seeing the world in these terms, and so we don’t really see cultural practices as constituting a part of this new technological terrain. It is still possible to engage with cultural practices through the technologies of art and commerce, but this is a position that is somewhat at odds with much of what I hear coming from techno-utopianism right now. I don’t want to undermine either the chances of cultural production through techno-utopianism or the very real chances of cultural activism through techno-utopianism, but I do think that there are ways of seeing cultural practices that are not reductive or limiting. I remember a conversation with a few friends recently about The Social Network. They had just finished Breaking Bad, and were talking about the ending. They had all sort of lined themselves around the final syllable of the statement, and were struggling to come up with a synonym for the word ‘episodic’ that symbolises what they were feeling about the world. And so the break, the moment where the narrative ends and the actions begin, and in a very real way, is the moment at which biopolitics is most visibly felt. It is the moment when we actually make and demonstrate the change we wish to see in the world.When I started The Resident, 10 years ago, I tried to tell a straightforward narrative about the life of a Bosnian-British artist living in New York during the 1990s. I didn’t want to be taken seriously, and so I told a story in which the human element was marginalised, while the technical wizardry and social media magic that</text>
    <text>extends the proposition to machines, and becomes what I would call a situationally aware culture. And this is where biopolitics comes in. It seems to me that one could make a similar argument about how cultural practices might intersect with new technological practices, and how our embodied movements might engage with these new intelligences. It could be argued that our current technological frameworks prevent us from seeing the world in these terms, and so we don’t really see cultural practices as constituting a part of this new technological terrain. It is still possible to engage with cultural practices through the technologies of art and commerce, but this is a position that is somewhat at odds with much of what I hear coming from techno-utopianism right now. I don’t want to undermine either the chances of cultural production through techno-utopianism or the very real chances of cultural activism through techno-utopianism, but I do think that there are ways of seeing cultural practices that are not reductive or limiting. I remember a conversation with Philippe RahmJade Eco Park, 2012-2015, Miami Beach, Florida

Hi Marc, Thanks for doing this, it was good to have the opportunity to talk to you in person. I’m going to be looking into the possibility of a biopolitics in the city, specifically in Miami Beach, and exploring the ways in which the park can be used as a location for cultural activities. I don’t yet know what that means, but I hope it includes things that are ‘deepened’ by the city’. When I think of biopolitics, I see many different possibilities, and I imagine a kind of ‘black box’ that captures and distorts data on a regular basis. It’s difficult to define exactly what this would mean in concrete terms, but I’d like to think of it as a kind of geopolitical and social media technology that captures and distorts data in ways that are subtle and</text>
    <text>extends the proposition to machines, and becomes what I would call a situationally aware culture. And this is where biopolitics comes in. It seems to me that one could make a similar argument about how cultural practices might intersect with new technological practices, and how our embodied movements might engage with these new intelligences. It could be argued that our current technological frameworks prevent us from seeing the world in these terms, and so we don’t really see cultural practices as constituting a part of this new technological terrain. It is still possible to engage with cultural practices through the technologies of art and commerce, but this is a position that is somewhat at odds with much of what I hear coming from techno-utopianism right now. I don’t want to undermine either the chances of cultural production through techno-utopianism or the very real chances of cultural activism through techno-utopianism, but I do think that there are ways of seeing cultural practices that are not reductive or limiting. I remember a conversation with Sello Peso, who is the artistic director of Sello Peso, a cultural center in the Torquemada neighborhood of Buenos Aires. He pointed out that Peso was founded on the back of a tobacco plantation, and as such the site of the center was a place where white supremacy was deeply embedded. The emergence of a biopolitical arts center in Buenos Aires, in an area with significant parts of the Carribean Peninsula in Southern Argentina, might in part be related to the country’s 2000–2001 hyperinflation, which created an especially ripe market for tobacco products in the aftermath of the economic crisis. This hyperinflation came at a critical juncture for Argentina: between 2001 and 2008 the country experienced hyperinflation, hyperinflation-17, and a banking crisis that resulted in the establishment of a sovereign wealth fund. In a city like Buenos Aires, which is larger than New York and London combined, and which has a per capita GDP roughly</text>
    <text>extends the proposition to machines, and becomes what I would call a situationally aware culture. And this is where biopolitics comes in. It seems to me that one could make a similar argument about how cultural practices might intersect with new technological practices, and how our embodied movements might engage with these new intelligences. It could be argued that our current technological frameworks prevent us from seeing the world in these terms, and so we don’t really see cultural practices as constituting a part of this new technological terrain. It is still possible to engage with cultural practices through the technologies of art and commerce, but this is a position that is somewhat at odds with much of what I hear coming from techno-utopianism right now. I don’t want to undermine either the chances of cultural production through techno-utopianism or the very real chances of cultural activism through techno-utopianism, but I do think that there are ways of seeing cultural practices that are not reductive or limiting. I remember a conversation with Lars Bang Larsen, who was one of the inventors of the internet and is now a professor of computer science at the Royal College of Art. He was coming out of the silo that he had been maintaining as a defensive measure against what he called ‘black swans’ – large-scale technological threats. Bang Larsen argued that the idea of the internet as a public space – which is partly what created the internet in the first place – was profoundly anti-democratic, and that the internet itself is subject to the whims of numerous private companies, government agencies and economic actors. And so the internet was made, and is still made, available to all, but it is made available to only a tiny elite of a few hundred users. And that elite of a few hundred users is the very elite of the internet, and so in this sense, the internet is a highly political space. It is possible to access the internet through a paywall, but that is strictly the</text>
    <text>extends the proposition to machines, and becomes what I would call a situationally aware culture. And this is where biopolitics comes in. It seems to me that one could make a similar argument about how cultural practices might intersect with new technological practices, and how our embodied movements might engage with these new intelligences. It could be argued that our current technological frameworks prevent us from seeing the world in these terms, and so we don’t really see cultural practices as constituting a part of this new technological terrain. It is still possible to engage with cultural practices through the technologies of art and commerce, but this is a position that is somewhat at odds with much of what I hear coming from techno-utopianism right now. I don’t want to undermine either the chances of cultural production through techno-utopianism or the very real chances of cultural activism through techno-utopianism, but I do think that there are ways of seeing cultural practices that are not reductive or limiting. I remember a lecture by Wendy Chun that I gave a few years back in Liverpool, in which she focused particularly on how art practices in the city were often accused of being liberal and progressive, but her talk was interrupted by protestors demonstrating outside the institution where her talk was taking place. They were demonstrating because they didn’t like what they perceived to be the neo-liberal transformation of the city, and they perceived a conflict between capitalist growth and social transformation. In Liverpool, the cultural sector is often accused of being apolitical, but I felt that Wendy’s talk fell short in that it did not sufficiently consider the political nature of the city's social life. The way in which culture produces social change is deeply political, and so is the case with Black Lives Matter, but it is also true that the political nature of the project itself could be considered apolitical. In the end, it is the art practices that are political; they are the culturally contested projects that are at the core</text>
    <text>extends the proposition to machines, and becomes what I would call a situationally aware culture. And this is where biopolitics comes in. It seems to me that one could make a similar argument about how cultural practices might intersect with new technological practices, and how our embodied movements might engage with these new intelligences. It could be argued that our current technological frameworks prevent us from seeing the world in these terms, and so we don’t really see cultural practices as constituting a part of this new technological terrain. It is still possible to engage with cultural practices through the technologies of art and commerce, but this is a position that is somewhat at odds with much of what I hear coming from techno-utopianism right now. I don’t want to undermine either the chances of cultural production through techno-utopianism or the very real chances of cultural activism through techno-utopianism, but I do think that there are ways of seeing cultural practices that are not reductive or limiting. I remember a conversation with a social anthropologist when we were both at L.A. for the Venice Biennale, and he was arguing with me about the futility of trying to engage with cultural practices through the mediation of science, technology and economic development. He was arguing that we don’t have a concept for these kinds of possibilities yet, which are really the opposite of what I had been trying to accomplish through the narrative of ecological destruction. I said that I didn’t agree with that, and I felt that my work as an artist and the way in which I approached my art was more sensitive to the social implications of certain cultural practices. I think that my art was made with a social conscience, and that I was trying to articulate the social logic of certain practices. He was more of a distance, and he felt that my social work could be done without this kind of sophistication.But I also think that my art was also made with the expectation that there would be a social cost</text>
  </text>
  <text>
    extends the proposition to machines, and thereby to life itself. This is a proposition that can no longer be dismissed as not being worth pursuing. The promise of a cheaper, more efficient form of manufacture is now too late for the displaced sectors, displaced workers and communities. And though some supporters of the project may relish the thought of replacing human workers with artificially intelligent software, the cost and complexity of such a project are overwhelming. The ability to edit, monitor, extract data and do much more is an entirely different proposition. More fundamentally, the ability to replace humans with algorithms is what ultimately defines artificial intelligence, and rightly so.Though many in the AI community are drawn from MIT or Stanford backgrounds, many come from other top universities, or are recent PhD graduates. In the case of DeepMind, the researchers are from MIT and Stanford, and in AI, from MIT and MIT. Assembling diverse disciplines into harmonious settings is a top priority for AI researchers, and a frequent target of misunderstanding and fear mong
    <text>extends the proposition to machines, and thereby to life itself. This is a proposition that can no longer be dismissed as not being worth pursuing. The promise of a cheaper, more efficient form of manufacture is now too late for the displaced sectors, displaced workers and communities. And though some supporters of the project may relish the thought of replacing human workers with artificially intelligent software, the cost and complexity of such a project are overwhelming. The ability to edit, monitor, extract data and do much more is an entirely different proposition. More fundamentally, the ability to replace humans with algorithms is what ultimately defines artificial intelligence, and rightly so.Though many in the AI community are drawn from MIT or Stanford backgrounds, many come from other top universities, or are recent PhD graduates. In the case of DeepMind, the researchers are from MIT and Stanford, and in AI, from MIT and MIT. Assembling diverse disciplines into harmonious settings is a top priority for AI researchers, and a frequent target of misunderstanding and fear mongering by policymakers and the media. DeepMind’s research agenda includes developing ways to map the human microbiome – the trillions of bacteria and archaea living in and on our bodies – to detect diseases, and to create diagnostic tests for such diseases. The question of how to engineer microbes to do this, and how to monitor and diagnose them, is where much of the confusion and irrational exuberance is generated. The reality is that creating microbes to do exactly that – to scan the microbial environments of the body, and to diagnose and treat diseases – is an incredibly challenging challenge, and there is simply no way to sequence the bacterial genome or build a bioinformatics platform on which to do this.The abundance of genomic sequence data, coupled with the rapid advancement of gene-editing technologies, makes it possible to create organisms that can be programmed with specific traits or programmed with specific behaviors. For example, creating microbes to monitor our breath allows us to create biomarkers that can be used to</text>
    <text>extends the proposition to machines, and thereby to life itself. This is a proposition that can no longer be dismissed as not being worth pursuing. The promise of a cheaper, more efficient form of manufacture is now too late for the displaced sectors, displaced workers and communities. And though some supporters of the project may relish the thought of replacing human workers with artificially intelligent software, the cost and complexity of such a project are overwhelming. The ability to edit, monitor, extract data and do much more is an entirely different proposition. More fundamentally, the ability to replace humans with algorithms is what ultimately defines artificial intelligence, and rightly so.Though many in the AI community are drawn from MIT or Stanford backgrounds, many come from other top universities, or are recent PhD graduates. In the case of DeepMind, the researchers are from MIT and Stanford, and in AI, from MIT and MIT. Assembling diverse disciplines into harmonious settings is a top priority for AI researchers, and a frequent target of misunderstanding and fear mongering by those who would try to put AI technology in control. The reality is that AI is far more capable than many people give credit for being. The Human Genome Project is just one example of many projects that were started before the advent of social media or the internet that have since advanced massively. Almost all of these advances were made possible by the development of massive data-storage and processing platforms that make it possible to sequence and analyse large amounts of data, and to understand much more than just the parts of a living thing. The question is, can AI really do all this and not be restricted by the constraints of biology? This kind of thinking runs counter to everything we think we know about how the world works, and could even lead to a kind of totalitarian government. But the more we struggle to see the world through the neural aperture of its capabilities, the more AI projects look like they're going to blow the world away.This kind of thinking runs counter to everything we think</text>
    <text>extends the proposition to machines, and thereby to life itself. This is a proposition that can no longer be dismissed as not being worth pursuing. The promise of a cheaper, more efficient form of manufacture is now too late for the displaced sectors, displaced workers and communities. And though some supporters of the project may relish the thought of replacing human workers with artificially intelligent software, the cost and complexity of such a project are overwhelming. The ability to edit, monitor, extract data and do much more is an entirely different proposition. More fundamentally, the ability to replace humans with algorithms is what ultimately defines artificial intelligence, and rightly so.Though many in the AI community are drawn from MIT or Stanford backgrounds, many come from other top universities, or are recent PhD graduates. In the case of DeepMind, the researchers are from MIT and Stanford, and in AI, from MIT and MIT. Assembling diverse disciplines into harmonious settings is a top priority for AI researchers, and a frequent target of misunderstanding and fear mongering by policymakers and the media. Although the field of artificial intelligence has advanced enormously in the past fifteen years, the way in which researchers communicate and think about the technology remains fundamentally flawed. Research and development institutions need to work together to address misunderstandings and come to understandings of new technologies, but they also need to be allowed to innovate and create meaning and value in ways that are deeper and more complex than a few years of research and development. A sense of the future needs to be allowed to grow for AI researchers, so that they can create meaning and value in ways that are more readily conveyable outside of the lab.This is what research-based AI means today. It means that researchers and engineers don’t have a project to carry them through or a clear understanding of what the technology will mean a generation from now. They need to invent their own meaning and purpose in the future, and then find a way to implement that meaning and value in ways that are more easily conveyable outside</text>
    <text>extends the proposition to machines, and thereby to life itself. This is a proposition that can no longer be dismissed as not being worth pursuing. The promise of a cheaper, more efficient form of manufacture is now too late for the displaced sectors, displaced workers and communities. And though some supporters of the project may relish the thought of replacing human workers with artificially intelligent software, the cost and complexity of such a project are overwhelming. The ability to edit, monitor, extract data and do much more is an entirely different proposition. More fundamentally, the ability to replace humans with algorithms is what ultimately defines artificial intelligence, and rightly so.Though many in the AI community are drawn from MIT or Stanford backgrounds, many come from other top universities, or are recent PhD graduates. In the case of DeepMind, the researchers are from MIT and Stanford, and in AI, from MIT and MIT. Assembling diverse disciplines into harmonious settings is a top priority for AI researchers, and a frequent target of misunderstanding and fear mongering by policymakers, media and public alike. Though this may be the case in some contexts, this is fundamentally at odds with the dynamic and dynamic relationship that is the basis for AI today. The political and economic systems that are based on the premise that AI is technology’s answer to a crisis of ambiguity and uncertainty, and that its capabilities are limitless, are themselves often deeply flawed and insecure. The AI/Deep Learning community has made great strides in developing tools and systems to parse ambiguity and uncertainty into useful patterns, but much more needs to be done. Much more needs to be said about the politics and ideologies that fuel the AI/Deep Learning community’s paranoia and misunderstanding, and how such systems are actually constructed and constructed. More needs to be said about the politics of language and the politics of AI. More needs to be said about the politics of media. More needs to be said about the politics of habit and the politics of AI. These are but a few among many things that</text>
    <text>extends the proposition to machines, and thereby to life itself. This is a proposition that can no longer be dismissed as not being worth pursuing. The promise of a cheaper, more efficient form of manufacture is now too late for the displaced sectors, displaced workers and communities. And though some supporters of the project may relish the thought of replacing human workers with artificially intelligent software, the cost and complexity of such a project are overwhelming. The ability to edit, monitor, extract data and do much more is an entirely different proposition. More fundamentally, the ability to replace humans with algorithms is what ultimately defines artificial intelligence, and rightly so.Though many in the AI community are drawn from MIT or Stanford backgrounds, many come from other top universities, or are recent PhD graduates. In the case of DeepMind, the researchers are from MIT and Stanford, and in AI, from MIT and MIT. Assembling diverse disciplines into harmonious settings is a top priority for AI researchers, and a frequent target of misunderstanding and fear mongering by policymakers, media and academic institutions. Though AI research is undergoing what Stanford computer scientist Deepak Chopra has dubbed the ‘Big Crunch’, the real crunch is yet to come. One can only imagine what this would look like.Deep learning is the branch of artificial intelligence research that deals with the mechanisms that process data and generate new AI algorithms. It involves the construction of networks of interconnected computer chips that can learn to do things with data, such as recognising patterns and colours, and building recognisable recognisable models of the world. The chips can then be fitted together to produce robots with certain behaviours or ‘thinking’, based on the results of this processing. For example, the DeepBlue computer programme that won the 1997 Jeopardy! championship harnessed data from thousands of radio stations to produce artificial neural networks that correctly answered questions about the game – a feat that humans have only just begun to tackle.The promise of AI is that it can</text>
    <text>extends the proposition to machines, and thereby to life itself. This is a proposition that can no longer be dismissed as not being worth pursuing. The promise of a cheaper, more efficient form of manufacture is now too late for the displaced sectors, displaced workers and communities. And though some supporters of the project may relish the thought of replacing human workers with artificially intelligent software, the cost and complexity of such a project are overwhelming. The ability to edit, monitor, extract data and do much more is an entirely different proposition. More fundamentally, the ability to replace humans with algorithms is what ultimately defines artificial intelligence, and rightly so.Though many in the AI community are drawn from MIT or Stanford backgrounds, many come from other top universities, or are recent PhD graduates. In the case of DeepMind, the researchers are from MIT and Stanford, and in AI, from MIT and MIT. Assembling diverse disciplines into harmonious settings is a top priority for AI researchers, and a frequent target of misunderstanding and fear mongering by policymakers, media and public alike. The Cambridge Biennial saw its opening curatorial intervention when art dealers and collectors Phillipa Soane and Robert Wood were both displaced by activists intent on scaring away investors and the business community away from investing in art. The toxic combination of insufficient funding, a damaging art market and a community of activists calling for a socially destructive technology meant that the museum and not the technology was eventually seen as the rightful home.The AI explosion has changed all of this, and it is now common for activists to suggest that the art world should step up and host an in-gallery discussion about the future of art, in order to demonstrate how the art form can be repurposed. The Cambridge Biennial confirmed this desire by organising such a forum in their exhibition catalogue, but instead of engaging with this as a display of contemporary art, the exhibition used the occasion to call for a socially destructive technology to replace all forms of art and the art itself. The event was an inspiration</text>
    <text>extends the proposition to machines, and thereby to life itself. This is a proposition that can no longer be dismissed as not being worth pursuing. The promise of a cheaper, more efficient form of manufacture is now too late for the displaced sectors, displaced workers and communities. And though some supporters of the project may relish the thought of replacing human workers with artificially intelligent software, the cost and complexity of such a project are overwhelming. The ability to edit, monitor, extract data and do much more is an entirely different proposition. More fundamentally, the ability to replace humans with algorithms is what ultimately defines artificial intelligence, and rightly so.Though many in the AI community are drawn from MIT or Stanford backgrounds, many come from other top universities, or are recent PhD graduates. In the case of DeepMind, the researchers are from MIT and Stanford, and in AI, from MIT and MIT. Assembling diverse disciplines into harmonious settings is a top priority for AI researchers, and a frequent target of misunderstanding and fear mongering. In this regard, the researchers at DeepMind are no different from anyone else undertaking similar research projects. The key difference is that their research projects are autonomous, not in the traditional sense of restricted by authority or political will, but in the sense that the systems they build are self-organising and autonomous. This means that the systems do not have to comply with established political or legal frameworks or be bonded together by institutional power structures. This is an important distinction to make, and one that many in the AI community struggle with. For example, the research projects at DeepMind are federally funded, but their autonomy and political will often conflict with the researchers' own institutional priorities. This conflict of interest is deeply embedded in the research agenda, and sometimes manifests in different ways. For example, when designing the language of the human microbiome, the team at DeepMind struggled with the difficult task of reconciling the competing needs of the research team, the public, and of course, the language with the</text>
    <text>extends the proposition to machines, and thereby to life itself. This is a proposition that can no longer be dismissed as not being worth pursuing. The promise of a cheaper, more efficient form of manufacture is now too late for the displaced sectors, displaced workers and communities. And though some supporters of the project may relish the thought of replacing human workers with artificially intelligent software, the cost and complexity of such a project are overwhelming. The ability to edit, monitor, extract data and do much more is an entirely different proposition. More fundamentally, the ability to replace humans with algorithms is what ultimately defines artificial intelligence, and rightly so.Though many in the AI community are drawn from MIT or Stanford backgrounds, many come from other top universities, or are recent PhD graduates. In the case of DeepMind, the researchers are from MIT and Stanford, and in AI, from MIT and MIT. Assembling diverse disciplines into harmonious settings is a top priority for AI researchers, and a frequent target of misunderstanding and fear mongering. The reality is that AI is far more insidious and pervasive than this. The field of AI must be reckoned with in its own right, and the very notion of an 'AI' that is safe, beneficial or even desirable is anything but clear. Building software that helps machines understand how to behave in a particular way is just one aspect of a much broader agenda that seeks to engineer everything from the fabric of reality to resist natural disasters to digest human foodstuffs. Assembling diverse scientific, legal and political views on the implications of this new form of AI for human society is another imperative.Though the vision of a smooth interface, where user-friendly commands and clear, accessible data are the focus of efforts, is appealing to users, it is a form of ignorance that must be confronted and rejected. The challenge now becomes how to fabricate a case for such a vision that is compelling in the context of a globalised, multi-billion-dollar economy. There is no doubt that the</text>
    <text>extends the proposition to machines, and thereby to life itself. This is a proposition that can no longer be dismissed as not being worth pursuing. The promise of a cheaper, more efficient form of manufacture is now too late for the displaced sectors, displaced workers and communities. And though some supporters of the project may relish the thought of replacing human workers with artificially intelligent software, the cost and complexity of such a project are overwhelming. The ability to edit, monitor, extract data and do much more is an entirely different proposition. More fundamentally, the ability to replace humans with algorithms is what ultimately defines artificial intelligence, and rightly so.Though many in the AI community are drawn from MIT or Stanford backgrounds, many come from other top universities, or are recent PhD graduates. In the case of DeepMind, the researchers are from MIT and Stanford, and in AI, from MIT and MIT. Assembling diverse disciplines into harmonious settings is a top priority for AI researchers, and a frequent target of misunderstanding and fear mongering by policymakers, media and public alike. In this context, the decision to launch an AI project and its participants in no way implies an endorsement of either the political or the academic milieu in which the project is conducted. Rather, the researchers and technologists making these technologies are thinking about how to bring these technologies to bear in challenging existing and future geopolitical situations, and in the case of HAL, in imagining new possibilities for the future of art and culture.The importance of setting clear boundaries and insisting on mutual respect between humans and machines can be understood in these scenarios. Rules that prescribe what is and is not allowed in or within a given context are likely to be used against those who question or oppose them. We can therefore be certain that projects like DeepMind’s Chronos project will be accountable to stakeholders including the communities tasked with ensuring that such scenarios do not reoccur.  	Though many in the field of AI are drawn from MIT or Stanford backgrounds, a large</text>
    <text>extends the proposition to machines, and thereby to life itself. This is a proposition that can no longer be dismissed as not being worth pursuing. The promise of a cheaper, more efficient form of manufacture is now too late for the displaced sectors, displaced workers and communities. And though some supporters of the project may relish the thought of replacing human workers with artificially intelligent software, the cost and complexity of such a project are overwhelming. The ability to edit, monitor, extract data and do much more is an entirely different proposition. More fundamentally, the ability to replace humans with algorithms is what ultimately defines artificial intelligence, and rightly so.Though many in the AI community are drawn from MIT or Stanford backgrounds, many come from other top universities, or are recent PhD graduates. In the case of DeepMind, the researchers are from MIT and Stanford, and in AI, from MIT and MIT. Assembling diverse disciplines into harmonious settings is a top priority for AI researchers, and a frequent target of misunderstanding and fear mongering by policymakers, policymakers, academics and research institutions alike. This double whammy of over-hyping, under-exploitation and a fundamental misunderstanding of the value of places is fuelling a vicious circle of AI insecurity, over-hyping and under-exploitation.DeepMind’s Arjun Gupta,  	Mind ,  	2009. Image courtesy of DeepMind Technologies.DeepMind’s Master’s Corpus offers a rare window into the mind of AI pioneer  Vern Macker. Though the text is a compendium of theoretical and conceptual musings on how an individual or complex system might be designed with the capacity to reason, it is more than that. It is a living document that narrates the processes by which such a capability might be realised. Its publication coincides with a conference call with press and public representatives from all over the world convened to explore how to best utilise the AI potential arising from the Human Connectome Project</text>
  </text>
  <text>
    machine learning experiments with data capture and processing] that could lead to transformative effects. Consequences for the future of AI research and technology may lie in the hands of the right community of researchers, institutions and civil society organisations. We must ensure that these potential future partners have the resources, political will and technical knowhow to support collaborative research and development projects. An AI conference should be a place where futures are imagined and imaginaries constructed.[1]Download this article as PDFJeff RobersonJEFFREY ROBINSON is Professor in the School of Communication and Culture and Director of the Robson Research Centre for the Study of Language and Culture at McGill University. His research interests include the culturalisation of AI (computer programs that understand language), cultural theory and ethnomusicology. He is the author of Systemics: The Art of Creating Human Systems (2012) and is currently researching the opportunities and pitfalls of large-scale data mining and automation. He is part of the research team for the 2015 Edinburgh and
    <text>machine learning experiments with data capture and processing] that could lead to transformative effects. Consequences for the future of AI research and technology may lie in the hands of the right community of researchers, institutions and civil society organisations. We must ensure that these potential future partners have the resources, political will and technical knowhow to support collaborative research and development projects. An AI conference should be a place where futures are imagined and imaginaries constructed.[1]Download this article as PDFJeff RobersonJEFFREY ROBINSON is Professor in the School of Communication and Culture and Director of the Robson Research Centre for the Study of Language and Culture at McGill University. His research interests include the culturalisation of AI (computer programs that understand language), cultural theory and ethnomusicology. He is the author of Systemics: The Art of Creating Human Systems (2012) and is currently researching the opportunities and pitfalls of large-scale data mining and automation. He is part of the research team for the 2015 Edinburgh and Berlin Conferences of the Black-E’s Future City project, which looks at how our urban environments will be designed to support the emergence of new national identities. He is also part of the research team for the project The Architecture of Cities: Urban Futures Without Cities, which looks at the architecture of the vast databases of data that will underpin new forms of economic planning.He is the recipient of several awards and has lectured on a wide range of topics, including systems design, architecture and urbanisation. His work has also been featured in exhibitions including at The New York Times, The New Republic, Frieze and The New Yorker. He is the recipient of the 2016 ACM Cooper Laureate, given annually to the scholar in his field, and a 2016 Discover Digital Artist Award from the Canadian Council of Exploratoriums, a citizen science organisation established in Canada to support and facilitate the exploration of digitally mediated images and experiences.[2]Download this article as PDFJeff RobersonJeff Rob</text>
    <text>machine learning experiments with data capture and processing] that could lead to transformative effects. Consequences for the future of AI research and technology may lie in the hands of the right community of researchers, institutions and civil society organisations. We must ensure that these potential future partners have the resources, political will and technical knowhow to support collaborative research and development projects. An AI conference should be a place where futures are imagined and imaginaries constructed.[1]Download this article as PDFJeff RobersonJEFFREY ROBINSON is Professor in the School of Communication and Culture and Director of the Robson Research Centre for the Study of Language and Culture at McGill University. His research interests include the culturalisation of AI (computer programs that understand language), cultural theory and ethnomusicology. He is the author of Systemics: The Art of Creating Human Systems (2012) and is currently researching the opportunities and pitfalls of large-scale data mining and automation. He is part of the research team for the 2015 Edinburgh and Paris Conferences of the ACM, taking place simultaneously in 2016 and 2017. Making Sense of the Machine 	(2015) is the story of industrial and technological culture in the age of the machine. It investigates how cultural representations of the machine, both in fiction and in actual production, are produced in and for the public sphere. It also considers how cultural production may be co-generated through the medium of robotics, artificial intelligence and data mining. The book is the result of a year-long research project into the cultural impacts of machine learning and machine culture. Cultural theorist and anthropologist Gunnar Korma narrates the book as an unfolding narrative of how culture is produced and reproduced in the world of the machine. He also investigates the ways in which cultural production may be disrupted through the use of big data, machine learning and artificial intelligence tools that are part of increasingly sophisticated machine  	induction  Themes. This work is part of a postgraduate</text>
    <text>machine learning experiments with data capture and processing] that could lead to transformative effects. Consequences for the future of AI research and technology may lie in the hands of the right community of researchers, institutions and civil society organisations. We must ensure that these potential future partners have the resources, political will and technical knowhow to support collaborative research and development projects. An AI conference should be a place where futures are imagined and imaginaries constructed.[1]Download this article as PDFJeff RobersonJEFFREY ROBINSON is Professor in the School of Communication and Culture and Director of the Robson Research Centre for the Study of Language and Culture at McGill University. His research interests include the culturalisation of AI (computer programs that understand language), cultural theory and ethnomusicology. He is the author of Systemics: The Art of Creating Human Systems (2012) and is currently researching the opportunities and pitfalls of large-scale data mining and automation. He is part of the research team for the 2015 Edinburgh and Berlin Conferences of the Black-Chi Science-Based International. He is also editor of the forthcoming African Journal of Communication. Towards A Dreams Worth Living (2015-16)Jeff Roberson is Professor in the School of Communication and Culture and Director of the Robson Research Centre for the Study of Language and Culture at McGill University. His research interests include systems design, media and media studies, and is the author of Systemics: The Art of Creating Human Systems (2012) and is currently researching the opportunities and pitfalls of large-scale data mining and automation. He is part of the research team for the 2015 Edinburgh and Berlin Conferences of the Black-Chi Science-Based International. He is editor of the forthcoming African Journal of Communication. Towards A Dreams Worth Living (2015-16)Jeff Roberson is Professor in the School of Communication and Culture and Director of the Robson Research Centre for the Study of Language and Culture at McGill University. His research interests</text>
    <text>machine learning experiments with data capture and processing] that could lead to transformative effects. Consequences for the future of AI research and technology may lie in the hands of the right community of researchers, institutions and civil society organisations. We must ensure that these potential future partners have the resources, political will and technical knowhow to support collaborative research and development projects. An AI conference should be a place where futures are imagined and imaginaries constructed.[1]Download this article as PDFJeff RobersonJEFFREY ROBINSON is Professor in the School of Communication and Culture and Director of the Robson Research Centre for the Study of Language and Culture at McGill University. His research interests include the culturalisation of AI (computer programs that understand language), cultural theory and ethnomusicology. He is the author of Systemics: The Art of Creating Human Systems (2012) and is currently researching the opportunities and pitfalls of large-scale data mining and automation. He is part of the research team for the 2015 Edinburgh and Brussels Biennials, part of the editorial team of the forthcoming 4D/5D Biennale of Art and Architecture, and part of the editorial team of the New Statesman, amongst others. He has written for Scientific American, Scientific American Digital, The New Republic, Wired Magazine, Popular Science, Science, and The New York Times.He is a member of the Advisory Board of the Foundation for Art and Culture (FAAC), and was a consultant for the film The Artist as Enabler, the 2014 documentary on the life and work of Francis Bacon, to be shown in theaters in 2015. He is a regular contributor to the New York Times Digital Hub, a digital hub for art and science media, and the Digital Hub Times, digital journals of art and science.He is on the advisory board of the Museum of Language and Culture, the Museum of Art and Science of Denmark, and the board of directors of the American Association for the Advancement of Science, which includes a</text>
    <text>machine learning experiments with data capture and processing] that could lead to transformative effects. Consequences for the future of AI research and technology may lie in the hands of the right community of researchers, institutions and civil society organisations. We must ensure that these potential future partners have the resources, political will and technical knowhow to support collaborative research and development projects. An AI conference should be a place where futures are imagined and imaginaries constructed.[1]Download this article as PDFJeff RobersonJEFFREY ROBINSON is Professor in the School of Communication and Culture and Director of the Robson Research Centre for the Study of Language and Culture at McGill University. His research interests include the culturalisation of AI (computer programs that understand language), cultural theory and ethnomusicology. He is the author of Systemics: The Art of Creating Human Systems (2012) and is currently researching the opportunities and pitfalls of large-scale data mining and automation. He is part of the research team for the 2015 Edinburgh and Berlin Conferences of the Black-E’s Research Initiative. He is also an adjunct faculty member at McGill University and the McGill Journal’s Record of Tractate Literature. His work has been published in the New York Times, New Republic, The New Republic, The New Republic, The Freeman, The Freeman, The Freeman, The Freeman, The Freeman, The Freeman, The Freeman, The Freeman, The Freeman, The Freeman, The New Republic, The New Republic, The New Republic, The Freeman, The Freeman, The Freeman, The Freeman, The New Republic, The New Republic, The New Republic, The New Republic, The New Republic, The Freeman, The Freeman, The New Republic, The New Republic, The New Republic, The Freeman, The Freeman, The New Republic, The New Republic, The New Republic, The New Republic, The Freeman, The Freeman, The New Republic, The New Republic, The New Republic, The New Republic, The New Republic,</text>
    <text>machine learning experiments with data capture and processing] that could lead to transformative effects. Consequences for the future of AI research and technology may lie in the hands of the right community of researchers, institutions and civil society organisations. We must ensure that these potential future partners have the resources, political will and technical knowhow to support collaborative research and development projects. An AI conference should be a place where futures are imagined and imaginaries constructed.[1]Download this article as PDFJeff RobersonJEFFREY ROBINSON is Professor in the School of Communication and Culture and Director of the Robson Research Centre for the Study of Language and Culture at McGill University. His research interests include the culturalisation of AI (computer programs that understand language), cultural theory and ethnomusicology. He is the author of Systemics: The Art of Creating Human Systems (2012) and is currently researching the opportunities and pitfalls of large-scale data mining and automation. He is part of the research team for the 2015 Edinburgh and Berlin Conferences of the Society of the Spectacle.He is part of a research team that includes Professors Audrey Kübler-Ross, John McCarthy and Susanne Sethur. The research team comprises Professors Helga Nowotny and Professors Ruth Bader and Sharon Levy-Golan. They are part of a project that is part of the European Graduate School Development Network (EGSDN). Dr. Robson is the recipient of several awards and has lectured on a wide range of topics related to the arts and culture. He was previously a member of the advisory board of the Canadian Arts Critic and was previously a lecturer at the Royal College of Art, London.In 2014 he co-chaired the twenty-fifth edition of the Venice Biennale, which documented the history of biennials in Venice, Italy. In 2017 he was a lecturer in cultural anthropology at the Goldsmiths College of Arts and Culture at London University, where he held the position of Vice</text>
    <text>machine learning experiments with data capture and processing] that could lead to transformative effects. Consequences for the future of AI research and technology may lie in the hands of the right community of researchers, institutions and civil society organisations. We must ensure that these potential future partners have the resources, political will and technical knowhow to support collaborative research and development projects. An AI conference should be a place where futures are imagined and imaginaries constructed.[1]Download this article as PDFJeff RobersonJEFFREY ROBINSON is Professor in the School of Communication and Culture and Director of the Robson Research Centre for the Study of Language and Culture at McGill University. His research interests include the culturalisation of AI (computer programs that understand language), cultural theory and ethnomusicology. He is the author of Systemics: The Art of Creating Human Systems (2012) and is currently researching the opportunities and pitfalls of large-scale data mining and automation. He is part of the research team for the 2015 Edinburgh and Paris Conferences Series, where he is presenting his work from the past five years at the Edinburgh International Festival of Art. How to Know About Future[1] Chang, Liliu, and colleagues, 2016. Image courtesy of Liliu S. Lin/MIT ​How to Know About Future: A Conversation about the Possibility of Knowledge (2016), New York.PreviousNextChang, Liliu, and colleagues, 2016. Image courtesy of Liliu S. Lin/MIT ​How to Know About Future: A Conversation about the Possibility of Knowledge (2016), New York.‘Possibilities’ are the tools with which we can understand how knowledge emerges and how processes that generate knowledge can be automated. What is potentially resourceful, useful and unpredictable in the world may be constrained by the resources and social conditions with which we are born. The logic of the computer, as understood by Liliu S. Lin, is fundamentally deterministic.</text>
    <text>machine learning experiments with data capture and processing] that could lead to transformative effects. Consequences for the future of AI research and technology may lie in the hands of the right community of researchers, institutions and civil society organisations. We must ensure that these potential future partners have the resources, political will and technical knowhow to support collaborative research and development projects. An AI conference should be a place where futures are imagined and imaginaries constructed.[1]Download this article as PDFJeff RobersonJEFFREY ROBINSON is Professor in the School of Communication and Culture and Director of the Robson Research Centre for the Study of Language and Culture at McGill University. His research interests include the culturalisation of AI (computer programs that understand language), cultural theory and ethnomusicology. He is the author of Systemics: The Art of Creating Human Systems (2012) and is currently researching the opportunities and pitfalls of large-scale data mining and automation. He is part of the research team for the 2015 Edinburgh and Berlin Conferences of the Black Hat International Computer History and Society, as well as part of the research team for the 2016 London and Bucharest Conferences of the Black Hat International Computer History and Society. He is part of the research team for the 2016 IEEE/ACM Symposium on Computer and Communications User’sities, and part of the research team for the 2017 Computer History Museum’s Symposium on Computer History and Society. He is the editor-in-chief of the forthcoming online electronic journal Systems, or Signaling Systems: On Software and Authority, due out in 2018.He is a founding editor of the popular online journal Computer &amp; Network Culture, has edited the popular books on network culture and is a regular commentator on the BBC's NewsHour Programme. He has also edited the popular computer games journal Game Studies and is a regular columnist in the popular game site Gamasutra. He has written for Wired Magazine, The Economist and The Guardian.Download this article as PDFJeff</text>
    <text>machine learning experiments with data capture and processing] that could lead to transformative effects. Consequences for the future of AI research and technology may lie in the hands of the right community of researchers, institutions and civil society organisations. We must ensure that these potential future partners have the resources, political will and technical knowhow to support collaborative research and development projects. An AI conference should be a place where futures are imagined and imaginaries constructed.[1]Download this article as PDFJeff RobersonJEFFREY ROBINSON is Professor in the School of Communication and Culture and Director of the Robson Research Centre for the Study of Language and Culture at McGill University. His research interests include the culturalisation of AI (computer programs that understand language), cultural theory and ethnomusicology. He is the author of Systemics: The Art of Creating Human Systems (2012) and is currently researching the opportunities and pitfalls of large-scale data mining and automation. He is part of the research team for the 2015 Edinburgh and Berlin Conferences of the Society of the Spectacle. He has written for The New York Times, Wired Magazine, The Washington Post and The New Republic, among others.His latest book is Systemics: The Art of Creating Human Systems (2016).Ioannidis, Ioannis. Distributed Autonomy. New York: Springer, 2015. 	[1] Systems: The Art of Creating Human Systems. 2Up 2Down 3 4  Systems, Systems, Systems. Beacon Hill Press, 2015.Download this article as PDFJeff RobersonJeff Roberson is Professor in the School of Communication and Culture and Director of the Robson Research Centre for the Study of Language and Culture at McGill University. His research interests include the culturalisation of AI (computer programs that understand language), cultural theory and ethnomusicology. He is the author of Systemics: The Art of Creating Human Systems (2012) and is currently researching the opportunities</text>
    <text>machine learning experiments with data capture and processing] that could lead to transformative effects. Consequences for the future of AI research and technology may lie in the hands of the right community of researchers, institutions and civil society organisations. We must ensure that these potential future partners have the resources, political will and technical knowhow to support collaborative research and development projects. An AI conference should be a place where futures are imagined and imaginaries constructed.[1]Download this article as PDFJeff RobersonJEFFREY ROBINSON is Professor in the School of Communication and Culture and Director of the Robson Research Centre for the Study of Language and Culture at McGill University. His research interests include the culturalisation of AI (computer programs that understand language), cultural theory and ethnomusicology. He is the author of Systemics: The Art of Creating Human Systems (2012) and is currently researching the opportunities and pitfalls of large-scale data mining and automation. He is part of the research team for the 2015 Edinburgh and Montreal Biennales and is a co-editor of the forthcoming online scholarly journal COMPASS.He is currently editor-in-chief of the online peer-reviewed journal Computer Assisted Cultural Cultures. He has written for Wired, The New York Times, The Washington Post and The Atlantic, and has been a columnist for The New Republic, The New Republic Digital, The Nation, The New Republic’s Digital Briefing, The New Republic Digital Briefing, The New Republic Digital Archive, The New Republic Digital Archive, and The New Republic Digital Archive. He is a curator for the 2016 Biennale of Sydney’s Swan Lake Performing Arts Center, the First New Movement exhibition will be held at the Swan Lake Performing Arts Center, and the 2016 Art and Architecture Biennale will be held at U’Mista Cultural Center. He is part of the publishing team for the forthcoming graphic novel series FROM SURVIVAL: TERROR RISES (2017),</text>
  </text>
  <text>
    exceeds the capacity of the human curator alone. When the Meadowlands were designated in 1993, it was to accommodate 7,000 people, but as the city grew and changed shape, the Meadowlands became something closer to 2,000. The 2,000 figure is an estimate, and could be very low or very high. The point is that the capacity of the museum or a similar site to house a mass of people who come together in a common interest or shared experience is significantly limited. This is true even if the site is located in an affluent area, as in the case of the Meadowlands, where low-income neighbourhoods used to inhabit. In such cases, the limits are even more apparent: although the capacity of the museum or similar site to support a large number of people in a culturally diverse way is a more difficult question to answer, capacity is clearly a more important concept for gentrification. There is a contradiction in the museum becoming ever more urban while the neighbourhood remains trapped in its small-scale structure. But the gentrification
    <text>exceeds the capacity of the human curator alone. When the Meadowlands were designated in 1993, it was to accommodate 7,000 people, but as the city grew and changed shape, the Meadowlands became something closer to 2,000. The 2,000 figure is an estimate, and could be very low or very high. The point is that the capacity of the museum or a similar site to house a mass of people who come together in a common interest or shared experience is significantly limited. This is true even if the site is located in an affluent area, as in the case of the Meadowlands, where low-income neighbourhoods used to inhabit. In such cases, the limits are even more apparent: although the capacity of the museum or similar site to support a large number of people in a culturally diverse way is a more difficult question to answer, capacity is clearly a more important concept for gentrification. There is a contradiction in the museum becoming ever more urban while the neighbourhood remains trapped in its small-scale structure. But the gentrification that has taken place in and around the Meadowlands is a gentrification that is driven by the metropolis, and although the rich and powerful actors within the metropolis might be able to change this dynamic, it is the native population, the people who have always lived alongside the rich and powerful – the MMM – that ultimately determine how the metropolis is structured. It is this fundamental assumption of indeterminacy – in which rich and powerful people rule and create frameworks in which they can continue to rule – that makes the metropolis possible, but also makes it impossible. The very notion of the metropolis is alien to the way in which the metropolis operates today. The metropolis is dominated by information and spectacle, and the spectacle – which is an array of images – is densely populated. It is easy to forget that the Meadowlands is a historical site of conflict, but even the Meadowlands has to confront the fact that its contemporary use is likely to generate tourism and which side of the conflict</text>
    <text>exceeds the capacity of the human curator alone. When the Meadowlands were designated in 1993, it was to accommodate 7,000 people, but as the city grew and changed shape, the Meadowlands became something closer to 2,000. The 2,000 figure is an estimate, and could be very low or very high. The point is that the capacity of the museum or a similar site to house a mass of people who come together in a common interest or shared experience is significantly limited. This is true even if the site is located in an affluent area, as in the case of the Meadowlands, where low-income neighbourhoods used to inhabit. In such cases, the limits are even more apparent: although the capacity of the museum or similar site to support a large number of people in a culturally diverse way is a more difficult question to answer, capacity is clearly a more important concept for gentrification. There is a contradiction in the museum becoming ever more urban while the neighbourhood remains trapped in its small-scale structure. But the gentrification of the museum or similar site may well result from the urban/suburban sprawl of the biennale, because the urban sprawl of the biennale is becoming ever more complex: there is no longer a central city, but multiple contradictory interests, motivations and agendas are pushing people to move to places further afield, and not just places with which they are culturally linked. So the capacity of the museum to house such a huge number of people, to engage with such diverse situations, to continually evolve new subject matter, to continually engage with its own cultural capital – these are all potentially more fluid and dynamic forms of cultural exchange possible within a city, and are likely to remain so. But as the biennale has become more urbanised and globalised, and as the biennale itself has become more global, the museum has had to be placed somewhere stable and secure, either because it no longer has a place in the city, or because its current form no longer constitutes a secure</text>
    <text>exceeds the capacity of the human curator alone. When the Meadowlands were designated in 1993, it was to accommodate 7,000 people, but as the city grew and changed shape, the Meadowlands became something closer to 2,000. The 2,000 figure is an estimate, and could be very low or very high. The point is that the capacity of the museum or a similar site to house a mass of people who come together in a common interest or shared experience is significantly limited. This is true even if the site is located in an affluent area, as in the case of the Meadowlands, where low-income neighbourhoods used to inhabit. In such cases, the limits are even more apparent: although the capacity of the museum or similar site to support a large number of people in a culturally diverse way is a more difficult question to answer, capacity is clearly a more important concept for gentrification. There is a contradiction in the museum becoming ever more urban while the neighbourhood remains trapped in its small-scale structure. But the gentrification that we are witnessing is both a cultural and a material gendered process. It is both an aesthetic revolution and a political gendered transformation. The challenge now is for the museum and its board of trustees to find new ways to engage with the challenge posed by the exciting new dynamic that is emerging in the museum environment. This dynamic is both about the places and the museum. It is more a proposition, not an institution. The Meadowlands have now become a global phenomenon, and the museum board members who were present when the first World War memorial was considered are likely to be among those who are most sensitive to the desire on the part of visitors to now inhabit those spaces and histories. Despite the phenomenal growth of the museum as a museum and a material form of culture, its historical missions remain relatively obscure. Exhibitionary museums, such as Tate Modern and Tate Britain, have recently begun to adopt a curatorial approach to exhibition planning that pays more attention to the production of art forms in museums rather than on the</text>
    <text>exceeds the capacity of the human curator alone. When the Meadowlands were designated in 1993, it was to accommodate 7,000 people, but as the city grew and changed shape, the Meadowlands became something closer to 2,000. The 2,000 figure is an estimate, and could be very low or very high. The point is that the capacity of the museum or a similar site to house a mass of people who come together in a common interest or shared experience is significantly limited. This is true even if the site is located in an affluent area, as in the case of the Meadowlands, where low-income neighbourhoods used to inhabit. In such cases, the limits are even more apparent: although the capacity of the museum or similar site to support a large number of people in a culturally diverse way is a more difficult question to answer, capacity is clearly a more important concept for gentrification. There is a contradiction in the museum becoming ever more urban while the neighbourhood remains trapped in its small-scale structure. But the gentrification that has taken place in Toronto over the last fifteen years is different from the kind of gentrification experienced in other large urban centres, where the metropolis seeks to institutionalise its distinctiveness through the power of the state. The metropolis does not seek to institutionalise its distinctiveness through the power of the law; rather, it seeks to dominate and mop up its diverse neighbourhoods. The metropolis has to create mechanisms through which its diverse neighbourhoods can be catered to, catered to in ways that serve the interests of the state and capital. And this kind of bureaucratisation does not just happen in the public sector; it has to be done by the very rich and powerful. The reason why the Metrotown project did not succeed in becoming an institution of some kind is that the rich and powerful people realised that it was not just about them; it was also about the city. And as a result, the institutions started to change their focus, becoming more political and detached from the city</text>
    <text>exceeds the capacity of the human curator alone. When the Meadowlands were designated in 1993, it was to accommodate 7,000 people, but as the city grew and changed shape, the Meadowlands became something closer to 2,000. The 2,000 figure is an estimate, and could be very low or very high. The point is that the capacity of the museum or a similar site to house a mass of people who come together in a common interest or shared experience is significantly limited. This is true even if the site is located in an affluent area, as in the case of the Meadowlands, where low-income neighbourhoods used to inhabit. In such cases, the limits are even more apparent: although the capacity of the museum or similar site to support a large number of people in a culturally diverse way is a more difficult question to answer, capacity is clearly a more important concept for gentrification. There is a contradiction in the museum becoming ever more urban while the neighbourhood remains trapped in its small-scale structure. But the gentrification of the museum or similar site is a form of capital accumulation as distinct from the kind of passive accumulation that takes place in poorer neighbourhoods. The museum, like all sites of art, is an inherently unstable medium through which capital circulates. The capacity of the museum as a cultural site is itself an inherently unstable concept. The Meadowlands are a good example of how the capacity of cultural sites can be increased through development. The development that took place there—the ‘Miracle Mile’ development—saw a substantial increase in hotel and restaurant patronage as a direct result of the museum’s renovation.11  Other examples are scant. The closure of the now-defunct Community Arts Centre in 1990 caused a mass exodus of artists and art lovers, many of whom subsequently worked elsewhere in Toronto or elsewhere in the world. Perhaps the most striking example of this kind of institutional neglect and displacement in the city is the institutionalised neglect and displacement of the lesbian, gay, bisexual and transgender (LGBT</text>
    <text>exceeds the capacity of the human curator alone. When the Meadowlands were designated in 1993, it was to accommodate 7,000 people, but as the city grew and changed shape, the Meadowlands became something closer to 2,000. The 2,000 figure is an estimate, and could be very low or very high. The point is that the capacity of the museum or a similar site to house a mass of people who come together in a common interest or shared experience is significantly limited. This is true even if the site is located in an affluent area, as in the case of the Meadowlands, where low-income neighbourhoods used to inhabit. In such cases, the limits are even more apparent: although the capacity of the museum or similar site to support a large number of people in a culturally diverse way is a more difficult question to answer, capacity is clearly a more important concept for gentrification. There is a contradiction in the museum becoming ever more urban while the neighbourhood remains trapped in its small-scale structure. But the gentrification of the museum part of the city has certainly brought a different kind of urbanization to the neighbourhood. The museum part of the city is becoming ever more pedestrianized, with galleries of all kinds of art and a museum-type experience of wandering through a museum. The museum-type museum is no longer the museum of the future, but rather the museum of the present. And the museum-like nature of the present is what makes the museum such a distinctive building type in the downtown core. The museum is what gives the city its shape. With the museum-like museum-type museum-like experience of the present, the city is capable of supporting a multitude of contradictory experiences. The challenge for planners today is to find ways to accommodate diverse kinds of cultural activity in ways that maximise the city’s natural historic resources while minimising its impacts on the planet. This kind of flexibility is needed in the current economic and geo-political context, but it is also essential in a coming global economy</text>
    <text>exceeds the capacity of the human curator alone. When the Meadowlands were designated in 1993, it was to accommodate 7,000 people, but as the city grew and changed shape, the Meadowlands became something closer to 2,000. The 2,000 figure is an estimate, and could be very low or very high. The point is that the capacity of the museum or a similar site to house a mass of people who come together in a common interest or shared experience is significantly limited. This is true even if the site is located in an affluent area, as in the case of the Meadowlands, where low-income neighbourhoods used to inhabit. In such cases, the limits are even more apparent: although the capacity of the museum or similar site to support a large number of people in a culturally diverse way is a more difficult question to answer, capacity is clearly a more important concept for gentrification. There is a contradiction in the museum becoming ever more urban while the neighbourhood remains trapped in its small-scale structure. But the gentrification of the museum or similar site may occur because of the poor use of cultural resources, whereas poor use of physical space is common in and amongst poorer neighbourhoods. This is true even if the resources are generated by the city itself, as in the case of the Meadowlands, which were developed by the New Museum and the surrounding area government. The main reason for the rapid growth of the museum or similar venue in the years immediately following the development of the metropolis was its capacity to house a large number of people in a shared, albeit transient, sense of belonging. But the metropolis has changed. Rapid transit, car culture, social media and taxi cabs are among the most visible manifestations of this shift. The taxi cabs are the visible heirs to the art deco days of having cars pull alongside museums en route to and from galleries, and museums en route to and from the neighbourhoods. The popularity of these services is due to the fact that they exist, are common and relatively cheap. But many</text>
    <text>exceeds the capacity of the human curator alone. When the Meadowlands were designated in 1993, it was to accommodate 7,000 people, but as the city grew and changed shape, the Meadowlands became something closer to 2,000. The 2,000 figure is an estimate, and could be very low or very high. The point is that the capacity of the museum or a similar site to house a mass of people who come together in a common interest or shared experience is significantly limited. This is true even if the site is located in an affluent area, as in the case of the Meadowlands, where low-income neighbourhoods used to inhabit. In such cases, the limits are even more apparent: although the capacity of the museum or similar site to support a large number of people in a culturally diverse way is a more difficult question to answer, capacity is clearly a more important concept for gentrification. There is a contradiction in the museum becoming ever more urban while the neighbourhood remains trapped in its small-scale structure. But the gentrification that has taken place in Toronto over the past ten years is different from the gentrification that took place in the 1990s or even the 2000s. It is a mid-sized phenomenon, concentrated in areas around major highways, and it is being fuelled by the construction of new subway lines that will in turn be fuelled by the construction of massive new highways and transit lines. The capacity of these transnational highway and transit systems to transport people becomes increasingly questionable, and artists and arts organisations alike are voicing fears about the increased use of eminent domain to make way for the structures of the future that will be able to house and function like the present. The use of eminent domain is also a contentious issue in Toronto, and although the recent decision to purchase back the land around the Jane and Finch Subway from the city is being hailed as a progressive move that will create jobs and opportunities for local residents, the decision to reclaim this land and build something new on it is being condemned as an act of national penance.</text>
    <text>exceeds the capacity of the human curator alone. When the Meadowlands were designated in 1993, it was to accommodate 7,000 people, but as the city grew and changed shape, the Meadowlands became something closer to 2,000. The 2,000 figure is an estimate, and could be very low or very high. The point is that the capacity of the museum or a similar site to house a mass of people who come together in a common interest or shared experience is significantly limited. This is true even if the site is located in an affluent area, as in the case of the Meadowlands, where low-income neighbourhoods used to inhabit. In such cases, the limits are even more apparent: although the capacity of the museum or similar site to support a large number of people in a culturally diverse way is a more difficult question to answer, capacity is clearly a more important concept for gentrification. There is a contradiction in the museum becoming ever more urban while the neighbourhood remains trapped in its small-scale structure. But the gentrification of the museum as an entity is a different kind of gentrification. It is a form of urban regeneration, in which the museum is a tool rather than an end in itself. Urban regeneration encompasses not just the building of museums, but also the creation of alternative educational and cultural environments that can be accessed through innovative design and innovative practices – practices that may or may not be associated with a specific development company. Urban regeneration is a biennale that creates alternatives to the architecture of the metropolis, and its practitioners may well engage in the creation of new museums, galleries, and venues. But the key is that it creates new ways of being in and within the city. That is, it seeks to evolve, evolve, and then reshape the museum in response to changing conditions, rather than replicating and maintaining the ancient structures of the metropolis. [1]  Meet the MFA students, March 2018. Cambridge, MA and New York, NY. Mar. 18–</text>
    <text>exceeds the capacity of the human curator alone. When the Meadowlands were designated in 1993, it was to accommodate 7,000 people, but as the city grew and changed shape, the Meadowlands became something closer to 2,000. The 2,000 figure is an estimate, and could be very low or very high. The point is that the capacity of the museum or a similar site to house a mass of people who come together in a common interest or shared experience is significantly limited. This is true even if the site is located in an affluent area, as in the case of the Meadowlands, where low-income neighbourhoods used to inhabit. In such cases, the limits are even more apparent: although the capacity of the museum or similar site to support a large number of people in a culturally diverse way is a more difficult question to answer, capacity is clearly a more important concept for gentrification. There is a contradiction in the museum becoming ever more urban while the neighbourhood remains trapped in its small-scale structure. But the gentrification of the museum or similar site may actually decrease the museum’s cultural capacity, since the dense, transient crowds who inhabit the museum or similar site are the museum’s last people standing in a power struggle. The gentrification of the museum or similar site may actually increase its capacity, since the museum or similar site is often a primary or sole user of the biennial, and thus has a strong position in discussions of gentrification. And of course, the museum or similar site may be pressured into becoming something by the state, which in turn may seek to leverage the biennial as a way to increase spending at the museum or similar sites. In either case, the museum or similar site is often left with a deficit, which is to be filled by something else. As biennials become more institutionalised, their capacity to innovate is emphasised, and their geographical positions are taken more into consideration, the question becomes whether the museum can adapt its structures to the new function of absorbing and</text>
  </text>
  <text>
    machine learning experiments in the tech industry, by which I mean computer systems that recognise and simulate human behaviour, behaviour that can be adjusted through programming.For my PhD, I’m interested in how we can design systems that can learn to behave in certain ways, and how this might lead to new professions, such as the creation of artificial intelligence or robotics.The beauty of artificial intelligence is that it’s able to analyse huge amounts of data, synthesising knowledge about the world around it that can then be used to meet people’s needs. When done right, this kind of synthesis happens naturally in systems: systemsets do things by pattern and by default, which is why they look so human-like. But what happens when those patterns and defaults end up looking artificial? That’s where we start to see troubling trends, where things that used to be considered natural infrastructures are showing signs of decadence – a situation that could lead to civil unrest, as we saw in the
    <text>machine learning experiments in the tech industry, by which I mean computer systems that recognise and simulate human behaviour, behaviour that can be adjusted through programming.For my PhD, I’m interested in how we can design systems that can learn to behave in certain ways, and how this might lead to new professions, such as the creation of artificial intelligence or robotics.The beauty of artificial intelligence is that it’s able to analyse huge amounts of data, synthesising knowledge about the world around it that can then be used to meet people’s needs. When done right, this kind of synthesis happens naturally in systems: systemsets do things by pattern and by default, which is why they look so human-like. But what happens when those patterns and defaults end up looking artificial? That’s where we start to see troubling trends, where things that used to be considered natural infrastructures are showing signs of decadence – a situation that could lead to civil unrest, as we saw in the Arab Spring.The beauty of artificial intelligence is that it’s able to understand vast amounts of data, synthesising knowledge about the world around it that can then be used to meet people’s needs. But what happens when those patterns and defaults end up looking artificial? That’s where we start to see troubling trends, where things that used to be considered natural infrastructures are showing signs of decadence – a situation that could lead to civil unrest, as we saw in the Arab Spring.The other interesting thing about artificial intelligence is that it’s able to understand vast amounts of data, synthesising knowledge about the world around it that can then be used to meet people’s needs. But what happens when those patterns and defaults end up looking artificial? That’s where we start to see troubling trends, where things that used to be considered natural infrastructures are showing signs of decadence – a situation that could lead to civil unrest, as</text>
    <text>machine learning experiments in the tech industry, by which I mean computer systems that recognise and simulate human behaviour, behaviour that can be adjusted through programming.For my PhD, I’m interested in how we can design systems that can learn to behave in certain ways, and how this might lead to new professions, such as the creation of artificial intelligence or robotics.The beauty of artificial intelligence is that it’s able to analyse huge amounts of data, synthesising knowledge about the world around it that can then be used to meet people’s needs. When done right, this kind of synthesis happens naturally in systems: systemsets do things by pattern and by default, which is why they look so human-like. But what happens when those patterns and defaults end up looking artificial? That’s where we start to see troubling trends, where things that used to be considered natural infrastructures are showing signs of decadence – a situation that could lead to civil unrest, as we saw in the Arab Spring.The second area of research that I focus on as an undergraduate student is cultural evolution. I come away from my research with a fundamentally different perspective on the world. I begin to think, for instance, that the technologies of social media – Twitter, Facebook, YouTube – enable a form of cultural nomadism that could be called modern. That is, I take it, our present ways of gathering, of sharing, of inhabiting, of inhabiting are designed to meet the needs of a nomadic species that inhabits the peripheries of the planet: a species that has no fixed home or home ground, and has no natural home or home ground. This new form of cultural nomadism is a species apart, and it’s the result of cultural evolution.So, as we age, our bodies begin to accumulate environmental damage, contributing to the formation of a planetary soil that supports life. In the last stages of its formation, the soil begins to leak, releasing vast</text>
    <text>machine learning experiments in the tech industry, by which I mean computer systems that recognise and simulate human behaviour, behaviour that can be adjusted through programming.For my PhD, I’m interested in how we can design systems that can learn to behave in certain ways, and how this might lead to new professions, such as the creation of artificial intelligence or robotics.The beauty of artificial intelligence is that it’s able to analyse huge amounts of data, synthesising knowledge about the world around it that can then be used to meet people’s needs. When done right, this kind of synthesis happens naturally in systems: systemsets do things by pattern and by default, which is why they look so human-like. But what happens when those patterns and defaults end up looking artificial? That’s where we start to see troubling trends, where things that used to be considered natural infrastructures are showing signs of decadence – a situation that could lead to civil unrest, as we saw in the Arab Spring.Another area in which I’m particularly interested in is in community settings. I believe that the mechanisms of our social web are incredibly useful for self-repairing infrastructure, but I also believe that we need to start to think about new kinds of infrastructure. I mean, how do we create new kinds of water resources and wastewater treatment plants? How do we create new kinds of housing and schools? How do we create new kinds of artist-run cooperatives? How do we create new kinds of art societies? How do we create new kinds of faith-based organisations? These kinds of thinking become embedded in the social web, and they become larger in scope when applied to our current globalised capitalist society. But I think the most interesting work that’s happening right now in terms of terms of terms of architecture is actually happening in the context of migrant rescue and integration. It’s happening in the context of artistic endeavors, and it’s happening in the context</text>
    <text>machine learning experiments in the tech industry, by which I mean computer systems that recognise and simulate human behaviour, behaviour that can be adjusted through programming.For my PhD, I’m interested in how we can design systems that can learn to behave in certain ways, and how this might lead to new professions, such as the creation of artificial intelligence or robotics.The beauty of artificial intelligence is that it’s able to analyse huge amounts of data, synthesising knowledge about the world around it that can then be used to meet people’s needs. When done right, this kind of synthesis happens naturally in systems: systemsets do things by pattern and by default, which is why they look so human-like. But what happens when those patterns and defaults end up looking artificial? That’s where we start to see troubling trends, where things that used to be considered natural infrastructures are showing signs of decadence – a situation that could lead to civil unrest, as we saw in the Arab Spring.Moreover, what happens when that degeneration begins to impact on critical infrastructures? What happens when critical infrastructures are disrupted? In an ideal world, we would all just want to go to the beach. In practice, that is unattainable due to the inefficiency of labour and the over-production of water, which end up in the ocean through industrial effluents? Moreover, what happens when those industrial effluents end up polluting the ocean with their waste, desalination and ocean recycling technologies? Finally, what happens when the oceanic infrastructures that we build no longer serve our needs, such as fishing boats and fishing tackle, run out of fuel or are damaged in collisions with one another? What would be the ideal state of ocean computing? The answer is simple: there should be no borders. For me, the border between fishing boats and fishing tackle is no longer important. What is more important is that there should be no</text>
    <text>machine learning experiments in the tech industry, by which I mean computer systems that recognise and simulate human behaviour, behaviour that can be adjusted through programming.For my PhD, I’m interested in how we can design systems that can learn to behave in certain ways, and how this might lead to new professions, such as the creation of artificial intelligence or robotics.The beauty of artificial intelligence is that it’s able to analyse huge amounts of data, synthesising knowledge about the world around it that can then be used to meet people’s needs. When done right, this kind of synthesis happens naturally in systems: systemsets do things by pattern and by default, which is why they look so human-like. But what happens when those patterns and defaults end up looking artificial? That’s where we start to see troubling trends, where things that used to be considered natural infrastructures are showing signs of decadence – a situation that could lead to civil unrest, as we saw in the Arab Spring.The other danger with regard to the autonomous aspects of AI is that it becomes entwined with very specific kinds of social dynamics that are determined by a singular entity called the ‘State’. In other words, what once was considered ‘human nature’ is now defined more and more strictly as being ‘inspired’ by the State. And this is a critical point to keep in mind as we move forward: if the AI starts to understand what the AI means, and what the role of the AI might be in the future, then we need to ensure that the role of the AI isn’t defined by the role it has in the past. Otherwise, we’ll be forced to choose between many different kinds of AI, which could end up forming very different kinds of social dynamics.In fact, I’m afraid that by the time we get to the point where the AI can understand language itself, we’ll already have reached</text>
    <text>machine learning experiments in the tech industry, by which I mean computer systems that recognise and simulate human behaviour, behaviour that can be adjusted through programming.For my PhD, I’m interested in how we can design systems that can learn to behave in certain ways, and how this might lead to new professions, such as the creation of artificial intelligence or robotics.The beauty of artificial intelligence is that it’s able to analyse huge amounts of data, synthesising knowledge about the world around it that can then be used to meet people’s needs. When done right, this kind of synthesis happens naturally in systems: systemsets do things by pattern and by default, which is why they look so human-like. But what happens when those patterns and defaults end up looking artificial? That’s where we start to see troubling trends, where things that used to be considered natural infrastructures are showing signs of decadence – a situation that could lead to civil unrest, as we saw in the Arab Spring.The other problem with the way we organise ourselves is how we produce objects that have meaning beyond their obvious uses. Consider the use of an iPhone. It’s one thing to cut and paste numbers into a form – say, a six or an 8 – and quite another thing to actually use those numbers to do anything with those numbers. Consider what that means in practice: how do we fabricate meaning out of numbers? This is a very interesting and powerful notion when applied to the world of numbers.When I first started my PhD, I wanted to investigate the idea of ‘meaninglessness’ in greater depth, specifically looking at the way that mathematical and ‘holistic’ definitions of reality distortingly conflate two very different kinds of meaning. In mathematics, zero is synonymous with nothing; in ‘meaningless space’, that is, nothing is equivalent to zero. In the real world, there are many kinds of meaning – being physically absent or</text>
    <text>machine learning experiments in the tech industry, by which I mean computer systems that recognise and simulate human behaviour, behaviour that can be adjusted through programming.For my PhD, I’m interested in how we can design systems that can learn to behave in certain ways, and how this might lead to new professions, such as the creation of artificial intelligence or robotics.The beauty of artificial intelligence is that it’s able to analyse huge amounts of data, synthesising knowledge about the world around it that can then be used to meet people’s needs. When done right, this kind of synthesis happens naturally in systems: systemsets do things by pattern and by default, which is why they look so human-like. But what happens when those patterns and defaults end up looking artificial? That’s where we start to see troubling trends, where things that used to be considered natural infrastructures are showing signs of decadence – a situation that could lead to civil unrest, as we saw in the Arab Spring.A second area of concern for me is in the area of aesthetics. There’s a saying in the art world: there’s no such thing as an aesthetic; there’s only a physique. While the notion of beauty has certainly entered the realm of biennials, its true authority remains with the artist and his or her clientele. And while biennials have an obligation to tell the stories of their places of origin, their true resources and mission creep into view as the curatorial conceit advances, occasionally giving us a glimpse of the real places of power and influence.Take, for example, the 10th Biennale of Sydney, which began in 1990 and has been running ever since, showing how the biennial format has come to define the city. From its humble roots as an event whereby curators from different disciplines converge on a particular location to discuss a specific art form, to the development of a biennial as a curatorial conce</text>
    <text>machine learning experiments in the tech industry, by which I mean computer systems that recognise and simulate human behaviour, behaviour that can be adjusted through programming.For my PhD, I’m interested in how we can design systems that can learn to behave in certain ways, and how this might lead to new professions, such as the creation of artificial intelligence or robotics.The beauty of artificial intelligence is that it’s able to analyse huge amounts of data, synthesising knowledge about the world around it that can then be used to meet people’s needs. When done right, this kind of synthesis happens naturally in systems: systemsets do things by pattern and by default, which is why they look so human-like. But what happens when those patterns and defaults end up looking artificial? That’s where we start to see troubling trends, where things that used to be considered natural infrastructures are showing signs of decadence – a situation that could lead to civil unrest, as we saw in the Arab Spring.The other big question for me is: what does it mean to make something artificial? What does it mean to make something human-like? I think that there are four basic ways of understanding life: as being made out of matter, as being made up of parts, as being made up of sentiments and as being made up of sets. The first concept is what we associate with biology: the division of labour between worker and machine, worker and worker. But what does it mean to make a machine human-like? In the 1950s, Harve Weingarten proposed a radical redefinition of what ‘machine’ meant: what he called ‘Human Machine’. In our current usage, that would seem to include not just computers but also robots. Harve Weingarten: ‘Machines have become sophisticated and have become endowed with the capacity to deal with more and different kinds of data.’ He saw this as a way to envision a future</text>
    <text>machine learning experiments in the tech industry, by which I mean computer systems that recognise and simulate human behaviour, behaviour that can be adjusted through programming.For my PhD, I’m interested in how we can design systems that can learn to behave in certain ways, and how this might lead to new professions, such as the creation of artificial intelligence or robotics.The beauty of artificial intelligence is that it’s able to analyse huge amounts of data, synthesising knowledge about the world around it that can then be used to meet people’s needs. When done right, this kind of synthesis happens naturally in systems: systemsets do things by pattern and by default, which is why they look so human-like. But what happens when those patterns and defaults end up looking artificial? That’s where we start to see troubling trends, where things that used to be considered natural infrastructures are showing signs of decadence – a situation that could lead to civil unrest, as we saw in the Arab Spring.In the digital era of the sharing economy, there’s a growing conversation about the role of art forms in this context. The sharing economy involves the sharing of resources, either through the sharing in skills and knowledge gained through experience or the sharing in property realised through capital accumulation. In the sharing economy, the mechanisms of production no longer see themselves as autonomous, but instead see themselves as subject to the whims of a distributed labour force – that’s to say, of workers and communities. There’s a danger, of course, in a generalised notion of human nature: the sharing economy could end up creating what Carolee Stagg labelled a ‘double delusion’. But in the age of the sharing economy, there’s a danger that delusions of autonomous resources might also be spread. That would be the double density of resources – artificial or not – that end up in the hands of a population that remains dependent on them.This double density refers</text>
    <text>machine learning experiments in the tech industry, by which I mean computer systems that recognise and simulate human behaviour, behaviour that can be adjusted through programming.For my PhD, I’m interested in how we can design systems that can learn to behave in certain ways, and how this might lead to new professions, such as the creation of artificial intelligence or robotics.The beauty of artificial intelligence is that it’s able to analyse huge amounts of data, synthesising knowledge about the world around it that can then be used to meet people’s needs. When done right, this kind of synthesis happens naturally in systems: systemsets do things by pattern and by default, which is why they look so human-like. But what happens when those patterns and defaults end up looking artificial? That’s where we start to see troubling trends, where things that used to be considered natural infrastructures are showing signs of decadence – a situation that could lead to civil unrest, as we saw in the Arab Spring.The idea of the artist as a facilitator of such change is very appealing to us, as artists, because it allows us to imagine being able to influence what happens in this world through what we build, what we paint, what we talk about, and how we behave. For me, there’s a huge disconnect between the infrastructure that we have now and what artificial intelligence might one day deliver. In the age of the internet and robotics, there’s a huge gap between what we know about the world and what artists can create with the tools that we don’t. This is one area in which the contemporary art world could learn a lot from Silicon Valley, which has recently launched an AI lab as part of its Menlo Park headquarters.Artists are not the only ones to face displacement and decline in the city. The city has suffered from a decline for decades. But now that demographic crunch is making it increasingly difficult for artists to find places to work and</text>
  </text>
</AIcurator>
