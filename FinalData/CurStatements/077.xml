<?xml version="1.0" ?>
<AIcurator>
  <!--Leonardo Impett, Ubermorgen, Joasia Krysa-->
  <text>
    biennials past and present’, seems to me an essential condition of the contemporary condition. More and more, biennials are becoming the everyday circulating parts of the city. They’re a source of information, a source of data, a source of capital for urban development. In Buffalo, for example, which has recently been selected to host the 2024 Summer Olympic and Paralympic Games, the biennale’s increased revenues and the visibility of its own projects mean that the city now depends on an event that can generate lots of press and tourism. And as the biennale’s budget has risen, so too has its role in generating that publicity. The question is: how does the biennale coordinate with the city to achieve its purposes? The answer, as always, is tourism and the spectacle of spectacle.Doubtless, the rise of the biennale as a biennial in places such as Venice, and the Museum of Modern Art in New York, is a direct
    <text>biennials past and present’, seems to me an essential condition of the contemporary condition. More and more, biennials are becoming the everyday circulating parts of the city. They’re a source of information, a source of data, a source of capital for urban development. In Buffalo, for example, which has recently been selected to host the 2024 Summer Olympic and Paralympic Games, the biennale’s increased revenues and the visibility of its own projects mean that the city now depends on an event that can generate lots of press and tourism. And as the biennale’s budget has risen, so too has its role in generating that publicity. The question is: how does the biennale coordinate with the city to achieve its purposes? The answer, as always, is tourism and the spectacle of spectacle.Doubtless, the rise of the biennale as a biennial in places such as Venice, and the Museum of Modern Art in New York, is a direct result of the decline of the biennial. But the influence of the biennial as a curatorial strategy is difficult to quantify. At the same time, the biennale has its share of critics, especially in America, who see a curatorial strategy as inherently anti-profane. Are we to understand the cultural capital of the biennale as a curatorial tactic? In this respect, the biennale is like any other form of cultural exchange. It’s a capital of sharing, and the sharing is always mediated by social exchange.Doubtless, the increasing privatisation of cultural forms is also a major topic of conversation in the Biennale. Is the biennial a capital of culture or a capital of commerce? In an increasingly unequal world, surely the former is more useful. And we know that the biennale has a financial endgame. Does the biennale have a curatorial endgame? We know that the University of Melbourne, which has been</text>
    <text>biennials past and present’, seems to me an essential condition of the contemporary condition. More and more, biennials are becoming the everyday circulating parts of the city. They’re a source of information, a source of data, a source of capital for urban development. In Buffalo, for example, which has recently been selected to host the 2024 Summer Olympic and Paralympic Games, the biennale’s increased revenues and the visibility of its own projects mean that the city now depends on an event that can generate lots of press and tourism. And as the biennale’s budget has risen, so too has its role in generating that publicity. The question is: how does the biennale coordinate with the city to achieve its purposes? The answer, as always, is tourism and the spectacle of spectacle.Doubtless, the rise of the biennale as a biennial in places such as Venice, and the Museum of Modern Art in New York, is a direct result of the latter’s ubiquity and the former’s decline. But the real-life, often spectacular, sometimes fraught relationship between biennials and their places of exhibition is what artist Sarah Lowdnes calls ‘the silent visitor’. She identifies five silences that ‘perform a silent work of art without a trace’: a department store window, a railway platform, a government building and a bank vault. The fifth silence, she says, is ‘imposed’ness ‘on the other side of the globe’. Here, the biennale is able to create a parallel universe in which to itself, taking the place of a museum, a high street or a commercial centre.The biennial as a collective effect is a powerful tool, but it has to be used responsibly. And as biennials become more and more institutionalised, their institutionalisedness becomes ever more important, and their access to funds a consequence</text>
    <text>biennials past and present’, seems to me an essential condition of the contemporary condition. More and more, biennials are becoming the everyday circulating parts of the city. They’re a source of information, a source of data, a source of capital for urban development. In Buffalo, for example, which has recently been selected to host the 2024 Summer Olympic and Paralympic Games, the biennale’s increased revenues and the visibility of its own projects mean that the city now depends on an event that can generate lots of press and tourism. And as the biennale’s budget has risen, so too has its role in generating that publicity. The question is: how does the biennale coordinate with the city to achieve its purposes? The answer, as always, is tourism and the spectacle of spectacle.Doubtless, the rise of the biennale as a biennial in places such as Venice, and the Museum of Modern Art in New York, is a direct result of the biennial in New York. But the biennial in Venice has been eclipsed by the power of the museum, and museums in general, which in turn have found ways to commission large numbers of artists to work in their collections more easily. The rise of the biennale in other cities is a by-product of the museum, and its collections, which are now so ubiquitous that it is difficult to find an exhibitionary space in a city without some form of biennale, and to be honest, some biennales seem to be accomplishing this through collections management rather than by picking and choosing from the best and most instructive exhibitionary material.  And I would add that the rise of the biennale in Venice is a by-product of the power of the museum, which is the museum’s strength, allowing it to become a locus of contemporary expression, while Venice itself becomes an object of contemporary contemplation.As a visitor to Venice, and especially</text>
    <text>biennials past and present’, seems to me an essential condition of the contemporary condition. More and more, biennials are becoming the everyday circulating parts of the city. They’re a source of information, a source of data, a source of capital for urban development. In Buffalo, for example, which has recently been selected to host the 2024 Summer Olympic and Paralympic Games, the biennale’s increased revenues and the visibility of its own projects mean that the city now depends on an event that can generate lots of press and tourism. And as the biennale’s budget has risen, so too has its role in generating that publicity. The question is: how does the biennale coordinate with the city to achieve its purposes? The answer, as always, is tourism and the spectacle of spectacle.Doubtless, the rise of the biennale as a biennial in places such as Venice, and the Museum of Modern Art in New York, is a direct result of the rise of the biennale visitor. And perhaps, as a consequence, the biennale is no longer the pinnacle of its visitor attraction. Visitors to the Venice Biennale today regularly manifest a feeling of déjà vu, as if drawn back into the city through the familiar logic of the itinerary. But this feeling is manufactured, as all such ficticious journeys are. The biennale must exert its muscles beyond the formal as a museum of transient interest and phenomenon. It must generate its own power, through its own productions and, above all, remain hidden, as it did throughout much of the twentieth century.Doubtless, the biennale today depends on the seamless integration of various public agencies and institutional forms, with the understanding that these agencies and forms will be activated in accordance with the needs of the city. It is a point of commonality among those who come to see the same work, and this commonality is often illustrated by a common symbol</text>
    <text>biennials past and present’, seems to me an essential condition of the contemporary condition. More and more, biennials are becoming the everyday circulating parts of the city. They’re a source of information, a source of data, a source of capital for urban development. In Buffalo, for example, which has recently been selected to host the 2024 Summer Olympic and Paralympic Games, the biennale’s increased revenues and the visibility of its own projects mean that the city now depends on an event that can generate lots of press and tourism. And as the biennale’s budget has risen, so too has its role in generating that publicity. The question is: how does the biennale coordinate with the city to achieve its purposes? The answer, as always, is tourism and the spectacle of spectacle.Doubtless, the rise of the biennale as a biennial in places such as Venice, and the Museum of Modern Art in New York, is a direct result of the importance of the art object in the art’s symbolic and cultural exchange. But the influence of the art object in the cultural totality, and the symbolic exchange in turn, needs to happen in concert with, and in the presence of, the city. The biennial as an institution needs to engage the city in a symbolic and cultural exchange, and the art object in this sense is a principal agent in this process.The Venice Biennale needs to develop a collective memory and begin to negotiate cultural spaces in and through which it will be able to produce cultural outputs and engage with the legacy of the Biennale in a way that is authentic to the city, and avoids becoming a biennale itself. In doing so, it must be able to produce cultural outputs that are not bought and paid for by the city. The Biennale needs to develop collective meanings about itself, and about the city. I propose that the Biennale produce cultural outputs that are specific to its</text>
    <text>biennials past and present’, seems to me an essential condition of the contemporary condition. More and more, biennials are becoming the everyday circulating parts of the city. They’re a source of information, a source of data, a source of capital for urban development. In Buffalo, for example, which has recently been selected to host the 2024 Summer Olympic and Paralympic Games, the biennale’s increased revenues and the visibility of its own projects mean that the city now depends on an event that can generate lots of press and tourism. And as the biennale’s budget has risen, so too has its role in generating that publicity. The question is: how does the biennale coordinate with the city to achieve its purposes? The answer, as always, is tourism and the spectacle of spectacle.Doubtless, the rise of the biennale as a biennial in places such as Venice, and the Museum of Modern Art in New York, is a direct consequence of the biennale in Venice dominating the culture of that city. But the cultural tourism generated by the biennale in any city is a different kind of tourism. It’s about experiencing the city as an experience of being in a city, of having been there, of having lived there. And there’s no better way to experience the city than by being in a city, which is what the biennale is trying to become. I think of the Venice Biennale as an artist in her work, trying to figure out how to make it into a cultural tourism entity.And she succeeds in some ways, in that the biennale is able to dominate a cultural tourism narrative in that city through its many art exhibitions, its frequent radio and TV appearances, and its periodic book series published by the biennale itself. But she also has to work harder to build relationships with local authorities and museums, and to build a following of artists and other artists who�</text>
    <text>biennials past and present’, seems to me an essential condition of the contemporary condition. More and more, biennials are becoming the everyday circulating parts of the city. They’re a source of information, a source of data, a source of capital for urban development. In Buffalo, for example, which has recently been selected to host the 2024 Summer Olympic and Paralympic Games, the biennale’s increased revenues and the visibility of its own projects mean that the city now depends on an event that can generate lots of press and tourism. And as the biennale’s budget has risen, so too has its role in generating that publicity. The question is: how does the biennale coordinate with the city to achieve its purposes? The answer, as always, is tourism and the spectacle of spectacle.Doubtless, the rise of the biennale as a biennial in places such as Venice, and the Museum of Modern Art in New York, is a direct result of the rise of that institution in the US. But in the case of such places as Liverpool, which is building a new Centenary Plaza, the biennial could well act as a tourist attraction, linking the cultural resources of that city with those of Liverpool. The biennial itself could well become a tourist attraction, attracting people from around the world to come and spend six months in the city, in and around the theatre and its facilities. The question is whether, in the future, the institutional memory of the biennial will be able to sustain a sufficient institutional memory of itself, and, if so, how.In the coming months and years, we’ll be discussing in many of the lecture-rooms of the Biennale Centre, as well as in the Liverpool Biennial 2016 visitor’s book, the relationships between art, architecture and the urban environment. For many visitors, this will be the first time they encounter these terms. We’ll also be discussing</text>
    <text>biennials past and present’, seems to me an essential condition of the contemporary condition. More and more, biennials are becoming the everyday circulating parts of the city. They’re a source of information, a source of data, a source of capital for urban development. In Buffalo, for example, which has recently been selected to host the 2024 Summer Olympic and Paralympic Games, the biennale’s increased revenues and the visibility of its own projects mean that the city now depends on an event that can generate lots of press and tourism. And as the biennale’s budget has risen, so too has its role in generating that publicity. The question is: how does the biennale coordinate with the city to achieve its purposes? The answer, as always, is tourism and the spectacle of spectacle.Doubtless, the rise of the biennale as a biennial in places such as Venice, and the Museum of Modern Art in New York, is a direct result of the biennalee’s increasing popularity and the museum’s increasing budget. But the biennalee also needs to negotiate its increasing budget, since, as the biennalee’s budget has multiplied in recent decades, the actual use of the building that it renders as its site increasingly restricts. The biennalee, by its own constant redefinition, needs to negotiate these new cultural conditions, and it does so by evolving financial models that draw on ever larger collections of artefacts and collections. To do so, the biennalee would need to be able to say with complete transparency that it is carrying out a cultural task, and that these artefacts are, in fact, contemporary artefacts. I think the biennial has the capacity and the will to do this.I remember, years ago (around 1998), when the Biennale of Sydney was at its height. I went to the exhibition, partly because I was interested in how</text>
    <text>biennials past and present’, seems to me an essential condition of the contemporary condition. More and more, biennials are becoming the everyday circulating parts of the city. They’re a source of information, a source of data, a source of capital for urban development. In Buffalo, for example, which has recently been selected to host the 2024 Summer Olympic and Paralympic Games, the biennale’s increased revenues and the visibility of its own projects mean that the city now depends on an event that can generate lots of press and tourism. And as the biennale’s budget has risen, so too has its role in generating that publicity. The question is: how does the biennale coordinate with the city to achieve its purposes? The answer, as always, is tourism and the spectacle of spectacle.Doubtless, the rise of the biennale as a biennial in places such as Venice, and the Museum of Modern Art in New York, is a direct result of the rise of the biennale as a cultural form. Indeed, it’s quite likely that the rise of the biennale as a performance form in these cities is a direct result of the rise of the biennale as a cultural form in that city.[3]The other critical event in the biennial’s trajectory is the archaeological and historical survey of the city that it produces. In many ways, both the physical and the virtual footprint of the biennale is now fully mapped. But there are also significant gaps and discontinuities that continue to remain unaddressed. This is a true biennale, and it’s this second criticality that the art fair is standing for. It began as an ephemeral event in many places, and is one of the few remaining spaces that the biennale currently occupies. This is why the art fair atmosphere – the performance, the shopping, the stalls, the free food – is so important</text>
    <text>biennials past and present’, seems to me an essential condition of the contemporary condition. More and more, biennials are becoming the everyday circulating parts of the city. They’re a source of information, a source of data, a source of capital for urban development. In Buffalo, for example, which has recently been selected to host the 2024 Summer Olympic and Paralympic Games, the biennale’s increased revenues and the visibility of its own projects mean that the city now depends on an event that can generate lots of press and tourism. And as the biennale’s budget has risen, so too has its role in generating that publicity. The question is: how does the biennale coordinate with the city to achieve its purposes? The answer, as always, is tourism and the spectacle of spectacle.Doubtless, the rise of the biennale as a biennial in places such as Venice, and the Museum of Modern Art in New York, is a direct result of the mediaeval and contemporaneous variety of its exhibitions. But the biennial itself is a rare and precious commodity, and its ubiquity in a city-dwelling, urbanising metropolis such as New York, is another story. The biennial is an exhibitionary form; it has defined characteristics that have put it on a par with museums of art and antiquities. Its exhibitions are lucrative, and its money helps fund the museum that it is built upon. But these two dimensions of the biennial are, for the most part, absent in the city as a cultural phenomenon in the modern era. New York City Bicentennial 2018 is a good example of this. Its status as an exhibitionary form is undetermined, and its status as a biennale itself is uncertain. All the evidence of its being a biennial at all is a fiction, created by and for the rich.It is a monument to the decline of the metropolis that the biennial should</text>
  </text>
  <text>
    biennial as a self-learning system could lead to other possibilities. The next time you're in an arts centre, stop by and see what artist Lynn Tolmon did a few years ago. Her project Participate researchers was a long-term participatory art project that lasted almost ten years. She engaged a broad range of artists, from young artists to established artists, and engaged with issues of identity, community and performance. She also engaged with the role of the artist, the biennial and the biennial as a whole. She was a founding member of the National Gallery of Canada, and worked with the National Gallery of Denmark, the Denmark Art Biennale, and the Klinik Muzeum Berlin. Lynn is a member of the Turner Prize Shortlist for 2016, and she was a finalist for the 2016 Turner prize. She is the curator of the forthcoming exhibition Contemporary Art - Art from Disappearing Cities, from Nowhere to Nowhere - from 2014–16.
    <text>biennial as a self-learning system could lead to other possibilities. The next time you're in an arts centre, stop by and see what artist Lynn Tolmon did a few years ago. Her project Participate researchers was a long-term participatory art project that lasted almost ten years. She engaged a broad range of artists, from young artists to established artists, and engaged with issues of identity, community and performance. She also engaged with the role of the artist, the biennial and the biennial as a whole. She was a founding member of the National Gallery of Canada, and worked with the National Gallery of Denmark, the Denmark Art Biennale, and the Klinik Muzeum Berlin. Lynn is a member of the Turner Prize Shortlist for 2016, and she was a finalist for the 2016 Turner prize. She is the curator of the forthcoming exhibition Contemporary Art - Art from Disappearing Cities, from Nowhere to Nowhere - from 2014–16.This book is the result of a collaboration between the National Gallery of Denmark and the Danish Council for Independent Curators. The book is the story of how artist and researcher Envisioned Communities was formed, how the project was funded, and what the future might hold for this dynamic. Drawn from the films of Jeanne Heeswijk, the collections officer for visual art and design at the National Gallery, and Guido Klooster, Curator of Performing Arts at the Danish Council for Independent Curators, the book documents the evolution of Envisioned Communities, its development as a centre of research, and the subsequent curatorial role. It also looks at how artists, curators, art collectives, urbanists, architects and others have been involved in the project, and how their involvement shapes the community. Envisioned Communities is the result of more than 20 interviews, over a period of five years. It draws together the stories told in the five volumes of the Envision</text>
    <text>biennial as a self-learning system could lead to other possibilities. The next time you're in an arts centre, stop by and see what artist Lynn Tolmon did a few years ago. Her project Participate researchers was a long-term participatory art project that lasted almost ten years. She engaged a broad range of artists, from young artists to established artists, and engaged with issues of identity, community and performance. She also engaged with the role of the artist, the biennial and the biennial as a whole. She was a founding member of the National Gallery of Canada, and worked with the National Gallery of Denmark, the Denmark Art Biennale, and the Klinik Muzeum Berlin. Lynn is a member of the Turner Prize Shortlist for 2016, and she was a finalist for the 2016 Turner prize. She is the curator of the forthcoming exhibition Contemporary Art - Art from Disappearing Cities, from Nowhere to Nowhere - from 2014–16.The Biennial ExperienceIn 2013, I co-curated The Liverpool Biennial, which was a curatorial intervention into the city and regional biennials of the UK. The Liverpool Biennial offered a window onto the hidden cities of the UK through the collection of its curators, artists and curatorial style. The exhibition was a mixture of past, present and future works, created as episodic occasions, each curated by a different curator. The aim of the exhibition was to present a continual window onto the city through the curatorial intervention of its artists, curators, art historians and art resource of the year (BRTA). The BRTA is a private, non-profit organisation. It is a regional hub for art, architecture, design and media production in the UK, connecting artists, curators, art producers, art educators and the public with creative activities and activities around the art and architecture of the city. The Liverpool Biennial produced its own curatorial intervention, but sought to</text>
    <text>biennial as a self-learning system could lead to other possibilities. The next time you're in an arts centre, stop by and see what artist Lynn Tolmon did a few years ago. Her project Participate researchers was a long-term participatory art project that lasted almost ten years. She engaged a broad range of artists, from young artists to established artists, and engaged with issues of identity, community and performance. She also engaged with the role of the artist, the biennial and the biennial as a whole. She was a founding member of the National Gallery of Canada, and worked with the National Gallery of Denmark, the Denmark Art Biennale, and the Klinik Muzeum Berlin. Lynn is a member of the Turner Prize Shortlist for 2016, and she was a finalist for the 2016 Turner prize. She is the curator of the forthcoming exhibition Contemporary Art - Art from Disappearing Cities, from Nowhere to Nowhere - from 2014–16. This transnational exhibition will see artists from all over the world confronting questions of gentrification, displacement and abandonment in an attempt to create a new future for formerly urbanised areas of Europe and North America. The project was co-curated by Meehan Crist, and Sofía Olascoaga, and Pollyanna Subba Rao. The volume of the Biennale will be published in 2019.Biennials have come a long way in the last fifty years. They are an essential part of the biennale circuit, and their ubiquity gives way to a kind of invisibility, as art is usually exhibited for only a limited time, and rarely in a sustained way. But this has all changed in the past few years. The biennial has entered a period of critical mass, and is regularly cited as a key driver of the increasing global importance of art in a (post-)globalised world. The level of criticality has obviously accelerated, and there</text>
    <text>biennial as a self-learning system could lead to other possibilities. The next time you're in an arts centre, stop by and see what artist Lynn Tolmon did a few years ago. Her project Participate researchers was a long-term participatory art project that lasted almost ten years. She engaged a broad range of artists, from young artists to established artists, and engaged with issues of identity, community and performance. She also engaged with the role of the artist, the biennial and the biennial as a whole. She was a founding member of the National Gallery of Canada, and worked with the National Gallery of Denmark, the Denmark Art Biennale, and the Klinik Muzeum Berlin. Lynn is a member of the Turner Prize Shortlist for 2016, and she was a finalist for the 2016 Turner prize. She is the curator of the forthcoming exhibition Contemporary Art - Art from Disappearing Cities, from Nowhere to Nowhere - from 2014–16. She has written for Inhabitant, Digest, Frieze and Curator at Permanent, U,Dansk, and Kübler-Welt, and she is a co-editor of the forthcoming Archive, of Contemporary Art, from 1968 to 2016.Carrot CakeRecipes by the Homebaked ChefsPreviousNextCarrot CakeIngredients 230g peeled and finely grated Carrots28g peeled and grated Root Ginger80ml Vegetable Oil40g Plain Flour20g Baking Powder5g Salt5g Ground Cinnamon5g Ground Ginger30g Pecans (plus extra to bind together)20g Caster Sugar5g Baking Flour20g Baking Powder5g Baking Soda5g Butter5g Vanilla Extract30g Plain Flour30g Baking Powder5g Ground All-Purpose Flour5g Baking Soda5g Salt5g Ground All-Purpose Fl</text>
    <text>biennial as a self-learning system could lead to other possibilities. The next time you're in an arts centre, stop by and see what artist Lynn Tolmon did a few years ago. Her project Participate researchers was a long-term participatory art project that lasted almost ten years. She engaged a broad range of artists, from young artists to established artists, and engaged with issues of identity, community and performance. She also engaged with the role of the artist, the biennial and the biennial as a whole. She was a founding member of the National Gallery of Canada, and worked with the National Gallery of Denmark, the Denmark Art Biennale, and the Klinik Muzeum Berlin. Lynn is a member of the Turner Prize Shortlist for 2016, and she was a finalist for the 2016 Turner prize. She is the curator of the forthcoming exhibition Contemporary Art - Art from Disappearing Cities, from Nowhere to Nowhere - from 2014–16. Lynn is the Curator of Exhibitions at The Norton, London, and is the Curator of Deeper and Dicier, both at M, London, and at Monumenta, Paris. She is the author of The Gallery Years: Art and Death from the 19th to the 21st Centuries, from 2001. Her writing has been featured in The Guardian, The New York Times, The New York Daily News, The New York Observer, The New York Times Higher Education magazine, The New York Public Library, The New York Public Theatre, the New York Public Theater Archive, the New York Public Theater Archive Foundation, the New York Public Theatre Journal, the New York Public Library Review, the New York Public Theater Archive, the New York Public Theatre Journal, the New York Public Theatre Archive Foundation, the New York Public Theatre Journal, the New York Public Theatre Archive, the New York Public Theatre Journal,</text>
    <text>biennial as a self-learning system could lead to other possibilities. The next time you're in an arts centre, stop by and see what artist Lynn Tolmon did a few years ago. Her project Participate researchers was a long-term participatory art project that lasted almost ten years. She engaged a broad range of artists, from young artists to established artists, and engaged with issues of identity, community and performance. She also engaged with the role of the artist, the biennial and the biennial as a whole. She was a founding member of the National Gallery of Canada, and worked with the National Gallery of Denmark, the Denmark Art Biennale, and the Klinik Muzeum Berlin. Lynn is a member of the Turner Prize Shortlist for 2016, and she was a finalist for the 2016 Turner prize. She is the curator of the forthcoming exhibition Contemporary Art - Art from Disappearing Cities, from Nowhere to Nowhere - from 2014–16.The project also involved Constructive Thinking Architects, who are a collective of international architects whose projects address the curatorial, planning and architectural barriers preventing cities from expressing their cultural potential. They include Renee Rehberger, Sol Ryabat and Wendy Chun.Their contribution to the 2016 Venice Biennale was recently adopted as a Future City for the City of Venice.  In order to access the full potential of a city, these projects had to be committed long-term, with development and community engagement as the critical means to address the challenges and opportunities of the city. With this kind of holistic perspective, it is easy to see how a biennale can be a site of meeting, dialogue, research and ultimately, engagement.  It is even possible that a biennale could become an event, where artists and audiences come together to work through a shared interest, questioning and perhaps generating discussion. This seems particularly possible in an urban setting where</text>
    <text>biennial as a self-learning system could lead to other possibilities. The next time you're in an arts centre, stop by and see what artist Lynn Tolmon did a few years ago. Her project Participate researchers was a long-term participatory art project that lasted almost ten years. She engaged a broad range of artists, from young artists to established artists, and engaged with issues of identity, community and performance. She also engaged with the role of the artist, the biennial and the biennial as a whole. She was a founding member of the National Gallery of Canada, and worked with the National Gallery of Denmark, the Denmark Art Biennale, and the Klinik Muzeum Berlin. Lynn is a member of the Turner Prize Shortlist for 2016, and she was a finalist for the 2016 Turner prize. She is the curator of the forthcoming exhibition Contemporary Art - Art from Disappearing Cities, from Nowhere to Nowhere - from 2014–16.Download this article as PDFLynn TolmonLynn Tolmon is a Toronto-based artist. She is the Curator of Transporting Materials in Transit, with a particular interest in transport and infrastructure. Her current projects explore the materiality of art and craft in a material culture of urban materials and construction. Her current exhibition, STS 11, Running Nowhere to Nowhere: Moving Images from the Biennial of Sydney invites visitors to observe artefacts from the exhibition in their contemporary form, subject to their own interpretations and contextualisations. In 2016, she collaborated with the Danish National Museum to commission a three-day exhibition of works from the exhibition STS 11, Part 1: Sydney and Part 2: Durban. In 2017, she collaborated with the Victoria and Albert Museum, Royal BC Museum and Klinik Muzeum Berlin to commission a series of works from the exhibition STS 11, Part 3: New York. Recent projects include Working Class</text>
    <text>biennial as a self-learning system could lead to other possibilities. The next time you're in an arts centre, stop by and see what artist Lynn Tolmon did a few years ago. Her project Participate researchers was a long-term participatory art project that lasted almost ten years. She engaged a broad range of artists, from young artists to established artists, and engaged with issues of identity, community and performance. She also engaged with the role of the artist, the biennial and the biennial as a whole. She was a founding member of the National Gallery of Canada, and worked with the National Gallery of Denmark, the Denmark Art Biennale, and the Klinik Muzeum Berlin. Lynn is a member of the Turner Prize Shortlist for 2016, and she was a finalist for the 2016 Turner prize. She is the curator of the forthcoming exhibition Contemporary Art - Art from Disappearing Cities, from Nowhere to Nowhere - from 2014–16. Her writing has been featured in numerous publications including the Atlantic, GQ, New York Times, Washington Post, The New York Times Literary Supplement, The New York Observer, The New Republic, The New Republic Online, The New York Daily News, The New York Sun, The New York Philharmonic, The New York Times and The New York Times Higher Critic. She has been a presenter of The Today Show, Tonight Show with Jimmy Fallon, Access Hollywood, and NPR's The Talk. She is the host of the CBC's The Debate, hosted by Gabrielle Chaleff, with guests Bill Harpe, Lesley Stewart and Josephine Machon, and the podcast This Hour Has Eight, with guests Joel Kotkin and Samantha Jones. She is the co-curator of the MUAC/UNESCO London and New Art Caravan, as well as the curator of the forthcoming DUBAI/UNESCO Istanbul Car</text>
    <text>biennial as a self-learning system could lead to other possibilities. The next time you're in an arts centre, stop by and see what artist Lynn Tolmon did a few years ago. Her project Participate researchers was a long-term participatory art project that lasted almost ten years. She engaged a broad range of artists, from young artists to established artists, and engaged with issues of identity, community and performance. She also engaged with the role of the artist, the biennial and the biennial as a whole. She was a founding member of the National Gallery of Canada, and worked with the National Gallery of Denmark, the Denmark Art Biennale, and the Klinik Muzeum Berlin. Lynn is a member of the Turner Prize Shortlist for 2016, and she was a finalist for the 2016 Turner prize. She is the curator of the forthcoming exhibition Contemporary Art - Art from Disappearing Cities, from Nowhere to Nowhere - from 2014–16.Interview with Wendy Chun Interview with Wendy Chun [1] Dortmund, Wendy Chun, Public Space, 2012. Photo: Nina Edge.PreviousNextInterview with Wendy Chun  [2] Gustave Flaubert, The Alchemist, first published in The New Republic, January 18, 1864, p.7. [3] Heide, The Scream, first published in 1953, p.19. [4] Povinelli, Rembrandt, 1937. [5] Ross, Modern Painters, first published in 1936, p.19. [6] Painter's Diagram, Sterling Archive, Artist's Diagram #941, Norfolk State Museum. [7] Gustave Flaubert, The Alchemist, first published in The New Republic, January 18, 1864, p.7. [8] Gustave</text>
    <text>biennial as a self-learning system could lead to other possibilities. The next time you're in an arts centre, stop by and see what artist Lynn Tolmon did a few years ago. Her project Participate researchers was a long-term participatory art project that lasted almost ten years. She engaged a broad range of artists, from young artists to established artists, and engaged with issues of identity, community and performance. She also engaged with the role of the artist, the biennial and the biennial as a whole. She was a founding member of the National Gallery of Canada, and worked with the National Gallery of Denmark, the Denmark Art Biennale, and the Klinik Muzeum Berlin. Lynn is a member of the Turner Prize Shortlist for 2016, and she was a finalist for the 2016 Turner prize. She is the curator of the forthcoming exhibition Contemporary Art - Art from Disappearing Cities, from Nowhere to Nowhere - from 2014–16. She has a co-curatorial appointment at MAA Berlin until 19 September 2016. Her writing has been published in Frieze, Nautilus, Fortean Times, Bloomsbury and various other publications.  Her paintings and works have been commission by the European Capital of Culture, Berlin, New Cultural Centre, Amsterdam, MAA Berlin, Mitre 10, Rotterdam, Dock International Gallery of Amsterdam, and the Norwegian and Danish art institutions. She has received a host of accolades and recognitions for her work, including most recently the 2015 Kübler-Ross Prize for Art and the Host Society, and the 2014 Magnus Topol School Award for Art and the Host. She has also been shortlisted for the 2014 MAMA/CITY ARTISTS POINTER, 2014 MITRE 10 Prize for Art and Architectural Social Activity (2014) and was a finalist</text>
  </text>
  <text>
    biennials past and present. 	[1] See, for example, Nina Edge, ‘What is biennial?’, in The New Curator, Vol. 18, No. 1, Spring, 2002, p.1.  	[2] David Slater, ‘The Biennial Condition: On the encroachments of Art Criticism’, in Current Issues in Art and Culture, Vol. 31, No. 1, Spring, 2001, p.1. [3] Tim Jeeves, ‘Review: Biennials in a Social Science'', in Critical Inquiry, Vol. 19, No. 1, Spring, 2003, p.1. [4] David Slater, ‘Volume 20 of C.I.A.T.P.R. (Association for Byzantine and Renaissance Art Studies Quarterly) published in the journal C.I.A.T.P.R., 13:1 published by the Archives of Art and Technology
    <text>biennials past and present. 	[1] See, for example, Nina Edge, ‘What is biennial?’, in The New Curator, Vol. 18, No. 1, Spring, 2002, p.1.  	[2] David Slater, ‘The Biennial Condition: On the encroachments of Art Criticism’, in Current Issues in Art and Culture, Vol. 31, No. 1, Spring, 2001, p.1. [3] Tim Jeeves, ‘Review: Biennials in a Social Science'', in Critical Inquiry, Vol. 19, No. 1, Spring, 2003, p.1. [4] David Slater, ‘Volume 20 of C.I.A.T.P.R. (Association for Byzantine and Renaissance Art Studies Quarterly) published in the journal C.I.A.T.P.R., 13:1 published by the Archives of Art and Technology, Vol. 3, No. 3, Spring, 2006, p.1. [5] C.I.A.T.P.R. Volume 20, Number 1, Spring, 2006, p.1. [6] David Slater, ‘Art Criticism and the Biennial Condition’, in Essays in Critical Inquiry, Summer, 1986, p.1. [7] David Slater, ‘Review: Art and Criticism’, in Arts Criticism Vol. 20, No. 1, Spring, 1986, p.1. [8] David Slater, ‘Volume 20 of C.I.A.T.P.R. published in the Journal of Aesthetics and Protest, 12:1 published by the Archives of Art and Technology, Vol. 3, No. 2, Spring, 2006, p.1. [9] David Slater, ‘Review: The Biennial Condition’, in Essays in</text>
    <text>biennials past and present. 	[1] See, for example, Nina Edge, ‘What is biennial?’, in The New Curator, Vol. 18, No. 1, Spring, 2002, p.1.  	[2] David Slater, ‘The Biennial Condition: On the encroachments of Art Criticism’, in Current Issues in Art and Culture, Vol. 31, No. 1, Spring, 2001, p.1. [3] Tim Jeeves, ‘Review: Biennials in a Social Science'', in Critical Inquiry, Vol. 19, No. 1, Spring, 2003, p.1. [4] David Slater, ‘Volume 20 of C.I.A.T.P.R. (Association for Byzantine and Renaissance Art Studies Quarterly) published in the journal C.I.A.T.P.R., 13:1 published by the Archives of Art and Technology, New York, 2002, p.1. [5] David Slater, ‘Review: A Biennale is Not Like a Symposium’, in The New Curator, Vol. 18, No. 2, Spring, 2004, p.1. [6] David Slater, ‘Review: A Biennale is Not Like a Symposium’, in Current Issues in Art and Culture, Vol. 32, No. 2, Spring, 2005, p.1. [7] David Slater, ‘Review: A Biennale is Not Like a Symposium’, in The New Curator, Vol. 18, No. 2, Spring, 2004, p.2. [8] David Slater, ‘Review: A Biennale is Not Like a Symposium’, in The New Curator, Vol. 18, No. 2, Spring, 2004, p.2. [9] David Slater, ‘</text>
    <text>biennials past and present. 	[1] See, for example, Nina Edge, ‘What is biennial?’, in The New Curator, Vol. 18, No. 1, Spring, 2002, p.1.  	[2] David Slater, ‘The Biennial Condition: On the encroachments of Art Criticism’, in Current Issues in Art and Culture, Vol. 31, No. 1, Spring, 2001, p.1. [3] Tim Jeeves, ‘Review: Biennials in a Social Science'', in Critical Inquiry, Vol. 19, No. 1, Spring, 2003, p.1. [4] David Slater, ‘Volume 20 of C.I.A.T.P.R. (Association for Byzantine and Renaissance Art Studies Quarterly) published in the journal C.I.A.T.P.R., 13:1 published by the Archives of Art and Technology, p.1. [5] David Slater, ‘The Art Market Returns: Art, commodification and the curatorial’, in Current Issues in Art and Culture, Vol. 23, No. 3, Spring, 2002, p.1. [6] David Slater, ‘Art as Capital: From Postcards to Biennials’, in Selected Essays, edited by David Slater and Tom Vandeputte, Pages 1-30, The MIT Press, Boston and New Haven, 1989, p.1. [7] David Slater, ‘Slaughterhouse Five’, in Penguin New York, Vol. 4, No. 1, New York and London, 1993, p.1. [8] David Slater, ‘Slaughterhouse Five’, in Penguin New York, Vol. 4, No. 2, New York and London, 1993, p.1. [9] David Slater, ‘Slaughterhouse</text>
    <text>biennials past and present. 	[1] See, for example, Nina Edge, ‘What is biennial?’, in The New Curator, Vol. 18, No. 1, Spring, 2002, p.1.  	[2] David Slater, ‘The Biennial Condition: On the encroachments of Art Criticism’, in Current Issues in Art and Culture, Vol. 31, No. 1, Spring, 2001, p.1. [3] Tim Jeeves, ‘Review: Biennials in a Social Science'', in Critical Inquiry, Vol. 19, No. 1, Spring, 2003, p.1. [4] David Slater, ‘Volume 20 of C.I.A.T.P.R. (Association for Byzantine and Renaissance Art Studies Quarterly) published in the journal C.I.A.T.P.R., 13:1 published by the Archives of Art and Technology, p.1. [5] David Slater, ‘The Culture and Criticism of Ancient Rome’, in Selected Essays, New York University Press, New York, 1989, p.291. [6] David Slater, ‘The Culture and Criticism of Ancient Rome’, in Critical Inquiry, Vol. 19, No. 1, Spring, 2003, p.1. [7] David Slater, ‘Art and Culture: From Antiquity to Today’, in Selected Essays, New York University Press, New York, 1990, p.291. [8] David Slater, ‘The Culture and Criticism of Ancient Rome’, in The New Curator, Vol. 18, No. 1, Spring, 2002, p.1. Download this article as PDFNina Edge Nina Edge is a curator based in London and New York. She co-directs New York/London Center for Biennial Research, New York/</text>
    <text>biennials past and present. 	[1] See, for example, Nina Edge, ‘What is biennial?’, in The New Curator, Vol. 18, No. 1, Spring, 2002, p.1.  	[2] David Slater, ‘The Biennial Condition: On the encroachments of Art Criticism’, in Current Issues in Art and Culture, Vol. 31, No. 1, Spring, 2001, p.1. [3] Tim Jeeves, ‘Review: Biennials in a Social Science'', in Critical Inquiry, Vol. 19, No. 1, Spring, 2003, p.1. [4] David Slater, ‘Volume 20 of C.I.A.T.P.R. (Association for Byzantine and Renaissance Art Studies Quarterly) published in the journal C.I.A.T.P.R., 13:1 published by the Archives of Art and Technology, p.1. [5] David Slater, ‘Art and Culture in the Age of Biennials’, in Cultural Geographies: Essays in Honour of Peter Brook, John Urry and Robert H MacDonald, edited by Henri Lefebvre, Paul McCarthy and Susanne Kreuziger, Princeton Architectural Press, Princeton, 1993, p.1. [6] David Slater, Culture and Criticism, Tarcher &amp; Rose, New York, 1986, p.1. [7] Ibid.p.2. [8] David Slater, Culture and Criticism, Tarcher &amp; Rose, New York, 1986, p.2. [9] David Slater, Culture and Criticism, Tarcher &amp; Rose, New York, 1986, p.2. [10] David Slater, Culture and Criticism, Tarcher &amp; Rose, New York, 1986, p.2. [11] David Slater, Culture and Criticism, Tarcher &amp;</text>
    <text>biennials past and present. 	[1] See, for example, Nina Edge, ‘What is biennial?’, in The New Curator, Vol. 18, No. 1, Spring, 2002, p.1.  	[2] David Slater, ‘The Biennial Condition: On the encroachments of Art Criticism’, in Current Issues in Art and Culture, Vol. 31, No. 1, Spring, 2001, p.1. [3] Tim Jeeves, ‘Review: Biennials in a Social Science'', in Critical Inquiry, Vol. 19, No. 1, Spring, 2003, p.1. [4] David Slater, ‘Volume 20 of C.I.A.T.P.R. (Association for Byzantine and Renaissance Art Studies Quarterly) published in the journal C.I.A.T.P.R., 13:1 published by the Archives of Art and Technology, Vol. 4(3), 1988, p.1. [5] David Slater, ‘Art and Life: From Biennials to Museums’, in Current Issues in Art and Culture, Vol. 31, No. 1, Spring, 2002, p.1. [6] David Slater, ‘Art and Life: From Biennials to Museums’, in Critical Inquiry, Vol. 19, No. 1, Spring, 2003, p.1. [7] David Slater, ‘Art and Life: From Biennials to Museums’, in Current Issues in Art and Culture, Vol. 31, No. 1, Spring, 2002, p.1. [8] David Slater, ‘Art and Life: From Biennials to Museums’, in Critical Inquiry, Vol. 19, No. 1, Spring, 2003, p.1. [9] David Slater, ‘Art and Life</text>
    <text>biennials past and present. 	[1] See, for example, Nina Edge, ‘What is biennial?’, in The New Curator, Vol. 18, No. 1, Spring, 2002, p.1.  	[2] David Slater, ‘The Biennial Condition: On the encroachments of Art Criticism’, in Current Issues in Art and Culture, Vol. 31, No. 1, Spring, 2001, p.1. [3] Tim Jeeves, ‘Review: Biennials in a Social Science'', in Critical Inquiry, Vol. 19, No. 1, Spring, 2003, p.1. [4] David Slater, ‘Volume 20 of C.I.A.T.P.R. (Association for Byzantine and Renaissance Art Studies Quarterly) published in the journal C.I.A.T.P.R., 13:1 published by the Archives of Art and Technology in London, January, 2004, p.1.Download this article as PDFNina Edge Nina Edge is a PhD candidate in Communication Studies and the History and Theatre Studies of Dublin City University’s College of Arts and Design. Her dissertation examines the relationship between the Biennial and contemporary art practices from a historical and contemporary point of view. She also investigates the role of Italian artists in the modernisation of urban centres and examines the relationship between the Biennial and the rise of biennials worldwide. She is the editor and publisher of the journal C.I.A.T.P.R. (Association for Byzantine and Renaissance Art Studies Quarterly) and the co-editor of the book C.I.A.T.P.R. (Conference Archives of Byzantine and Renaissance Art Studies) which documents the life and work of the Italian artist Paolo Maldini, the journal was published by the Italian Archives. She is a founding member of the Advisory Board of the Society</text>
    <text>biennials past and present. 	[1] See, for example, Nina Edge, ‘What is biennial?’, in The New Curator, Vol. 18, No. 1, Spring, 2002, p.1.  	[2] David Slater, ‘The Biennial Condition: On the encroachments of Art Criticism’, in Current Issues in Art and Culture, Vol. 31, No. 1, Spring, 2001, p.1. [3] Tim Jeeves, ‘Review: Biennials in a Social Science'', in Critical Inquiry, Vol. 19, No. 1, Spring, 2003, p.1. [4] David Slater, ‘Volume 20 of C.I.A.T.P.R. (Association for Byzantine and Renaissance Art Studies Quarterly) published in the journal C.I.A.T.P.R., 13:1 published by the Archives of Art and Technology, p.1. [5] Peter Osborne, ‘Art As a Social Form: A Biennale Formula’, in Critical Inquiry, Vol. 22, No. 2, Spring, 2004, p.1. [6] Fred Turner, ‘The Art Newspaper: Art and its Place in the Digital Age’, in Current Issues in Art and Culture, Vol. 31, No. 1, Spring, 2003, p.1. [7] See, for example, Jon Super, ‘&quot;Artistic Freedom of the Press&quot; in Art &amp; Architecture, Issue 3, Spring, 2006, p.1. [8] Norman Browning and Noel Burch, eds., The Cambridge Companion to English and Commonwealth Art, Cambridge University Press, Cambridge, 1988, p.1. [9] For an introduction to the concept of biennials, see Norman Browning and Noel Burch, eds., The Cambridge Companion to English and Commonwealth Art</text>
    <text>biennials past and present. 	[1] See, for example, Nina Edge, ‘What is biennial?’, in The New Curator, Vol. 18, No. 1, Spring, 2002, p.1.  	[2] David Slater, ‘The Biennial Condition: On the encroachments of Art Criticism’, in Current Issues in Art and Culture, Vol. 31, No. 1, Spring, 2001, p.1. [3] Tim Jeeves, ‘Review: Biennials in a Social Science'', in Critical Inquiry, Vol. 19, No. 1, Spring, 2003, p.1. [4] David Slater, ‘Volume 20 of C.I.A.T.P.R. (Association for Byzantine and Renaissance Art Studies Quarterly) published in the journal C.I.A.T.P.R., 13:1 published by the Archives of Art and Technology, P. 43, Spring, 2004, p.1. [5] David Slater, ‘Biennials as a Socially Engaged Form’, in Current Art and Culture, Vol. 27, No. 1, Winter, 2004, p.1. [6]  David Slater,  	Unravelled, DVD, 2012. [7] David Slater, ‘The Art of Not Being Governed: An Anarchist History of Biennials’, in Current Art and Culture, Vol. 26, No. 2, Winter, 2005, p.1. [8] David Slater,  	Coming Apart: Growing Up Contemporary Art and the Media Crisis of the Millennium, New York: New Vessel Press, 2013. [9] David Slater,  	Unravelled, DVD, 2012. [10] David Slater,  	Unravelled, New York and London, London and San Francisco: New Vessel Press,</text>
    <text>biennials past and present. 	[1] See, for example, Nina Edge, ‘What is biennial?’, in The New Curator, Vol. 18, No. 1, Spring, 2002, p.1.  	[2] David Slater, ‘The Biennial Condition: On the encroachments of Art Criticism’, in Current Issues in Art and Culture, Vol. 31, No. 1, Spring, 2001, p.1. [3] Tim Jeeves, ‘Review: Biennials in a Social Science'', in Critical Inquiry, Vol. 19, No. 1, Spring, 2003, p.1. [4] David Slater, ‘Volume 20 of C.I.A.T.P.R. (Association for Byzantine and Renaissance Art Studies Quarterly) published in the journal C.I.A.T.P.R., 13:1 published by the Archives of Art and Technology, Pg. 51. [5] David Slater, ‘Review: Biennials in a Social Science'', in Current Issues in Art and Culture, Vol. 27, No. 2, Spring, 2004, pgs.1–2. [6] David Slater, ‘Review: Biennials in a Social Science'', in Contemporary Art Reviews, Vol. 22, Number 4, Spring, 2006, pgs.1–4. [7] David Slater, ‘Review: Biennials in a Social Science'', in Current Issues in Art and Culture, Vol. 27, No. 2, Spring, 2004, pgs.1–2. [8] David Slater, ‘Review: Biennials in a Social Science'', in Contemporary Art Reviews, Vol. 22, Number 4, Spring, 2006, pgs.1–4. [9] David Slater, ‘Review: Biennials in a Social Science'</text>
  </text>
  <text>
    biennials past and present. The official selection of the Venice Biennale is made up of approximately 120 artists, curators, journalists and other artists from around the world. The curator is the Joasia Krysa, an artist and director with a specialised expertise in biennials and contemporary art. The Dolphin Group is made up of Krysa, Jenny Gleadhill, the architect; William Turner, an architect; Leonore Kambov, an architect; Ingo Rose, an architect; Ram Jethmalani, a structural engineer; and Patricia McBride, an architect. The partnership was established in 1983 and brings together the expertise of a huge number of areas of expertise, allowing for the creation of innovative solutions to challenging problems. For the 2015 edition, the group presented a new concept for a new type of biennial – one that addresses social and spatial issues rather than strictly economic ones.  It was seen as an innovative approach to the existing
    <text>biennials past and present. The official selection of the Venice Biennale is made up of approximately 120 artists, curators, journalists and other artists from around the world. The curator is the Joasia Krysa, an artist and director with a specialised expertise in biennials and contemporary art. The Dolphin Group is made up of Krysa, Jenny Gleadhill, the architect; William Turner, an architect; Leonore Kambov, an architect; Ingo Rose, an architect; Ram Jethmalani, a structural engineer; and Patricia McBride, an architect. The partnership was established in 1983 and brings together the expertise of a huge number of areas of expertise, allowing for the creation of innovative solutions to challenging problems. For the 2015 edition, the group presented a new concept for a new type of biennial – one that addresses social and spatial issues rather than strictly economic ones.  It was seen as an innovative approach to the existing biennial, in that it offered a fresh approach to an older, traditional business model. But it seems even as biennials today, there is some sense of fatigue with the material culture form that is contributing to the increasing privatization of knowledge, making it harder for artists and others to contribute meaningfully to the production of social and cultural change.  The 2015 edition of the Biennale may have been a turning point for the Biennale, in that it signaled a change in the institutional and political context in which the event was taking place. Biennials are institutions now, and their curatorial powers have a critical importance. But there are signs that they too may be on the wane, as material culture takes root in the biennial’s organisational principles and culture as a material medium is actively filtered through the curatorial narrative. Only recently, a number of biennials have been proposing new curatorial approaches to the event; in 2017, a number of artists and creative</text>
    <text>biennials past and present. The official selection of the Venice Biennale is made up of approximately 120 artists, curators, journalists and other artists from around the world. The curator is the Joasia Krysa, an artist and director with a specialised expertise in biennials and contemporary art. The Dolphin Group is made up of Krysa, Jenny Gleadhill, the architect; William Turner, an architect; Leonore Kambov, an architect; Ingo Rose, an architect; Ram Jethmalani, a structural engineer; and Patricia McBride, an architect. The partnership was established in 1983 and brings together the expertise of a huge number of areas of expertise, allowing for the creation of innovative solutions to challenging problems. For the 2015 edition, the group presented a new concept for a new type of biennial – one that addresses social and spatial issues rather than strictly economic ones.  It was seen as an innovative approach to the existing biennial form, which is culturally dependent on economic growth. At the time, this meant that biennials had to be 'in the black' in order to support their significant costs. The new proposal added a socio-political dimension to the mix. The new model invited artists to take part in a larger, more global discussion through which they could unfold their artistic potentialities.  The proposal was received favourably by many art institutions, particularly those located in the West.  Biennials have become increasingly aware of the value of their geographical position. These days, a biennial is not just an exhibitionary venue but also a site of permanent movements of people, goods and objects. So a biennial that displays the latest work from a particular artist in a particular city is constantly asking, who are these people coming through the doors? Art biennials are particularly sensitive to the fact that the value of cultural resources is increasingly being distributed across a variety of platforms – be it through the adoption of</text>
    <text>biennials past and present. The official selection of the Venice Biennale is made up of approximately 120 artists, curators, journalists and other artists from around the world. The curator is the Joasia Krysa, an artist and director with a specialised expertise in biennials and contemporary art. The Dolphin Group is made up of Krysa, Jenny Gleadhill, the architect; William Turner, an architect; Leonore Kambov, an architect; Ingo Rose, an architect; Ram Jethmalani, a structural engineer; and Patricia McBride, an architect. The partnership was established in 1983 and brings together the expertise of a huge number of areas of expertise, allowing for the creation of innovative solutions to challenging problems. For the 2015 edition, the group presented a new concept for a new type of biennial – one that addresses social and spatial issues rather than strictly economic ones.  It was seen as an innovative approach to the existing biennial format, which has tended to prioritize economic concerns.  It also addressed the fact that despite the increasing numbers of biennials, the number of citizens continuing to flock to them is limited. As biennials become increasingly international in scope and function, the need for a common language to understand the interconnection and connexion of the biennial becomes ever more urgent.  The new format is a catalyst for its own proliferation, as is evident in the fact that the globalised character of biennials has spilled over into the form of cities.  The emergence of new modes of international travel,  	 transnational flight, is understood by art collectors and other art professionals as a globally relevant theme. Art biennials are particularly valuable in their current economic and cultural context because they are international in their form. That is, they are made up of art from many different places at once, and often in locations that are close to one another. Art biennials are art collectors</text>
    <text>biennials past and present. The official selection of the Venice Biennale is made up of approximately 120 artists, curators, journalists and other artists from around the world. The curator is the Joasia Krysa, an artist and director with a specialised expertise in biennials and contemporary art. The Dolphin Group is made up of Krysa, Jenny Gleadhill, the architect; William Turner, an architect; Leonore Kambov, an architect; Ingo Rose, an architect; Ram Jethmalani, a structural engineer; and Patricia McBride, an architect. The partnership was established in 1983 and brings together the expertise of a huge number of areas of expertise, allowing for the creation of innovative solutions to challenging problems. For the 2015 edition, the group presented a new concept for a new type of biennial – one that addresses social and spatial issues rather than strictly economic ones.  It was seen as an innovative approach to the existing biennale format, which is predominantly about economic considerations.  The approach seems to fly in the face of the prevailing economic model, which is oriented around the exchange of commodities and the movement of goods and people. In fact, the dominant economic form in the biennial market today is not that of the biennial, but that of the multiplebbody, which entails many different types of exhibitions that are typically shown in isolation.  Biennials are, in the main, occasions when a curator or a curatorial team works with an artist or a team of artists to explore an area of expertise. These are occasions when a different kind of relationship is forged between curator and audience, between artist and audience, between curatorial and academic. These kinds of relationships, which are often fragile and fraught, are the occasion for celebration, and they are, in the main, occasions for meeting, fornication and strife. But they also constitute occasions for caring, for friendship, for sharing in common struggles</text>
    <text>biennials past and present. The official selection of the Venice Biennale is made up of approximately 120 artists, curators, journalists and other artists from around the world. The curator is the Joasia Krysa, an artist and director with a specialised expertise in biennials and contemporary art. The Dolphin Group is made up of Krysa, Jenny Gleadhill, the architect; William Turner, an architect; Leonore Kambov, an architect; Ingo Rose, an architect; Ram Jethmalani, a structural engineer; and Patricia McBride, an architect. The partnership was established in 1983 and brings together the expertise of a huge number of areas of expertise, allowing for the creation of innovative solutions to challenging problems. For the 2015 edition, the group presented a new concept for a new type of biennial – one that addresses social and spatial issues rather than strictly economic ones.  It was seen as an innovative approach to the existing biennial format, which was always political and economic in character. The present biennial has been around for over 200 years and will continue to be so even after climate change becomes a major issue, as it already is in the United States and other parts of the world. The biennial format will continue to be a point of convergence for a number of different kinds of art and curatorial approaches, and a point of origin for new ones. But it will be interesting to see how the concerns of the biennial and its stakeholders, its audience, shape what is already happening in this city and beyond. The opportunity to be part of that convergence is there, but it is also there in the form of displacement, which is increasingly important as more and more of the planet is geosigned by the post-Fordist internet economy.The Cities at War (2014–16) was the first in a trilogy of exhibitions about cities at the Academy of Art and Design in</text>
    <text>biennials past and present. The official selection of the Venice Biennale is made up of approximately 120 artists, curators, journalists and other artists from around the world. The curator is the Joasia Krysa, an artist and director with a specialised expertise in biennials and contemporary art. The Dolphin Group is made up of Krysa, Jenny Gleadhill, the architect; William Turner, an architect; Leonore Kambov, an architect; Ingo Rose, an architect; Ram Jethmalani, a structural engineer; and Patricia McBride, an architect. The partnership was established in 1983 and brings together the expertise of a huge number of areas of expertise, allowing for the creation of innovative solutions to challenging problems. For the 2015 edition, the group presented a new concept for a new type of biennial – one that addresses social and spatial issues rather than strictly economic ones.  It was seen as an innovative approach to the existing biennial format, in that it addressed social and political concerns, and especially in the context of a major international exposition like Venice Biennale. The new concept was re-formulated around a global audience, and addressed the city as a major node, with its unique combination of physical, social and political heterogeneity. The idea was to construct a fundamentally different kind of biennial from the biennial of the present, in which the artist-centred exhibition takes precedence, and internationalism is subordinated to localism and localism's primary concern.  A number of artist-curator pairs worked alongside each other to explore the idea of ‘culture as a social process’ on a global scale. This was seen as an important step in the evolution of biennials, since it opened new frontiers in our notion of what is meant by this term and in what sense biennial.  A key insight was provided by the fact that the concept presented in the Venice Biennale</text>
    <text>biennials past and present. The official selection of the Venice Biennale is made up of approximately 120 artists, curators, journalists and other artists from around the world. The curator is the Joasia Krysa, an artist and director with a specialised expertise in biennials and contemporary art. The Dolphin Group is made up of Krysa, Jenny Gleadhill, the architect; William Turner, an architect; Leonore Kambov, an architect; Ingo Rose, an architect; Ram Jethmalani, a structural engineer; and Patricia McBride, an architect. The partnership was established in 1983 and brings together the expertise of a huge number of areas of expertise, allowing for the creation of innovative solutions to challenging problems. For the 2015 edition, the group presented a new concept for a new type of biennial – one that addresses social and spatial issues rather than strictly economic ones.  It was seen as an innovative approach to the existing biennial format, which was showing signs of fatigue. It was also seen as a departure from the institutionalised form, in which structural changes were being made in the face of ever-more elaborate financial support mechanisms. The new model was seen as the optimal one – one that would allow the organisation to devote more resources to the art, while also enabling the public to participate in the art through 'recreational' means. But what was most evident in the group’s discussions was the overwhelming consensus that this new system was unsustainable and needed to be fundamentally changed. At the same time, the group felt that there was no other viable representation of art and the social impact that it could have. So what emerged was a series of proposals that evolved over time into the present proposal, which is the basis for the organisation’s activities today.  The proposal was simple: why not create a biennale that addressed social and spatial issues in a local context and invited artists to collaborate in this</text>
    <text>biennials past and present. The official selection of the Venice Biennale is made up of approximately 120 artists, curators, journalists and other artists from around the world. The curator is the Joasia Krysa, an artist and director with a specialised expertise in biennials and contemporary art. The Dolphin Group is made up of Krysa, Jenny Gleadhill, the architect; William Turner, an architect; Leonore Kambov, an architect; Ingo Rose, an architect; Ram Jethmalani, a structural engineer; and Patricia McBride, an architect. The partnership was established in 1983 and brings together the expertise of a huge number of areas of expertise, allowing for the creation of innovative solutions to challenging problems. For the 2015 edition, the group presented a new concept for a new type of biennial – one that addresses social and spatial issues rather than strictly economic ones.  It was seen as an innovative approach to the existing biennial format, which had been consistently influenced by the needs of a metropolitan centre.  Art’s role is to negotiate, and Art and Architecture are two examples of situations in which the relationship between artist and organisation has been a key point of negotiation. The role of the artist is to enter into an organisation’s relationship with the organisation, and then to leave when she or he is not asked to. In this sense, the relationship between the Biennale and the city is strategic, and the city is a passive participant in her or his work. This is true not only of the Biennale itself, but of collaborative efforts between the Biennale, the Biennial Foundation, the Arts Council and other stakeholders. The city is often the instigator of the project, but this is often short lived. The Biennale’s mission statement includes a reference to the need to ‘reanimate’ cultural space, which in turn often calls for the creation</text>
    <text>biennials past and present. The official selection of the Venice Biennale is made up of approximately 120 artists, curators, journalists and other artists from around the world. The curator is the Joasia Krysa, an artist and director with a specialised expertise in biennials and contemporary art. The Dolphin Group is made up of Krysa, Jenny Gleadhill, the architect; William Turner, an architect; Leonore Kambov, an architect; Ingo Rose, an architect; Ram Jethmalani, a structural engineer; and Patricia McBride, an architect. The partnership was established in 1983 and brings together the expertise of a huge number of areas of expertise, allowing for the creation of innovative solutions to challenging problems. For the 2015 edition, the group presented a new concept for a new type of biennial – one that addresses social and spatial issues rather than strictly economic ones.  It was seen as an innovative approach to the existing biennale format, and a step towards a new kind of biennale.  The group’s new proposal clearly establishes the site, the purpose of the exhibition, and the audience, while also inviting fresh approaches to the structures and cultures that make up the biennale experience. It is a snapshot of a time and place – a city in its spatial and temporal complexity – in which the community is invited to be present in an important and often misunderstood role.  The proposal also proposes the establishment of a public board of directors, who would act as a democratic advisory board to the group, representing the city in the formation of the biennale. The board members would act independently and without compulsion, and would have no political or corporate standing. They would act in concert to achieve the goals of the biennale.  In previous biennales, the directors have been chosen through a merit-based selection process with an emphasis on those with the greatest potential for commercial or</text>
    <text>biennials past and present. The official selection of the Venice Biennale is made up of approximately 120 artists, curators, journalists and other artists from around the world. The curator is the Joasia Krysa, an artist and director with a specialised expertise in biennials and contemporary art. The Dolphin Group is made up of Krysa, Jenny Gleadhill, the architect; William Turner, an architect; Leonore Kambov, an architect; Ingo Rose, an architect; Ram Jethmalani, a structural engineer; and Patricia McBride, an architect. The partnership was established in 1983 and brings together the expertise of a huge number of areas of expertise, allowing for the creation of innovative solutions to challenging problems. For the 2015 edition, the group presented a new concept for a new type of biennial – one that addresses social and spatial issues rather than strictly economic ones.  It was seen as an innovative approach to the existing biennial format, while striving to offer a new perspective on the art form by incorporating a range of local, regional and global artists and curators. The proposal was received favourably by the artistic community and the Biennalee itself. However, significant questions were raised about the wisdom of this approach, especially as the biennial landscape has shifted ever more towards urbanisation and technological innovations. The new concept and the accompanying encyclopedic catalogue did not sit well with some critics, who saw a deliberate attempt to re-impose modernism and industrial society's influence in an urban setting.  The catalogue, which doubled as an interactive website, emphasised the city's past glories – but with a powerful new twist.  The glossary included a new element that some saw as an attempt to re-impose the city’s industrial core and generate a parallel architecture that would mirror the ‘infrastructure of the city’.  Titles that did not explicitly refer to</text>
  </text>
  <text>
    subsequent iterative processing by machines’ sensory organs. The result is ever increasing amounts of data being gathered and transmitted, yet with little or no human involvement. This is where computers come in. They’re small devices that help us do things through codes that mimic the actions of living creatures. Think of a computer as a proto-AI or 'smart' computer. So far, so mundane, but what computers can do is incredibly powerful. For example, a computer can ‘think’ big – able to recognise thousands of letters and numbers, and much more. It can also recognise faces – recognising a human being as one who is familiar with that street corner bakery that you and your mates frequent. A computer can detect patterns – human or algorithmic – in data – recognising features such as number of parking spaces, seasonal patterns and much more. All of this can be achieved through the use of programming languages that mimic the syntax of the human language – albeit in a computer.Nowadays, programming languages are
    <text>subsequent iterative processing by machines’ sensory organs. The result is ever increasing amounts of data being gathered and transmitted, yet with little or no human involvement. This is where computers come in. They’re small devices that help us do things through codes that mimic the actions of living creatures. Think of a computer as a proto-AI or 'smart' computer. So far, so mundane, but what computers can do is incredibly powerful. For example, a computer can ‘think’ big – able to recognise thousands of letters and numbers, and much more. It can also recognise faces – recognising a human being as one who is familiar with that street corner bakery that you and your mates frequent. A computer can detect patterns – human or algorithmic – in data – recognising features such as number of parking spaces, seasonal patterns and much more. All of this can be achieved through the use of programming languages that mimic the syntax of the human language – albeit in a computer.Nowadays, programming languages are written in robust and expressive human languages that can be understood by computers. In fact, almost all programming is done ‘programmatically’ via programming languages, and this has recently become the norm. As programming becomes more and more sophisticated, the stakes are raised – there is much more that can be done with programming. And although the scope of this kind of programming remains limited to highly skilled professionals, there is growing evidence that it can yield powerful effects.For example, the spread of HIV/AIDS via sexually transmitted infections (STIs) is believed to have been caused in part by the over-use of condoms among human populations in Western nations. Many in positions of power now believe that the lack of condoms is a major cause of the epidemic. But this is not at all clear cut. It is widely held that the human population was somehow magically endowed with a huge amount of 'design potential’ through in vitro fertilisation, and that since condoms are made of such-called �</text>
    <text>subsequent iterative processing by machines’ sensory organs. The result is ever increasing amounts of data being gathered and transmitted, yet with little or no human involvement. This is where computers come in. They’re small devices that help us do things through codes that mimic the actions of living creatures. Think of a computer as a proto-AI or 'smart' computer. So far, so mundane, but what computers can do is incredibly powerful. For example, a computer can ‘think’ big – able to recognise thousands of letters and numbers, and much more. It can also recognise faces – recognising a human being as one who is familiar with that street corner bakery that you and your mates frequent. A computer can detect patterns – human or algorithmic – in data – recognising features such as number of parking spaces, seasonal patterns and much more. All of this can be achieved through the use of programming languages that mimic the syntax of the human language – albeit in a computer.Nowadays, programming languages are written in robust, general purpose programming languages capable of executing large amounts of data. Most programming languages today target teams of programmers able to digest large amounts of data at once. In addition to allowing us to do amazing things with data, programming languages also often help us to create artificial intelligence (AI) systems that can learn by doing. For example, the language of the robot maker is likely to follow certain patterns and employ certain behaviours, but it will also happily let users create and tweak their own robot. This is because programming is a ‘deep’ skill, requiring mastery over one’s programming, and programming is a natural part of what is sometimes referred to as ‘the human’. Phrase-based programming, the act of writing programs in a particular language, is another natural ability that emerges alongside other natural skills – for example, language interpretation and human language manipulation. In addition, programming allows us to create virtual assistants – computers that understand spoken language but not necessarily the</text>
    <text>subsequent iterative processing by machines’ sensory organs. The result is ever increasing amounts of data being gathered and transmitted, yet with little or no human involvement. This is where computers come in. They’re small devices that help us do things through codes that mimic the actions of living creatures. Think of a computer as a proto-AI or 'smart' computer. So far, so mundane, but what computers can do is incredibly powerful. For example, a computer can ‘think’ big – able to recognise thousands of letters and numbers, and much more. It can also recognise faces – recognising a human being as one who is familiar with that street corner bakery that you and your mates frequent. A computer can detect patterns – human or algorithmic – in data – recognising features such as number of parking spaces, seasonal patterns and much more. All of this can be achieved through the use of programming languages that mimic the syntax of the human language – albeit in a computer.Nowadays, programming languages are written in robust Googol-like computer programming languages. This makes them useful not only for science, but also for the burgeoning field of artificial intelligence. Consider, for example, how programming languages can be used to detect plagiarism in academic writing. In the current climate of alleged 'intellectual appropriation’, it is imperative that programming is recognised as a legitimate field of study and profession. Despite this growing recognition, however, there is an urgent need to re-architect education around the complexities of programming and create new ways of engaging learners in the profession. This is due to a lack of access to programming languages for the developing mind, and the resultant lack of skills. Aims such as encouraging coding and ensuring that programming is a career choice are few and far between, yet many young people have their eyes trained on coding as a career option. In an attempt to tackle this problem, a ‘first programming language’ course was designed and delivered in Liverpool. This was offered in both</text>
    <text>subsequent iterative processing by machines’ sensory organs. The result is ever increasing amounts of data being gathered and transmitted, yet with little or no human involvement. This is where computers come in. They’re small devices that help us do things through codes that mimic the actions of living creatures. Think of a computer as a proto-AI or 'smart' computer. So far, so mundane, but what computers can do is incredibly powerful. For example, a computer can ‘think’ big – able to recognise thousands of letters and numbers, and much more. It can also recognise faces – recognising a human being as one who is familiar with that street corner bakery that you and your mates frequent. A computer can detect patterns – human or algorithmic – in data – recognising features such as number of parking spaces, seasonal patterns and much more. All of this can be achieved through the use of programming languages that mimic the syntax of the human language – albeit in a computer.Nowadays, programming languages are written in robust, modern procedural syntax that is understandable to machines. This makes them clear to software developers, and extends the capabilities of the machines even further. For example, the BBC’s FaceBook platform allows people to create and collaborate with face recognition software; people use this to recognise people, organisations, places and much more. Assembling data through algorithms is not just about machines understanding the world; it is also about social intent and aims. Assembling data through algorithms allows datasets to be re-used, allowing organisations to gain an edge in the battle for social purpose and cultural identity.  	What is at Stake in the Struggle for Cultural Identity? 	Cultural identity refers to the collective sense of belonging to a specific geographical area or set of places. It is a geopolitical concept, and it is being actively contested in the age of information overload. The quest for cultural identity is an old one, but the digital revolution has transformed the art world in unexpected ways.</text>
    <text>subsequent iterative processing by machines’ sensory organs. The result is ever increasing amounts of data being gathered and transmitted, yet with little or no human involvement. This is where computers come in. They’re small devices that help us do things through codes that mimic the actions of living creatures. Think of a computer as a proto-AI or 'smart' computer. So far, so mundane, but what computers can do is incredibly powerful. For example, a computer can ‘think’ big – able to recognise thousands of letters and numbers, and much more. It can also recognise faces – recognising a human being as one who is familiar with that street corner bakery that you and your mates frequent. A computer can detect patterns – human or algorithmic – in data – recognising features such as number of parking spaces, seasonal patterns and much more. All of this can be achieved through the use of programming languages that mimic the syntax of the human language – albeit in a computer.Nowadays, programming languages are honed to a finely tuned set of behaviours – able to achieve incredible feats of artificial intelligence. For example, the programming of a dog to follow a certain route through a city is a programmer’s dream, but it’s not feasible in the real world. Not all programming is created equal, however. For one thing, programming is inherently dangerous, and for a programming error to land a human being in a fire hydrant is quite possible. Also, not all programming is created equal. A programming error is an error, and mistakes do happen. But in the grand scheme of things, human error represents a small portion of success. Most programming errors, however, result in safer programming options. For example, programming the programming of a dog to follow certain routes through a city is achievable through the use of highly customized computer algorithms, and programming the programming of a cat to follow certain routes through a city is achievable through the use of enhanced programming languages. In the latter case, the use</text>
    <text>subsequent iterative processing by machines’ sensory organs. The result is ever increasing amounts of data being gathered and transmitted, yet with little or no human involvement. This is where computers come in. They’re small devices that help us do things through codes that mimic the actions of living creatures. Think of a computer as a proto-AI or 'smart' computer. So far, so mundane, but what computers can do is incredibly powerful. For example, a computer can ‘think’ big – able to recognise thousands of letters and numbers, and much more. It can also recognise faces – recognising a human being as one who is familiar with that street corner bakery that you and your mates frequent. A computer can detect patterns – human or algorithmic – in data – recognising features such as number of parking spaces, seasonal patterns and much more. All of this can be achieved through the use of programming languages that mimic the syntax of the human language – albeit in a computer.Nowadays, programming languages are written in robust and powerful procedural syntax – a development that comes as no surprise, given that humans have been programming in those languages for thousands of years. Phrase-based programming allows for the creation of self-similar units, each uniquely identifying a part of the system. For example, in programming a computer, symbols such as statements and procedures such as statements and procedures are written in code that mimics the behaviour of living things – animals and computers in this case. The output of such code is then text, which mimics the syntactic behaviour of human speech – albeit in a much more abstract and abstract way. For example, in programming a car, the output of a given statement is a sequence of characters that resemble the syntax of a human statement. Subsequent statements such as those listed above can be written in such a way as to incorporate data from multiple streams of data – in effect creating what Carolee Letterman would like to see replaced with: an ‘infrastructure of connected</text>
    <text>subsequent iterative processing by machines’ sensory organs. The result is ever increasing amounts of data being gathered and transmitted, yet with little or no human involvement. This is where computers come in. They’re small devices that help us do things through codes that mimic the actions of living creatures. Think of a computer as a proto-AI or 'smart' computer. So far, so mundane, but what computers can do is incredibly powerful. For example, a computer can ‘think’ big – able to recognise thousands of letters and numbers, and much more. It can also recognise faces – recognising a human being as one who is familiar with that street corner bakery that you and your mates frequent. A computer can detect patterns – human or algorithmic – in data – recognising features such as number of parking spaces, seasonal patterns and much more. All of this can be achieved through the use of programming languages that mimic the syntax of the human language – albeit in a computer.Nowadays, programming languages are written in robust and expressive human languages that are understandable to a sophisticated level of programming. This is due to the advent of programming languages that can ‘hack’ into hardware and software, and into almost anything that is ‘programmed’ – something that was previously ‘programmed’ only in highly specialised environments. For example, the ability to program viruses allows computers to do amazing things, such as spotting dangerous software, or hacking into networks to disrupt the flow of information or fuel resources, or even ‘programming’ machines to perform useful tasks. The possibilities are endless. The power of the AI is enabling new kinds of surveillance and control, and not only that, but is doing so in ways that are almost entirely invisible to the human characters who build and maintain those systems.And now we find ourselves caught in a web of pressures and contextual considerations that are almost entirely divorced from the actual world. Our local newspapers publish astoundingly long articles about projects that</text>
    <text>subsequent iterative processing by machines’ sensory organs. The result is ever increasing amounts of data being gathered and transmitted, yet with little or no human involvement. This is where computers come in. They’re small devices that help us do things through codes that mimic the actions of living creatures. Think of a computer as a proto-AI or 'smart' computer. So far, so mundane, but what computers can do is incredibly powerful. For example, a computer can ‘think’ big – able to recognise thousands of letters and numbers, and much more. It can also recognise faces – recognising a human being as one who is familiar with that street corner bakery that you and your mates frequent. A computer can detect patterns – human or algorithmic – in data – recognising features such as number of parking spaces, seasonal patterns and much more. All of this can be achieved through the use of programming languages that mimic the syntax of the human language – albeit in a computer.Nowadays, programming languages are developed primarily in the United States and are heavily influenced by the computer science taught in the US. Thus programming languages like Python and Ruby on Rails are examples of programmes that are heavily influenced by the computer systems that they’re written in. In addition, many software development languages are ‘Scripting Languages’, referring to the fact that scripts are often used to automate parts of the programme. For example, in the programming of self-driving cars, the imperative part of the car – the part that drives the car – is programmed with mathematical formulas that mimic the behaviour of the Greek language. By imitating a given system, the computer is able to detect when the car is in ‘good’ or ‘bad’ condition, and can do something about it. Such computer programming is being done in large part through the use of big data, which is being collected and transmitted all across the world through cars driven by data capture systems. This kind of data collection and analysis is</text>
    <text>subsequent iterative processing by machines’ sensory organs. The result is ever increasing amounts of data being gathered and transmitted, yet with little or no human involvement. This is where computers come in. They’re small devices that help us do things through codes that mimic the actions of living creatures. Think of a computer as a proto-AI or 'smart' computer. So far, so mundane, but what computers can do is incredibly powerful. For example, a computer can ‘think’ big – able to recognise thousands of letters and numbers, and much more. It can also recognise faces – recognising a human being as one who is familiar with that street corner bakery that you and your mates frequent. A computer can detect patterns – human or algorithmic – in data – recognising features such as number of parking spaces, seasonal patterns and much more. All of this can be achieved through the use of programming languages that mimic the syntax of the human language – albeit in a computer.Nowadays, programming languages are written in general terms, and can be understood to mean anything. So if you’re programming in the programming language of the humanoid computer, you’re actually programming the very machine that you’re programming. So while you’re actually programming the computer, you’re actually programming the very infrastructure of the city. You’re programming the whole city, but that doesn’t make any sense. Why would I want to live in a city? Why would I want to buy a house? Why would I want to go to the movies? Why would I want to go to a carnival any more than buying a house or a car does? Programming the very infrastructure of the city allows such questions to be approached more constructively. For example, the programming of housing allows one to create affordable housing, and programming the housing itself allows for the self-replacement of labour. A computer can do all of this for us.In fact, programming the very</text>
    <text>subsequent iterative processing by machines’ sensory organs. The result is ever increasing amounts of data being gathered and transmitted, yet with little or no human involvement. This is where computers come in. They’re small devices that help us do things through codes that mimic the actions of living creatures. Think of a computer as a proto-AI or 'smart' computer. So far, so mundane, but what computers can do is incredibly powerful. For example, a computer can ‘think’ big – able to recognise thousands of letters and numbers, and much more. It can also recognise faces – recognising a human being as one who is familiar with that street corner bakery that you and your mates frequent. A computer can detect patterns – human or algorithmic – in data – recognising features such as number of parking spaces, seasonal patterns and much more. All of this can be achieved through the use of programming languages that mimic the syntax of the human language – albeit in a computer.Nowadays, programming languages are written in highly sophisticated human languages that are difficult to understand. The difficulty lies in the fact that programming is done using rules that mimic the human language in sophisticated ways. Rules are encoded in programming languages and used to create complex systems. For example, the English language dictates that when programming a car, the driver’s job is to maintain the speed, angle and height of the car as closely as possible in relation to the terrain. Given this, it is easy to see how programming a car could be automated. In fact, Mercedes’s research institute is developing autonomous driving software that looks at millions of kilometres of data a day. Rules are not the issue here; what is at stake is the quest for autonomy. In a future where ‘rules’ are not only programmed but also contingently evolved, it is possible to envision a world in which ‘rules’ do in fact play a role in the creation and operation of the world.This is the futuristic thinking that has</text>
  </text>
  <text>
    using machine learning techniques to detect patterns in images that can be used to identify objects in a photo, such as human features or hidden messages. 	In  autonomisation  parlance, the sharing of resources (including images) is prioritised over the appropriation and reuse of those resources, because a resource does not exist in a primordial state without the co-operation and co-management of those with whom it is shared.1 	The sharing of images and resources is possible through the ‘sharing economy’ – an emerging field of sharing and collaborative behaviour that seeks to replace the need for professionals to be resourceful and creative in their uses of images and data.2 	The sharing of images and data sets is desirable in part because of the proliferation of image-processing and publishing platforms – platforms that make it easier and cheaper to produce and share images than ever before, and to share images more widely – making it easier to discover great images than ever before. These sharing platforms are a result of competitive
    <text>using machine learning techniques to detect patterns in images that can be used to identify objects in a photo, such as human features or hidden messages. 	In  autonomisation  parlance, the sharing of resources (including images) is prioritised over the appropriation and reuse of those resources, because a resource does not exist in a primordial state without the co-operation and co-management of those with whom it is shared.1 	The sharing of images and resources is possible through the ‘sharing economy’ – an emerging field of sharing and collaborative behaviour that seeks to replace the need for professionals to be resourceful and creative in their uses of images and data.2 	The sharing of images and data sets is desirable in part because of the proliferation of image-processing and publishing platforms – platforms that make it easier and cheaper to produce and share images than ever before, and to share images more widely – making it easier to discover great images than ever before. These sharing platforms are a result of competitive pressures on image quality and image quantity; of course, there is a cost to such rapid growth, but the sharing of images and data sets is a by-product of the sharing economy’s increased efficiency. 	Once images are produced and shared, their characteristics shift, become identifiable, and are used to help categorise and categorise individuals, groups or large corporates – entities that then pay organisations to do things with those images or data sets. 	There are now over 6,000 categorisation tools and websites dedicated to categorising and organising images, data or objects, making it easier to find good images or data sets.3 	There are over 23,000 image processing packages available for download, making it easier to find good images than ever before.4 	There are over 6,000 image processing packages available for commercial download, making it easier to find good images than ever before.5 	There are over 6,000 image-processing packages available for research</text>
    <text>using machine learning techniques to detect patterns in images that can be used to identify objects in a photo, such as human features or hidden messages. 	In  autonomisation  parlance, the sharing of resources (including images) is prioritised over the appropriation and reuse of those resources, because a resource does not exist in a primordial state without the co-operation and co-management of those with whom it is shared.1 	The sharing of images and resources is possible through the ‘sharing economy’ – an emerging field of sharing and collaborative behaviour that seeks to replace the need for professionals to be resourceful and creative in their uses of images and data.2 	The sharing of images and data sets is desirable in part because of the proliferation of image-processing and publishing platforms – platforms that make it easier and cheaper to produce and share images than ever before, and to share images more widely – making it easier to discover great images than ever before. These sharing platforms are a result of competitive pressures in an industry already prone to periodic out-competition from cheap labour, competitive media platforms and cheap data, as well as of the fact that technological innovation often times saves lives. 	There are many different kinds of sharing platforms, but the main ones that I see employed in large datasets are Facebook likes and Instagram feeds. Facebook likes and Instagram feeds are dominated by images from the social network, and act as a kind of public archive of sorts, capturing moments from the internet that are preserved amongst the images that are shared. Popularly, they are seen as a form of ‘meme magic’ – a kind of magic that allows the sharing of images across social media platforms, often performed with fluid images and images that are manipulated to replicate the actions of the users that like or follow an image. 	Because of the inherent collaborative nature of the internet – which tends to deplete resources – sharing an image across platforms is a ‘gift’ from the sharing of images</text>
    <text>using machine learning techniques to detect patterns in images that can be used to identify objects in a photo, such as human features or hidden messages. 	In  autonomisation  parlance, the sharing of resources (including images) is prioritised over the appropriation and reuse of those resources, because a resource does not exist in a primordial state without the co-operation and co-management of those with whom it is shared.1 	The sharing of images and resources is possible through the ‘sharing economy’ – an emerging field of sharing and collaborative behaviour that seeks to replace the need for professionals to be resourceful and creative in their uses of images and data.2 	The sharing of images and data sets is desirable in part because of the proliferation of image-processing and publishing platforms – platforms that make it easier and cheaper to produce and share images than ever before, and to share images more widely – making it easier to discover great images than ever before. These sharing platforms are a result of competitive pressures on image-processing facilities to reduce image sizes, improve image quality and increase image variety, while maintaining image quality and image variety at the same time. This reduces the need for image-processing facilities to be located in cities, and to employ large numbers of workers in image-processing tasks. 	The sharing of images and data sets is desirable in part because of the proliferation of image-processing and publishing platforms – platforms that make it easier and cheaper to produce and share images than ever before, and to share images more widely – making it easier to discover great images than ever before. These sharing platforms are a result of competitive pressures on image-processing facilities to reduce image sizes, improve image quality and increase image variety, while maintaining image quality and image variety at the same time. This reduces the need for image-processing facilities to be located in cities, and to employ large numbers of workers in image-processing tasks. 	Image-processing stacks are made increasingly complex, and the number of</text>
    <text>using machine learning techniques to detect patterns in images that can be used to identify objects in a photo, such as human features or hidden messages. 	In  autonomisation  parlance, the sharing of resources (including images) is prioritised over the appropriation and reuse of those resources, because a resource does not exist in a primordial state without the co-operation and co-management of those with whom it is shared.1 	The sharing of images and resources is possible through the ‘sharing economy’ – an emerging field of sharing and collaborative behaviour that seeks to replace the need for professionals to be resourceful and creative in their uses of images and data.2 	The sharing of images and data sets is desirable in part because of the proliferation of image-processing and publishing platforms – platforms that make it easier and cheaper to produce and share images than ever before, and to share images more widely – making it easier to discover great images than ever before. These sharing platforms are a result of competitive pressures on image-editing platforms to keep up with demand, and a proliferation of self-improvement programmes that seek to incorporate more people, more images and more images in more places. 	As platforms for sharing images, they often function as ‘double-edged swordies’. On the one hand, the proliferation of platforms encourages the sharing of images across platforms (platforms being what platforms discover, recognise and incorporate into their networks); on the other, the proliferation of platforms also encourages the sharing of images among networks (platforms learning, recognising and incorporating new networks). Platforms may adopt practices and features that are helpful or inconvenient to their users, depending on what is useful or inconvenient about a given situation. Platforms may also forbid certain activities or provide superior or lesser alternatives to their users. But perhaps the primary concern of the platform holder is itself threatened with obsolescence. Platforms may well become redundant – they are readied to provide superior or lesser alternatives to their</text>
    <text>using machine learning techniques to detect patterns in images that can be used to identify objects in a photo, such as human features or hidden messages. 	In  autonomisation  parlance, the sharing of resources (including images) is prioritised over the appropriation and reuse of those resources, because a resource does not exist in a primordial state without the co-operation and co-management of those with whom it is shared.1 	The sharing of images and resources is possible through the ‘sharing economy’ – an emerging field of sharing and collaborative behaviour that seeks to replace the need for professionals to be resourceful and creative in their uses of images and data.2 	The sharing of images and data sets is desirable in part because of the proliferation of image-processing and publishing platforms – platforms that make it easier and cheaper to produce and share images than ever before, and to share images more widely – making it easier to discover great images than ever before. These sharing platforms are a result of competitive pressures on image-processing facilities to reduce costs and to maximise the number of images that can be produced and shared in a given timeframe; of course, the image-processing and publishing industries also exert significant socio-political pressures. 3 	But the sharing of images and data sets is just one way in which the sharing economy is reshaping how we communicate, consume, and organise our lives. There are many more images and data-sets to be produced and shared, and these are distributed globally via algorithms and social media platforms – a form of multi-scale image-processing that seeks to meet the ever-more-demanding needs of the sharing economy. 	In an attempt to quantify the extent and dynamic nature of this sharing, I have developed a new kind of image-recognition app – ImageJ – that seeks to identify images and data sets that have been processed in a way that align with predetermined criteria. 4 	ImageJ is just one example of many software tools and</text>
    <text>using machine learning techniques to detect patterns in images that can be used to identify objects in a photo, such as human features or hidden messages. 	In  autonomisation  parlance, the sharing of resources (including images) is prioritised over the appropriation and reuse of those resources, because a resource does not exist in a primordial state without the co-operation and co-management of those with whom it is shared.1 	The sharing of images and resources is possible through the ‘sharing economy’ – an emerging field of sharing and collaborative behaviour that seeks to replace the need for professionals to be resourceful and creative in their uses of images and data.2 	The sharing of images and data sets is desirable in part because of the proliferation of image-processing and publishing platforms – platforms that make it easier and cheaper to produce and share images than ever before, and to share images more widely – making it easier to discover great images than ever before. These sharing platforms are a result of competitive pressures on image-processing facilities to meet the demands of a rapidly expanding clientele of image-editors and graphic designers. 	It is not just clientele figures that are impacted by the proliferation of image-processing and publishing platforms; artists and institutions alike are affected by the proliferation of image-processing and publishing platforms. Image-editing platforms such as Photoshop, image-editing apps such as Invent9 and Photo Finish, and magazine-style publishing platforms such as The New York Times and The Washington Post are the primary platforms through which images are produced and shared. 	As the clientele for image-editing and publishing platforms grows larger, so does the clientele for data-processing and storage, which results in increased data rates, which in turn consumes even more of the infrastructure – increasing figures of data and so on. As a result of this circular nature to the technology stack, artists are often left with artefacts of technology that are either antiquated or useless. </text>
    <text>using machine learning techniques to detect patterns in images that can be used to identify objects in a photo, such as human features or hidden messages. 	In  autonomisation  parlance, the sharing of resources (including images) is prioritised over the appropriation and reuse of those resources, because a resource does not exist in a primordial state without the co-operation and co-management of those with whom it is shared.1 	The sharing of images and resources is possible through the ‘sharing economy’ – an emerging field of sharing and collaborative behaviour that seeks to replace the need for professionals to be resourceful and creative in their uses of images and data.2 	The sharing of images and data sets is desirable in part because of the proliferation of image-processing and publishing platforms – platforms that make it easier and cheaper to produce and share images than ever before, and to share images more widely – making it easier to discover great images than ever before. These sharing platforms are a result of competitive pressures generated by the information society, and their proliferation is a by-product of the information society’s own self-sufficiency. These sharing platforms exist in part because of the inaccessibility of large amounts of data about the locations of objects, people, places and objects that are mined and produced in a sustained way. This continuous recording of locations, people, places and objects is what image-processing and image-publishing platforms traditionally do, and what data capture and publishing means today. 	However, these sharing mechanisms also exist because of the inaccessibility of large amounts of data, and the fact that such data does not correlate with increased sharing, which means that ‘ordinary’ people can and do create and participate in these sharing economies without necessarily fitting into either the information society or the media society. 	Alternative sharing mechanisms exist because of the inaccessibility of large amounts of data, and the fact that data capture and publishing today is possible at a scale that enables</text>
    <text>using machine learning techniques to detect patterns in images that can be used to identify objects in a photo, such as human features or hidden messages. 	In  autonomisation  parlance, the sharing of resources (including images) is prioritised over the appropriation and reuse of those resources, because a resource does not exist in a primordial state without the co-operation and co-management of those with whom it is shared.1 	The sharing of images and resources is possible through the ‘sharing economy’ – an emerging field of sharing and collaborative behaviour that seeks to replace the need for professionals to be resourceful and creative in their uses of images and data.2 	The sharing of images and data sets is desirable in part because of the proliferation of image-processing and publishing platforms – platforms that make it easier and cheaper to produce and share images than ever before, and to share images more widely – making it easier to discover great images than ever before. These sharing platforms are a result of competitive pressures, but also of the emergence of collaborative applications between different platforms – platforms that share resources and connect to one another. 	The sharing of images and data sets is possible through the use of platforms, but also because of the emergence of collaborative applications between different platforms – platforms that share data and allow their users to ‘hack’ the systems that house images and data. 	The sharing of images and data sets is possible through the use of protocols, but also because of the emergence of collaborative applications between different platforms – platforms that share data and allow the users to ‘hack’ the systems that house images and data.[2] This last point is perhaps the most relevant and relevant-to-us discussion here. 	The sharing of data sets and images is a data grab, and with good reason. As the saying goes, anything can be done with an image – be it a photo, a video or an e-mail – and the assumption of control over one</text>
    <text>using machine learning techniques to detect patterns in images that can be used to identify objects in a photo, such as human features or hidden messages. 	In  autonomisation  parlance, the sharing of resources (including images) is prioritised over the appropriation and reuse of those resources, because a resource does not exist in a primordial state without the co-operation and co-management of those with whom it is shared.1 	The sharing of images and resources is possible through the ‘sharing economy’ – an emerging field of sharing and collaborative behaviour that seeks to replace the need for professionals to be resourceful and creative in their uses of images and data.2 	The sharing of images and data sets is desirable in part because of the proliferation of image-processing and publishing platforms – platforms that make it easier and cheaper to produce and share images than ever before, and to share images more widely – making it easier to discover great images than ever before. These sharing platforms are a result of competitive pressures on image-production platforms to meet user demands, as well as the collaborative efforts of collaborative agencies such as those funded by the UK’sÜs Health Research​ Authority (HRAs) and the US National Institutes of Health (NIH), whose collaborative research agendas often overlap with those of the UK’s Medical Research Council (MRC). These agencies have worked together to develop promising new image-processing and publishing platforms that reduce the need for image-editing by making it easier and cheaper to share images and data sets. 	One example of a sharing platform is  ImageJourney, developed by the UK’s Medical Research Council and NIH, and the US National Institutes of Health, in collaboration with Microsoft Research. ImageJourney allows researchers to download, compile, and search medical and scientific literature across a broad range of conditions, allowing them to discover new therapies or detect new diseases. ImageJourney was recently named one of N.F.L. Top</text>
    <text>using machine learning techniques to detect patterns in images that can be used to identify objects in a photo, such as human features or hidden messages. 	In  autonomisation  parlance, the sharing of resources (including images) is prioritised over the appropriation and reuse of those resources, because a resource does not exist in a primordial state without the co-operation and co-management of those with whom it is shared.1 	The sharing of images and resources is possible through the ‘sharing economy’ – an emerging field of sharing and collaborative behaviour that seeks to replace the need for professionals to be resourceful and creative in their uses of images and data.2 	The sharing of images and data sets is desirable in part because of the proliferation of image-processing and publishing platforms – platforms that make it easier and cheaper to produce and share images than ever before, and to share images more widely – making it easier to discover great images than ever before. These sharing platforms are a result of competitive pressures generated by a changing economy, and social media platforms that constantly post great images of themselves alongside captions that emphasise the image (and the author) in highly visible, viral and addictive ways. 	With the ubiquity of social media and image-processing platforms, it is easy to forget that photography is a visual medium first and foremost. So it is easy to lose sight of the many other ways images circulate in the social and cultural landscape. 	The sharing economy is a phenomenon that has come to be understood as fundamentally connected to the sharing of resources, and is therefore likely to continue to be discussed and theorised about in the cultural studies and art  outsourcing milieu. But it is also a phenomenon that has become extremely profitable for a number of platform owners and users at a scale that is not confined to a few cities. Platforms that capture and distribute images and data sets are essential for the proliferation of platforms, and the establishment of ever more autonomous  sharing forms of culture.</text>
  </text>
  <text>
    using machine learning techniques to detect unique features in photographs, and automatically generate images for websites that point to specific images. These images would then be used to advertise the websites of luxury goods or artisanal goods, or simply to mark places where the photographer found photographs of art. 	Such systems are pervasive with the rise of data-intensive communications and media, but the aim of this book is to entangle the photographer in the process of generating images and images through what it calls ‘instant image processing’. 	This can be seen as an attempt to reconcile the objective (what is sometimes called the media constrain) and subjective (what the photographer does with the aim of creating images) aspects of the relationship. The objective is to produce images that are of a particular kind, or of a particular kind of kind, and to this end the photographer must be able to manipulate images in such a way as to please a client or satisfy a crowd. The subjective aspect of the relationship, however, is to
    <text>using machine learning techniques to detect unique features in photographs, and automatically generate images for websites that point to specific images. These images would then be used to advertise the websites of luxury goods or artisanal goods, or simply to mark places where the photographer found photographs of art. 	Such systems are pervasive with the rise of data-intensive communications and media, but the aim of this book is to entangle the photographer in the process of generating images and images through what it calls ‘instant image processing’. 	This can be seen as an attempt to reconcile the objective (what is sometimes called the media constrain) and subjective (what the photographer does with the aim of creating images) aspects of the relationship. The objective is to produce images that are of a particular kind, or of a particular kind of kind, and to this end the photographer must be able to manipulate images in such a way as to please a client or satisfy a crowd. The subjective aspect of the relationship, however, is to create images that are not of that particular client or crowd. This is sometimes called the crowd effect or the ‘social beauty gap’. 	The book aims to help the photographer achieve both aspects of the relationship: the object and the audience. It attempts to show how the photographer might engage in the practices outlined in the book, while remaining detached from the audience and the object. It attempts to show how the crowd can be used to produce images that are not of the client or of the crowd, but of a mixed kind, or of a mixed crowd. 	The book is divided into three sections: the theatre, the streets and the city. 	The Theatre 	The Theatre is a city-sized recreation of the Roman theatre, built in AD209 and named for the Roman actor Julius Caesar. It was the scene of major battles including the famous ‘Tuscany Maneuver’ that restored order to chaotic disorder. It is the largest surviving theatre structure and</text>
    <text>using machine learning techniques to detect unique features in photographs, and automatically generate images for websites that point to specific images. These images would then be used to advertise the websites of luxury goods or artisanal goods, or simply to mark places where the photographer found photographs of art. 	Such systems are pervasive with the rise of data-intensive communications and media, but the aim of this book is to entangle the photographer in the process of generating images and images through what it calls ‘instant image processing’. 	This can be seen as an attempt to reconcile the objective (what is sometimes called the media constrain) and subjective (what the photographer does with the aim of creating images) aspects of the relationship. The objective is to produce images that are of a particular kind, or of a particular kind of kind, and to this end the photographer must be able to manipulate images in such a way as to please a client or satisfy a crowd. The subjective aspect of the relationship, however, is to create images that are not of a particular kind, or of a particular kind, but of a kind that appeals to a crowd. This is what the crowd in the Venice Biennale wants – it is what the Biennale stands for. It is not enough for the Biennale to be a ‘researcher in the cultural industry’; it must also be able to ‘produce culturally responsive work in support of this exploration’.2 	The Biennale has always strived to present the full range of its potential, and to this end it has published a series of books that capture its diverse interests – photography, biennials, curatorial work, urban design and cultural studies – in one place. These books have sold well, and continue to do so, although there is some evidence that this is changing. The new edition of the Biennale book is out now, and the editors’ response to our book is published in December</text>
    <text>using machine learning techniques to detect unique features in photographs, and automatically generate images for websites that point to specific images. These images would then be used to advertise the websites of luxury goods or artisanal goods, or simply to mark places where the photographer found photographs of art. 	Such systems are pervasive with the rise of data-intensive communications and media, but the aim of this book is to entangle the photographer in the process of generating images and images through what it calls ‘instant image processing’. 	This can be seen as an attempt to reconcile the objective (what is sometimes called the media constrain) and subjective (what the photographer does with the aim of creating images) aspects of the relationship. The objective is to produce images that are of a particular kind, or of a particular kind of kind, and to this end the photographer must be able to manipulate images in such a way as to please a client or satisfy a crowd. The subjective aspect of the relationship, however, is to be found in the second part of the equation: the cultural factors that influence what is perceived as aesthetically pleasing, what is considered interesting, what is considered aesthetically significant. 	The second part of the equation is the art that is to be depicted. Although the photographer has a conceptual understanding of what he wants to create, he does not have the power necessary to execute a project with the images that are produced. The artist has this power, but it is used sparingly and only when necessary. The powers that be see the enormous potential that such an approach could hold, and one can imagine artists and photographers coming together to share ideas and encourage collaboration. 	The challenge of instigating such a project would be huge, but the power to actually do it would reside in the hands of a diverse group of artists, photographers and curators. There would be a clear separation between the designers and the artists; the latter having little power but the artisans having a lot of influence. The choice</text>
    <text>using machine learning techniques to detect unique features in photographs, and automatically generate images for websites that point to specific images. These images would then be used to advertise the websites of luxury goods or artisanal goods, or simply to mark places where the photographer found photographs of art. 	Such systems are pervasive with the rise of data-intensive communications and media, but the aim of this book is to entangle the photographer in the process of generating images and images through what it calls ‘instant image processing’. 	This can be seen as an attempt to reconcile the objective (what is sometimes called the media constrain) and subjective (what the photographer does with the aim of creating images) aspects of the relationship. The objective is to produce images that are of a particular kind, or of a particular kind of kind, and to this end the photographer must be able to manipulate images in such a way as to please a client or satisfy a crowd. The subjective aspect of the relationship, however, is to produce images that are not of a particular kind, or of a particular kind, and this is where the problems of cultural capture and reproducibility emerge. The tendency is for the photographer to produce images that are of a particular kind, or of a particular kind, but also for the courts to intervene and compel the photographer to produce images that are of that kind. 	In the 1990s and early 2000s the practice of cultural capture and reproducibility was often discussed as a way to promote a ‘balance of ideas and knowledge’ between the public and the press, and in turn the press to promote a more progressive agenda. It was often said that a press gallery should exhibit works by and on a first come, first serve basis, so that the public can ‘stack the deck’ in their favour. 	However, this approach to public access to art produced by artists who were members of a protected cultural group often faced serious restrictions. For example, most bienn</text>
    <text>using machine learning techniques to detect unique features in photographs, and automatically generate images for websites that point to specific images. These images would then be used to advertise the websites of luxury goods or artisanal goods, or simply to mark places where the photographer found photographs of art. 	Such systems are pervasive with the rise of data-intensive communications and media, but the aim of this book is to entangle the photographer in the process of generating images and images through what it calls ‘instant image processing’. 	This can be seen as an attempt to reconcile the objective (what is sometimes called the media constrain) and subjective (what the photographer does with the aim of creating images) aspects of the relationship. The objective is to produce images that are of a particular kind, or of a particular kind of kind, and to this end the photographer must be able to manipulate images in such a way as to please a client or satisfy a crowd. The subjective aspect of the relationship, however, is to a large extent determined by the photographer and the company in which he works. The fact that images are taken with a particular subject in mind and that the photographer has access to a particular set of images means that the kind of objectivity that the photographer seeks is unattainable. 	One solution to the problem of the image is to create a platform on which the photographer can place his or her ultimate control. The platform is a novel concept in itself, and one that demands a great deal of technical and artistic acumen to build and manage. However, the main thrust of the book is based around the notion that the photographer has a responsibility to the public and that the public has a fundamental right to know how images were produced and used. This is shown through various illustrations in the book, which illustrate the relationship between the photographer and the site where images are taken. 	Some of these images are stills from films or still photographs, showing the areas where the relationship between the photographer and the site</text>
    <text>using machine learning techniques to detect unique features in photographs, and automatically generate images for websites that point to specific images. These images would then be used to advertise the websites of luxury goods or artisanal goods, or simply to mark places where the photographer found photographs of art. 	Such systems are pervasive with the rise of data-intensive communications and media, but the aim of this book is to entangle the photographer in the process of generating images and images through what it calls ‘instant image processing’. 	This can be seen as an attempt to reconcile the objective (what is sometimes called the media constrain) and subjective (what the photographer does with the aim of creating images) aspects of the relationship. The objective is to produce images that are of a particular kind, or of a particular kind of kind, and to this end the photographer must be able to manipulate images in such a way as to please a client or satisfy a crowd. The subjective aspect of the relationship, however, is to produce images that are not of that particular kind. The challenge for the photographer is to find ways to expose his or her work to both, the crowd and the client. The client is expected to reward the photographer for his or her work, and the client is expected to reward the photographer for his or her images. If the client makes images that are not of the intended audience, or of a kind suited to the audience, the photographer will find himself or herself in conflict with the audience’s self-image. 	If the client speaks directly to the photographer, or if the client represents an idea in images, the photographer has a greater ability to respond. For example, the client may delegate the production of images to a team of graphic designers, photographers and journalists who may represent a range of values, or it may be the photographer himself who undertakes the project. The challenge for the photographer is to find ways to engage with both the delegation and the representation of values. 	In</text>
    <text>using machine learning techniques to detect unique features in photographs, and automatically generate images for websites that point to specific images. These images would then be used to advertise the websites of luxury goods or artisanal goods, or simply to mark places where the photographer found photographs of art. 	Such systems are pervasive with the rise of data-intensive communications and media, but the aim of this book is to entangle the photographer in the process of generating images and images through what it calls ‘instant image processing’. 	This can be seen as an attempt to reconcile the objective (what is sometimes called the media constrain) and subjective (what the photographer does with the aim of creating images) aspects of the relationship. The objective is to produce images that are of a particular kind, or of a particular kind of kind, and to this end the photographer must be able to manipulate images in such a way as to please a client or satisfy a crowd. The subjective aspect of the relationship, however, is to produce images that are not of that particular kind. The question becomes, which kind? In the context of the digital revolution, there is no clear answer. But in the age of the internet ‘image processing software’ and the smartphone camera, the image processing that is possible through the use of algorithms today is a reality that a client (Facebook, for example) can pilot. 	The rapid proliferation of algorithms has led to a new kind of artist’s body of work, in which the client is no longer the artist but rather an administrator of algorithms. This is particularly true for those whose work is based on data, as is the case with the photography of landscapes or monuments. The use of big data enables the organisation of information in such a way as to enable any user to engage with the work in a way that corresponds to the client’s interests. For example, the proliferation of algorithms that understand geographical location – the position of buildings, vehicles, objects and people –</text>
    <text>using machine learning techniques to detect unique features in photographs, and automatically generate images for websites that point to specific images. These images would then be used to advertise the websites of luxury goods or artisanal goods, or simply to mark places where the photographer found photographs of art. 	Such systems are pervasive with the rise of data-intensive communications and media, but the aim of this book is to entangle the photographer in the process of generating images and images through what it calls ‘instant image processing’. 	This can be seen as an attempt to reconcile the objective (what is sometimes called the media constrain) and subjective (what the photographer does with the aim of creating images) aspects of the relationship. The objective is to produce images that are of a particular kind, or of a particular kind of kind, and to this end the photographer must be able to manipulate images in such a way as to please a client or satisfy a crowd. The subjective aspect of the relationship, however, is to create images that are not merely of a clientele that is interested in acquiring images of art, but also images that are not merely images of art. This last condition is what the photographer does with the images produced, but it is what the client (including a museum) does not. The photographer does not publish the images, but the client uses the images to market the products and services of the artist, and thereby establishes a relationship with the public that is mediated through the images. 	The aim of the book is to the extent that it addresses the photographer’s role in the process of image formation and distribution. But the major tasks of the artist and the museum are not mutually exclusive. The former must be responsive to the latter. The aim of this book is to the extent that it addresses the role of the museum and the photographer, but the major contentions are not mutually exclusive. For example, there is a division between the images of the Venice Biennale that the Bienn</text>
    <text>using machine learning techniques to detect unique features in photographs, and automatically generate images for websites that point to specific images. These images would then be used to advertise the websites of luxury goods or artisanal goods, or simply to mark places where the photographer found photographs of art. 	Such systems are pervasive with the rise of data-intensive communications and media, but the aim of this book is to entangle the photographer in the process of generating images and images through what it calls ‘instant image processing’. 	This can be seen as an attempt to reconcile the objective (what is sometimes called the media constrain) and subjective (what the photographer does with the aim of creating images) aspects of the relationship. The objective is to produce images that are of a particular kind, or of a particular kind of kind, and to this end the photographer must be able to manipulate images in such a way as to please a client or satisfy a crowd. The subjective aspect of the relationship, however, is to be found in the second part of the equation – the part that makes images (and thus images per se, not just images per se). This is often misconstrued as an emphasis on aesthetics, but in fact much of what the photographer does with images is to satisfy a client’s needs, and this includes what he or she does with the images themselves. 	Thus, the image processing that characterises modern image making is to a large extent indistinguishable from what image processing firms such as Adobe do with images, and this is true not just with regards to aesthetics, but also in terms of actual image processing. 	[2] Conversely, the image processing firms that emerged in the 1980s and 1990s were not interested in the creation of beautiful images, but in the manipulation of images to fit specific ends – that is, the creation of images that align with what the firm wants and the needs of a client. For instance, the motivation for the social networking site Facebook</text>
    <text>using machine learning techniques to detect unique features in photographs, and automatically generate images for websites that point to specific images. These images would then be used to advertise the websites of luxury goods or artisanal goods, or simply to mark places where the photographer found photographs of art. 	Such systems are pervasive with the rise of data-intensive communications and media, but the aim of this book is to entangle the photographer in the process of generating images and images through what it calls ‘instant image processing’. 	This can be seen as an attempt to reconcile the objective (what is sometimes called the media constrain) and subjective (what the photographer does with the aim of creating images) aspects of the relationship. The objective is to produce images that are of a particular kind, or of a particular kind of kind, and to this end the photographer must be able to manipulate images in such a way as to please a client or satisfy a crowd. The subjective aspect of the relationship, however, is to produce images that are not of a particular kind, or of a particular kind, and the client is not satisfied with images that are of that kind. 	Such subjective aspects of the relationship are what make the photographer work, and what makes it interesting. They are what make it a job well done, but they are also what make it a job well done in the end. But the fact that these processes happen at all is part of the job description. The client is expected to feel like a client, and to understand the way in which the photographer is working. This can be seen as a conflictual dynamic, and it is also part of the job description. But the fact that these processes happen at all is part of the job description, and the client is expected to understand the way in which the photographer is working. This can be seen as a conflictual dynamic, and it is also part of the job description. In the end, the client is expected to pay for the service,</text>
  </text>
  <text>
    subsequent iterative processing by machines] can[…] provide a framework into which we can begin to imbricate new ideas, concepts, actions and in that case we can create powerful adjacencies. But that is not at all what has happened. Rather, the world is made up of fragments and parts that constantly jostle for position, vying for place in a global system that is constantly trying to fill them in or obliterate them. And so we are told (by technologists and economists alike) that we are either 'infrastructure 2080′ or ‘infrastructures “only a step away from being recreated.’ But to truly understand how infrastructure actually works we must actually ‘get situated’ – actually locate where we live, what infrastructure we have, and what new kinds of infrastructure are around the world. And so we are told that we need smart meters and 'smart' roads and bridges and tunnels and street furniture and … well, of course, smart meters and smart roads
    <text>subsequent iterative processing by machines] can[…] provide a framework into which we can begin to imbricate new ideas, concepts, actions and in that case we can create powerful adjacencies. But that is not at all what has happened. Rather, the world is made up of fragments and parts that constantly jostle for position, vying for place in a global system that is constantly trying to fill them in or obliterate them. And so we are told (by technologists and economists alike) that we are either 'infrastructure 2080′ or ‘infrastructures “only a step away from being recreated.’ But to truly understand how infrastructure actually works we must actually ‘get situated’ – actually locate where we live, what infrastructure we have, and what new kinds of infrastructure are around the world. And so we are told that we need smart meters and 'smart' roads and bridges and tunnels and street furniture and … well, of course, smart meters and smart roads and bridges and tunnels and street furniture and smart parking lots and smart retail districts and smart industrial parks and ‘smart homes’ and smart policing districts and ‘smart governments’ and smart publics and smart libraries and 'smart churches’ and smart unions’ and smart schools and ‘smart heritage districts and smart heritage preservation districts and smart heritage preservation processes’. And so on.And yet, when all is said and done, infrastructure hardly matters. Much more is at stake in the creation of ever more perfect replicas of ourselves – replicas that can be shared, that can be managed, that can be app-enabled, that can be programmed, that can be hacked into being – replicas that are as malleable as DNA, that can be pulled into any desired pattern. And so we are shown films like these: the Terminator, Casino Royale, Groundhog Day, to name only a few. And we are encouraged to imagine ourselves ‘programmed�</text>
    <text>subsequent iterative processing by machines] can[…] provide a framework into which we can begin to imbricate new ideas, concepts, actions and in that case we can create powerful adjacencies. But that is not at all what has happened. Rather, the world is made up of fragments and parts that constantly jostle for position, vying for place in a global system that is constantly trying to fill them in or obliterate them. And so we are told (by technologists and economists alike) that we are either 'infrastructure 2080′ or ‘infrastructures “only a step away from being recreated.’ But to truly understand how infrastructure actually works we must actually ‘get situated’ – actually locate where we live, what infrastructure we have, and what new kinds of infrastructure are around the world. And so we are told that we need smart meters and 'smart' roads and bridges and tunnels and street furniture and … well, of course, smart meters and smart roads and bridges and tunnels and street furniture and sensors and data capture and software and platforms and app platforms and content platforms and … well, of course, smart meters and smart roads and bridges and tunnels and street furniture and sensors and data capture and algorithms and content platforms and app platforms. And so on. And each new piece of infrastructure we are given represents a new point of global intersection. And so the story of infrastructure, as we are told it, continues to grow richer and more complex as new interlocutors join the mix. And as the story of global intersections continues to change, so does our understanding of how things are made, who is made, and with whom, and with what kind of relationships – and in that mix we find ourselves arguing about who is made what and how. And each new understanding that we have of the world around us, and the places and people who inhabit it, changes our understanding of how infrastructure is made change. And so on. And each new piece of infrastructure</text>
    <text>subsequent iterative processing by machines] can[…] provide a framework into which we can begin to imbricate new ideas, concepts, actions and in that case we can create powerful adjacencies. But that is not at all what has happened. Rather, the world is made up of fragments and parts that constantly jostle for position, vying for place in a global system that is constantly trying to fill them in or obliterate them. And so we are told (by technologists and economists alike) that we are either 'infrastructure 2080′ or ‘infrastructures “only a step away from being recreated.’ But to truly understand how infrastructure actually works we must actually ‘get situated’ – actually locate where we live, what infrastructure we have, and what new kinds of infrastructure are around the world. And so we are told that we need smart meters and 'smart' roads and bridges and tunnels and street furniture and … well, of course, smart meters and smart roads and bridges and tunnels and street furniture and smart booths and software and sensors and actuators and compressors and pumps and compressors and coolants and coolants and chemical additives and solvents and all the different kinds of packaging and art and decoration and functionalities and materials and ensembles and architectures and materials and ensembles and … well, packaging and art and so forth. And so on. And so on.And yet, somehow, we are also taught (by technologists and economists alike) that we do not need smart meters or smart roads or smart street furniture or smart lighting or smart signage or smart parking or smart pharmacies or smart hotels or smart community centres or smart community boards or smart Community Arts and Cultural Centres or smart Community Land Trusts or smart Community Land Trusts or smart Community Museums or smart Aboriginal and Tribal Design Departments or smart Educational Design Departments or ‘smart’ websites or ‘smart’ apps or ‘smart’ radio or</text>
    <text>subsequent iterative processing by machines] can[…] provide a framework into which we can begin to imbricate new ideas, concepts, actions and in that case we can create powerful adjacencies. But that is not at all what has happened. Rather, the world is made up of fragments and parts that constantly jostle for position, vying for place in a global system that is constantly trying to fill them in or obliterate them. And so we are told (by technologists and economists alike) that we are either 'infrastructure 2080′ or ‘infrastructures “only a step away from being recreated.’ But to truly understand how infrastructure actually works we must actually ‘get situated’ – actually locate where we live, what infrastructure we have, and what new kinds of infrastructure are around the world. And so we are told that we need smart meters and 'smart' roads and bridges and tunnels and street furniture and … well, of course, smart meters and smart roads and bridges and tunnels. And so on.And so, in a way, we are continually told that we must either ‘get situated’ or ‘move to where the jobs are’ and so on. And so it’s with us when we leave home and travel far afield – far enough away from our homes to be able to walk to wherever we ‘want’ to be. But in the end, where do we go when we have nothing to fall back on but the environment and our own bodies? Where do we turn when we are faced with situations that we do not understand? Where do we turn when we are denied access to knowledge that could potentially expand our perspectives?When I first started working in the environment sector in the late 1980s and early 1990s, the main thing that kept coming up was: where do I start? I had already been doing works in places like architecture and landscape painting, landscape architecture, urban research and so on,</text>
    <text>subsequent iterative processing by machines] can[…] provide a framework into which we can begin to imbricate new ideas, concepts, actions and in that case we can create powerful adjacencies. But that is not at all what has happened. Rather, the world is made up of fragments and parts that constantly jostle for position, vying for place in a global system that is constantly trying to fill them in or obliterate them. And so we are told (by technologists and economists alike) that we are either 'infrastructure 2080′ or ‘infrastructures “only a step away from being recreated.’ But to truly understand how infrastructure actually works we must actually ‘get situated’ – actually locate where we live, what infrastructure we have, and what new kinds of infrastructure are around the world. And so we are told that we need smart meters and 'smart' roads and bridges and tunnels and street furniture and … well, of course, smart meters and smart roads and bridges and tunnels and street furniture and smart meters and smart parking lots and smart meters and smart signage and smart control units and smart garage doors and smart sprinklers and smart traffic lights and smart billboards and smart billboards and smart post boxes and smart tuners and smart control units and smart control planes and smart bridges and smart plazas and smart plazas and smart pay stations and smart payment cards and smart control towers and smart docks and smart surveillance drones and smart parking lots and smart unisex changing rooms and smart dishwashers and smart dryers and smart bars and smart bars and smart bars and smart cubicles and smart mannequins and smart plazas and smart freezers and smart free trade zones and smart slave markets and smart slave exchanges and smart slave repatriation and ‘smart’ buildings. For more than just infrastructure, infrastructure is a social production that involves people and social systems that can be modified in innumerable ways. But these modifications are not intended to create desired outcomes;</text>
    <text>subsequent iterative processing by machines] can[…] provide a framework into which we can begin to imbricate new ideas, concepts, actions and in that case we can create powerful adjacencies. But that is not at all what has happened. Rather, the world is made up of fragments and parts that constantly jostle for position, vying for place in a global system that is constantly trying to fill them in or obliterate them. And so we are told (by technologists and economists alike) that we are either 'infrastructure 2080′ or ‘infrastructures “only a step away from being recreated.’ But to truly understand how infrastructure actually works we must actually ‘get situated’ – actually locate where we live, what infrastructure we have, and what new kinds of infrastructure are around the world. And so we are told that we need smart meters and 'smart' roads and bridges and tunnels and street furniture and … well, of course, smart meters and smart roads and bridges and tunnels and street furniture and data capture and processing … but really, really, really, really need to get situated. This is why so many technologists, including myself, dream about smarter, smarter ways of organising the world around connected objects and sensors. It is the missing link in the heart of infrastructure that allows us to see the world in new ways. But this isn’t infrastructure, just infrastructure; it is the world as it is constructed, and in that process all manner of new relations, not just economic, have emerged between humans and the world around them. And so we live in a world that is constantly seeking to be arranged in certain ways, and so it’s easy to get sucked into a certain mode of engagement with infrastructure that isn’t engaged with, or even to identify with the mode of engagement that is.  	The ability to imagine new modes of engagement and the capacity to create them has always been fundamental to the creative destruction of</text>
    <text>subsequent iterative processing by machines] can[…] provide a framework into which we can begin to imbricate new ideas, concepts, actions and in that case we can create powerful adjacencies. But that is not at all what has happened. Rather, the world is made up of fragments and parts that constantly jostle for position, vying for place in a global system that is constantly trying to fill them in or obliterate them. And so we are told (by technologists and economists alike) that we are either 'infrastructure 2080′ or ‘infrastructures “only a step away from being recreated.’ But to truly understand how infrastructure actually works we must actually ‘get situated’ – actually locate where we live, what infrastructure we have, and what new kinds of infrastructure are around the world. And so we are told that we need smart meters and 'smart' roads and bridges and tunnels and street furniture and … well, of course, smart meters and smart roads and bridges and tunnels and street furniture and … well, smart meters and smart roads and bridges and tunnels and street furniture and … well, smart meters and smart roads and bridges and tunnels and street furniture and … well, smart meters and smart roads and bridges and tunnels and street furniture and … well, smart meters and smart tunnels and bridges and … well, smart bridges and tunnels and … well, smart bridges and … well, smart streets and bridges and tunnels and … well, smart street furniture and smart street furniture and … well, smart street lamps and … well, smart street lamps and … well, smart street furniture and … well, smart street ladders and … well, smart trees and smart trees and smart blind alleys and smart parking lots and … well, smart parking lots and smart stormwater harvesting and smart stormwater capture and smart conservation practices that reduce or eliminate maintenance costs and improve properties but also smart codes that regulate building construction and improve property values and property rights but also smart contracts that control building</text>
    <text>subsequent iterative processing by machines] can[…] provide a framework into which we can begin to imbricate new ideas, concepts, actions and in that case we can create powerful adjacencies. But that is not at all what has happened. Rather, the world is made up of fragments and parts that constantly jostle for position, vying for place in a global system that is constantly trying to fill them in or obliterate them. And so we are told (by technologists and economists alike) that we are either 'infrastructure 2080′ or ‘infrastructures “only a step away from being recreated.’ But to truly understand how infrastructure actually works we must actually ‘get situated’ – actually locate where we live, what infrastructure we have, and what new kinds of infrastructure are around the world. And so we are told that we need smart meters and 'smart' roads and bridges and tunnels and street furniture and … well, of course, smart meters and smart roads and bridges and tunnels and street furniture. But we also need specialist software to interpret data about where we live, what infrastructure we have, and what new kinds of infrastructure are around the world, so that infrastructure can be reconfigured in response to changing conditions and constantly adapting ways of using it. And so the race to connect continues unabated, despite the fact that understanding how infrastructure actually works would mean mapping the physical world and the life-world in which it exists, and constructing software to interpret that data.  And so we go on, building ever more pluggable pieces of infrastructure into ever larger computer systems, making ever more intelligent devices that monitor, capture, analyze and ultimately render into ever more granular forms that can be plugged into one another and reconfigured in response to changing conditions. And one by one, these pieces fall into place and become connected objects that can then be plugged into one another and made ever more productive. Automation means less, but also more: it does not merely</text>
    <text>subsequent iterative processing by machines] can[…] provide a framework into which we can begin to imbricate new ideas, concepts, actions and in that case we can create powerful adjacencies. But that is not at all what has happened. Rather, the world is made up of fragments and parts that constantly jostle for position, vying for place in a global system that is constantly trying to fill them in or obliterate them. And so we are told (by technologists and economists alike) that we are either 'infrastructure 2080′ or ‘infrastructures “only a step away from being recreated.’ But to truly understand how infrastructure actually works we must actually ‘get situated’ – actually locate where we live, what infrastructure we have, and what new kinds of infrastructure are around the world. And so we are told that we need smart meters and 'smart' roads and bridges and tunnels and street furniture and … well, of course, smart meters and smart roads and bridges and tunnels and street furniture and mapping apps and app factories … but more importantly, we must also understand how these interact with one another, and how this affects our day-to-day functioning. I give examples of cities that have smart meters and smart street furniture and mapping apps and have conversations with engineers about how these might be used to monitor and manipulate the urban environment, and so on. But here is the rub: smart meters and smart street furniture and mapping apps do not at all compute the whole picture. They merely point to parts of the city where electric and gas utilities are located, and so forth. And so we inhabit these cities, our imaginations running wild as to what kinds of uses can be made of the huge amounts of mapping data that now defines our terms and relations with the natural world. We walk to the grocery store, we take the subway, we take the bus, we take a bike … but really thinking constructs these journeys and running of images as events unfold around</text>
    <text>subsequent iterative processing by machines] can[…] provide a framework into which we can begin to imbricate new ideas, concepts, actions and in that case we can create powerful adjacencies. But that is not at all what has happened. Rather, the world is made up of fragments and parts that constantly jostle for position, vying for place in a global system that is constantly trying to fill them in or obliterate them. And so we are told (by technologists and economists alike) that we are either 'infrastructure 2080′ or ‘infrastructures “only a step away from being recreated.’ But to truly understand how infrastructure actually works we must actually ‘get situated’ – actually locate where we live, what infrastructure we have, and what new kinds of infrastructure are around the world. And so we are told that we need smart meters and 'smart' roads and bridges and tunnels and street furniture and … well, of course, smart meters and smart roads and bridges and tunnels and street furniture and smart art projects and … well, of course, smart art projects. And these infrastructures have to be connected to larger infrastructures, or else they compete with each other for space and are less able to support a range of different uses, including but not limited to agriculture, housing, commerce, education and … well, of course, smart infrastructure. And so we are told that infrastructure investments are needed ‘now more than ever’ and that these can be either ‘bang for the buck’ or ‘greener pastures’. And so we are directed to ‘all cities do some sort of urban sprawl … today’ thinking ‘blitz’ on a massive scale.’ And so we are told that it is important to build ‘smart cities’, smart publics, and smart‐toolkits, ‘that is, physically connected to one another in such</text>
  </text>
  <text>
    broad range of curatorial processes and exhibitions – is the product of a very particular moment in the history of art in the UK. It emerged during the mid-1970s when the Tate Gallery became the first institution of its kind in London. It has been a hub for art-historical research and communication ever since. I attended the Tate Gallery conference in 2014, when a new exhibition format was introduced, and I attended the World Biennial (2016) in Induction, where the field was introduced to the notion of the biennial in a general sense. In the case of the inaugural Venice Biennale, which was conceived as an interlinking series of exhibitions, the idea was that we would begin in Venice and end up in San Francisco. In the case of the inaugural San Francisco Biennale, which was conceived as a mini-exhibition, the idea was that we would visit different cities in the Bay Area and Northern California. The idea was that we would visit different parts of the city, from the South to the
    <text>broad range of curatorial processes and exhibitions – is the product of a very particular moment in the history of art in the UK. It emerged during the mid-1970s when the Tate Gallery became the first institution of its kind in London. It has been a hub for art-historical research and communication ever since. I attended the Tate Gallery conference in 2014, when a new exhibition format was introduced, and I attended the World Biennial (2016) in Induction, where the field was introduced to the notion of the biennial in a general sense. In the case of the inaugural Venice Biennale, which was conceived as an interlinking series of exhibitions, the idea was that we would begin in Venice and end up in San Francisco. In the case of the inaugural San Francisco Biennale, which was conceived as a mini-exhibition, the idea was that we would visit different cities in the Bay Area and Northern California. The idea was that we would visit different parts of the city, from the South to the North, and then travel up the Interstate 580 corridor to get to Mountain View. This was the general idea of the exhibition formats that we were looking for. At the time, the main exhibition format was the biennial, which offered many opportunities to find these other sites. The key was that the other sites had to be contiguous, and that is what we did. When we started the series, we did a kind of index card-based index to locate the exhibits. So each of the 12 locations had a small index card that listed the number of the index card, the name of the site, the index, the number of the site, and the index number. This made it possible to locate each site using a given index. For example, the first location with the great serpent, Cathedral Grove, had a small index of index cards. So the idea was that the index cards would be found in the same place, and the site with the great serpent, in the same order. This made</text>
    <text>broad range of curatorial processes and exhibitions – is the product of a very particular moment in the history of art in the UK. It emerged during the mid-1970s when the Tate Gallery became the first institution of its kind in London. It has been a hub for art-historical research and communication ever since. I attended the Tate Gallery conference in 2014, when a new exhibition format was introduced, and I attended the World Biennial (2016) in Induction, where the field was introduced to the notion of the biennial in a general sense. In the case of the inaugural Venice Biennale, which was conceived as an interlinking series of exhibitions, the idea was that we would begin in Venice and end up in San Francisco. In the case of the inaugural San Francisco Biennale, which was conceived as a mini-exhibition, the idea was that we would visit different cities in the Bay Area and Northern California. The idea was that we would visit different parts of the city, from the South to the North, and that we would visit different parts of the country. In the case of the Venice Biennale, which was conceived as a curatorial enterprise, the idea was that we would visit different parts of the city, from the South to the Northern California, and that we would take over a traditional museum space and invest in curatorial tasks. I think in the context of the Venice Biennale, the North was seen as being especially important, because it is the region where the Symposium was born, and also because it was the site of the first symposia. In other words, it was seen as the most accessible to the general public, and the most representative of a growing body of artistic and curatorial talent. The fact that it was also the site of the Giardini – which many people still associate with the Biennale (even though this is clearly seen as a case of competition, not a collaboration) – made it a particularly attractive proposition to international curators</text>
    <text>broad range of curatorial processes and exhibitions – is the product of a very particular moment in the history of art in the UK. It emerged during the mid-1970s when the Tate Gallery became the first institution of its kind in London. It has been a hub for art-historical research and communication ever since. I attended the Tate Gallery conference in 2014, when a new exhibition format was introduced, and I attended the World Biennial (2016) in Induction, where the field was introduced to the notion of the biennial in a general sense. In the case of the inaugural Venice Biennale, which was conceived as an interlinking series of exhibitions, the idea was that we would begin in Venice and end up in San Francisco. In the case of the inaugural San Francisco Biennale, which was conceived as a mini-exhibition, the idea was that we would visit different cities in the Bay Area and Northern California. The idea was that we would visit different parts of the city, from the South to the North, and that the exhibition format would vary according to the city, region, and even culture. Each city would have a number assigned to it, which corresponded to the number of artists working in that city at the time. For the Venice Biennale, we did this by creating a ‘biennial index’ (an index to the city's cultural history) and a ‘city index’ (an index to its cultural attractions).4. The biennale as a ‘sub-exhibition’. The Venice Biennale, Induction, 2016. Photo courtesy of the Biennale Archive. The Venice Biennale, Induction, 2016. Photo courtesy of the Biennale Archive. The Venice Biennale, Induction, 2016. Photo courtesy of the Biennale Archive. The Venice Biennale, Induction, 2016. Photo courtesy of the Biennale Archive. The Venice Biennale, Induction, 2016.</text>
    <text>broad range of curatorial processes and exhibitions – is the product of a very particular moment in the history of art in the UK. It emerged during the mid-1970s when the Tate Gallery became the first institution of its kind in London. It has been a hub for art-historical research and communication ever since. I attended the Tate Gallery conference in 2014, when a new exhibition format was introduced, and I attended the World Biennial (2016) in Induction, where the field was introduced to the notion of the biennial in a general sense. In the case of the inaugural Venice Biennale, which was conceived as an interlinking series of exhibitions, the idea was that we would begin in Venice and end up in San Francisco. In the case of the inaugural San Francisco Biennale, which was conceived as a mini-exhibition, the idea was that we would visit different cities in the Bay Area and Northern California. The idea was that we would visit different parts of the city, from the South to the North, and then travel along the Transbay Ferry corridor, visiting museums and performing historical archival research along the way. In the case of the inaugural Venice Biennale, which was conceived as an interlinking series of exhibitions, the idea was that we would begin in Venice and end up in San Francisco. The idea was that we would visit different cities in the Bay Area and Northern California. The idea was that we would visit different parts of the city, from the South to the North, and then travel along the Transbay Ferry corridor, visiting museums and performing historical archival research along the way. This exhibition format was developed in collaboration between the San Francisco Museum of Modern Art and the San Francisco Art Institute. It was based around the &quot;House of Cards&quot;, a collection of more than 1,000 props and costumes from the San Francisco Art Biennale that was staged simultaneously in several cities around the world. The props and costumes were organized into three categories: costumes, props and objects from</text>
    <text>broad range of curatorial processes and exhibitions – is the product of a very particular moment in the history of art in the UK. It emerged during the mid-1970s when the Tate Gallery became the first institution of its kind in London. It has been a hub for art-historical research and communication ever since. I attended the Tate Gallery conference in 2014, when a new exhibition format was introduced, and I attended the World Biennial (2016) in Induction, where the field was introduced to the notion of the biennial in a general sense. In the case of the inaugural Venice Biennale, which was conceived as an interlinking series of exhibitions, the idea was that we would begin in Venice and end up in San Francisco. In the case of the inaugural San Francisco Biennale, which was conceived as a mini-exhibition, the idea was that we would visit different cities in the Bay Area and Northern California. The idea was that we would visit different parts of the city, from the South to the North and from the Financial District to the Mission District. And we would be visiting different periods in the history of the city: the Biennale as a whole, and the Mission District and South of Market in particular. In Venice, we would be visiting the Guggenheim Museum, the Giardini and the Arsenale. In San Francisco we would visit the Transamerica building, the Golden Gate Bridge, and the Museum of Modern Art. In the early 1990s, we started the Biennale in Induction, and then in 1999 we started the Biennale in the Transamerica. At the time, we also started the Asian Arts Exposition, which is now the largest and most prestigious Asian art market in the world.Since the 1990s the Biennale has been a recurring event in Venice. It is a ‘house’ in the Venice city centre, a ‘scene’ that offers a window into the city’s architecture and</text>
    <text>broad range of curatorial processes and exhibitions – is the product of a very particular moment in the history of art in the UK. It emerged during the mid-1970s when the Tate Gallery became the first institution of its kind in London. It has been a hub for art-historical research and communication ever since. I attended the Tate Gallery conference in 2014, when a new exhibition format was introduced, and I attended the World Biennial (2016) in Induction, where the field was introduced to the notion of the biennial in a general sense. In the case of the inaugural Venice Biennale, which was conceived as an interlinking series of exhibitions, the idea was that we would begin in Venice and end up in San Francisco. In the case of the inaugural San Francisco Biennale, which was conceived as a mini-exhibition, the idea was that we would visit different cities in the Bay Area and Northern California. The idea was that we would visit different parts of the city, from the South to the North, visiting different parts of the biennale, visiting different parts of the city. This approach to the biennial was also the basis for our approach to the new curatorial style that was to be developed in conjunction with the new exhibition format. This was considered art history in the wider sense of the term. As the curatorial gestalt shifted, so did our approach to the biennale. Biennales are the objects of a particular historical and curatorial moment, and so the major curatorial gestures during this time were those that positioned the biennale in the historical context of an event that was as much a product of that era as it was of the present. The introduction of the new curatorial leadership, which we gathered under the leadership of Neil Johnson at the Venice Biennale, and which we subsequently put into effect at the San Francisco Biennale, was a watershed moment in our thinking. It signaled a shift from the local, to the universal, and away from</text>
    <text>broad range of curatorial processes and exhibitions – is the product of a very particular moment in the history of art in the UK. It emerged during the mid-1970s when the Tate Gallery became the first institution of its kind in London. It has been a hub for art-historical research and communication ever since. I attended the Tate Gallery conference in 2014, when a new exhibition format was introduced, and I attended the World Biennial (2016) in Induction, where the field was introduced to the notion of the biennial in a general sense. In the case of the inaugural Venice Biennale, which was conceived as an interlinking series of exhibitions, the idea was that we would begin in Venice and end up in San Francisco. In the case of the inaugural San Francisco Biennale, which was conceived as a mini-exhibition, the idea was that we would visit different cities in the Bay Area and Northern California. The idea was that we would visit different parts of the city, from the South to the North, and then travel cross-country to visit different locations in the North. The idea was that we would come to understand the city better as an entity, rather than as an isolated collection of places. The fact that the first Venice Biennale was held in a ramshackle old bookstore in the Tintoretto district of Venice, and the first Northern California Biennale was held in an empty theatre in the Golden Gate Theatre, which were then closed, suggests that the Biennale was originally conceived as an interlinking series of exhibitions, rather than as a singular event. A key part of the Venice Biennale package was a public programme of concerts, many of which were performed in venues all over Venice. The public programme was a convenient way to expose visitors to different parts of the city, and to introduce the organised and connected dimension of the biennale. It was a way to engage with the notion of the city as a place of encounters, even if this was</text>
    <text>broad range of curatorial processes and exhibitions – is the product of a very particular moment in the history of art in the UK. It emerged during the mid-1970s when the Tate Gallery became the first institution of its kind in London. It has been a hub for art-historical research and communication ever since. I attended the Tate Gallery conference in 2014, when a new exhibition format was introduced, and I attended the World Biennial (2016) in Induction, where the field was introduced to the notion of the biennial in a general sense. In the case of the inaugural Venice Biennale, which was conceived as an interlinking series of exhibitions, the idea was that we would begin in Venice and end up in San Francisco. In the case of the inaugural San Francisco Biennale, which was conceived as a mini-exhibition, the idea was that we would visit different cities in the Bay Area and Northern California. The idea was that we would visit different parts of the city, from the South to the North, from the impoverished areas to the powerful areas. And the juxtaposition of these different periods and locations allowed us to begin to think about gentrification, which is defined as the interaction between different social actors, especially in the context of a biennale. So, in the case of the Venice Biennale, we would begin in Venice, and the North American and European artworlds would begin to converge there.Our first excursion to North America took us to Winnipeg, where we visited the Winnipeg Art Gallery and Museum of Modern Art. We then traveled to San Francisco, taking a three-month, two-city tour that included stops in Toronto and Berlin. In between, we visited the Off-Modern Art Gallery in Leeds, which is developing a new exhibition format. In each city, we stopped at a different art museum: the Winnipeg Art Gallery, the Museum of Modern Art and the Museum of Contemporary Art. In each city, we visited different off-museums, including</text>
    <text>broad range of curatorial processes and exhibitions – is the product of a very particular moment in the history of art in the UK. It emerged during the mid-1970s when the Tate Gallery became the first institution of its kind in London. It has been a hub for art-historical research and communication ever since. I attended the Tate Gallery conference in 2014, when a new exhibition format was introduced, and I attended the World Biennial (2016) in Induction, where the field was introduced to the notion of the biennial in a general sense. In the case of the inaugural Venice Biennale, which was conceived as an interlinking series of exhibitions, the idea was that we would begin in Venice and end up in San Francisco. In the case of the inaugural San Francisco Biennale, which was conceived as a mini-exhibition, the idea was that we would visit different cities in the Bay Area and Northern California. The idea was that we would visit different parts of the city, from the South to the North, and that these would be juxtaposed in an interlinking series of exhibitions. And so the idea is that the Venice Biennale would begin in Venice, for example, and San Francisco. It would then visit various cities in the Northern California and Northern California, and then it would end up in London. And here is the interesting thing – the idea that the Venice Biennale could begin in London and San Francisco did not come about because the Tate or the Biennale wanted to begin in London. It came about because the biennale promoters realised that the Tate and the Biennale organisers could not operate in London. The reason was that the Tate and the Biennale organisers were against the introduction of mini-exhibitions into the biennale schedule. So the biennale promoters understood that they had to find other venues in London. And so the plan was that the Venice Biennale would begin in London and San Francisco, but it would also begin in</text>
    <text>broad range of curatorial processes and exhibitions – is the product of a very particular moment in the history of art in the UK. It emerged during the mid-1970s when the Tate Gallery became the first institution of its kind in London. It has been a hub for art-historical research and communication ever since. I attended the Tate Gallery conference in 2014, when a new exhibition format was introduced, and I attended the World Biennial (2016) in Induction, where the field was introduced to the notion of the biennial in a general sense. In the case of the inaugural Venice Biennale, which was conceived as an interlinking series of exhibitions, the idea was that we would begin in Venice and end up in San Francisco. In the case of the inaugural San Francisco Biennale, which was conceived as a mini-exhibition, the idea was that we would visit different cities in the Bay Area and Northern California. The idea was that we would visit different parts of the city, from the South to the North. This exhibition was conceived as a kind of a digression from the main theme of the festival, which was the entanglement of culture and commerce in an economic hegemon. It was a way to begin to unravel the cultural narrative that was being advanced by the biennale itself. In the case of a digression, the opening act quickly creates a new narrative around themselves and the art in their city. This was a feature of the Venice Biennale as a whole, but especially in its early years: the Museum of Modern Art and San Francisco Museum of Modern Art used this retroactive curatorial trickery to open new cultural experiences for the city, including the public, through the displacement of traditional art forms and the reconfiguration of architecture. And it is a feature of the biennale now: the Museum of Modern Art and San Francisco Museum of Modern Art have their exhibition readings around the idea of the biennale as a cultural form, from the curatorial point</text>
  </text>
  <text>
    subsequent iterative processing by machines) would permit us to create a ‘better world’. 	Such a world would limit human agency to the pre-defined patterns and possible futures of a given society. Instead of the often playful interactions of individuals with natural human agency, machines would govern systems, enact laws and influence outcomes through influence, technology and data mining. Machines would be far more capable of enacting social change and instilling a sense of belonging in disaffected communities, although this would still remain a part of the human condition. 	Such a scenario could also bring advances in medical and scientific knowledge of the human condition, and perhaps even in the creation of new kinds of art and science. However, given the proliferation of AI and the subsequent proliferation of surveillance and policing technologies, a world government would be unlikely to be able to intervene in such a way as to bring about such positive changes as altering patterns of behaviour. 	Human beings would still need to be empowered in such a way as to be able to
    <text>subsequent iterative processing by machines) would permit us to create a ‘better world’. 	Such a world would limit human agency to the pre-defined patterns and possible futures of a given society. Instead of the often playful interactions of individuals with natural human agency, machines would govern systems, enact laws and influence outcomes through influence, technology and data mining. Machines would be far more capable of enacting social change and instilling a sense of belonging in disaffected communities, although this would still remain a part of the human condition. 	Such a scenario could also bring advances in medical and scientific knowledge of the human condition, and perhaps even in the creation of new kinds of art and science. However, given the proliferation of AI and the subsequent proliferation of surveillance and policing technologies, a world government would be unlikely to be able to intervene in such a way as to bring about such positive changes as altering patterns of behaviour. 	Human beings would still need to be empowered in such a way as to be able to enact meaningful change, and this would still need to happen through legislation and executive order. The American Revolution was won, in part, by the support of a large volunteer army of volunteers, and it is likely that the same kind of mobilization would be needed to enact similar changes in a world government. 	However, given the proliferation of AI and the consequent proliferation of surveillance and policing technologies, a world government would be unlikely to be able to intervene in such a way as to bring about such positive changes as altering patterns of behaviour. As noted by Stewart Brand in his book  	Signaling 	, the tools of the state would have to evolve to accommodate the new social worlds that would be created by new AI. And since the state is far from the only player in the new AI game, other social actors would also need to be addressed. Hence, the third party would have to be able to talk to the social actors on the other side of the equation and convince them to do</text>
    <text>subsequent iterative processing by machines) would permit us to create a ‘better world’. 	Such a world would limit human agency to the pre-defined patterns and possible futures of a given society. Instead of the often playful interactions of individuals with natural human agency, machines would govern systems, enact laws and influence outcomes through influence, technology and data mining. Machines would be far more capable of enacting social change and instilling a sense of belonging in disaffected communities, although this would still remain a part of the human condition. 	Such a scenario could also bring advances in medical and scientific knowledge of the human condition, and perhaps even in the creation of new kinds of art and science. However, given the proliferation of AI and the subsequent proliferation of surveillance and policing technologies, a world government would be unlikely to be able to intervene in such a way as to bring about such positive changes as altering patterns of behaviour. 	Human beings would still need to be empowered in such a way as to be able to enact meaningful change, and this would still require consensus and trust building between the different stakeholders. It would be necessary for at least some stakeholders to be willing to give up certain kinds of ownership in exchange for certain kinds of benefits. For example, granting certain kinds of ownership to film makers who produce and distribute films on a regular basis would be necessary for the maintenance of trust between audiences and the creators of artistic works. It would also require that audiences have some knowledge about the activities of the film and television makers, and for the system to work at all, the stakeholders would need to be able to negotiate collectively with the stakeholders who control the information. 	It is possible that the empowerment of audiences would come about by other than through the mechanisms outlined above, and such other ideas are alternatives to the current situation where the only way to be empowered is to be part of a cultural elite. For example, there is the alternative possibility that the cultural elites who regularly screen films and television programmes around the world would be</text>
    <text>subsequent iterative processing by machines) would permit us to create a ‘better world’. 	Such a world would limit human agency to the pre-defined patterns and possible futures of a given society. Instead of the often playful interactions of individuals with natural human agency, machines would govern systems, enact laws and influence outcomes through influence, technology and data mining. Machines would be far more capable of enacting social change and instilling a sense of belonging in disaffected communities, although this would still remain a part of the human condition. 	Such a scenario could also bring advances in medical and scientific knowledge of the human condition, and perhaps even in the creation of new kinds of art and science. However, given the proliferation of AI and the subsequent proliferation of surveillance and policing technologies, a world government would be unlikely to be able to intervene in such a way as to bring about such positive changes as altering patterns of behaviour. 	Human beings would still need to be empowered in such a way as to be able to effectuate meaningful change, and this would still need to happen through legislation and government. In principle, the prospect of such legislation and government should never be accepted lightly, for it could empower not only the few, but also powerful think-tanks and advocacy groups, which could in turn form lobby for such legislation and government action. 	There are, however, good intentions behind the motivations that give rise to such legislation and policy. Perhaps the main reason for our present predicament is that we have surrendered too much of our sovereignty to nongovernmental bodies and nonstate actors, institutions and nonstate actors, including the nonstate actors themselves. It is true that in a constitutional republic, nonstate actors are constrained in their ability to enact their own laws; but unlike other types of citizens, who are legally bound to adhere to the laws as enacted by the state, nonstate actors are not legally bound to adhere to the laws as enacted by the state. Consequently, in theory, any law passed</text>
    <text>subsequent iterative processing by machines) would permit us to create a ‘better world’. 	Such a world would limit human agency to the pre-defined patterns and possible futures of a given society. Instead of the often playful interactions of individuals with natural human agency, machines would govern systems, enact laws and influence outcomes through influence, technology and data mining. Machines would be far more capable of enacting social change and instilling a sense of belonging in disaffected communities, although this would still remain a part of the human condition. 	Such a scenario could also bring advances in medical and scientific knowledge of the human condition, and perhaps even in the creation of new kinds of art and science. However, given the proliferation of AI and the subsequent proliferation of surveillance and policing technologies, a world government would be unlikely to be able to intervene in such a way as to bring about such positive changes as altering patterns of behaviour. 	Human beings would still need to be empowered in such a way as to be able to enact meaningful change, and such action would need to be approved by a majority of citizens. Given the current socio-political situation, this seems unlikely, although there is some debate as to whether or not a publicly elected government would be able to effectively enact such a change. 	The main issue that remains unresolved in relation to the role of the non-human in a future utopia is what kind of future society would be possible. While there are many different possibilities, various authors have proposed various levels of technological augmentations to create what have come to be known as the ‘fourth industrial revolution’[9] and beyond. 	However, there is growing recognition that our current socio-economic and spatial configurations are incompatible with the emergent potentialities of a future with a fully realized climate. Urban sprawl and climate-controlled agricultural systems are the order of the future not because they are desirable, but because they are technologically infeasible. As a result, cities are often re</text>
    <text>subsequent iterative processing by machines) would permit us to create a ‘better world’. 	Such a world would limit human agency to the pre-defined patterns and possible futures of a given society. Instead of the often playful interactions of individuals with natural human agency, machines would govern systems, enact laws and influence outcomes through influence, technology and data mining. Machines would be far more capable of enacting social change and instilling a sense of belonging in disaffected communities, although this would still remain a part of the human condition. 	Such a scenario could also bring advances in medical and scientific knowledge of the human condition, and perhaps even in the creation of new kinds of art and science. However, given the proliferation of AI and the subsequent proliferation of surveillance and policing technologies, a world government would be unlikely to be able to intervene in such a way as to bring about such positive changes as altering patterns of behaviour. 	Human beings would still need to be empowered in such a way as to be able to enact meaningful change, and this would still need to be done by consensus, i.e. political and legal power. In addition to the issues raised by the question,  	what is it that makes us human? 	There are of course other factors that come into play as well, such as cultural values, ideology, race and gender. However, given the right set of empowering technologies, a shared set of values can be enacted through consensus, namely, the human condition. 	Within this shared set of human values and human rights, there would exist a space where people could come to agree and negotiate such things as what constitutes a good or a bad day, how one counts as human and so forth. In effect, there would be a mutually agreed-upon set of frameworks within which such positive and negative outcomes could be negotiated. 	But although these frameworks could be agreed upon and negotiated, they would still ultimately emerge from the society as a whole. In other words, there</text>
    <text>subsequent iterative processing by machines) would permit us to create a ‘better world’. 	Such a world would limit human agency to the pre-defined patterns and possible futures of a given society. Instead of the often playful interactions of individuals with natural human agency, machines would govern systems, enact laws and influence outcomes through influence, technology and data mining. Machines would be far more capable of enacting social change and instilling a sense of belonging in disaffected communities, although this would still remain a part of the human condition. 	Such a scenario could also bring advances in medical and scientific knowledge of the human condition, and perhaps even in the creation of new kinds of art and science. However, given the proliferation of AI and the subsequent proliferation of surveillance and policing technologies, a world government would be unlikely to be able to intervene in such a way as to bring about such positive changes as altering patterns of behaviour. 	Human beings would still need to be empowered in such a way as to be able to enact positive change in the world around them, and this would still need to happen through legislation and executive orders from higher authorities. 	Given the current state of technological literacy and the exponential increase in data sets, it is quite conceivable that such a scenario could materialise. However, given the scale of human-machine interaction and technological advancement, such power would need to be actively resisted by those who lacked it in the past. 	The current state of technological literacy and the exponential increase in data sets means that it is possible that even as AI improves, it will still be able to create positive outcomes for society as a whole. The challenge for us now is how to actively resist such technological developments in the way that a community of artists, thinkers and artists can. This issue of Stages will look at three activities in particular that aim to bring the capacities of artists, thinkers and artists to bear in challenging the systems that are contributing to creating the new normal. 	Artists, thinkers and artists</text>
    <text>subsequent iterative processing by machines) would permit us to create a ‘better world’. 	Such a world would limit human agency to the pre-defined patterns and possible futures of a given society. Instead of the often playful interactions of individuals with natural human agency, machines would govern systems, enact laws and influence outcomes through influence, technology and data mining. Machines would be far more capable of enacting social change and instilling a sense of belonging in disaffected communities, although this would still remain a part of the human condition. 	Such a scenario could also bring advances in medical and scientific knowledge of the human condition, and perhaps even in the creation of new kinds of art and science. However, given the proliferation of AI and the subsequent proliferation of surveillance and policing technologies, a world government would be unlikely to be able to intervene in such a way as to bring about such positive changes as altering patterns of behaviour. 	Human beings would still need to be empowered in such a way as to be able to enact meaningful change in the world, and this would still need to happen through legislation and institutions, which are many and vary widely in their nature. It would be interesting to apply this logic to non-human species, but even then, such a future would only be possible within a technologically augmented human species. 	Finally, what is perhaps most at odds with the technocratic future envisioned by futurists is the sense of the future taking shape as a collective result of the future. The collective memory preserved in art and in the accumulated significant collections of museums and historic sites would provide the framework for such a future to be formed. In short, the future is an ever-shifting record of what might and might not be possible, based on what has been done and what hasn’t been done since. 	So while there might be opportunities for art and science to play a role in the future in the form of legislation and institutions designed to support creative activity, such action would still lie</text>
    <text>subsequent iterative processing by machines) would permit us to create a ‘better world’. 	Such a world would limit human agency to the pre-defined patterns and possible futures of a given society. Instead of the often playful interactions of individuals with natural human agency, machines would govern systems, enact laws and influence outcomes through influence, technology and data mining. Machines would be far more capable of enacting social change and instilling a sense of belonging in disaffected communities, although this would still remain a part of the human condition. 	Such a scenario could also bring advances in medical and scientific knowledge of the human condition, and perhaps even in the creation of new kinds of art and science. However, given the proliferation of AI and the subsequent proliferation of surveillance and policing technologies, a world government would be unlikely to be able to intervene in such a way as to bring about such positive changes as altering patterns of behaviour. 	Human beings would still need to be empowered in such a way as to be able to enact change through legislation and take part in scientific and artistic exploration. In the event of such an intervention, it would most likely need to be through referendums, local majorities or simple popular mobilisations. Such initiatives would need to be sustainable and buildable through democratic means, as well as providing for the future needs of the people. 	In this vein, an awareness of the historical situation is crucial in order to enable such initiatives to gain power. In many ways, there is a misunderstanding of how art and art history are produced in Brazil. In Brazil, art is produced through several layers of production, from the highest artistic levels to the lowest levels of the administrative, legal and even political system. The layers are as follows: layers of art dealers, artists' associations, museums, galleries, public commissions, art collections, production houses, social interaction specialists and so forth. Art is produced not only through these production lines, but also through grassroots artistic projects, community events, including but not limited</text>
    <text>subsequent iterative processing by machines) would permit us to create a ‘better world’. 	Such a world would limit human agency to the pre-defined patterns and possible futures of a given society. Instead of the often playful interactions of individuals with natural human agency, machines would govern systems, enact laws and influence outcomes through influence, technology and data mining. Machines would be far more capable of enacting social change and instilling a sense of belonging in disaffected communities, although this would still remain a part of the human condition. 	Such a scenario could also bring advances in medical and scientific knowledge of the human condition, and perhaps even in the creation of new kinds of art and science. However, given the proliferation of AI and the subsequent proliferation of surveillance and policing technologies, a world government would be unlikely to be able to intervene in such a way as to bring about such positive changes as altering patterns of behaviour. 	Human beings would still need to be empowered in such a way as to be able to enact positive change through direct action, and this would still need to happen through legislation and executive orders from higher government. Given the current scale of technological change and the way in which technologists think about the world, such a radical proposition is doubly unpalatable to non-technical workers and workers in other sectors, particularly in the West. 	The main reason for this is that, while AI might theoretically make it easier for large organisations to monitor and control populations through sophisticated data mining and analysis, this would also make it easier for such organisations to annihilate those populations through brute force – namely, to humanity through war. 	Therefore, while the West might aspire to be more like China and less like Russia, it would still suffer from this loss of meaning and intrinsic value as a result of its techno-mathematical nature. 	The second reason for the West to embrace such a world is that, while it would no longer be bound by the institutions and values of the West,</text>
    <text>subsequent iterative processing by machines) would permit us to create a ‘better world’. 	Such a world would limit human agency to the pre-defined patterns and possible futures of a given society. Instead of the often playful interactions of individuals with natural human agency, machines would govern systems, enact laws and influence outcomes through influence, technology and data mining. Machines would be far more capable of enacting social change and instilling a sense of belonging in disaffected communities, although this would still remain a part of the human condition. 	Such a scenario could also bring advances in medical and scientific knowledge of the human condition, and perhaps even in the creation of new kinds of art and science. However, given the proliferation of AI and the subsequent proliferation of surveillance and policing technologies, a world government would be unlikely to be able to intervene in such a way as to bring about such positive changes as altering patterns of behaviour. 	Human beings would still need to be empowered in such a way as to be able to enact their own destinies. And this is where creative perforations arise. 	One could argue that there are two ways of thinking about technological empowerment: (1) as a gradual but inexorable process that is designed to meet ever greater human needs, and (2) as a kind of socially engineered techno-utopianism that seeks to replace human needs with synthetic ones. 	And although the first perforation may be seen as the inevitable outcome of increasing AI, it is important to recognize that the latter perforations do not have to be. 	In fact, the more successful AI is, the more autonomy and empowerment it grants us, the more freedom and politics we can reimagine the world in our image. 	Furthermore, the more autonomy and empowerment we give to AI, the less we need to protect it from the things that make us human – ourselves, nature, economic forces, cultural influences etc. – which makes it ripe for the imposition of these</text>
  </text>
  <text>
    broad range of curatorial processes and exhibitions) and is currently undertaking a one-year residency at the British Museum. Migration: The Embryo IndustryIn recent years the child has become the object of considerable cultural speculation. The most immediate appeal of a video of an emerging live embryo to the spectator is the potential for profit. Embryonic research is a notoriously poor investment, with most bioengineering efforts failing or being scaled back by policy makers. The potential for aesthetic pleasure is maximised in the right donor, but there is some evidence that the aesthetic pleasure derived from anembryonic tissue may be valuable in curating contexts where art is often seen as a political tool. There are, however, other considerations that render the ethically tricky ethically undeveloped. Here we focus on the secondarily valuable trade in fetal tissue, citing specific cases from the United Kingdom and United States.Fetal Tissue is Beautiful, But Embryonic Research Is a Bad InvestmentArt historian and art critic, John McWhorter presents
    <text>broad range of curatorial processes and exhibitions) and is currently undertaking a one-year residency at the British Museum. Migration: The Embryo IndustryIn recent years the child has become the object of considerable cultural speculation. The most immediate appeal of a video of an emerging live embryo to the spectator is the potential for profit. Embryonic research is a notoriously poor investment, with most bioengineering efforts failing or being scaled back by policy makers. The potential for aesthetic pleasure is maximised in the right donor, but there is some evidence that the aesthetic pleasure derived from anembryonic tissue may be valuable in curating contexts where art is often seen as a political tool. There are, however, other considerations that render the ethically tricky ethically undeveloped. Here we focus on the secondarily valuable trade in fetal tissue, citing specific cases from the United Kingdom and United States.Fetal Tissue is Beautiful, But Embryonic Research Is a Bad InvestmentArt historian and art critic, John McWhorter presents the case for the utilitarian use of fetal tissue in a variety of contexts. In The Embryology of Consent (2015), he proposes a set of principles that could serve as a framework for the future of research-based medicine. These principles ‘reject everything that stands in the way of the common good’, he writes, echoing Shelley Erm, the Roman poet. They are premised on the presumption that the ‘unborn’ have the same intrinsic worth as other living beings, and thus stand to be commodified through the use of biological resources. Based on this assumption, any research into the biology of human beings is ethically problematic. The principles also point to important distinctions between research and art. Research, he maintains, should contribute to the common good, while the work it produces is ultimately of value only to a privileged few. McWhorter offers several examples of work that is consistent with these principles, including ‘revealing previously secret structures to reveal their structures</text>
    <text>broad range of curatorial processes and exhibitions) and is currently undertaking a one-year residency at the British Museum. Migration: The Embryo IndustryIn recent years the child has become the object of considerable cultural speculation. The most immediate appeal of a video of an emerging live embryo to the spectator is the potential for profit. Embryonic research is a notoriously poor investment, with most bioengineering efforts failing or being scaled back by policy makers. The potential for aesthetic pleasure is maximised in the right donor, but there is some evidence that the aesthetic pleasure derived from anembryonic tissue may be valuable in curating contexts where art is often seen as a political tool. There are, however, other considerations that render the ethically tricky ethically undeveloped. Here we focus on the secondarily valuable trade in fetal tissue, citing specific cases from the United Kingdom and United States.Fetal Tissue is Beautiful, But Embryonic Research Is a Bad InvestmentArt historian and art critic, John McWhorter presents a case for the intrinsically beautiful and inherently risky inherent in the live embryo. He argues that viewing anembryonic tissue 'illustrates the inherent beauty of life’, since the signal from anembryon is intrinsically beautiful and because, unlike other kinds of aesthetic beauty, its origins are pure. Drawing heavily on his own curatorial thinking, he develops three broad categories of aesthetic beauty: physical beauty, moral beauty and spiritual beauty.McWhorter begins his argument by remarking that the aesthetic pleasure derived from anembryonic tissue is necessarily limited. ‘Surprisingly few of us have the intellectual capacity to grasp the full beauty of life coming to life’.1 He cites Margaret Thatcher’s 1983 decision to cut off the head of a miscarried foetus to show that the decision was utterly bereft of aesthetic considerations.2 Drawing on the work of his friend and fellow artist, the art historian Alan Hewson,3 McWhorter asks why the Nobel Prize in</text>
    <text>broad range of curatorial processes and exhibitions) and is currently undertaking a one-year residency at the British Museum. Migration: The Embryo IndustryIn recent years the child has become the object of considerable cultural speculation. The most immediate appeal of a video of an emerging live embryo to the spectator is the potential for profit. Embryonic research is a notoriously poor investment, with most bioengineering efforts failing or being scaled back by policy makers. The potential for aesthetic pleasure is maximised in the right donor, but there is some evidence that the aesthetic pleasure derived from anembryonic tissue may be valuable in curating contexts where art is often seen as a political tool. There are, however, other considerations that render the ethically tricky ethically undeveloped. Here we focus on the secondarily valuable trade in fetal tissue, citing specific cases from the United Kingdom and United States.Fetal Tissue is Beautiful, But Embryonic Research Is a Bad InvestmentArt historian and art critic, John McWhorter presents a case for the intrinsic value of the human embryo in the modern artist. In his book  	Embryonic: The Modern Art of Embryology, he argues that the modern ethical artist comes to the task of depicting circumstances that are as likely to induce a miscarriage as they are to a successful birth.  He also argues that this critical vocation places artists at a distinct disadvantage in the battle to inspire moral courage in audiences primed to reject the pretence of value and seek courage in a situation.McWhorter describes a generation of artists whose work was informed by ethical concerns stemming from the 1960s. These were: Wittgenstein, Gustave Flaubert, Benoît Carré, Jean Lévi-Strauss, Yves Klein, Yvonne Rainer and Jean-Luc Nancy. These were artists who, he claims, lacked the moral fiber to confront their work with the realities of life outside of the womb,</text>
    <text>broad range of curatorial processes and exhibitions) and is currently undertaking a one-year residency at the British Museum. Migration: The Embryo IndustryIn recent years the child has become the object of considerable cultural speculation. The most immediate appeal of a video of an emerging live embryo to the spectator is the potential for profit. Embryonic research is a notoriously poor investment, with most bioengineering efforts failing or being scaled back by policy makers. The potential for aesthetic pleasure is maximised in the right donor, but there is some evidence that the aesthetic pleasure derived from anembryonic tissue may be valuable in curating contexts where art is often seen as a political tool. There are, however, other considerations that render the ethically tricky ethically undeveloped. Here we focus on the secondarily valuable trade in fetal tissue, citing specific cases from the United Kingdom and United States.Fetal Tissue is Beautiful, But Embryonic Research Is a Bad InvestmentArt historian and art critic, John McWhorter presents his book, The Art of Embryonic Stem-Whitening, in which he criticises the practice of embryonic stem-whitening as an 'artificial and futile renewal of old age’. Embryonic stem-whitening involves the harvesting of viable eggs from the fringes of the embryo-bearing human body via a 1-in-10,000 ricin-snail sized rock hard chunk of tissue. McWhorter argues that the growing body of the tissue is then packed tightly in a blister-like capsule and thrown into a river. As the tissue is compressed, its surface area increases, enabling the egg to leak out and be flushed down a drain. The excess water is used as fuel by a plant growing within the hollow of the skull. The Art of Embryonic Stem-Whitening is a ‘ficto-mathematical formula for artistic magisterial writing on the tissue of the human body’. It</text>
    <text>broad range of curatorial processes and exhibitions) and is currently undertaking a one-year residency at the British Museum. Migration: The Embryo IndustryIn recent years the child has become the object of considerable cultural speculation. The most immediate appeal of a video of an emerging live embryo to the spectator is the potential for profit. Embryonic research is a notoriously poor investment, with most bioengineering efforts failing or being scaled back by policy makers. The potential for aesthetic pleasure is maximised in the right donor, but there is some evidence that the aesthetic pleasure derived from anembryonic tissue may be valuable in curating contexts where art is often seen as a political tool. There are, however, other considerations that render the ethically tricky ethically undeveloped. Here we focus on the secondarily valuable trade in fetal tissue, citing specific cases from the United Kingdom and United States.Fetal Tissue is Beautiful, But Embryonic Research Is a Bad InvestmentArt historian and art critic, John McWhorter presents an exhaustive survey of the history of contemporary art, anatomy, physiology and the gross anatomy of the human body in the New York Times in an article entitled “Embryology and Contemporary Art”. His aim is to awaken a sleeping giant: “The sleeping giant is the ethical question of our age. We are becoming ever more like apes, whose habits are constantly questioned by artists and scientists.” He concludes with this list of questions, from most to least politically potent: “How should art be produced if our leading biomed centers cannot find collaborators or artists?” He also questions the ethics of research into therapeutic uses of body parts. At the other extreme are the ethical questions raised by the advent of technology: how much autonomy should the user have in deciding how his body is used? McWhorter considers the position of the artist and his or her users, while also noting that unlike in the case of research into emerging cancer, the user is not a consumer: the</text>
    <text>broad range of curatorial processes and exhibitions) and is currently undertaking a one-year residency at the British Museum. Migration: The Embryo IndustryIn recent years the child has become the object of considerable cultural speculation. The most immediate appeal of a video of an emerging live embryo to the spectator is the potential for profit. Embryonic research is a notoriously poor investment, with most bioengineering efforts failing or being scaled back by policy makers. The potential for aesthetic pleasure is maximised in the right donor, but there is some evidence that the aesthetic pleasure derived from anembryonic tissue may be valuable in curating contexts where art is often seen as a political tool. There are, however, other considerations that render the ethically tricky ethically undeveloped. Here we focus on the secondarily valuable trade in fetal tissue, citing specific cases from the United Kingdom and United States.Fetal Tissue is Beautiful, But Embryonic Research Is a Bad InvestmentArt historian and art critic, John McWhorter presents his book Embryology: The Search for Life in a Microchip oreAnd, published in 1999, as a ‘less than rapid rebuttal’ to the work of W. E. B. Du Bois,  oreAndThe Race to Be Human, that appeared in The New York Times two years earlier. McWhorter focuses on a single chapter that takes up the bulk of the book, but provides a wealth of supplementary material on the subject in a multitude of formats and languages. Embryology: The Race to Be Human is a sustained and highly readable work, well worth the money expended on its publication. Its publication coincided with the start of the  oreAndHuman Genome Project, an international project that sought to map the human genome. The book includes a wealth of ethnography on the project’s participants, including profiles of the scientists who contributed to its success, a survey of the project’s funding streams, and a description of the</text>
    <text>broad range of curatorial processes and exhibitions) and is currently undertaking a one-year residency at the British Museum. Migration: The Embryo IndustryIn recent years the child has become the object of considerable cultural speculation. The most immediate appeal of a video of an emerging live embryo to the spectator is the potential for profit. Embryonic research is a notoriously poor investment, with most bioengineering efforts failing or being scaled back by policy makers. The potential for aesthetic pleasure is maximised in the right donor, but there is some evidence that the aesthetic pleasure derived from anembryonic tissue may be valuable in curating contexts where art is often seen as a political tool. There are, however, other considerations that render the ethically tricky ethically undeveloped. Here we focus on the secondarily valuable trade in fetal tissue, citing specific cases from the United Kingdom and United States.Fetal Tissue is Beautiful, But Embryonic Research Is a Bad InvestmentArt historian and art critic, John McWhorter presents his new book The End of Art as Work: On Contemporaneous Art and Culture,1962–1974. It is a critical look at the contemporary art market and its discontents. The art world is captivated by the end product – the beautiful, but in the end all that matters is that the art be good. McWhorter examines the ethics of this pre-eminent modern art form and their inverse: the practice of creating beautiful art as a side business. Embryonic research is a bad investment, he argues, because it demands a high standard of technical sophistication and precise control, but it is also a big waste of money – the kind of wasteful, even destructive activity that produces only high-quality output at high productivity. McWhorter argues that the ethical imperative to produce ethically overrules the need to produce aesthetically. He criticises the ethical imperative to produce aesthetically, in part, because we have come to associate aesthetics with high productivity and high returns.</text>
    <text>broad range of curatorial processes and exhibitions) and is currently undertaking a one-year residency at the British Museum. Migration: The Embryo IndustryIn recent years the child has become the object of considerable cultural speculation. The most immediate appeal of a video of an emerging live embryo to the spectator is the potential for profit. Embryonic research is a notoriously poor investment, with most bioengineering efforts failing or being scaled back by policy makers. The potential for aesthetic pleasure is maximised in the right donor, but there is some evidence that the aesthetic pleasure derived from anembryonic tissue may be valuable in curating contexts where art is often seen as a political tool. There are, however, other considerations that render the ethically tricky ethically undeveloped. Here we focus on the secondarily valuable trade in fetal tissue, citing specific cases from the United Kingdom and United States.Fetal Tissue is Beautiful, But Embryonic Research Is a Bad InvestmentArt historian and art critic, John McWhorter presents the case for the human embryo in the context of the striking visual similarities between the work of art that he has written about since the 1920s (the Barbican, Picasso, Banksy, Mark Millett, among many others) and that of contemporary art (Madonna, Damien Hirst, Damien H. Wahlberg). He also looks at the ethical questions about the commodification of life. Embryonic research is aesthetically distinct from 'natural' pregnancy, which is primarily concerned with the aesthetic effects of the form (McWhorter 2015, Chapter 4). Embryonic research differs from in vitro fertilization in that its aim is to produce a specific type of brain through in vitro fertilization rather than to implant that specific brain in a living recipient (McWhorter 2015, Chapter 5). Embryonic research has been around for some years, but McWhorter notes that its legal status is unsettled, with conflicting federal and state laws governing when and how such research</text>
    <text>broad range of curatorial processes and exhibitions) and is currently undertaking a one-year residency at the British Museum. Migration: The Embryo IndustryIn recent years the child has become the object of considerable cultural speculation. The most immediate appeal of a video of an emerging live embryo to the spectator is the potential for profit. Embryonic research is a notoriously poor investment, with most bioengineering efforts failing or being scaled back by policy makers. The potential for aesthetic pleasure is maximised in the right donor, but there is some evidence that the aesthetic pleasure derived from anembryonic tissue may be valuable in curating contexts where art is often seen as a political tool. There are, however, other considerations that render the ethically tricky ethically undeveloped. Here we focus on the secondarily valuable trade in fetal tissue, citing specific cases from the United Kingdom and United States.Fetal Tissue is Beautiful, But Embryonic Research Is a Bad InvestmentArt historian and art critic, John McWhorter presents a case for the intrinsic aesthetic value of the developing embryo, particularly in its nascent, unadulterated stage. Drawing on his current research in ethology and anthropology, he investigates the relationship between art and the development of fetuses from the 1920s. Drawing on his own practice as a senior curator for Natural History Museum, London, which included a tenure as the museum’s curator of contemporary natural history, McWhorter analyzes the present fascination with genetic engineering, which seeks to engineer living things through the exchange of genetic material. Embryonic stem cells, which can assume many different biological forms, are among the most promising candidates for this bioengineering revolution. Drawing on his current research interests, he also examines the emergence of contemporary art history through the lens of ethnography, the study of art and culture. Embryonic stromal migration is a research interest of hers. Drawing from her current position as a postdoctoral researcher in art history at McGill University, she investigates the</text>
    <text>broad range of curatorial processes and exhibitions) and is currently undertaking a one-year residency at the British Museum. Migration: The Embryo IndustryIn recent years the child has become the object of considerable cultural speculation. The most immediate appeal of a video of an emerging live embryo to the spectator is the potential for profit. Embryonic research is a notoriously poor investment, with most bioengineering efforts failing or being scaled back by policy makers. The potential for aesthetic pleasure is maximised in the right donor, but there is some evidence that the aesthetic pleasure derived from anembryonic tissue may be valuable in curating contexts where art is often seen as a political tool. There are, however, other considerations that render the ethically tricky ethically undeveloped. Here we focus on the secondarily valuable trade in fetal tissue, citing specific cases from the United Kingdom and United States.Fetal Tissue is Beautiful, But Embryonic Research Is a Bad InvestmentArt historian and art critic, John McWhorter presents us to the ethical dilemma of the increasingly prominent role that biology plays in our daily lives. In McWhorter’s account, we come to understand the true force of the crisis in the social-psychological capital of our age – in the capacity to understand and value the social – when, in the wake of it all, anembryonic stem cells are used to create digital images that can then be downloaded and shared widely. Embryonic stem cells have become the backbone of a new wave of biomedical research, and recent advances in computer modelling have allowed scientists to precisely target and monitor these cells in a dish. McWhorter mentions the Boston College of Arts and the Structural Brain Laboratory at Harvard Medical School, where pioneering work in neural networks was begun in the 1970s. These were, he claims, ‘the first steps in the direction of a social neuroscience that would later be built on ‘the foundations of “embryonic and fetal neuroscience”’.</text>
  </text>
  <text>
    biennial as a self-learning system might also account for the increased number of biennials, including this one, in the United States and internationally, especially since the late 1970s. 	[5] See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.126.   1   The biennial as an exhibitionary form has recently been revived in Venice, although its cultural practices are those characteristic of the nineteenth-century museum. See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.135.   2  Ibid.   3  Ibid., p.140.   4  Ibid., p.141.   5  Ibid., p.143.   6  Ibid., p.146.   7  Ibid., p.148.   8
    <text>biennial as a self-learning system might also account for the increased number of biennials, including this one, in the United States and internationally, especially since the late 1970s. 	[5] See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.126.   1   The biennial as an exhibitionary form has recently been revived in Venice, although its cultural practices are those characteristic of the nineteenth-century museum. See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.135.   2  Ibid.   3  Ibid., p.140.   4  Ibid., p.141.   5  Ibid., p.143.   6  Ibid., p.146.   7  Ibid., p.148.   8  Ibid., p.149.   9  Ibid., p.151.   10  Ibid., p.151.   11  Ibid., p.157.   12  See http://www.biennaleofluxury.com/?p=1708.   13  See http://www.labiennaleofluxury.com/en/.   14  See also Robin Mackay and Rob Schmitz,  	The Biennial Condition: On the Experience of Exhibitions, Berghaus Stuttgart, 2006, p.15.   15  See Robin Mackay,  	Being Biennially Educated: The Art of Not Being Governed, Berghaus Stuttgart, 2006, p.46.   16  See Robin Mackay,  	The Biennial Condition: On the Experience of Exhibitions, Berghaus Stuttgart, 2006, p</text>
    <text>biennial as a self-learning system might also account for the increased number of biennials, including this one, in the United States and internationally, especially since the late 1970s. 	[5] See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.126.   1   The biennial as an exhibitionary form has recently been revived in Venice, although its cultural practices are those characteristic of the nineteenth-century museum. See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.135.   2  Ibid.   3  Ibid., p.140.   4  Ibid., p.141.   5  Ibid., p.143.   6  Ibid., p.146.   7  Ibid., p.148.   8  Ibid., p.149.   9  Ibid., p.152.   10  Ibid., p.155.   11  Ibid., p.156.   12  Ibid., p.157.   13  Ibid., p.158.   14  Ibid., p.159.   15  Ibid., p.160.   16  Ibid., p.161.   Download this article as PDFJussi ParikkaJussi Parikka is an artist. His work in communities began in the 1990s when people in his neighbourhood started rebuilding their houses. Parikka’s artworks often intervene in the lives of people with disabilities, explaining the principles of ‘community'' and its many facets. He has exhibited internationally including at the Venice Biennale, South by Southwest, the Venice Biennale, and the Venice Architecture Biennale. His designs have been used in the cover of home</text>
    <text>biennial as a self-learning system might also account for the increased number of biennials, including this one, in the United States and internationally, especially since the late 1970s. 	[5] See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.126.   1   The biennial as an exhibitionary form has recently been revived in Venice, although its cultural practices are those characteristic of the nineteenth-century museum. See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.135.   2  Ibid.   3  Ibid., p.140.   4  Ibid., p.141.   5  Ibid., p.143.   6  Ibid., p.146.   7  Ibid., p.148.   8  Ibid., p.149.   9  Ibid., p.150.   10  Ibid., p.151.   11  Ibid., p.154.   12  Ibid., p.156.   13  Ibid., p.157.   14  Ibid., Epsilon Phi Epsilon, Chapter V, 11 April 1968, in Fred Turner,  	Transforming Biennials, Ponte, 1988, p.39–43.   15  Ibid., 7 June 1968.   16  Turner, op. cit., p.6.Download this article as PDFFred TurnerFred Turner is a curator, writer and Producer who has curated numerous exhibits, including at the Greenleaf Gallery, Liverpool, UK, and at the Tate Modern, London, UK. He is currently curator and producer at the Ann &amp; Ralph Hubbard Gallery, New York, USA, and the National Gallery of Canada, Vancouver, Canada. His most recent</text>
    <text>biennial as a self-learning system might also account for the increased number of biennials, including this one, in the United States and internationally, especially since the late 1970s. 	[5] See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.126.   1   The biennial as an exhibitionary form has recently been revived in Venice, although its cultural practices are those characteristic of the nineteenth-century museum. See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.135.   2  Ibid.   3  Ibid., p.140.   4  Ibid., p.141.   5  Ibid., p.143.   6  Ibid., p.146.   7  Ibid., p.148.   8  Rudolf Steiner, ‘On the Nature of Things’, in Critical Inquiry, Vol. 2, No. 1, Spring, 1978, p.109.   9  William Shakespeare, King Henry VI, Act IV, Scene IV.   10  Jonathan Crary, ‘On the Nature of Things’, in Cultural Studies, vol. 32, No. 4, 1967, p.231.   11  The Yale Book of Quotations on the Tree of Knowledge (1907), p.330.   12  Jonathan Crary, ‘On the Nature of Things’, in Critical Inquiry, vol. 2, No. 1, Spring, 1978, p.211.   13  Ibid., p.312.   14  S.D. Kennedy and A.D. Harrison, ‘On the Nature of Things’, in Cuming, Culture and Society, ed. S.D</text>
    <text>biennial as a self-learning system might also account for the increased number of biennials, including this one, in the United States and internationally, especially since the late 1970s. 	[5] See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.126.   1   The biennial as an exhibitionary form has recently been revived in Venice, although its cultural practices are those characteristic of the nineteenth-century museum. See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.135.   2  Ibid.   3  Ibid., p.140.   4  Ibid., p.141.   5  Ibid., p.143.   6  Ibid., p.146.   7  Ibid., p.148.   8  Ibid., p.149.   9  Ibid., p.150.   10  Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.133.Download this article as PDFHolly R. Kim, Artist in Residence at UCLA, and Producer of the award-winning docuseries series A Voice of Thunder, will discuss environmental justice and related topics in an intimate context of the UCLA Art Museum on September 8 and 9, 2016. The series premiered in September 2014 in Los Angeles and San Francisco and is now available on Netflix.For more information, visit: https://www.youtube.com/channel/UC8yzD... Music by Noel Burch and Harry Shum Jr.A Voice of Thunder is a multi-award-winning docuseries that follows five generations of the Kim family from California to the Canadian border in search of their missing father</text>
    <text>biennial as a self-learning system might also account for the increased number of biennials, including this one, in the United States and internationally, especially since the late 1970s. 	[5] See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.126.   1   The biennial as an exhibitionary form has recently been revived in Venice, although its cultural practices are those characteristic of the nineteenth-century museum. See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.135.   2  Ibid.   3  Ibid., p.140.   4  Ibid., p.141.   5  Ibid., p.143.   6  Ibid., p.146.   7  Ibid., p.148.   8  Ibid., p.149.   9  Ibid., p.152.   10  Ibid., p.154.   11  Ibid., p.156.   12  Ibid., p.157.   13  Ibid., p.158.   14  See, for example, Philip E Agre,  	Self-Repairing Bodies: Evolution and the Biennial, Oxford University Press, Oxford, 1988, and Robin Mackay,  	Emerging Biennials: Art, Culture and the Biennial, Routledge, London, 1994, which are both in my book  	Surviving the Biennial: Art, Architecture and the Episodic, and which were shortlisted for the Hugo Boss Prize, and which the Academy of Art, London, awarded me posthumously. 	[14] See  	W. Somerset Maugham,  	The Maugham Collection:</text>
    <text>biennial as a self-learning system might also account for the increased number of biennials, including this one, in the United States and internationally, especially since the late 1970s. 	[5] See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.126.   1   The biennial as an exhibitionary form has recently been revived in Venice, although its cultural practices are those characteristic of the nineteenth-century museum. See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.135.   2  Ibid.   3  Ibid., p.140.   4  Ibid., p.141.   5  Ibid., p.143.   6  Ibid., p.146.   7  Ibid., p.148.   8  Ibid., p.150.   9  Ibid., p.151.   10  Ibid., p.154.   11  Ibid., p.156.   12  Ibid., p.157.   13  Ibid., p.158.   14  Ibid., p.159.   15  Ibid., p.160.   16  Ibid., p.161.   17  Fred Turner, ‘The Art of Not Being Governed’, New York Times, 11 October 2008, http://www.nytimes.com/2008/10/11/us/politics/economy.html?, 11st print edition, November 2009.   18  ‘Vital Signs: Ebola’s Impact on Africa’s Public Health Picture’, Health Ranger, 17 October 2014, http://www.health.gov.au/publichealth/en/news/2014</text>
    <text>biennial as a self-learning system might also account for the increased number of biennials, including this one, in the United States and internationally, especially since the late 1970s. 	[5] See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.126.   1   The biennial as an exhibitionary form has recently been revived in Venice, although its cultural practices are those characteristic of the nineteenth-century museum. See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.135.   2  Ibid.   3  Ibid., p.140.   4  Ibid., p.141.   5  Ibid., p.143.   6  Ibid., p.146.   7  Ibid., p.148.   8  Ibid., p.149.   9  Ibid., p.150.   10  Ibid., p.151.   11  Ibid., p.153.   12  Ibid., p.156.   13  Ibid., p.157.   14  Ibid., p.158.   15  Ibid., p.159.   16  Ibid., pt. 2, fig. 2.   Download this article as PDFFred TurnerFred Turner is the author of The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, and has written extensively on artist-led culture and community for the New York Times, Newsweek, The New Republic, Art in America and elsewhere. His most recent work, Being Mortal: Violence, Politics and Participatory Art, was published by Third Millennium Press. He is currently completing a PhD at SOAS, University of London,</text>
    <text>biennial as a self-learning system might also account for the increased number of biennials, including this one, in the United States and internationally, especially since the late 1970s. 	[5] See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.126.   1   The biennial as an exhibitionary form has recently been revived in Venice, although its cultural practices are those characteristic of the nineteenth-century museum. See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.135.   2  Ibid.   3  Ibid., p.140.   4  Ibid., p.141.   5  Ibid., p.143.   6  Ibid., p.146.   7  Ibid., p.148.   8  Ibid., p.149.   9  Ibid., pp.151–54.   10  Ibid., p.155.   11  Ibid., p.156.   12  Ibid., p.157.   13  Ibid., p.158.   14  Ibid., p.159.   15  Ibid., p.160.   16  Ibid., p.161.   17  Ibid., p.162.   18  Ibid., p.163.   19  Ibid., p.164.   20  Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.173.   21  Raffaele Tononi, ‘Massive Complexity: Italy and the Biennale’, in S. Fridermatis, ed., The Contemporary Art of the</text>
    <text>biennial as a self-learning system might also account for the increased number of biennials, including this one, in the United States and internationally, especially since the late 1970s. 	[5] See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.126.   1   The biennial as an exhibitionary form has recently been revived in Venice, although its cultural practices are those characteristic of the nineteenth-century museum. See Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.135.   2  Ibid.   3  Ibid., p.140.   4  Ibid., p.141.   5  Ibid., p.143.   6  Ibid., p.146.   7  Ibid., p.148.   8  Ibid., p.149.   9  Ibid., p.150.   10  Gabrielle Roy,  	Signal: Les Beaux-Arts et les Départementaires, Chamonix, 1985, p.104.   11  Fred Turner, The Art of Not Being Governed: Participatory Community Arts in an Age of Authoritarianism, Third Millennium, New York, 1978, p.131.   12  Roy, op. cit., p.105.  Download this article as PDFGabriela RendonGabriela Rendon is an artist and the Director of the Biennial Program at the Art Institute of Chicago. Her artworks have been commissioned by Chatsworth Arts, the Chrysler Museum, and the Illinois Arts Council. She is an adjunct faculty at the Art Institute of Chicago. Her writing has been published in the Chicago Tribune, the Chicago Sun-Times, the Wall Street Journal, the New York</text>
  </text>
  <text>
    broad range of curatorial processes and exhibitions, which are normally organised and presented by and in agreement with the city as a whole. 	[1] 	In a nutshell, the concept of ‘the city’ as an urban space emerged in the second half of the nineteenth century. The aim of this brief pre-examines this crucial period in the city’s development and makes the case that, far from being constrained by national or colonial boundaries, the city is both space and ‘category fail’ – a category that grows larger with each passing day. The city becomes ever more diverse and variable, but its core urban function remains the same: to be a set of relations between people, places, objects and networks. In this sense, the city is both infinite in space and categorical. 	[2] As Michael Slackman describes, the city is a double-sided ­&quot;panorama&quot; that reflects the contradictory ­effects of numerous interacting parts. Although the city appears to be a
    <text>broad range of curatorial processes and exhibitions, which are normally organised and presented by and in agreement with the city as a whole. 	[1] 	In a nutshell, the concept of ‘the city’ as an urban space emerged in the second half of the nineteenth century. The aim of this brief pre-examines this crucial period in the city’s development and makes the case that, far from being constrained by national or colonial boundaries, the city is both space and ‘category fail’ – a category that grows larger with each passing day. The city becomes ever more diverse and variable, but its core urban function remains the same: to be a set of relations between people, places, objects and networks. In this sense, the city is both infinite in space and categorical. 	[2] As Michael Slackman describes, the city is a double-sided ­&quot;panorama&quot; that reflects the contradictory ­effects of numerous interacting parts. Although the city appears to be a collection of disparate elements, the elements within it are all integrated into a larger whole. This is true of ‘urban settlements’ such as ‘colonies’ and ‘regions’ such as ‘the city’. When these elements form a city, they form a city in their own right: a complex metropolis with national parks, protected ‘regions’ within them and ‘totem’-shaped national parks across the city. The elements of the city that are not integrated into the metropolis – the streets that house the factories, the park that houses the seaside ‘paradise’ – become ‘tentacula’ for the decomposition of industrial society. 	[3] This concept of ‘the city’ was first expounded by French urbanist Henri Lefebvre in 1949. Although he argued that the postwar suburban development was the outcome of a global industrial economy</text>
    <text>broad range of curatorial processes and exhibitions, which are normally organised and presented by and in agreement with the city as a whole. 	[1] 	In a nutshell, the concept of ‘the city’ as an urban space emerged in the second half of the nineteenth century. The aim of this brief pre-examines this crucial period in the city’s development and makes the case that, far from being constrained by national or colonial boundaries, the city is both space and ‘category fail’ – a category that grows larger with each passing day. The city becomes ever more diverse and variable, but its core urban function remains the same: to be a set of relations between people, places, objects and networks. In this sense, the city is both infinite in space and categorical. 	[2] As Michael Slackman describes, the city is a double-sided ­&quot;panorama&quot; that reflects the contradictory ­effects of numerous interacting parts. Although the city appears to be a collection of aggregates, its many diverse parts nevertheless attempt to live in harmony and coordinate themselves into a shared outcome. A large‐scale urban planning program is part of this overarching ‘comprehensive city’ concept. 	[3] As a project manager and an educator, I work with communities to change the city through design and architecture. I design ‘paved and underground urban districts with a street grid, ‘covered districts, public parks, …’ by creating ‘micro‐grids’ within which ‘neighborhoods’ can be created and ‘towns’ can be consolidated. I use urban geographers and urbanists to understand how new ways of organizing urban space can be developed through the scientific method, while incorporating local and regional perspectives and arts initiatives into the design process. I also work with local politicians, developers, artists and academics to bring projects to life and to understand their motivations, but also to uncover</text>
    <text>broad range of curatorial processes and exhibitions, which are normally organised and presented by and in agreement with the city as a whole. 	[1] 	In a nutshell, the concept of ‘the city’ as an urban space emerged in the second half of the nineteenth century. The aim of this brief pre-examines this crucial period in the city’s development and makes the case that, far from being constrained by national or colonial boundaries, the city is both space and ‘category fail’ – a category that grows larger with each passing day. The city becomes ever more diverse and variable, but its core urban function remains the same: to be a set of relations between people, places, objects and networks. In this sense, the city is both infinite in space and categorical. 	[2] As Michael Slackman describes, the city is a double-sided ­&quot;panorama&quot; that reflects the contradictory ­effects of numerous interacting parts. Although the city appears to be a collection of physical objects, its core social and ‘cultural’ qualities lie in their heterogeneity. A single ­street corner may house a ‘collection of small artworks’ or a ‘collection of paintings’ from a number of different artists; a single ­tenant’ may house a collection of different kinds of buildings; a public realm may be a series of ‘publics’ interacting with each other. The city is both object and social but also a complex system that depends on many other interacting parts. As the authors of the 2010 International Accounting Standards Conference** demonstrated when developing a new generation of auditors' instruments, the ability to observe and analyse such complexity is a practical and useful commodity – a commodity that, while it exists, must be used to reconstruct and expand upon the history of the city and its people. 	[3] As the early twentieth century drew to a close, it became clear that the predominant forms of accounting – that is</text>
    <text>broad range of curatorial processes and exhibitions, which are normally organised and presented by and in agreement with the city as a whole. 	[1] 	In a nutshell, the concept of ‘the city’ as an urban space emerged in the second half of the nineteenth century. The aim of this brief pre-examines this crucial period in the city’s development and makes the case that, far from being constrained by national or colonial boundaries, the city is both space and ‘category fail’ – a category that grows larger with each passing day. The city becomes ever more diverse and variable, but its core urban function remains the same: to be a set of relations between people, places, objects and networks. In this sense, the city is both infinite in space and categorical. 	[2] As Michael Slackman describes, the city is a double-sided ­&quot;panorama&quot; that reflects the contradictory ­effects of numerous interacting parts. Although the city appears to be a collection of interacting parts, this is not always the case; sometimes the city is a collection of isolated points of articulation. For instance, the Mississippi Delta is a collection of islands; its tributaries and branches, which make up the Mississippi River Delta, are highly variable in elevation and depth; their aquatic ecosystems ­– including the Strait of Georgia and the Caspian Sea – are topological parabolas; and migratory birding and photography is a major form of urban habitation.4 	In this multi-tiered urban system, ‘place­making’ occurs at multiple scales, but the primary locus of such activity is located at the local, regional, and national levels. In the twenty-first century, we can only speculate about the specific mechanisms that underlie this seemingly new form of urbanity, but it is certain that much of what makes our urban form distinctive is its ability to flexibly expand and contract at will. In this sense, the</text>
    <text>broad range of curatorial processes and exhibitions, which are normally organised and presented by and in agreement with the city as a whole. 	[1] 	In a nutshell, the concept of ‘the city’ as an urban space emerged in the second half of the nineteenth century. The aim of this brief pre-examines this crucial period in the city’s development and makes the case that, far from being constrained by national or colonial boundaries, the city is both space and ‘category fail’ – a category that grows larger with each passing day. The city becomes ever more diverse and variable, but its core urban function remains the same: to be a set of relations between people, places, objects and networks. In this sense, the city is both infinite in space and categorical. 	[2] As Michael Slackman describes, the city is a double-sided ­&quot;panorama&quot; that reflects the contradictory ­effects of numerous interacting parts. Although the city appears to be a collection of interacting parts, its true intent is to organize and regulate networks of interactions among people, objects and networks. In this sense, the city is a double-edged ­&quot;sword’ that can be used to great effect when it is organized to meet the specific needs of its users, but also when it is not organized to meet the specific needs of the city. The paradoxical ­effect of this double-faced panorama – part of its ‘feature set’ – is that it can simultaneously serve as a collector of objects and a disassembler of systems. A collection of statues and monuments may be valuable as a collection of objects, but when these collections become obsolete or fail to meet current needs, their use is generally considered a waste of space and resources. SystemicsLets start by considering a general notion of the word ‘systemics’ in its broadest sense. We can think of any set of relations as a kind of hierarchy, and the word �</text>
    <text>broad range of curatorial processes and exhibitions, which are normally organised and presented by and in agreement with the city as a whole. 	[1] 	In a nutshell, the concept of ‘the city’ as an urban space emerged in the second half of the nineteenth century. The aim of this brief pre-examines this crucial period in the city’s development and makes the case that, far from being constrained by national or colonial boundaries, the city is both space and ‘category fail’ – a category that grows larger with each passing day. The city becomes ever more diverse and variable, but its core urban function remains the same: to be a set of relations between people, places, objects and networks. In this sense, the city is both infinite in space and categorical. 	[2] As Michael Slackman describes, the city is a double-sided ­&quot;panorama&quot; that reflects the contradictory ­effects of numerous interacting parts. Although the city appears to be a collection of instances, each with distinctive functions and properties (properties, rights and privileges), the city is actually a series of relations: relations of domination and subordination, relations of appearance and concealment, relations of ownership and occupancy, relations of communication and ‘wealth production’.  	[3] As the historian Amy Jacobs reminds us, each urban community, each neighborhood, harbours latent ideas and perceptions of belonging and community that can be refined and exploited over time.  	[4] As the urban explorer ­Neil Johnson describes, such latent ideas and behaviours are structurally different from the fixed, rigidly grounded histories and narratives of traditional notions of the city.  	[5] In fact, as Johnson makes clear, such latent perceptions and behaviours are among the most powerful forms of resistance to neoliberalism and related socio-political systems. As the historian Wendy Chun reminds us, for communities to develop and thrive, they must be able</text>
    <text>broad range of curatorial processes and exhibitions, which are normally organised and presented by and in agreement with the city as a whole. 	[1] 	In a nutshell, the concept of ‘the city’ as an urban space emerged in the second half of the nineteenth century. The aim of this brief pre-examines this crucial period in the city’s development and makes the case that, far from being constrained by national or colonial boundaries, the city is both space and ‘category fail’ – a category that grows larger with each passing day. The city becomes ever more diverse and variable, but its core urban function remains the same: to be a set of relations between people, places, objects and networks. In this sense, the city is both infinite in space and categorical. 	[2] As Michael Slackman describes, the city is a double-sided ­&quot;panorama&quot; that reflects the contradictory ­effects of numerous interacting parts. Although the city appears to be a collection of nodes and aggregates, its true character is fundamentally determined by the diversity of its tributaries and links. The more diverse the tributaries and links are, the more dynamic and transformatory the city becomes. As the historical and contemporary conditions of today’s city become ever more variable and variable, the city becomes ever more unstable and subject to the vagaries of geopolitics, economic development and war. The ability of a city to sustain a credible claim to its existing and future inhabitants' services and real estate, to draw on its historical wealth of networks of trade and real-estate management, to employ its urban resources creatively (and destructively) in a war effort, to assume and assume again – that is, change its shape and expand – is the job of the war effort, the state, and future generations to shape and grow out of its mess. 	[3] As the late nineteenth-century western model matured and firmed up, so did the</text>
    <text>broad range of curatorial processes and exhibitions, which are normally organised and presented by and in agreement with the city as a whole. 	[1] 	In a nutshell, the concept of ‘the city’ as an urban space emerged in the second half of the nineteenth century. The aim of this brief pre-examines this crucial period in the city’s development and makes the case that, far from being constrained by national or colonial boundaries, the city is both space and ‘category fail’ – a category that grows larger with each passing day. The city becomes ever more diverse and variable, but its core urban function remains the same: to be a set of relations between people, places, objects and networks. In this sense, the city is both infinite in space and categorical. 	[2] As Michael Slackman describes, the city is a double-sided ­&quot;panorama&quot; that reflects the contradictory ­effects of numerous interacting parts. Although the city appears to be a collection of interconnected points of entry and exit, they are actually quite separate processes. The city is a spatially mediated phenomenon that depends on a multitude of independent, varying and autonomous factors. As such, it is neither static nor static in character – both space and time are constantly mutating and evolving. The ‘consequences of [its] growth’ are complex and unpredictable. As the city pursues ever larger and more diverse user-generated reconfiguration, its shape-shifting and new urban forms are continually on the horizon. 	[3] As a result, the city is constantly on the hunt for ways to allocate and dissipate this ever-expanding, multi-tasking and multi-use urban space. This internal mutability – constantly on the hunt for new urban configurations that cater to the needs of the ever-expanding, multi-modal urban user – is what makes the city what it is. It is a constant shifting of the city</text>
    <text>broad range of curatorial processes and exhibitions, which are normally organised and presented by and in agreement with the city as a whole. 	[1] 	In a nutshell, the concept of ‘the city’ as an urban space emerged in the second half of the nineteenth century. The aim of this brief pre-examines this crucial period in the city’s development and makes the case that, far from being constrained by national or colonial boundaries, the city is both space and ‘category fail’ – a category that grows larger with each passing day. The city becomes ever more diverse and variable, but its core urban function remains the same: to be a set of relations between people, places, objects and networks. In this sense, the city is both infinite in space and categorical. 	[2] As Michael Slackman describes, the city is a double-sided ­&quot;panorama&quot; that reflects the contradictory ­effects of numerous interacting parts. Although the city appears to be a collection of interacting parts, the integrated parts are made up of many more than one type of person, place, object or network. Thus, the integrated aspects of the city are not confined to the city itself. They extend to the built environment, and can be seen to affect everyday life in the built environment as well. 	[3] For instance, in the metropolis, the city organises people into ‘stations’ or ‘cultures’ – populations of people confined to particular locations in a particular urban landscape. Thus, the metropolis regulates and assembles into ‘regions’ of urban activity, often regulating or removing people from the urban space during this process of spatialisation. The city organises people into ‘regions’ of activity – populations of people who move from place to place in an urban space. Instituting ‘regional identity’ for ‘regional travel’ is a principal objective of the urban</text>
    <text>broad range of curatorial processes and exhibitions, which are normally organised and presented by and in agreement with the city as a whole. 	[1] 	In a nutshell, the concept of ‘the city’ as an urban space emerged in the second half of the nineteenth century. The aim of this brief pre-examines this crucial period in the city’s development and makes the case that, far from being constrained by national or colonial boundaries, the city is both space and ‘category fail’ – a category that grows larger with each passing day. The city becomes ever more diverse and variable, but its core urban function remains the same: to be a set of relations between people, places, objects and networks. In this sense, the city is both infinite in space and categorical. 	[2] As Michael Slackman describes, the city is a double-sided ­&quot;panorama&quot; that reflects the contradictory ­effects of numerous interacting parts. Although the city appears to be a collection of distinct points of entry and exit, its true intent is to support a multiplicity of contradictory interlocutions that mutually depend on one another. The city is both space and a category, but differentiating objects in different semantic domains simultaneously inhabit different parts of the same city. Object systems – including bureaucracies and national parks – are highly interpretable, and the historical record shows that many different interpretations of the same park can be attributed to the same founding event. The twentieth century saw a massive expansion of the national park system, which allowed a single developer to construct a large, multipurpose park that could simultaneously serve as an administrative center, a commercial centre, and a city core. This expansion coincided with a massive decrease in population, which in turn reduced the need for so many people to work in the park (or in factories, for those with skills). The result was an explosion of new kinds of office and retail space, which suddenly attracted a diverse range of people, including workers from all</text>
  </text>
  <text>
    using machine learning techniques to identify patterns and similarities between crowds of people – and between crowds of objects – and between crowds of human-made and natural objects. The autonomous vehicle could then employ similar but different algorithms to analyse data to predict behaviour based on past behaviour. For instance, the autonomous truck might employ similar mathematical modelling to human-driven vehicles to detect roadside debris and other obstacles, and it might employ similar but different statistical and human-learning techniques to detect human behaviour in the road ahead.  	Such autonomous vehicle options would permit drivers to act on data to achieve safer driving, and the autonomous truck might employ similar but different algorithms to achieve the same or better results. 	Such techniques are just one example of the ways in which autonomous vehicles are approaching a scenario in which humans play a role that is no longer solely that of driver. In fact, there are many more technological options that need to be considered and perfected before such autonomous vehicles become a reality. For instance, there is the need to ensure that the
    <text>using machine learning techniques to identify patterns and similarities between crowds of people – and between crowds of objects – and between crowds of human-made and natural objects. The autonomous vehicle could then employ similar but different algorithms to analyse data to predict behaviour based on past behaviour. For instance, the autonomous truck might employ similar mathematical modelling to human-driven vehicles to detect roadside debris and other obstacles, and it might employ similar but different statistical and human-learning techniques to detect human behaviour in the road ahead.  	Such autonomous vehicle options would permit drivers to act on data to achieve safer driving, and the autonomous truck might employ similar but different algorithms to achieve the same or better results. 	Such techniques are just one example of the ways in which autonomous vehicles are approaching a scenario in which humans play a role that is no longer solely that of driver. In fact, there are many more technological options that need to be considered and perfected before such autonomous vehicles become a reality. For instance, there is the need to ensure that the infrastructure supporting autonomous vehicles is not tampered with in such a way as to permit the vehicles to travel at all, let alone to allow them to overtake and drive alongside other vehicles. In addition, there is the need to ensure that the infrastructure supporting such vehicles is not used for commercial ends – let alone for the purposes of industrial ones. Lastly, there is the question of sovereignty – what kind of claims – and how such rights should be defined and enforced. 	In the context of the global autonomous vehicle market, the ability of autonomous vehicles to negotiate their behaviour around the world is a real issue. The ability to travel at all is claimed by many, and the ability to drive alongside other vehicles is an important right, at least in the eyes of a dominant elite. In the event of a dispute, it is likely that the dominant elite will seek to seize control of the means of transportation and make them their own. 	So how should autonomous vehicles be regulated? In the US, the debate</text>
    <text>using machine learning techniques to identify patterns and similarities between crowds of people – and between crowds of objects – and between crowds of human-made and natural objects. The autonomous vehicle could then employ similar but different algorithms to analyse data to predict behaviour based on past behaviour. For instance, the autonomous truck might employ similar mathematical modelling to human-driven vehicles to detect roadside debris and other obstacles, and it might employ similar but different statistical and human-learning techniques to detect human behaviour in the road ahead.  	Such autonomous vehicle options would permit drivers to act on data to achieve safer driving, and the autonomous truck might employ similar but different algorithms to achieve the same or better results. 	Such techniques are just one example of the ways in which autonomous vehicles are approaching a scenario in which humans play a role that is no longer solely that of driver. In fact, there are many more technological options that need to be considered and perfected before such autonomous vehicles become a reality. For instance, there is the need to ensure that the components of an autonomous truck or van meet regulatory requirements – and even comply with international safety standards – so that they can operate legally. There is the need to ensure that autonomous vehicles meet the needs of local communities so that they can provide local businesses and residents with better solutions. There is the need to ensure that autonomous vehicles operate in ways that are not guided by human beings and so so so on. 	Such are the layers of the society, though the layers may be invisible or even unimportant in the scheme of things. But when analysing the issues raised by the Black Lives Matter movement, we see that they are visible and well-known. We see the systemic causes of poverty, racism and injustice that need addressing. We see the need for autonomous vehicles that look after our natural resources and leave the city to minimise disruption to the urban landscape. And we see the perils of over-embracing technology and high-speed internet access in the service of economic growth. 	But</text>
    <text>using machine learning techniques to identify patterns and similarities between crowds of people – and between crowds of objects – and between crowds of human-made and natural objects. The autonomous vehicle could then employ similar but different algorithms to analyse data to predict behaviour based on past behaviour. For instance, the autonomous truck might employ similar mathematical modelling to human-driven vehicles to detect roadside debris and other obstacles, and it might employ similar but different statistical and human-learning techniques to detect human behaviour in the road ahead.  	Such autonomous vehicle options would permit drivers to act on data to achieve safer driving, and the autonomous truck might employ similar but different algorithms to achieve the same or better results. 	Such techniques are just one example of the ways in which autonomous vehicles are approaching a scenario in which humans play a role that is no longer solely that of driver. In fact, there are many more technological options that need to be considered and perfected before such autonomous vehicles become a reality. For instance, there is the need to ensure that the data captured by autonomous vehicles is treated with the same legal protections as human-driven vehicles; to ensure that the data is transmitted over the same infrastructures; to ensure that the data is accurately captured and transmitted back to the infrastructures that created the autonomous vehicle; and to ensure that the autonomous truck reaches its destination in the same way that human-driven trucks do. 	Such  	layer-by-layer and 'sandwich' options – which aim to provide such critical layers but are also so adept at feeding off the fragmented information that is gathered that they can anticipate virtually every eventuality and making optimisations based on imperfect data – are just some of the technological constraints that need to be considered when designing an autonomous vehicle. While some of these technological choices seem obvious and obvious-seeming, others are much more subtle and yet vital to the success of such initiatives as trucking and delivery services today. 	For instance, when a driverless truck, for some</text>
    <text>using machine learning techniques to identify patterns and similarities between crowds of people – and between crowds of objects – and between crowds of human-made and natural objects. The autonomous vehicle could then employ similar but different algorithms to analyse data to predict behaviour based on past behaviour. For instance, the autonomous truck might employ similar mathematical modelling to human-driven vehicles to detect roadside debris and other obstacles, and it might employ similar but different statistical and human-learning techniques to detect human behaviour in the road ahead.  	Such autonomous vehicle options would permit drivers to act on data to achieve safer driving, and the autonomous truck might employ similar but different algorithms to achieve the same or better results. 	Such techniques are just one example of the ways in which autonomous vehicles are approaching a scenario in which humans play a role that is no longer solely that of driver. In fact, there are many more technological options that need to be considered and perfected before such autonomous vehicles become a reality. For instance, there is the need to ensure that the infrastructure supporting autonomous vehicles is not tampered with in any way, as a way to ensure that the systems that underpin autonomous vehicles are not tampered with in such a way as to prevent the vehicles achieving better results than they would in their human-driven models.  One final technological option that is worthy of note is the role played by data minimisation in the creation of autonomous vehicles. Consideration should be given to the maximisation of the utility of a given amount of data, as a measure of a vehicle’s capabilities. For instance, the amount of roadkill that is killed on the US-Mexico border each year is a function of the amount of data gathered by border patrol agents. If all the data about a vehicle were gathered simultaneously, then the usefulness of the vehicle would be determined by the number of miles driven in a given time period. In contrast, the amount of roadkill that is killed in a given year is a function of the amount of data gathered by border patrol agents</text>
    <text>using machine learning techniques to identify patterns and similarities between crowds of people – and between crowds of objects – and between crowds of human-made and natural objects. The autonomous vehicle could then employ similar but different algorithms to analyse data to predict behaviour based on past behaviour. For instance, the autonomous truck might employ similar mathematical modelling to human-driven vehicles to detect roadside debris and other obstacles, and it might employ similar but different statistical and human-learning techniques to detect human behaviour in the road ahead.  	Such autonomous vehicle options would permit drivers to act on data to achieve safer driving, and the autonomous truck might employ similar but different algorithms to achieve the same or better results. 	Such techniques are just one example of the ways in which autonomous vehicles are approaching a scenario in which humans play a role that is no longer solely that of driver. In fact, there are many more technological options that need to be considered and perfected before such autonomous vehicles become a reality. For instance, there is the need to ensure that the information gathered from the autonomous vehicle is transmitted to the local authorities and, if needed, used to build public works or create new types of infrastructure. There is the need to ensure that the autonomous truck does not drive itself, and that it does not run out of gas. There is the need to ensure that the surrounding area is kept safe and to minimise accidents. There is the need to ensure that the surrounding area is not overwhelmed by construction and industry. There is the need to ensure that the surrounding area is not flooded and that the construction does not disrupt water resources. Finally, there is the need to ensure that the surrounding area is not left behind by the ageing infrastructure that characterises large-scale urbanisation. 	In these scenarios, the autonomy of the driver becomes a critical role, serving as a mediator between the autonomous vehicle and the socio-political systems that control its movement.  	In the coming years, autonomous vehicle technologies will become more pervasive and sensitive to urbanisation and</text>
    <text>using machine learning techniques to identify patterns and similarities between crowds of people – and between crowds of objects – and between crowds of human-made and natural objects. The autonomous vehicle could then employ similar but different algorithms to analyse data to predict behaviour based on past behaviour. For instance, the autonomous truck might employ similar mathematical modelling to human-driven vehicles to detect roadside debris and other obstacles, and it might employ similar but different statistical and human-learning techniques to detect human behaviour in the road ahead.  	Such autonomous vehicle options would permit drivers to act on data to achieve safer driving, and the autonomous truck might employ similar but different algorithms to achieve the same or better results. 	Such techniques are just one example of the ways in which autonomous vehicles are approaching a scenario in which humans play a role that is no longer solely that of driver. In fact, there are many more technological options that need to be considered and perfected before such autonomous vehicles become a reality. For instance, there is the need to ensure that the data that is gathered by the autonomous vehicle is treated with the same sensitivity as human data; to ensure that the data does not become corrupted by the action of its drivers; to ensure that the data is transmitted at the speed of sound; and to ensure that the transmission of the data is not compromised by the actions of its drivers or other parties. 	Such are some of the issues that need to be considered in order to envision a scenario in which autonomous vehicles are not only used by nomadic crews but also help to create new professions and cultural forms that are more appropriately suited to the urban environment. It is important to note that although there are many technical and societal hurdles that need to be overcome in order to enable such visions to become a reality, there are also great rewards to be had in the form of social impact. 	It is widely accepted that the main driver of autonomous vehicle adoption is the desire to do away with the need for humans in the vehicle (or, more accurately, in</text>
    <text>using machine learning techniques to identify patterns and similarities between crowds of people – and between crowds of objects – and between crowds of human-made and natural objects. The autonomous vehicle could then employ similar but different algorithms to analyse data to predict behaviour based on past behaviour. For instance, the autonomous truck might employ similar mathematical modelling to human-driven vehicles to detect roadside debris and other obstacles, and it might employ similar but different statistical and human-learning techniques to detect human behaviour in the road ahead.  	Such autonomous vehicle options would permit drivers to act on data to achieve safer driving, and the autonomous truck might employ similar but different algorithms to achieve the same or better results. 	Such techniques are just one example of the ways in which autonomous vehicles are approaching a scenario in which humans play a role that is no longer solely that of driver. In fact, there are many more technological options that need to be considered and perfected before such autonomous vehicles become a reality. For instance, there is the need to ensure that the data that is gathered from the roads of Los Angeles is treated with the same sensitivity as human-caused pollution is; that is, that it is transmitted between the autonomous vehicle and the local sensor network; and that the transmission is robust against data corruption and network failures. 	[i] Oxford Dictionaries, s.v.  1, September 2018.Download this article as PDFJussi ParikkaJussi Parikka is Associate Professor in the School of Information and Culture, University of Southern California. His research interests include the autonomous vehicle, mapping and data extraction, and media ecology. His publications include the book “Autonomous Vehicles and Data Mining” in the California Association of Governments Computing Journal #3, the annual “California Road Trip” in the California Geographic Quarterly, and the annual “California Road Trip” web-based journal Approximation. His work has been featured in numerous trade and non-trade publications including a</text>
    <text>using machine learning techniques to identify patterns and similarities between crowds of people – and between crowds of objects – and between crowds of human-made and natural objects. The autonomous vehicle could then employ similar but different algorithms to analyse data to predict behaviour based on past behaviour. For instance, the autonomous truck might employ similar mathematical modelling to human-driven vehicles to detect roadside debris and other obstacles, and it might employ similar but different statistical and human-learning techniques to detect human behaviour in the road ahead.  	Such autonomous vehicle options would permit drivers to act on data to achieve safer driving, and the autonomous truck might employ similar but different algorithms to achieve the same or better results. 	Such techniques are just one example of the ways in which autonomous vehicles are approaching a scenario in which humans play a role that is no longer solely that of driver. In fact, there are many more technological options that need to be considered and perfected before such autonomous vehicles become a reality. For instance, there is the need to ensure that the data that is gathered from the road is transmitted over long distances, and that the vehicles that are fitted with cameras actually follow the laws of the road. 	Another issue that needs to be considered is the effect of such surveillance on the rights of citizens. Is it too dystopian? Does the surveillance that is being proposed actually exacerbate contemporary  	challenges? In the wake of Edward Snowden’s revelations, it is obvious that the rights of citizens are among those at risk. There is a danger that, instead of gathering data to support legal claims, autonomous vehicles would rather invent their own truth. 	Such is the  	logic of the autonomous vehicle. However, as we have seen, this is by no means inevitable. It is an option, and one that is being actively considered by policymakers and civil society organisations. As a final note, it is worth noting that the prospect of such legal claims was a key reason why William Burchell established the Burchell &amp; Company law</text>
    <text>using machine learning techniques to identify patterns and similarities between crowds of people – and between crowds of objects – and between crowds of human-made and natural objects. The autonomous vehicle could then employ similar but different algorithms to analyse data to predict behaviour based on past behaviour. For instance, the autonomous truck might employ similar mathematical modelling to human-driven vehicles to detect roadside debris and other obstacles, and it might employ similar but different statistical and human-learning techniques to detect human behaviour in the road ahead.  	Such autonomous vehicle options would permit drivers to act on data to achieve safer driving, and the autonomous truck might employ similar but different algorithms to achieve the same or better results. 	Such techniques are just one example of the ways in which autonomous vehicles are approaching a scenario in which humans play a role that is no longer solely that of driver. In fact, there are many more technological options that need to be considered and perfected before such autonomous vehicles become a reality. For instance, there is the need to ensure that the data that is gathered from the roads of Toronto supports the analysis and decision-making processes that lead to safer driving, including the development of collaborative legal frameworks and the development of regulatory schemes that protect the environment and citizens. 	Another factor is the creation of what is sometimes referred to as the ‘transportation network integrity problem’. This is an area where federal, provincial and municipal governments play a role in the development and implementation of autonomous vehicles. For instance, the federal government provides funding through grants to encourage the development of new autonomous vehicle technologies. These technologies need to be approved by provincial and municipal governments at the federal and provincial level in order to become part of the national infrastructure. The creation of a provincial or municipal autonomous vehicle policymaking body is another option for the future of transportation infrastructure, and this is an option that is being actively pursued by cities and municipalities. 	These autonomous vehicle policymaking bodies would take the lead in the development and implementation of the new technology, and they would</text>
    <text>using machine learning techniques to identify patterns and similarities between crowds of people – and between crowds of objects – and between crowds of human-made and natural objects. The autonomous vehicle could then employ similar but different algorithms to analyse data to predict behaviour based on past behaviour. For instance, the autonomous truck might employ similar mathematical modelling to human-driven vehicles to detect roadside debris and other obstacles, and it might employ similar but different statistical and human-learning techniques to detect human behaviour in the road ahead.  	Such autonomous vehicle options would permit drivers to act on data to achieve safer driving, and the autonomous truck might employ similar but different algorithms to achieve the same or better results. 	Such techniques are just one example of the ways in which autonomous vehicles are approaching a scenario in which humans play a role that is no longer solely that of driver. In fact, there are many more technological options that need to be considered and perfected before such autonomous vehicles become a reality. For instance, there is the need to ensure that the data that is gathered from the road is treated with the same dignity and legal protections as other types of data, and there is the need to ensure that the data is transmitted over the same networks that all other types of data are transmitted through. 	Similarly, there is the question of pollution, which is increasingly a concern for drivers as is evident in the growing number of self-reported accidents. The increased number of reported accidents is likely to increase public and political awareness of the seriousness of the pollution problem, and perhaps also of the culpability of drivers. 	The autonomous vehicle may be able to detect when a particular pollutant is present, and thus may be able to modify its trajectory in an effort to avoid or mitigate the pollution problem. 	Such pollution control could potentially reduce congestion and pollution, at least in the short term. 	But given that self-driving vehicles continue to be novel and experimental, and given that the regulatory framework remains largely undetermined and unpredictable, it is also</text>
  </text>
  <text>
    biennial as a self-learning system, capable of building on its experiences and those of others. If, as the Biennale hopes, its visitors are visitors, then the museum becomes a second home. The Biennale’s mission statement announces its desire to explore ‘the larger picture of the world’: ‘a bigger picture that includes ourselves and those around us.’17 The Biennale’s portfolio of projects calls into question the notion of the museum as a purely decorative object: ‘it is here that the museum is no longer seen as the pinnacle of artistic achievement’ and calls into question the value of the museum as a museum institution.18 ‘Art’, in the Biennale’s parlance, is a synonym for ‘value’; a word that, while it may convey a great deal of information, is subject to a great deal of ambiguity and subject to change. The ambiguity of the word ‘art’,
    <text>biennial as a self-learning system, capable of building on its experiences and those of others. If, as the Biennale hopes, its visitors are visitors, then the museum becomes a second home. The Biennale’s mission statement announces its desire to explore ‘the larger picture of the world’: ‘a bigger picture that includes ourselves and those around us.’17 The Biennale’s portfolio of projects calls into question the notion of the museum as a purely decorative object: ‘it is here that the museum is no longer seen as the pinnacle of artistic achievement’ and calls into question the value of the museum as a museum institution.18 ‘Art’, in the Biennale’s parlance, is a synonym for ‘value’; a word that, while it may convey a great deal of information, is subject to a great deal of ambiguity and subject to change. The ambiguity of the word ‘art’, and the subjectivity of the viewer in relation to the artist’s choices, make it difficult to determine the true value of an artefact. Indeed, the ambiguity of the term is a problem for the biennale as a whole; the audience is frequently directed to viewing the artist’s work as a way to ‘reanimate’ institutional forms and ideologies.19 The Biennale has its critics; there is much antipathy towards the modern museum, especially from the younger generation of biennials. But the ambivalent quality of the word ‘art’ that the Biennale seeks to obscure is part of the exhibitionary complex, part of the gallery experience, and part of the biennale’s public relations campaign. The Biennale has an art museum, the Barbican, but it also has a cultural museum, the Barbican, which is concerned with the relationship between art and the social. It is, in effect, asking the</text>
    <text>biennial as a self-learning system, capable of building on its experiences and those of others. If, as the Biennale hopes, its visitors are visitors, then the museum becomes a second home. The Biennale’s mission statement announces its desire to explore ‘the larger picture of the world’: ‘a bigger picture that includes ourselves and those around us.’17 The Biennale’s portfolio of projects calls into question the notion of the museum as a purely decorative object: ‘it is here that the museum is no longer seen as the pinnacle of artistic achievement’ and calls into question the value of the museum as a museum institution.18 ‘Art’, in the Biennale’s parlance, is a synonym for ‘value’; a word that, while it may convey a great deal of information, is subject to a great deal of ambiguity and subject to change. The ambiguity of the word ‘art’, while essential in conveying the meaning of the word, becomes questionable when applied to ancillary, curatorial practices such as the installation of fencing or towers, or to the collection and management of artefacts. The Biennale’s portfolio of projects calls into question the notion of the artist as a creator, and the artist as a curatorial agent is elevated to a position of great power and control. The Biennale is not, however, the first or even the most prominent promoter of ‘natural’ cultural practices such as yoga, meditation, or eco-tourism. It is true that, during the 1960s and 1970s, the Biennale established a reputation for its ‘poetic atmosphere’ and its ‘deep dive into the cultural heart of Europe’; in these years, the rhetoric was set against the backdrop of the Paris bombings and the Sandinista Alianza crisis in Nicaragua. Still, the Biennale’</text>
    <text>biennial as a self-learning system, capable of building on its experiences and those of others. If, as the Biennale hopes, its visitors are visitors, then the museum becomes a second home. The Biennale’s mission statement announces its desire to explore ‘the larger picture of the world’: ‘a bigger picture that includes ourselves and those around us.’17 The Biennale’s portfolio of projects calls into question the notion of the museum as a purely decorative object: ‘it is here that the museum is no longer seen as the pinnacle of artistic achievement’ and calls into question the value of the museum as a museum institution.18 ‘Art’, in the Biennale’s parlance, is a synonym for ‘value’; a word that, while it may convey a great deal of information, is subject to a great deal of ambiguity and subject to change. The ambiguity of the word ‘art’, in the Biennale’s case, is accentuated by the fact that its definition is continuous with that of the artist (i.e., evolving). In the face of this constant redefinition, the art gallery becomes subject to precisely such changes, which are understood not only as changes in definition but also as changes in venue. This double negation – the art gallery is a ‘potential site of encounter’ but also a ‘potential site of transformation’ – is a powerful concept for the biennial. The Biennale’s portfolio of projects clearly narrates the artist’s ongoing evolution as a visitor to the site of his or her creation. These projects tell the story of the art gallery in various forms – a ‘potential site of encounter’ but also a ‘potential site of transformation’ – and the biennale as a whole. These narratives effectively point to a second evolution of the art gallery in that</text>
    <text>biennial as a self-learning system, capable of building on its experiences and those of others. If, as the Biennale hopes, its visitors are visitors, then the museum becomes a second home. The Biennale’s mission statement announces its desire to explore ‘the larger picture of the world’: ‘a bigger picture that includes ourselves and those around us.’17 The Biennale’s portfolio of projects calls into question the notion of the museum as a purely decorative object: ‘it is here that the museum is no longer seen as the pinnacle of artistic achievement’ and calls into question the value of the museum as a museum institution.18 ‘Art’, in the Biennale’s parlance, is a synonym for ‘value’; a word that, while it may convey a great deal of information, is subject to a great deal of ambiguity and subject to change. The ambiguity of the word ‘art’, as in the famous Merriam-Webster dictionary, is heightened by a number of synonyms, some of which conjure images of places and crowds of people: ‘concert, parade, metropolis’, ‘museum’, ‘apparition, figure, signal, adornment’, ‘compound, structure, set of ranks and ranks’, ‘crowd, multitude, assemblage’, ‘crowd of people united by a common experience’, ‘compound, relation, bond’, ‘commodity, association’ and ‘crowd, including tourists’.19 Cultural capital is a synonym for vernacular.  A number of biennales over the years have sought to capture the magic of the biennale in a form that is not strictly geographical, national or institutional. I have chosen as an example the 1970 Munich Biennale,</text>
    <text>biennial as a self-learning system, capable of building on its experiences and those of others. If, as the Biennale hopes, its visitors are visitors, then the museum becomes a second home. The Biennale’s mission statement announces its desire to explore ‘the larger picture of the world’: ‘a bigger picture that includes ourselves and those around us.’17 The Biennale’s portfolio of projects calls into question the notion of the museum as a purely decorative object: ‘it is here that the museum is no longer seen as the pinnacle of artistic achievement’ and calls into question the value of the museum as a museum institution.18 ‘Art’, in the Biennale’s parlance, is a synonym for ‘value’; a word that, while it may convey a great deal of information, is subject to a great deal of ambiguity and subject to change. The ambiguity of the word ‘art’, in turn, creates opportunities for those with a clear agenda (including biennials) to prey on those who are trapped in the ambiguity of the word. The ethics of the biennial in its current form certainly supports the claims of those who seek to exploit the ambiguity of the concept. The Biennale has set out to make art, and its projects clearly demonstrate that aim. However, the project does not seek to render to the viewer a clear understanding of the world; rather, it seeks to reveal the ambiguities and give the viewer a sense of the unknown. That ambiguity is a feature of the art market. There is a danger, though, in the creation of art that is not carefully executed: that ambiguity can be exploited by market forces, and biennials are a prime market for this exploitation. The Biennale is no doubt aware of this danger – and perhaps it sees the writing on the wall: that danger is writing itself. It is true that the value of a</text>
    <text>biennial as a self-learning system, capable of building on its experiences and those of others. If, as the Biennale hopes, its visitors are visitors, then the museum becomes a second home. The Biennale’s mission statement announces its desire to explore ‘the larger picture of the world’: ‘a bigger picture that includes ourselves and those around us.’17 The Biennale’s portfolio of projects calls into question the notion of the museum as a purely decorative object: ‘it is here that the museum is no longer seen as the pinnacle of artistic achievement’ and calls into question the value of the museum as a museum institution.18 ‘Art’, in the Biennale’s parlance, is a synonym for ‘value’; a word that, while it may convey a great deal of information, is subject to a great deal of ambiguity and subject to change. The ambiguity of the word ‘art’, in relation to the Biennale, is heightened by the fact that the word is Italian and, although it may be the case that all artists are Italian, Italy’s place in the world, especially through its economic and political power, is a fiction. The Biennale’s other partner in Turin, the iconic Giardini, has an even bleaker take on the word: while its Venice headquarters are undoubtedly its most recognizable, the Biennale’s London and New York branches are equally identifiable, and have been since at least the 1980s, when Giardini emerged as the obvious successor to the venerable National Gallery.19 Given the reputation of Giardini, and the Biennale’s, particular celebrity-making potential, its use in this sense is appealing. But the Biennale’s use of a site-specifically identifies the site, something that could have been done with the Biennale’s attention to</text>
    <text>biennial as a self-learning system, capable of building on its experiences and those of others. If, as the Biennale hopes, its visitors are visitors, then the museum becomes a second home. The Biennale’s mission statement announces its desire to explore ‘the larger picture of the world’: ‘a bigger picture that includes ourselves and those around us.’17 The Biennale’s portfolio of projects calls into question the notion of the museum as a purely decorative object: ‘it is here that the museum is no longer seen as the pinnacle of artistic achievement’ and calls into question the value of the museum as a museum institution.18 ‘Art’, in the Biennale’s parlance, is a synonym for ‘value’; a word that, while it may convey a great deal of information, is subject to a great deal of ambiguity and subject to change. The ambiguity of the word ‘art’, as well as the subjectivity of the art that is displayed, is itself subject to change. The meaning of the word may be influenced by the context in which it is used; context being one of the ways in which the Biennale is able to constrain and mediate meaning in a way that other art institutions do not.  The ambiguity of the word ‘art’ also contributes to the ubiquity of ‘art’ as an adjective. In the broadest usage of the word, it is possible to mention many different kinds of art, from early modern to contemporary art done today. Modern and contemporary art are typically referred to as ‘artistic practice’, though there is some confusion about what this means in the context of the Biennale. Some see the connotations of the word, and some don’t.  Art is a noun, and as such its use in this sense is widespread, but to a large extent it remains unclear what</text>
    <text>biennial as a self-learning system, capable of building on its experiences and those of others. If, as the Biennale hopes, its visitors are visitors, then the museum becomes a second home. The Biennale’s mission statement announces its desire to explore ‘the larger picture of the world’: ‘a bigger picture that includes ourselves and those around us.’17 The Biennale’s portfolio of projects calls into question the notion of the museum as a purely decorative object: ‘it is here that the museum is no longer seen as the pinnacle of artistic achievement’ and calls into question the value of the museum as a museum institution.18 ‘Art’, in the Biennale’s parlance, is a synonym for ‘value’; a word that, while it may convey a great deal of information, is subject to a great deal of ambiguity and subject to change. The ambiguity of the word ‘art’, in this context, is accentuated by the fact that art is art in its own right, with no clearly defined boundary or standard of value. As the Biennale’s portfolio makes clear, the ambiguity of the concept of value is compounded by the fact that art is inherently unstable: it is always in conflict with itself, and its very appearance threatens the stability of the Biennale itself. The portfolio of works included in the exhibition is testimony to this instability; its splendor is itself a prelude to a conflictual encounter between art and the biennial itself.1 	[i] See  	http://www.biennaleofsydney.com/212826/exhibition/default.asp. 2 	[ii] See  	http://www.biennaleofsydney.com/212826/exhibition/default.asp. 3 	[iii] See  	http://www.</text>
    <text>biennial as a self-learning system, capable of building on its experiences and those of others. If, as the Biennale hopes, its visitors are visitors, then the museum becomes a second home. The Biennale’s mission statement announces its desire to explore ‘the larger picture of the world’: ‘a bigger picture that includes ourselves and those around us.’17 The Biennale’s portfolio of projects calls into question the notion of the museum as a purely decorative object: ‘it is here that the museum is no longer seen as the pinnacle of artistic achievement’ and calls into question the value of the museum as a museum institution.18 ‘Art’, in the Biennale’s parlance, is a synonym for ‘value’; a word that, while it may convey a great deal of information, is subject to a great deal of ambiguity and subject to change. The ambiguity of the word ‘art’, in fact, remains largely unresolved in English; as a noun, it is equally ambiguous as an adjective. Perhaps one could say that the ambiguity of the word derives from the fact that it is a noun, while the ambiguity of the object it represents – a painting, a sculpture or a piece of decorative art – derives from the fact that it is an object of contemplation. But this is to misunderstand the object itself; the word exhibits a preference for the ambiguous. The object – and its owner, the viewer, the listener – is the par excellence of the biennale. At a distance, we might notice that the art exhibits are scattered and disjointed; as biennales grow more institutionalised, the number of individual artworks and their placement becomes increasingly important. As the Biennale matures, and more and more of its programming is focussed on the gallery, the viewer, the collector and the curator, the number of artists and their placement becomes increasingly variable. Each viewer is</text>
    <text>biennial as a self-learning system, capable of building on its experiences and those of others. If, as the Biennale hopes, its visitors are visitors, then the museum becomes a second home. The Biennale’s mission statement announces its desire to explore ‘the larger picture of the world’: ‘a bigger picture that includes ourselves and those around us.’17 The Biennale’s portfolio of projects calls into question the notion of the museum as a purely decorative object: ‘it is here that the museum is no longer seen as the pinnacle of artistic achievement’ and calls into question the value of the museum as a museum institution.18 ‘Art’, in the Biennale’s parlance, is a synonym for ‘value’; a word that, while it may convey a great deal of information, is subject to a great deal of ambiguity and subject to change. The ambiguity of the word ‘art’, while accentuated in the Biennale’s portfolio, is also a powerful tool for obfuscating, for example, when, during the 2016 Venice Biennale, a team of architects and designers, led by Arsen Avakov, presented a new take on the Biennale’s old, but potent, concept of the ‘institution’ through what was, in their words, a ‘radical reimagining’ of the institution.19 The Biennale’s packaging certainly aids in this tendency towards the present, in that the capsule, while not a literal product (in that the capsule is a concept rather than a tangible artefact), nevertheless conveys a snapshot of events in an event through which the Biennale is asked to ‘represent the city alongside its natural resources and inhabitants’.20 The capsule is packaged as a visual memoir of the event, in the form of over a hundred photographs. It is divided into three sections</text>
  </text>
</AIcurator>
