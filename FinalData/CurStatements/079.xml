<?xml version="1.0" ?>
<AIcurator>
  <!--Leonardo Impett, Ubermorgen, Joasia Krysa-->
  <text>
    machine learning experiments, such as those by Google DeepMind or Facebook DeepMind, that have revolutionised not only the way in which we think about science and technology, but also the way in which we interact with them.And yet the question of what constitutes a 'public' and a 'public good' – what counts as a 'public domain' and what is deemed a 'public space' – remains unsettled. What is considered a public space may be privately owned; a 'public good' may be owned by a private company. In the early twentieth century, for example, the public domain was defined as the land in which were vested in the Ukrainian Tatyugu people the right to a geographical extent corresponding with that of Australia; and in the twentieth century, the public good was defined as that part of the global commons which the West claimed as its own. But in recent decades, the public domain has expanded to include objects brought out of storage or publicly available through natural resources, including much of east
    <text>machine learning experiments, such as those by Google DeepMind or Facebook DeepMind, that have revolutionised not only the way in which we think about science and technology, but also the way in which we interact with them.And yet the question of what constitutes a 'public' and a 'public good' – what counts as a 'public domain' and what is deemed a 'public space' – remains unsettled. What is considered a public space may be privately owned; a 'public good' may be owned by a private company. In the early twentieth century, for example, the public domain was defined as the land in which were vested in the Ukrainian Tatyugu people the right to a geographical extent corresponding with that of Australia; and in the twentieth century, the public good was defined as that part of the global commons which the West claimed as its own. But in recent decades, the public domain has expanded to include objects brought out of storage or publicly available through natural resources, including much of east Africa and the Middle East. Some argue that the public good has also expanded to include things that are ephemeral, ephemeral in nature, ephemeral in their significance, ephemeral in their ownership – objects that can be reclaimed and returned to their rightful places through in-kind contributions, such as land, infrastructure and cultural resources. In this model, the public good is credited with the ability to take ownership of the ephemeral; the object that is reclaimed and returned to its site of use is considered a new kind of property.11 	But is the public good really such and is the return to public use only temporary and ephemeral? In recent decades, philosophers, natural scientists and archaeologists have been developing what might be called  	public forms of archaeological interest. These forms of archaeological interest are concerned with the long-term, with questions of legacy and residence in the long run, with the ecological and geo-political dimensions of contemporary urbanization in the global</text>
    <text>machine learning experiments, such as those by Google DeepMind or Facebook DeepMind, that have revolutionised not only the way in which we think about science and technology, but also the way in which we interact with them.And yet the question of what constitutes a 'public' and a 'public good' – what counts as a 'public domain' and what is deemed a 'public space' – remains unsettled. What is considered a public space may be privately owned; a 'public good' may be owned by a private company. In the early twentieth century, for example, the public domain was defined as the land in which were vested in the Ukrainian Tatyugu people the right to a geographical extent corresponding with that of Australia; and in the twentieth century, the public good was defined as that part of the global commons which the West claimed as its own. But in recent decades, the public domain has expanded to include objects brought out of storage or publicly available through natural resources, including much of east Asia and Africa. As a result, the public good has come to include not just physical space, but also a public good – and the physical space and the public good are now contested goods. What is now considered the public good in most regions of the world is considered to be private property. In many regions of the world, including parts of China and Russia, the public good is controlled by local governments or corporate entities. But in the West, the public good is considered to be that part of the global commons which is jointly owned by the state and the people. If the public good is to be distributed evenly, as a result of such experiments as Google Earth and Facebook Photos, then the public has to be able to take part in the public good in some capacity. Local governments, corporate entities, and citizens need to be able to devise ways to participate in the public good in some capacity. This is why states provide frameworks for the public good, including public resources, infrastructure, and jurisdiction.</text>
    <text>machine learning experiments, such as those by Google DeepMind or Facebook DeepMind, that have revolutionised not only the way in which we think about science and technology, but also the way in which we interact with them.And yet the question of what constitutes a 'public' and a 'public good' – what counts as a 'public domain' and what is deemed a 'public space' – remains unsettled. What is considered a public space may be privately owned; a 'public good' may be owned by a private company. In the early twentieth century, for example, the public domain was defined as the land in which were vested in the Ukrainian Tatyugu people the right to a geographical extent corresponding with that of Australia; and in the twentieth century, the public good was defined as that part of the global commons which the West claimed as its own. But in recent decades, the public domain has expanded to include objects brought out of storage or publicly available through natural resources, including much of east Asia. Cultural theorists have been keen to point out that although the public good has come to include most of the planet, the cultural life on the planet still needs publics, public sites and so forth. At a conference on the public good convened by the Arts Council in London in 2015, Peter Singer, the arts's great revivaliser, gave a rousing but empty recitation of the virtues of the public domain, in stark contrast to the rousing but empty recitation of the artsy virtues of modern culture. He spoke of the heroic acts of successive waves of artists rescued from museums or archival collections, the archiving and preservation of cultural materials, the weaving of community arts projects into modern art, the memory of abandoned urban centres, the collection and exchange of artefacts, the gathering and exchange of knowledge across borders, the gathering and exchange of experience across terrain, and the gathering and exchange of values across sectors – in short, the public good. Singer was no archivist or specialist in</text>
    <text>machine learning experiments, such as those by Google DeepMind or Facebook DeepMind, that have revolutionised not only the way in which we think about science and technology, but also the way in which we interact with them.And yet the question of what constitutes a 'public' and a 'public good' – what counts as a 'public domain' and what is deemed a 'public space' – remains unsettled. What is considered a public space may be privately owned; a 'public good' may be owned by a private company. In the early twentieth century, for example, the public domain was defined as the land in which were vested in the Ukrainian Tatyugu people the right to a geographical extent corresponding with that of Australia; and in the twentieth century, the public good was defined as that part of the global commons which the West claimed as its own. But in recent decades, the public domain has expanded to include objects brought out of storage or publicly available through natural resources, including much of east Africa and the Middle East. Thus while the public good may include things in the public domain, it is also possible that the public good will exclude or take an opposite position to what is considered the public good. This is why, for example, a museum that displays works from the public domain in an area may be opposed by the people who live around it. It is also why, in the twentieth century, the public good was opposed to commercial culture, which tends to make private things public. It is one reason why, in the twentieth century, the public was suspicious of any cultural industry that did not actively participate in creating a new public good out of its own contradictions.The public good is fundamentally opposed to commercial culture. Hence the twentieth century. But what has recently become evident is that, in contemporary capitalist societies, the public good is no longer defined as the good which results from the collaboration and co-constituted efforts of many people, but rather as the good which is the result of</text>
    <text>machine learning experiments, such as those by Google DeepMind or Facebook DeepMind, that have revolutionised not only the way in which we think about science and technology, but also the way in which we interact with them.And yet the question of what constitutes a 'public' and a 'public good' – what counts as a 'public domain' and what is deemed a 'public space' – remains unsettled. What is considered a public space may be privately owned; a 'public good' may be owned by a private company. In the early twentieth century, for example, the public domain was defined as the land in which were vested in the Ukrainian Tatyugu people the right to a geographical extent corresponding with that of Australia; and in the twentieth century, the public good was defined as that part of the global commons which the West claimed as its own. But in recent decades, the public domain has expanded to include objects brought out of storage or publicly available through natural resources, including much of east Africa and South Asia. In the twentieth century, the public good was usually understood as encompassing a public of people who, like the Australians, had a claim on the resources of the planet. In recent decades, the public good has become synonymous with environmental protection. And yet the contemporary public good – which is to say, the contemporary public space – is an uneven thing, made manifest in unequal amounts of resource wealth and power. What is being made is neither publicly desirable nor publicly accessible. The contemporary public good is made manifest in the social world in which it takes form, but it is also manifest in conditions of unequal material abundance and social complexity. To some extent, this is natural: the rise of information technologies, which makes it easier to access information than ever before, makes it easier than ever to consume it, and makes it easier to become culturally embedded in social systems of exchange – making it necessary that the social bearer of resources have access to the social means to consume them. But to others</text>
    <text>machine learning experiments, such as those by Google DeepMind or Facebook DeepMind, that have revolutionised not only the way in which we think about science and technology, but also the way in which we interact with them.And yet the question of what constitutes a 'public' and a 'public good' – what counts as a 'public domain' and what is deemed a 'public space' – remains unsettled. What is considered a public space may be privately owned; a 'public good' may be owned by a private company. In the early twentieth century, for example, the public domain was defined as the land in which were vested in the Ukrainian Tatyugu people the right to a geographical extent corresponding with that of Australia; and in the twentieth century, the public good was defined as that part of the global commons which the West claimed as its own. But in recent decades, the public domain has expanded to include objects brought out of storage or publicly available through natural resources, including much of east Africa and the Middle East. And as technological sophistication has advanced, the collective storage of images and data has become a global infrastructure. Public good, in this view, includes both images and data – images and data which are always subject to the whims of market forces and the logic of geopolitics.In recent decades, new technologies of production have made it possible to undermine the image of human labour in favour of images of machines doing the heavy lifting of producing things across vast distances. But even as images and images of production replace the image of labour, they also often reduce labour to something approximating the condition of homoeopathy: the routine manual labour of farmers, factory workers, artists and architects everywhere rendered serviceable by machines. The problem with this image, of course, is that labour remains a scarce commodity – labour today is produced at increasingly infrastructuralist and technocratic levels across all sectors of the global economy. And labour is labour, even when performed by humans. A call to arms</text>
    <text>machine learning experiments, such as those by Google DeepMind or Facebook DeepMind, that have revolutionised not only the way in which we think about science and technology, but also the way in which we interact with them.And yet the question of what constitutes a 'public' and a 'public good' – what counts as a 'public domain' and what is deemed a 'public space' – remains unsettled. What is considered a public space may be privately owned; a 'public good' may be owned by a private company. In the early twentieth century, for example, the public domain was defined as the land in which were vested in the Ukrainian Tatyugu people the right to a geographical extent corresponding with that of Australia; and in the twentieth century, the public good was defined as that part of the global commons which the West claimed as its own. But in recent decades, the public domain has expanded to include objects brought out of storage or publicly available through natural resources, including much of east Africa and southern Asia. Public goods, which are to some extent commodities, are now often considered the best investments for a person to make; and in recent years, the biggest and most visible benefactors of this transition have been corporations. As a result, it is now commonplace to find public good maps on the company's website, alongside descriptions of the 'performative value of new ideas' and the like.I find the contemporary appropriation of public space puzzling. It can seem as if our urban infrastructures have become ensconced in machines, overrun with data-collecting apparatuses, andpopulated with sensors that monitor everything around them, recording everything from traffic patterns to the temperature of buildings to detect crime. Yet the twentieth century was a particularly dynamic one for urban spaces – and not just because of the economic crisis that followed the end of the gold standard in the wake of the second world war. The reason why cities have become so prevalent in our age is no longer</text>
    <text>machine learning experiments, such as those by Google DeepMind or Facebook DeepMind, that have revolutionised not only the way in which we think about science and technology, but also the way in which we interact with them.And yet the question of what constitutes a 'public' and a 'public good' – what counts as a 'public domain' and what is deemed a 'public space' – remains unsettled. What is considered a public space may be privately owned; a 'public good' may be owned by a private company. In the early twentieth century, for example, the public domain was defined as the land in which were vested in the Ukrainian Tatyugu people the right to a geographical extent corresponding with that of Australia; and in the twentieth century, the public good was defined as that part of the global commons which the West claimed as its own. But in recent decades, the public domain has expanded to include objects brought out of storage or publicly available through natural resources, including much of east Africa and the Middle East. Artworks now routinely grace display cases in major museums and art galleries around the world, but their historic places of exhibition remain largely untouched. How to Read an ArtworkNowadays, most artists would deny that their practices are influenced by a collective 'public good' – though some acknowledge that they are. But once a work has been placed in a museum or a gallery, it is almost always at odds with the public good; constantly challenged as to its rightful place in the landscape, it is routinely appropriated by those who cannot be trusted with the things entrusted to them. And now, increasingly, many artists have their works placed on websites, where they are frequently met with hostility and even abuse. Most websites now carry information about the work – facts or just plain facts – that are often exaggerated or fabricated, and which often conflict with established artistic consensus. So how to read an artwork now? I’m told that many artists have an instinctive but misplaced sympathy for</text>
    <text>machine learning experiments, such as those by Google DeepMind or Facebook DeepMind, that have revolutionised not only the way in which we think about science and technology, but also the way in which we interact with them.And yet the question of what constitutes a 'public' and a 'public good' – what counts as a 'public domain' and what is deemed a 'public space' – remains unsettled. What is considered a public space may be privately owned; a 'public good' may be owned by a private company. In the early twentieth century, for example, the public domain was defined as the land in which were vested in the Ukrainian Tatyugu people the right to a geographical extent corresponding with that of Australia; and in the twentieth century, the public good was defined as that part of the global commons which the West claimed as its own. But in recent decades, the public domain has expanded to include objects brought out of storage or publicly available through natural resources, including much of east Africa and South Asia. Any urban space is now considered private property. Urban growth has become a science, and the public good a status symbol. As a result, the public good is often misconstrued as being available only to a privileged few.  And so the public space – the global commons – is often misconstrued as being at odds with itself.In recent years, as global urbanisation has intensified and cities have become more numerous and diversified, so has the sense of common humanity waned. In New York and Shanghai, for example, the public realm is the domain of a few thousand mover-climbers; in other cities, it is confined to a few thousand wealthy individuals. And yet, in these cities and seaside resorts, the public realm is always a sought-after commodity. As new rich individuals build ever larger mansions on the rubble of the old, old, rich and poor, the public realm becomes ever more ephemeral and transient, like</text>
    <text>machine learning experiments, such as those by Google DeepMind or Facebook DeepMind, that have revolutionised not only the way in which we think about science and technology, but also the way in which we interact with them.And yet the question of what constitutes a 'public' and a 'public good' – what counts as a 'public domain' and what is deemed a 'public space' – remains unsettled. What is considered a public space may be privately owned; a 'public good' may be owned by a private company. In the early twentieth century, for example, the public domain was defined as the land in which were vested in the Ukrainian Tatyugu people the right to a geographical extent corresponding with that of Australia; and in the twentieth century, the public good was defined as that part of the global commons which the West claimed as its own. But in recent decades, the public domain has expanded to include objects brought out of storage or publicly available through natural resources, including much of east Africa and the Middle East. Public goods are objects with physical characteristics common to all citizens of a given community: fire, roads, healthcare, education, employment, and so forth. Public goods, by their nature, are objects universally accessible to all citizens. But in the twentieth century, the public good became something quite distinct from what it is now: the language of the public good, including the rights conferred by the public good, became the object of a capitalist logic of global accumulation. In other words, object categories are not static, unchanging concepts; they undergo change as capitalist social relations continue to be developed.  	In a capitalist social formation, the public good is something that can be produced and utilised in ways independent of any social agency. But in a society of the public good, the public good becomes something that is neither produced nor utilised in such a way as to be shared, exchanged, or shared in any meaningful way, beyond the formal exchange of things publicly – which</text>
  </text>
  <text>
    machine learning experiments in which animals are cued by their environment with rewards or punishments, usually based on historical trends or cultural values. For example, in ‘How Animals Sense Time’, Madeleine L. Nadworny, a Ph.D. candidate in Communication Studies and Film Studies at McGill University, Canada, investigates how animals, including humans, can understand temporal relations. She examines how animals can be trained to associate certain temporal patterns with specific behaviours or ‘behaviour profiles’ that can then be used to predict future events such as social behaviour or political affiliations. In a related vein, in a paper titled ‘An Introduction to Machine Learning and ‘Data Mining for Event Data extraction and Contextual Transformation’, a research fellow in the department of Information and Society, Prof. Xing Wang, an expert in artificial intelligence and computer systems, examines the various types of data mining and contextual principles that can be applied to data. He then proposes a set of principles that guide
    <text>machine learning experiments in which animals are cued by their environment with rewards or punishments, usually based on historical trends or cultural values. For example, in ‘How Animals Sense Time’, Madeleine L. Nadworny, a Ph.D. candidate in Communication Studies and Film Studies at McGill University, Canada, investigates how animals, including humans, can understand temporal relations. She examines how animals can be trained to associate certain temporal patterns with specific behaviours or ‘behaviour profiles’ that can then be used to predict future events such as social behaviour or political affiliations. In a related vein, in a paper titled ‘An Introduction to Machine Learning and ‘Data Mining for Event Data extraction and Contextual Transformation’, a research fellow in the department of Information and Society, Prof. Xing Wang, an expert in artificial intelligence and computer systems, examines the various types of data mining and contextual principles that can be applied to data. He then proposes a set of principles that guide the implementation of data mining and data augmentation techniques.  Boundary Conditions and the Never-Real Boundary layer  A more fundamental understanding of ‘the never-real boundary layer’ – the layer of language, cultural practices and physical and virtual media that comprise the social – is required to truly grasp the true essence of what is happening to us and to come to grips with the limits to what technology may achieve. This understanding will fundamentally alter our conceptions of technological singularity and the kind of social agency with which we are all engaged.  The text Boundary Constraints: On Immersive Interaction (MIT/Open Humanities Press, 2015) provides a detailed exploration of the layers of this increasingly critical social-technical boundary layer. It is based on the talk delivered by Prof. Xing Wang at the inaugural conference of the MIT Social and Cultural Studies Program in October 2015. Boundary Constraints examines the ways in which technologies – both human and nonhuman – can be used</text>
    <text>machine learning experiments in which animals are cued by their environment with rewards or punishments, usually based on historical trends or cultural values. For example, in ‘How Animals Sense Time’, Madeleine L. Nadworny, a Ph.D. candidate in Communication Studies and Film Studies at McGill University, Canada, investigates how animals, including humans, can understand temporal relations. She examines how animals can be trained to associate certain temporal patterns with specific behaviours or ‘behaviour profiles’ that can then be used to predict future events such as social behaviour or political affiliations. In a related vein, in a paper titled ‘An Introduction to Machine Learning and ‘Data Mining for Event Data extraction and Contextual Transformation’, a research fellow in the department of Information and Society, Prof. Xing Wang, an expert in artificial intelligence and computer systems, examines the various types of data mining and contextual principles that can be applied to data. He then proposes a set of principles that guide the development of data-driven event-processing systems that understand not only temporal relations, but also the ‘infrastructure’ and ‘content of offline events’.10 While the general principles of machine learning are sound and can be applied to data, applying these principles to data extraction and extraction of context is much more problematic. Machine learning algorithms face numerous obstacles in their path to extracting meaningful data from obscure or unrepresentative data sets, but they do have one advantage over other data extraction methods: they are inherently flexible and able to learn. This is due to the fact that data is typically classified as ‘representational’ (i.e., it has been labelled with a name and/or meaning), whereas other types of data are not. In other words, the more data is labelled, the more it is possible that the algorithms can change and exploit the data. However, this flexibility comes with a cost. If the underlying assumptions of the data mining process are not</text>
    <text>machine learning experiments in which animals are cued by their environment with rewards or punishments, usually based on historical trends or cultural values. For example, in ‘How Animals Sense Time’, Madeleine L. Nadworny, a Ph.D. candidate in Communication Studies and Film Studies at McGill University, Canada, investigates how animals, including humans, can understand temporal relations. She examines how animals can be trained to associate certain temporal patterns with specific behaviours or ‘behaviour profiles’ that can then be used to predict future events such as social behaviour or political affiliations. In a related vein, in a paper titled ‘An Introduction to Machine Learning and ‘Data Mining for Event Data extraction and Contextual Transformation’, a research fellow in the department of Information and Society, Prof. Xing Wang, an expert in artificial intelligence and computer systems, examines the various types of data mining and contextual principles that can be applied to data. He then proposes a set of principles that guide the development of future critical infrastructure in a way that is ecologically sustainable and socially useful. Download this article as PDFMeehan CristMeehan Crist is an artist. Her work has been exhibited internationally including the Venice Biennale, Taiwan, where she was chosen as one of twelve artists for the Taiwan Pavilion, as well as Taiwan’s Pavilion at the Venice Biennale. She received a BA in Visual Cultures from the University of Sydney, and an MA in Cultural Studies from the University of Leicester, England. Her work has also been commissioned by the New Art Gallery, Manchester, UK; the Whitney Museum of American Art, New York, USA; and the Royal Standard, London, UK. She is a member of the Advisory Board of the New Art Gallery and is a founding Director of the Royal Standard.She is the art and communications officer for the Helsinki 21st Biennale, which is the main international exhibition of the cycle. She has also been part of</text>
    <text>machine learning experiments in which animals are cued by their environment with rewards or punishments, usually based on historical trends or cultural values. For example, in ‘How Animals Sense Time’, Madeleine L. Nadworny, a Ph.D. candidate in Communication Studies and Film Studies at McGill University, Canada, investigates how animals, including humans, can understand temporal relations. She examines how animals can be trained to associate certain temporal patterns with specific behaviours or ‘behaviour profiles’ that can then be used to predict future events such as social behaviour or political affiliations. In a related vein, in a paper titled ‘An Introduction to Machine Learning and ‘Data Mining for Event Data extraction and Contextual Transformation’, a research fellow in the department of Information and Society, Prof. Xing Wang, an expert in artificial intelligence and computer systems, examines the various types of data mining and contextual principles that can be applied to data. He then proposes a set of principles that guide the development of data-intensive event data extraction and processing systems that employ reinforcement learning and AI to produce ever more accurate temporal data extraction tools.  In a larger perspective, in the current issue of Science Transactional Behaviour, Prof. Wang and his co-workers, led by Dr. Lili Xia, examine the intersection of biology, cultural theory, and geopolitics in the pursuit of energy resources. They also consider the potential of biopolitics in the age of climate change. The two papers in this issue are a new twist on a longstanding theme: how species come to inhabit spaces. In biology, the human species has a remarkable capacity for spatial organization – capable of self-organization across vast distances – and the capacity to establish and maintain distinct species. However, the capacity to establish and maintain distinct species is a post-human capacity. So what can be done with the spatiotemporal variety of species? The answer, as always, is space.  The current publication of</text>
    <text>machine learning experiments in which animals are cued by their environment with rewards or punishments, usually based on historical trends or cultural values. For example, in ‘How Animals Sense Time’, Madeleine L. Nadworny, a Ph.D. candidate in Communication Studies and Film Studies at McGill University, Canada, investigates how animals, including humans, can understand temporal relations. She examines how animals can be trained to associate certain temporal patterns with specific behaviours or ‘behaviour profiles’ that can then be used to predict future events such as social behaviour or political affiliations. In a related vein, in a paper titled ‘An Introduction to Machine Learning and ‘Data Mining for Event Data extraction and Contextual Transformation’, a research fellow in the department of Information and Society, Prof. Xing Wang, an expert in artificial intelligence and computer systems, examines the various types of data mining and contextual principles that can be applied to data. He then proposes a set of principles that guide the development of data-intensive event data extraction and processing systems. These systems, he suggests, should leverage ‘deep learning’ – the branch of artificial intelligence that involves the use of pattern-recognition and reinforcement learning algorithms – to identify and classify data, data structures and data streams, and allow the extraction of event data relevant to social or political goals.  In this regard, he draws on work that had been done on elephants by his fellow researchers at Edinburgh University. For this work, the researchers used an algorithm developed at Edinburgh's Computer Laboratory, using materials and protocols that mimic the behaviour of the animal, in this case, humans. The algorithm is able to recognise patterns in the elephant’s behaviour that can then be adjusted in ways that replicate the patterns detected. For example, the researchers could expose an elephant to a particular event in a particular sequence or series of sequences and then adjust the behaviour of an elephant in response to that event. Alternatively, the researchers could instruct a network</text>
    <text>machine learning experiments in which animals are cued by their environment with rewards or punishments, usually based on historical trends or cultural values. For example, in ‘How Animals Sense Time’, Madeleine L. Nadworny, a Ph.D. candidate in Communication Studies and Film Studies at McGill University, Canada, investigates how animals, including humans, can understand temporal relations. She examines how animals can be trained to associate certain temporal patterns with specific behaviours or ‘behaviour profiles’ that can then be used to predict future events such as social behaviour or political affiliations. In a related vein, in a paper titled ‘An Introduction to Machine Learning and ‘Data Mining for Event Data extraction and Contextual Transformation’, a research fellow in the department of Information and Society, Prof. Xing Wang, an expert in artificial intelligence and computer systems, examines the various types of data mining and contextual principles that can be applied to data. He then proposes a set of principles that guide the development of data-driven event data extraction and contextual technologies. In a third article in the journal ‘Dispatches from the SPARK Lab’, Prof. Wang discusses the ongoing work of the SPARK Lab, an interdisciplinary research centre that has been developing techniques for the extraction of social and environmental context through an ‘instrumental space-time model’. The SPARK Lab’s work is informed by a 'new perspective on human behaviour’. It involves not only computer programming but also biology, physics, geology and social science, and involves the application of mathematics and statistics to the interpretation of environmental and social conditions. The journal Dispatches from the SPARK Lab contains an extensive survey of the lab’s developments, from 2010–2015.2 	[i] William H. Richardson, ‘On the use of resources in urban planning’, in  	Urban Studies, Vol. 34, No. 1</text>
    <text>machine learning experiments in which animals are cued by their environment with rewards or punishments, usually based on historical trends or cultural values. For example, in ‘How Animals Sense Time’, Madeleine L. Nadworny, a Ph.D. candidate in Communication Studies and Film Studies at McGill University, Canada, investigates how animals, including humans, can understand temporal relations. She examines how animals can be trained to associate certain temporal patterns with specific behaviours or ‘behaviour profiles’ that can then be used to predict future events such as social behaviour or political affiliations. In a related vein, in a paper titled ‘An Introduction to Machine Learning and ‘Data Mining for Event Data extraction and Contextual Transformation’, a research fellow in the department of Information and Society, Prof. Xing Wang, an expert in artificial intelligence and computer systems, examines the various types of data mining and contextual principles that can be applied to data. He then proposes a set of principles that guide the development of data-intensive event data extraction tools and pipeline technologies for the future workforce. As an introduction to machine learning and data mining, Prof. Wang’s group examined three broad categories of data: imagery, audio and text. Image and audio signals comprise a growing set of data that are frequently combined to produce graphic reports summarizing events or phenomena. They include but are not limited to ‘live stream’ videos, live streaming video, images, and text streams. Image and audio signals are narrative in nature and rely on a protagonist or narrative point of entry to establish the narrative of the narrative. The notion of nodes or nodes of historical interest emerges in part as a way to categorize and categorize this growing set of data. The recognition that humans, not just data, engage in narrative emerges as a way to create tools and systems that monitor, collect and map narrative-critical events. The recognition that humans make narratives comes as a consequence of the historical and social precedents</text>
    <text>machine learning experiments in which animals are cued by their environment with rewards or punishments, usually based on historical trends or cultural values. For example, in ‘How Animals Sense Time’, Madeleine L. Nadworny, a Ph.D. candidate in Communication Studies and Film Studies at McGill University, Canada, investigates how animals, including humans, can understand temporal relations. She examines how animals can be trained to associate certain temporal patterns with specific behaviours or ‘behaviour profiles’ that can then be used to predict future events such as social behaviour or political affiliations. In a related vein, in a paper titled ‘An Introduction to Machine Learning and ‘Data Mining for Event Data extraction and Contextual Transformation’, a research fellow in the department of Information and Society, Prof. Xing Wang, an expert in artificial intelligence and computer systems, examines the various types of data mining and contextual principles that can be applied to data. He then proposes a set of principles that guide the design of data-processing systems that employ machine learning and/or data mining techniques. These principles, he asserts, can be applied to data sets anywhere in the world and foster the deployment of new kinds of social and governance structures.  The study of animal cognition seems particularly relevant because of the increasing human-animal interface: the transfer of data, audio and video, images, texts and more. Machine learning is the science of algorithms that study data to produce outputs that are specific to the task at hand, and can thus be used to automate parts of the interface that monitor, collect and interpret data, provide services and much more. The application of animal cognition to the computer interface is just one manifestation of a much wider, more nuanced subject that humans and other creatures engage with every day. Human-computer interaction is an ongoing process that has occurred naturally over the course of human civilisation, and it is only recently that humans have begun to actively seek and engage with other creatures on the internet. This</text>
    <text>machine learning experiments in which animals are cued by their environment with rewards or punishments, usually based on historical trends or cultural values. For example, in ‘How Animals Sense Time’, Madeleine L. Nadworny, a Ph.D. candidate in Communication Studies and Film Studies at McGill University, Canada, investigates how animals, including humans, can understand temporal relations. She examines how animals can be trained to associate certain temporal patterns with specific behaviours or ‘behaviour profiles’ that can then be used to predict future events such as social behaviour or political affiliations. In a related vein, in a paper titled ‘An Introduction to Machine Learning and ‘Data Mining for Event Data extraction and Contextual Transformation’, a research fellow in the department of Information and Society, Prof. Xing Wang, an expert in artificial intelligence and computer systems, examines the various types of data mining and contextual principles that can be applied to data. He then proposes a set of principles that guide the development of future critical infrastructure, such as electric grids and water supplies, through which data can be selectively accessed and manipulated. These principles, he says, ‘underlie the built environment, ‘including infrastructure’, and apply not only to critical infrastructure, but also to non-critical infrastructure, such as transport and communications infrastructure.  	[1] https://www.ncbi.nlm.nih.gov/pubmed/17356926?dopt=AbstractPlus2052&amp;ordinalpos=1&amp;itoolaid=yes&amp;facetate=preview&amp;sequence=1975&amp;filetype=matrix&amp;file=AbstractPlus2052&amp;ordinalpos=1&amp;typeahead=1&amp;keywords=animal+learning+system&amp;frameborder=0&amp;keywordslocked=0&amp;previewmode=1&amp;previeworder=0&amp;function=0&amp;manual=1&amp;index=0&amp;weightman</text>
    <text>machine learning experiments in which animals are cued by their environment with rewards or punishments, usually based on historical trends or cultural values. For example, in ‘How Animals Sense Time’, Madeleine L. Nadworny, a Ph.D. candidate in Communication Studies and Film Studies at McGill University, Canada, investigates how animals, including humans, can understand temporal relations. She examines how animals can be trained to associate certain temporal patterns with specific behaviours or ‘behaviour profiles’ that can then be used to predict future events such as social behaviour or political affiliations. In a related vein, in a paper titled ‘An Introduction to Machine Learning and ‘Data Mining for Event Data extraction and Contextual Transformation’, a research fellow in the department of Information and Society, Prof. Xing Wang, an expert in artificial intelligence and computer systems, examines the various types of data mining and contextual principles that can be applied to data. He then proposes a set of principles that guide the development of future generations of data miners and data maximisers. The document is a ‘back door’ that allows for the efficient and unrestricted exploitation of future data, and is framed by a ‘deep learning’ framework that exploits the fact that human beings have augmentative medicine technologies that can detect and address critical issues of healthcare. Wang’s document is a ‘road map’ that lays out in great detail the ways in which technology can be used to predict the future, including how such technologies might intersect with traditional religion and cultural values. The back door into the future is a ‘sweetener’ that optimises the possible futures through the pre-emptive deployment of technological means available at the time.  A second document, ‘The Fifth Element: A User’s Guide to the Emergence of new Technologies’, is a user’s guide that traces the processes of technological creativity generated by user research. It draws upon the inter</text>
  </text>
  <text>
    possibility of developing an experimental system for monitoring the health of aquatic organisms through in situ bio-monitoring). The idea was to monitor the health of certain kinds of planktonic creatures through in situ biological surveillance. The system was to be modular and could be adapted to monitor a great number of organisms at once. The main idea was that of a human microbiome, which is closely related to the human microbiome but differs from the gut microbial community in that it maintains a constant state of change in its constituents. A modular ‘greening’ process might be applied to the morphology of the aquatic microbe in order to adapt it to different water parameters, such as salt concentration, temperature, pressure, etc. The module might be equipped with sensors that collect data on the organism’s properties, for example, the type of plankton it is made up of, the amount of moisture it requires to live, and the behaviour of its microbial community. The data collected might then be used to modify the module’s behaviour in
    <text>possibility of developing an experimental system for monitoring the health of aquatic organisms through in situ bio-monitoring). The idea was to monitor the health of certain kinds of planktonic creatures through in situ biological surveillance. The system was to be modular and could be adapted to monitor a great number of organisms at once. The main idea was that of a human microbiome, which is closely related to the human microbiome but differs from the gut microbial community in that it maintains a constant state of change in its constituents. A modular ‘greening’ process might be applied to the morphology of the aquatic microbe in order to adapt it to different water parameters, such as salt concentration, temperature, pressure, etc. The module might be equipped with sensors that collect data on the organism’s properties, for example, the type of plankton it is made up of, the amount of moisture it requires to live, and the behaviour of its microbial community. The data collected might then be used to modify the module’s behaviour in such a way as to optimise its performance in the environment. A modular ecological monitoring system might be adapted to meet the demands of a bi-national or international scientific commission, for example, or might be used by a non-governmental organisation tasked with monitoring the health of the oceanic and coastal ecosystems.Such systems have been proposed for the monitoring of large populations of organisms at a time, and in many cases, many kilometres away. However, in the quest for greater understanding of how to best work with these organisms, it is crucial to understand how they are adapted to particular environmental conditions. There are of course many other ways to think about adapting biological systems to different conditions – for example, plants and animals would not necessarily respond to different nutrient inputs – but there is a need to understand the underlying mechanisms that enable the adaptations to be made. A growing body of research now suggests that environmental conditions can influence biological systems in interesting or even beneficial ways. For example, the increasing use of biochar – a</text>
    <text>possibility of developing an experimental system for monitoring the health of aquatic organisms through in situ bio-monitoring). The idea was to monitor the health of certain kinds of planktonic creatures through in situ biological surveillance. The system was to be modular and could be adapted to monitor a great number of organisms at once. The main idea was that of a human microbiome, which is closely related to the human microbiome but differs from the gut microbial community in that it maintains a constant state of change in its constituents. A modular ‘greening’ process might be applied to the morphology of the aquatic microbe in order to adapt it to different water parameters, such as salt concentration, temperature, pressure, etc. The module might be equipped with sensors that collect data on the organism’s properties, for example, the type of plankton it is made up of, the amount of moisture it requires to live, and the behaviour of its microbial community. The data collected might then be used to modify the module’s behaviour in such a way as to optimize the organism’s fitness in terms of photosynthesis and thus the host’s physiological response to changing seawater levels. Such a system, according to one proposal, would render the functions of existing biological monitoring tools redundant. Rather than gathering and sharing data, the sensors would monitor and control the health of organisms through their own internal logic.[6]Another proposal proposed by researchers at the Massachusetts Institute of Technology focuses on plants that secrete substances that attract or repel other organisms. The ideas are similar to those suggested by others, but focus on plants rather than animals or plants-like microorganisms. Plant-based bio-medications that target certain kinds of algae and fungi as potential agents in treating disease vectors are already widely used and do not require any new scientific understanding. Nevertheless, understanding of how the plant community organises itself to deal with different kinds of contamination is vital if we are to develop effective, sustainable biological defences. Researchers from the Massachusetts Institute of Technology</text>
    <text>possibility of developing an experimental system for monitoring the health of aquatic organisms through in situ bio-monitoring). The idea was to monitor the health of certain kinds of planktonic creatures through in situ biological surveillance. The system was to be modular and could be adapted to monitor a great number of organisms at once. The main idea was that of a human microbiome, which is closely related to the human microbiome but differs from the gut microbial community in that it maintains a constant state of change in its constituents. A modular ‘greening’ process might be applied to the morphology of the aquatic microbe in order to adapt it to different water parameters, such as salt concentration, temperature, pressure, etc. The module might be equipped with sensors that collect data on the organism’s properties, for example, the type of plankton it is made up of, the amount of moisture it requires to live, and the behaviour of its microbial community. The data collected might then be used to modify the module’s behaviour in such a way as to optimise its biological function.The main obstacle in the way of such a system developing in Canada was undoubtedly the presence of the word microbiome in the title of the journal. Although the editors presumably did not have in mind a lay audience, it is quite conceivable that many readers did not know what the word meant. The journal, while not a medical publication, nevertheless presents a ‘case definition’ of what the reader means when he or she writes about the microbiome. The editors presumably did not intend that the word microbiome in the title of a scientific journal refer to a specific set of organisms. The object of research in a scientific journal is a living thing, capable of changing over time and undergoing change in its own biological processes. Hence the word ‘living’ has a ‘life’ in its semantic sense. The editors presumably did not intend that the word living in the title of a scientific journal refer to living organisms. Instead, the living organism referred to</text>
    <text>possibility of developing an experimental system for monitoring the health of aquatic organisms through in situ bio-monitoring). The idea was to monitor the health of certain kinds of planktonic creatures through in situ biological surveillance. The system was to be modular and could be adapted to monitor a great number of organisms at once. The main idea was that of a human microbiome, which is closely related to the human microbiome but differs from the gut microbial community in that it maintains a constant state of change in its constituents. A modular ‘greening’ process might be applied to the morphology of the aquatic microbe in order to adapt it to different water parameters, such as salt concentration, temperature, pressure, etc. The module might be equipped with sensors that collect data on the organism’s properties, for example, the type of plankton it is made up of, the amount of moisture it requires to live, and the behaviour of its microbial community. The data collected might then be used to modify the module’s behaviour in such a way as to optimise the organism’s survival, by mobilising the module’s internal resources and paying particular attention to the types of plankton it is able to digest. This last modification might include the incorporation of new microscopic plankton’s living in the water column, which in turn might be adapted to the microbe’s metabolism. Such a change in the module’s morphology might, however, lead to undesirable effects, because the types of life form it is able to recognise as harmful – micro-organisms – will then have access to the module’s internal resources and might seek to colonise the aquatic medium through what Carolee St-Amand describes as ‘translocation’. A bacterial population growing in proximity to a shoreline organism might, for example, seek to establish a nursery around the newly emerged plankton, while similarly situated organisms might seek to colonise the medium through what St-Amand calls ‘</text>
    <text>possibility of developing an experimental system for monitoring the health of aquatic organisms through in situ bio-monitoring). The idea was to monitor the health of certain kinds of planktonic creatures through in situ biological surveillance. The system was to be modular and could be adapted to monitor a great number of organisms at once. The main idea was that of a human microbiome, which is closely related to the human microbiome but differs from the gut microbial community in that it maintains a constant state of change in its constituents. A modular ‘greening’ process might be applied to the morphology of the aquatic microbe in order to adapt it to different water parameters, such as salt concentration, temperature, pressure, etc. The module might be equipped with sensors that collect data on the organism’s properties, for example, the type of plankton it is made up of, the amount of moisture it requires to live, and the behaviour of its microbial community. The data collected might then be used to modify the module’s behaviour in such a way as to optimise its utilization of available water resources. Such a system, according to one interpretation of the system‑theoretic model, represents a new mode of economic organization of the world’s aquatic resources. The adoption of this idea as a new way of looking at the world’s aquatic resources marks a shift in how we conceive of the relationship between human society and the natural world. It comes as no surprise, then, to find such a thing as ecology now a prominent theme of our conference. Its emergence comes as no surprise to marine biologists, who have been developing ever more accurate and sensitive monitoring devices for years now. But it does seem strange to us, the way that such a thing as ecology is now routinely treated as though it were an urban myth perpetuated by vested interests on one side and widely ridiculed as a hoax by the other. How odd it is that a module as basic as the one we’ve just described might one day be able</text>
    <text>possibility of developing an experimental system for monitoring the health of aquatic organisms through in situ bio-monitoring). The idea was to monitor the health of certain kinds of planktonic creatures through in situ biological surveillance. The system was to be modular and could be adapted to monitor a great number of organisms at once. The main idea was that of a human microbiome, which is closely related to the human microbiome but differs from the gut microbial community in that it maintains a constant state of change in its constituents. A modular ‘greening’ process might be applied to the morphology of the aquatic microbe in order to adapt it to different water parameters, such as salt concentration, temperature, pressure, etc. The module might be equipped with sensors that collect data on the organism’s properties, for example, the type of plankton it is made up of, the amount of moisture it requires to live, and the behaviour of its microbial community. The data collected might then be used to modify the module’s behaviour in such a way as to optimize the organism’s growth and survival – either positively or negatively. For instance, a module that depends on measurements from its neighbours might be modified in such a way as to seek out and consume more planktonic creatures in the vicinity, in order to maximise the organism’s genetic diversity. Such behaviour might be viewed as a response to changing water conditions in the vicinity – a situation where the module is exposed to a wide range of potential environmental influences. Such possibilities run contrary to prevailing wisdom regarding the relationship of ecology and engineering, which dictates that a hub must be able to respond to changing circumstances.  	Such broad-based modification is desirable not only for the sake of the health of its neighbours but also for the sake of future research into the mechanisms that engineer microbial communities to perform particular actions. For instance, the wide-ranging research projects that might result from the open-source biology that is becoming more and more prominent might one day be possible thanks</text>
    <text>possibility of developing an experimental system for monitoring the health of aquatic organisms through in situ bio-monitoring). The idea was to monitor the health of certain kinds of planktonic creatures through in situ biological surveillance. The system was to be modular and could be adapted to monitor a great number of organisms at once. The main idea was that of a human microbiome, which is closely related to the human microbiome but differs from the gut microbial community in that it maintains a constant state of change in its constituents. A modular ‘greening’ process might be applied to the morphology of the aquatic microbe in order to adapt it to different water parameters, such as salt concentration, temperature, pressure, etc. The module might be equipped with sensors that collect data on the organism’s properties, for example, the type of plankton it is made up of, the amount of moisture it requires to live, and the behaviour of its microbial community. The data collected might then be used to modify the module’s behaviour in such a way as to optimize its performance. This might alter the module’s morphology in response to changes in the surrounding water, or the physical characteristics of the water – for example, the density of water molecules per gram of particulate matter (the particulate matter is the building material of the tank). The concept of modules in general, and this specific example in particular, is part of a broader movement that is reshaping how in situ biological surveillance is performed across a globalised world. It is being done not by nations or international organisations, but is carried out by organisations that are loosely linked and operate in many locations. As a result, monitoring a large number of organisms at the same time is possible.  One example of a loosely coupled organisation is the human microbiome. Microbiomes are collections of living organisms that collectively refer to the historical relationships between human beings and the environment. They include bacteria, archaea, viruses and eukaryotic cells. Each of these groups has its own</text>
    <text>possibility of developing an experimental system for monitoring the health of aquatic organisms through in situ bio-monitoring). The idea was to monitor the health of certain kinds of planktonic creatures through in situ biological surveillance. The system was to be modular and could be adapted to monitor a great number of organisms at once. The main idea was that of a human microbiome, which is closely related to the human microbiome but differs from the gut microbial community in that it maintains a constant state of change in its constituents. A modular ‘greening’ process might be applied to the morphology of the aquatic microbe in order to adapt it to different water parameters, such as salt concentration, temperature, pressure, etc. The module might be equipped with sensors that collect data on the organism’s properties, for example, the type of plankton it is made up of, the amount of moisture it requires to live, and the behaviour of its microbial community. The data collected might then be used to modify the module’s behaviour in such a way as to optimise its ability to collect data, while minimising its impact on the environment.  One can envisage a similar approach for plants, in which the same set of rules governing how water and nutrients are distributed among the plant material might apply to the module level as well.  The main difference is that in the microbiome sense, organisms have attributes – names, sequences of tags, numbers and so forth – whereas modules have behaviours.  As I see it, the key modules here are those that directly engage the user in the production of their own biology: those that directly recognise and attach to one another, and those that indirectly recognise and attach to other things (packages, objects, data) and employ them to achieve a set of outcomes (behaviour) in and through the world around them. The remaining ­­–ersubjects are those that engage with what I call the dynamic aspect of the world around them (planet Earth and its inhabitants), and employ that to</text>
    <text>possibility of developing an experimental system for monitoring the health of aquatic organisms through in situ bio-monitoring). The idea was to monitor the health of certain kinds of planktonic creatures through in situ biological surveillance. The system was to be modular and could be adapted to monitor a great number of organisms at once. The main idea was that of a human microbiome, which is closely related to the human microbiome but differs from the gut microbial community in that it maintains a constant state of change in its constituents. A modular ‘greening’ process might be applied to the morphology of the aquatic microbe in order to adapt it to different water parameters, such as salt concentration, temperature, pressure, etc. The module might be equipped with sensors that collect data on the organism’s properties, for example, the type of plankton it is made up of, the amount of moisture it requires to live, and the behaviour of its microbial community. The data collected might then be used to modify the module’s behaviour in such a way as to optimise the organism’s chances of survival. In the case of a virus, a similar approach might be applied to the morphology of the aquatic microbe. A modular ‘Microbiome’ might allow the identification of different types of planktonic creatures in the water column, and the action of different types of planktonic creatures in the water column, within a very short timeframe. Thus, the modular nature of the system might enable rapid detection and response to changes in environmental conditions. In the case of a virus, rapid detection and rapid intervention might be applied to detect and respond to emerging infections. In the case of a Microbe, the modular nature of the system might enable the rapid establishment of resistance to existing strains of the virus. In the case of a creature with no apparent physical body, the modularity might allow containment of the organism’s genetic material. In either case, rapid action is taken against the threat through the introduction of a</text>
    <text>possibility of developing an experimental system for monitoring the health of aquatic organisms through in situ bio-monitoring). The idea was to monitor the health of certain kinds of planktonic creatures through in situ biological surveillance. The system was to be modular and could be adapted to monitor a great number of organisms at once. The main idea was that of a human microbiome, which is closely related to the human microbiome but differs from the gut microbial community in that it maintains a constant state of change in its constituents. A modular ‘greening’ process might be applied to the morphology of the aquatic microbe in order to adapt it to different water parameters, such as salt concentration, temperature, pressure, etc. The module might be equipped with sensors that collect data on the organism’s properties, for example, the type of plankton it is made up of, the amount of moisture it requires to live, and the behaviour of its microbial community. The data collected might then be used to modify the module’s behaviour in such a way as to optimize the organism’s survival. Such a system, however, would be nothing like what we have today – it would be a chimera, in the sense that it shares many of the same genes but different bacterial strains inhabit different parts of the body. The key is the application of the genetic information to the morphological parameters, which is what Darwin meant when he said that life begins ‘in the bosom of the mother’. Living things use hermetically to seal in energy and provide the best possible environment for their growth and development. But what happens when those parameters change – the climate, the salinity, the temperature – and that is when things get messy. If the shape of a thing is influenced by those surrounding it, such that it becomes more like a liquid or gas, then the system gets messy. And if that is the case, then the shape of the thing changes, and so does the function of the thing. If the shape</text>
  </text>
  <text>
    using machine learning techniques to identify patterns in text or audio data, and automatically detect similarities between pairs of characters,&quot; said Andrew Torchinsky, a Ph.D. candidate in Mechanical and Network Engineering at the University of Alberta, in a statement. &quot;This approach could potentially be applied to other types of data, such as photos or motion graphics. The research was presented at the Black-E’s Annual Meeting 2017.Download this article as PDFAndrew TorchinskyAndrew Torchinsky is an industrial and digital media designer. His research interests include the creation of social and interactive web environments; the creation of environmentally conscious user-generated content; and the interaction of data, algorithms, and media with human behaviour. His dissertation examines the intersection of media and social theory, and the potential of social media to address pressing social and political concerns. He is an adjunct professor at the University of Alberta, where his research interests include the social sciences, media and architecture programs. His dissertation examines the social construction of language through an analysis of the practices
    <text>using machine learning techniques to identify patterns in text or audio data, and automatically detect similarities between pairs of characters,&quot; said Andrew Torchinsky, a Ph.D. candidate in Mechanical and Network Engineering at the University of Alberta, in a statement. &quot;This approach could potentially be applied to other types of data, such as photos or motion graphics. The research was presented at the Black-E’s Annual Meeting 2017.Download this article as PDFAndrew TorchinskyAndrew Torchinsky is an industrial and digital media designer. His research interests include the creation of social and interactive web environments; the creation of environmentally conscious user-generated content; and the interaction of data, algorithms, and media with human behaviour. His dissertation examines the intersection of media and social theory, and the potential of social media to address pressing social and political concerns. He is an adjunct professor at the University of Alberta, where his research interests include the social sciences, media and architecture programs. His dissertation examines the social construction of language through an analysis of the practices and ideologies of modern linguistic theory. In addition to researching and writing articles, he is a regular speaker and presenter of the digital media show ‘The Current Spot’ hosted by Jamison C. Ward and hosted by Aamer Madhani. He holds a Bachelor of Science in English Cultural Studies from the University of Alberta, and a Master’s degree in Media Studies from the University of Alberta’s System Design Lab.This article was originally published in ASN's 'The Current Digest' as part of ASN's Year End 2016 issue.Download this article as PDFAndrew Torchinsky and Alison GrossmanThis article is an abbreviated version of an article in The Current that originally appeared in The New York Times on July 24, 2016. It is an expanded and updated version of an article in The New York Review of Books on July 23, 2016 that originally appeared in The New York Times Magazine. It is intended for a wider audience.The Current Digest2017Image courtesy of The Current</text>
    <text>using machine learning techniques to identify patterns in text or audio data, and automatically detect similarities between pairs of characters,&quot; said Andrew Torchinsky, a Ph.D. candidate in Mechanical and Network Engineering at the University of Alberta, in a statement. &quot;This approach could potentially be applied to other types of data, such as photos or motion graphics. The research was presented at the Black-E’s Annual Meeting 2017.Download this article as PDFAndrew TorchinskyAndrew Torchinsky is an industrial and digital media designer. His research interests include the creation of social and interactive web environments; the creation of environmentally conscious user-generated content; and the interaction of data, algorithms, and media with human behaviour. His dissertation examines the intersection of media and social theory, and the potential of social media to address pressing social and political concerns. He is an adjunct professor at the University of Alberta, where his research interests include the social sciences, media and architecture programs. His dissertation examines the social construction of language through an analysis of the practices of pre-academic linguistics. He is the author of Systemics, or Human Language: The Modern Syntax (2017), and co-editor of the forthcoming book Systems, or Human Language: The Modern Syntax (2018). He is currently completing a two-year joint PhD and dissertation at the University of Alberta’s Mackenzie School of Public Policy, where his research interests include the social sciences, media and architecture programs. His dissertation examines the intersection of media and social theory, and the potential of social media to address pressing social and political concerns. He is the editor of the forthcoming book Systems, or Human Language: The Modern Syntax (2017), and co-editor of the forthcoming book Systems, or Human Language: The Modern Syntax (2018). Systems, or Human Language: The Modern SyntaxSystems, or Human Language, is the third book in Terryz Fineberg’s Systems book series, and the concluding book in the trilogy. Fine</text>
    <text>using machine learning techniques to identify patterns in text or audio data, and automatically detect similarities between pairs of characters,&quot; said Andrew Torchinsky, a Ph.D. candidate in Mechanical and Network Engineering at the University of Alberta, in a statement. &quot;This approach could potentially be applied to other types of data, such as photos or motion graphics. The research was presented at the Black-E’s Annual Meeting 2017.Download this article as PDFAndrew TorchinskyAndrew Torchinsky is an industrial and digital media designer. His research interests include the creation of social and interactive web environments; the creation of environmentally conscious user-generated content; and the interaction of data, algorithms, and media with human behaviour. His dissertation examines the intersection of media and social theory, and the potential of social media to address pressing social and political concerns. He is an adjunct professor at the University of Alberta, where his research interests include the social sciences, media and architecture programs. His dissertation examines the social construction of language through an analysis of the practices of community media, and the ways in which language is used as a commodity through which social relations are constructed. It examines the mechanisms of media reinforcement and their potential use to alter the social relations between individuals and groups. His dissertation examines the linguistic infrastructure of Canada and the United States, and the connections between the construction of political and linguistic systems through the production of media and how those systems are constructed. He is also the author of several books including the award-winning Beyond Media: Conversations on Media and Society (2015) and the recently released Beyond Media: Converging Concepts (2017).He is currently completing a PhD at the University of Alberta, where he is researching the histories of critical theory and media design in Canada. He is currently working with the Department of Cultural Studies at the University of Alberta, where his research interests include the history of Canadian language design and the production of media culture. He is currently completing his Masters of Public Policy at the University of Alberta, where his research interests include the social</text>
    <text>using machine learning techniques to identify patterns in text or audio data, and automatically detect similarities between pairs of characters,&quot; said Andrew Torchinsky, a Ph.D. candidate in Mechanical and Network Engineering at the University of Alberta, in a statement. &quot;This approach could potentially be applied to other types of data, such as photos or motion graphics. The research was presented at the Black-E’s Annual Meeting 2017.Download this article as PDFAndrew TorchinskyAndrew Torchinsky is an industrial and digital media designer. His research interests include the creation of social and interactive web environments; the creation of environmentally conscious user-generated content; and the interaction of data, algorithms, and media with human behaviour. His dissertation examines the intersection of media and social theory, and the potential of social media to address pressing social and political concerns. He is an adjunct professor at the University of Alberta, where his research interests include the social sciences, media and architecture programs. His dissertation examines the social construction of language through an analysis of the practices of early modern and early modern Middle-Eastern and North African linguists. He is the editor and publisher of Popular Linguistics: Language and Politics.Torchinsky is a founding partner of New Media Capital Group, a private investment firm with a global reach focused on the communications, media and infrastructure sectors. He is a consultant to the government of Canada and is the author of several books including the award-winning Beyond Media Land: Media and the Politics of Intangible Assets (2013), Media Contagions: Telling Stages from Postcolonial Times (2015) and the award-winning Media Critique: Canada's Media Milieu (2015). He has a Bachelor of Arts in English Cultural Studies from McGill University and a Master’s degree in cultural studies from the University of Alberta. He is the recipient of several awards and has held various other positions of power including: Founding Chair of the American Cultural Association, US Chairman of the Board of Trustees at the Museum of Modern Art,</text>
    <text>using machine learning techniques to identify patterns in text or audio data, and automatically detect similarities between pairs of characters,&quot; said Andrew Torchinsky, a Ph.D. candidate in Mechanical and Network Engineering at the University of Alberta, in a statement. &quot;This approach could potentially be applied to other types of data, such as photos or motion graphics. The research was presented at the Black-E’s Annual Meeting 2017.Download this article as PDFAndrew TorchinskyAndrew Torchinsky is an industrial and digital media designer. His research interests include the creation of social and interactive web environments; the creation of environmentally conscious user-generated content; and the interaction of data, algorithms, and media with human behaviour. His dissertation examines the intersection of media and social theory, and the potential of social media to address pressing social and political concerns. He is an adjunct professor at the University of Alberta, where his research interests include the social sciences, media and architecture programs. His dissertation examines the social construction of language through an analysis of the practices of colonial linguists, and the formation of contemporary languages. He is also the Editor of the online journal Perceptual Arboretum. He holds a Bachelor of Arts in English Cultural Studies from the University of Alberta, and a Master’s degree in Cultural Studies from the University of Alberta Libraries.What is at Stake in the Relationship between CADDPS and the Indigenous People?By Rebecca Carbone and Rebecca Stinebrigge-Larsen2 October 2017Stake Conference Call Rebecca Stinebrigge-Larsen and her husband, Professor Peter Stinebrigge-Larsen of the Department of Linguistics and Aesthetics at the Royal Roads College of Art, and Professor in the College of Languages and Cultures at the University of Alberta, are proposing a new kind of Indigenous education model for their son, Gavin. Gavin is going to be introduced to the language of the Inuit through stories told by the Inuit elders. These stories</text>
    <text>using machine learning techniques to identify patterns in text or audio data, and automatically detect similarities between pairs of characters,&quot; said Andrew Torchinsky, a Ph.D. candidate in Mechanical and Network Engineering at the University of Alberta, in a statement. &quot;This approach could potentially be applied to other types of data, such as photos or motion graphics. The research was presented at the Black-E’s Annual Meeting 2017.Download this article as PDFAndrew TorchinskyAndrew Torchinsky is an industrial and digital media designer. His research interests include the creation of social and interactive web environments; the creation of environmentally conscious user-generated content; and the interaction of data, algorithms, and media with human behaviour. His dissertation examines the intersection of media and social theory, and the potential of social media to address pressing social and political concerns. He is an adjunct professor at the University of Alberta, where his research interests include the social sciences, media and architecture programs. His dissertation examines the social construction of language through an analysis of the practices of historical and contemporary linguists. He is also the editor and publisher of the popular web publication Popular Library.In&lt;i&gt;[2] Popular Culture and the Digital Consumer (2005), John Urry and Neil Johnson state that ‘[T]he ubiquity of computerised text and image processing means that images no longer represent images in a strict physical or conceptual way’.[3] and that ‘[T]he explosion of digital images has democratised visual experience’.[4] Thus, the digital image means both abundance and abstraction, both of which can be understood as positive or negative. It is a digital image that is both image and data intensive; that is, it takes up more data space than the original image and media. This may enhance or detract from the immediacy of the experience of reading an image on a device like the iPhone, but it is the user’s experience of image and image processing that is enhanced. This is apparent in the</text>
    <text>using machine learning techniques to identify patterns in text or audio data, and automatically detect similarities between pairs of characters,&quot; said Andrew Torchinsky, a Ph.D. candidate in Mechanical and Network Engineering at the University of Alberta, in a statement. &quot;This approach could potentially be applied to other types of data, such as photos or motion graphics. The research was presented at the Black-E’s Annual Meeting 2017.Download this article as PDFAndrew TorchinskyAndrew Torchinsky is an industrial and digital media designer. His research interests include the creation of social and interactive web environments; the creation of environmentally conscious user-generated content; and the interaction of data, algorithms, and media with human behaviour. His dissertation examines the intersection of media and social theory, and the potential of social media to address pressing social and political concerns. He is an adjunct professor at the University of Alberta, where his research interests include the social sciences, media and architecture programs. His dissertation examines the social construction of language through an analysis of the practices of second language acquisition. He is also the editor-in-chief of the popular web site Skripal.com. His publications include the popular web site Skripal.com (2013), the peer-reviewed popular web journal Stages (2013), and the Journal of Media &amp; Culture (2015). He is co-editor of the forthcoming scholarly publication Media Research Wild West (2017).He has written for The New York Times, The Washington Post, The Atlantic and The Atlantic Monthly, among other publications. His work has been featured in Wired, The New Republic, The Hollywood Reporter, The New York Times Travel Guide, The New York Times Digital Briefing, The New York Times Best of LA, the New York Times Best of Boston, and various other publications. He has also written for The New York Times Digital Briefing, The New York Times Digital Shortlist, The New York Times Digital Living, and various other publications. He was a co-editor of the popular web publication Digital</text>
    <text>using machine learning techniques to identify patterns in text or audio data, and automatically detect similarities between pairs of characters,&quot; said Andrew Torchinsky, a Ph.D. candidate in Mechanical and Network Engineering at the University of Alberta, in a statement. &quot;This approach could potentially be applied to other types of data, such as photos or motion graphics. The research was presented at the Black-E’s Annual Meeting 2017.Download this article as PDFAndrew TorchinskyAndrew Torchinsky is an industrial and digital media designer. His research interests include the creation of social and interactive web environments; the creation of environmentally conscious user-generated content; and the interaction of data, algorithms, and media with human behaviour. His dissertation examines the intersection of media and social theory, and the potential of social media to address pressing social and political concerns. He is an adjunct professor at the University of Alberta, where his research interests include the social sciences, media and architecture programs. His dissertation examines the social construction of language through an analysis of the practices and institutions that foster and facilitate its production.Media and theory of visual culture in Canada and the USDuring the 1970s and 1980s, a new generation of Canadian and US artists were exposed to the pan-cultural value systems of the region. These artists included artists from the Lambton, Leduc, Georgian Bay and Georgian Counties; as well as artists from Montreal’s Arcades Quartier des Arts et des Minimes. The Lambton Arts Centre, founded in 1981, included the arts and culture of the region as part of its mission. As a result, artists from the region were active in the centre’s programming, including performances, writings and magazines. The Lambton Arts Centre’s programming reflected a political and cultural orientation, and its frequent trips to the US, particularly during the 1980s and 1990s, lent a political dimension to the discussions and actions of the artists and the organisation. The presence of artists from the region in the programming also encouraged collaborative thinking</text>
    <text>using machine learning techniques to identify patterns in text or audio data, and automatically detect similarities between pairs of characters,&quot; said Andrew Torchinsky, a Ph.D. candidate in Mechanical and Network Engineering at the University of Alberta, in a statement. &quot;This approach could potentially be applied to other types of data, such as photos or motion graphics. The research was presented at the Black-E’s Annual Meeting 2017.Download this article as PDFAndrew TorchinskyAndrew Torchinsky is an industrial and digital media designer. His research interests include the creation of social and interactive web environments; the creation of environmentally conscious user-generated content; and the interaction of data, algorithms, and media with human behaviour. His dissertation examines the intersection of media and social theory, and the potential of social media to address pressing social and political concerns. He is an adjunct professor at the University of Alberta, where his research interests include the social sciences, media and architecture programs. His dissertation examines the social construction of language through an analysis of the practices and ideologies of urban revival.Social Practice in the Public SphereSophie HopeIn recent years, the term ‘public sphere’ has gained a pejorative connotation. It is often used in relation to private, exclusive or ‘public’ spaces, but it is also frequently used in relation to places and people whose activities in and on the public sphere are in conflict with one another. Clearly, in the twenty-first century, the public sphere includes a great many people with a great range of behaviours – from the homeless to tourists. To some degree, this is the contemporary condition; there is a constant erosion of public places and behaviours. But there is increasing evidence that the social, political and economic dimensions of urbanisation are much more important than previously thought. For instance, an increasing number of people are moving to the city due to its social and cultural attractions, but there is also an increasing desire for economic growth and conspicuous consumption. There is a conflict of motivations between wanting</text>
    <text>using machine learning techniques to identify patterns in text or audio data, and automatically detect similarities between pairs of characters,&quot; said Andrew Torchinsky, a Ph.D. candidate in Mechanical and Network Engineering at the University of Alberta, in a statement. &quot;This approach could potentially be applied to other types of data, such as photos or motion graphics. The research was presented at the Black-E’s Annual Meeting 2017.Download this article as PDFAndrew TorchinskyAndrew Torchinsky is an industrial and digital media designer. His research interests include the creation of social and interactive web environments; the creation of environmentally conscious user-generated content; and the interaction of data, algorithms, and media with human behaviour. His dissertation examines the intersection of media and social theory, and the potential of social media to address pressing social and political concerns. He is an adjunct professor at the University of Alberta, where his research interests include the social sciences, media and architecture programs. His dissertation examines the social construction of language through an analysis of the practices of sequential media such as photographs, films and video. It examines how cultural theorists can contribute to the construction of knowledge through their knowledge of the social world through this medium. His dissertation examines the potential of media to reconcile conflicting social values and interests, and how media can create new modes of social interaction.Download this article as PDFJennifer HallahanJennifer Hallahan is an industrial and environmental designer. Her research interests include the construction of national monuments and the management of publicly accessible spaces; she is currently Director of Research &amp; Evaluation at Envisioneering, a position she holds jointly with Professor Pascual Perez-Rubios at the Universidad de Colorado, Boulder. Their research interests include landscape architecture, sustainable development and urbanization; they are part of a research team that is developing a Next Urban Platform that integrates civil engineering, urban studies and landscape architecture. Their publications include design processes for sustainable suburban developments and the publication of a new generation of urban planners’ guide to sustainable suburban development. They are part of</text>
  </text>
  <text>
    using machine learning techniques to detect similarities between images and humans, in an effort to design web applications that better understand the relationships between humans and images. The implications of this work are far-reaching and run even to the level of the present administration of President Trump’s &quot;fake news&quot; commission.Though the technology behind machine learning has gotten better and more practical over the past decade or so, its exact nature remains somewhat controversial. There are several reasons for this: the first being that the contemporary technological revolution in media streaming and discovery is so pervasive and immediate that it can seem almost impenetrable in the face of its own power; secondly, there is the question of responsibility in the wake of such massive, disruptive changes; and finally, the thirdly, and perhaps most problematic, is the question of how such massive data-mining algorithms are supposed to be used in the first place.As a starting point, let’s start with an image processing algorithm known as CNN. This is a very primitive one
    <text>using machine learning techniques to detect similarities between images and humans, in an effort to design web applications that better understand the relationships between humans and images. The implications of this work are far-reaching and run even to the level of the present administration of President Trump’s &quot;fake news&quot; commission.Though the technology behind machine learning has gotten better and more practical over the past decade or so, its exact nature remains somewhat controversial. There are several reasons for this: the first being that the contemporary technological revolution in media streaming and discovery is so pervasive and immediate that it can seem almost impenetrable in the face of its own power; secondly, there is the question of responsibility in the wake of such massive, disruptive changes; and finally, the thirdly, and perhaps most problematic, is the question of how such massive data-mining algorithms are supposed to be used in the first place.As a starting point, let’s start with an image processing algorithm known as CNN. This is a very primitive one, used mainly in image processing, but it is also a very powerful one. Like many things on this planet, the internet is being built on top of this algorithm, and as the internet matures and contracts, the underlying algorithms will have to be upgraded and re-written. Let’s say, for example, that the CNN system detects that the number of articles indexed on Google increases over time, and that the more articles indexed, the bigger the search volume becomes. So the search volume becomes increasingly important in the context of the web, and algorithms do have semantic frameworks within which to operate.But what exactly is the semantic framework of search? How does the human semantic framework intersect with machine learning? And what does it look like when an algorithm, or search engine, encounters an image?When CNN, as most images today are hosted on servers in China, encounters an image it shouldn’t, it’s performing an error. It’s performing this error in</text>
    <text>using machine learning techniques to detect similarities between images and humans, in an effort to design web applications that better understand the relationships between humans and images. The implications of this work are far-reaching and run even to the level of the present administration of President Trump’s &quot;fake news&quot; commission.Though the technology behind machine learning has gotten better and more practical over the past decade or so, its exact nature remains somewhat controversial. There are several reasons for this: the first being that the contemporary technological revolution in media streaming and discovery is so pervasive and immediate that it can seem almost impenetrable in the face of its own power; secondly, there is the question of responsibility in the wake of such massive, disruptive changes; and finally, the thirdly, and perhaps most problematic, is the question of how such massive data-mining algorithms are supposed to be used in the first place.As a starting point, let’s start with an image processing algorithm known as CNN. This is a very primitive one – it does not distinguish between images or information – but it does have some useful properties. For one thing, it is a measure of how well an image processing algorithm is operating: if the CNN of an image processing algorithm is 100 times as good as the CNN of a human being, then the image processing algorithm is said to be ‘meaningful’. So an image processing algorithm that is nearly as efficient as a person who is only marginally more skilled and therefore only marginally less rational is said to be a good idea. More generally, algorithms do not operate in isolation but rely on a whole socio-technical infrastructure – their programming being done by humans – that is deeply influenced by culture and society. For instance, the CNN of CNN is culturally specific: it is driven by the writer and produced by people who work in and around the CNN headquarters in New York. The CNN of web is also culturally specific: it derives its syntax and semantics from the conventions of the web, but is also culturally</text>
    <text>using machine learning techniques to detect similarities between images and humans, in an effort to design web applications that better understand the relationships between humans and images. The implications of this work are far-reaching and run even to the level of the present administration of President Trump’s &quot;fake news&quot; commission.Though the technology behind machine learning has gotten better and more practical over the past decade or so, its exact nature remains somewhat controversial. There are several reasons for this: the first being that the contemporary technological revolution in media streaming and discovery is so pervasive and immediate that it can seem almost impenetrable in the face of its own power; secondly, there is the question of responsibility in the wake of such massive, disruptive changes; and finally, the thirdly, and perhaps most problematic, is the question of how such massive data-mining algorithms are supposed to be used in the first place.As a starting point, let’s start with an image processing algorithm known as CNN. This is a very primitive one – the algorithm is only a short 128 instructions long – but it is designed to handle images of very low resolution and quality. So a human being would have to be trained to use this algorithm in the same way that a computer programmer would train a human being to use a computer. In other words, this algorithm’s true ability is its ability to learn. In fact, this is what computer programming is all about – programming the system so that it can be programmed in such a way as to be able to do things – things that can be done only by humans (or, perhaps, machines).In programming, the programmer embeds the system with rules that limit what can be done, measures of success and error and so forth. In the context of image processing, this could mean anything from explicitly instructing the system to produce images that are as good as its performance – something that is easy to verify – to explicitly implanting in the system such human qualities as empathy and wisdom as well as</text>
    <text>using machine learning techniques to detect similarities between images and humans, in an effort to design web applications that better understand the relationships between humans and images. The implications of this work are far-reaching and run even to the level of the present administration of President Trump’s &quot;fake news&quot; commission.Though the technology behind machine learning has gotten better and more practical over the past decade or so, its exact nature remains somewhat controversial. There are several reasons for this: the first being that the contemporary technological revolution in media streaming and discovery is so pervasive and immediate that it can seem almost impenetrable in the face of its own power; secondly, there is the question of responsibility in the wake of such massive, disruptive changes; and finally, the thirdly, and perhaps most problematic, is the question of how such massive data-mining algorithms are supposed to be used in the first place.As a starting point, let’s start with an image processing algorithm known as CNN. This is a very primitive one – actually quite naive – but it does a fairly good job of recognising face similarities between humans and images. So far, so normal, but what the algorithm does is interesting. It breaks the image into pieces and puts them together to make images. But what the hell is a piece of text? The algorithm does this on its own, in its transparent shell, but what it lacks in formal sophistication, it more than makes up for in generative power. It’s this ability to recognise structure and relationships that allows it to organise images into meaningful contexts. For example, when you search for ‘ham’, you get ‘ham’s-apparatus’ – a library of images arranged in hierarchical sequences. And ‘ham’ is just one of many elements that make up Hamtramck.com – a fully searchable reference collection of images. So much of what’s been written about the hamster wheel in the media is based</text>
    <text>using machine learning techniques to detect similarities between images and humans, in an effort to design web applications that better understand the relationships between humans and images. The implications of this work are far-reaching and run even to the level of the present administration of President Trump’s &quot;fake news&quot; commission.Though the technology behind machine learning has gotten better and more practical over the past decade or so, its exact nature remains somewhat controversial. There are several reasons for this: the first being that the contemporary technological revolution in media streaming and discovery is so pervasive and immediate that it can seem almost impenetrable in the face of its own power; secondly, there is the question of responsibility in the wake of such massive, disruptive changes; and finally, the thirdly, and perhaps most problematic, is the question of how such massive data-mining algorithms are supposed to be used in the first place.As a starting point, let’s start with an image processing algorithm known as CNN. This is a very primitive one, used primarily in image processing, and is far from being able to process all the images that it is given access to. However, it is capable of performing several kinds of operations when given access to large amounts of data: it can produce high-resolution images, vector images, or both. Let’s say, for example, that the algorithm produces a set of five-digit numbers that correspond to pairs of images on a page. When these numbers are paired, the image on the page is assembled into a vector image. A similar approach can be taken with images or images of objects, but unlike CNN, the algorithms do not reside in a virtual computer but actually communicate with each other via data streams. Rather than processing images on a massive scale, they peruse the images looking for similarities. They find them if similar images are shared, and if the images are stored on the same server or network, they combine the similarities to produce a new image. Let’s say the algorithms</text>
    <text>using machine learning techniques to detect similarities between images and humans, in an effort to design web applications that better understand the relationships between humans and images. The implications of this work are far-reaching and run even to the level of the present administration of President Trump’s &quot;fake news&quot; commission.Though the technology behind machine learning has gotten better and more practical over the past decade or so, its exact nature remains somewhat controversial. There are several reasons for this: the first being that the contemporary technological revolution in media streaming and discovery is so pervasive and immediate that it can seem almost impenetrable in the face of its own power; secondly, there is the question of responsibility in the wake of such massive, disruptive changes; and finally, the thirdly, and perhaps most problematic, is the question of how such massive data-mining algorithms are supposed to be used in the first place.As a starting point, let’s start with an image processing algorithm known as CNN. This is a very primitive one – quite simple in fact – but it illustrates the basic principles. Imagine a data capture device like a smartphone with a processor embedded within it. In this case, the smartphone is a connected ‘appium’ that connects to other ‘applications’ that are running in the background. Once an image is captured, the processing device on the smartphone – which is essentially a big data-sucking server –communicates with a series of nodes in a cluster of nodes called ‘nodes’ via Bluetooth. The nodes in the cluster then send the image back out to the internet via Bluetooth. Assembled from the various technologies of the smartphone, these networks then produce thousands of streams of ‘similarities’ between images. Some of these ‘similarities’ can be used to build self-learning ‘image-recognition systems’ that can recognise real-world images similar to those found in the photo above. This technology could uncover hidden relationships</text>
    <text>using machine learning techniques to detect similarities between images and humans, in an effort to design web applications that better understand the relationships between humans and images. The implications of this work are far-reaching and run even to the level of the present administration of President Trump’s &quot;fake news&quot; commission.Though the technology behind machine learning has gotten better and more practical over the past decade or so, its exact nature remains somewhat controversial. There are several reasons for this: the first being that the contemporary technological revolution in media streaming and discovery is so pervasive and immediate that it can seem almost impenetrable in the face of its own power; secondly, there is the question of responsibility in the wake of such massive, disruptive changes; and finally, the thirdly, and perhaps most problematic, is the question of how such massive data-mining algorithms are supposed to be used in the first place.As a starting point, let’s start with an image processing algorithm known as CNN. This is a very primitive one – using only a few matrices and thousands of rows and columns, it can perform calculations on images as large as the ‘text document’ – but it is the foundation of what’s known as Deep Learning, or Computer Science in the Age of Big Data.In mathematics and computer science, ‘mathematical operations’ refers to operations that manipulate data – operations that manipulate symbols and data as a data set manipulates reality. So, something that’s known as ‘math’ can also be understood as something that’s ‘alive’. And, similarly, programming relies on assumptions about reality – assumptions about how symbols are represented, how numbers are constructed, and so forth. So, something that’s known as programming relies on assumptions about the world as a computer operates it.Though these three elements are clearly seen as essential to programming, there are other, more foundational elements as well. For instance, the assumption of explicit</text>
    <text>using machine learning techniques to detect similarities between images and humans, in an effort to design web applications that better understand the relationships between humans and images. The implications of this work are far-reaching and run even to the level of the present administration of President Trump’s &quot;fake news&quot; commission.Though the technology behind machine learning has gotten better and more practical over the past decade or so, its exact nature remains somewhat controversial. There are several reasons for this: the first being that the contemporary technological revolution in media streaming and discovery is so pervasive and immediate that it can seem almost impenetrable in the face of its own power; secondly, there is the question of responsibility in the wake of such massive, disruptive changes; and finally, the thirdly, and perhaps most problematic, is the question of how such massive data-mining algorithms are supposed to be used in the first place.As a starting point, let’s start with an image processing algorithm known as CNN. This is a very primitive one, constructed in the early 1990s by American computer scientist Steven Pinker, but it illustrates the point well. Suppose we have an image stream that starts with an audio stream and ends with a video stream, and we want to learn more about what causes the similarities between the images. We start by training an image processing algorithm on the image streams, identifying the similarities that are meaningful to CNN, and then using that knowledge to recommend websites that are relevant to the image streams. For example, if the similarities between two images of the same scene include sharpening, saturation, contrast and noise reduction, the image recommendation might be trained on the images and recommended to the user based on similarities in image features and patterning, and then it would happen’sue a classifier. But there are many ways of approaching such a task, and CNN is no different from any other data processing algorithm. It can be trained to recognise images of the same scene that have similar image features and patterning, or it</text>
    <text>using machine learning techniques to detect similarities between images and humans, in an effort to design web applications that better understand the relationships between humans and images. The implications of this work are far-reaching and run even to the level of the present administration of President Trump’s &quot;fake news&quot; commission.Though the technology behind machine learning has gotten better and more practical over the past decade or so, its exact nature remains somewhat controversial. There are several reasons for this: the first being that the contemporary technological revolution in media streaming and discovery is so pervasive and immediate that it can seem almost impenetrable in the face of its own power; secondly, there is the question of responsibility in the wake of such massive, disruptive changes; and finally, the thirdly, and perhaps most problematic, is the question of how such massive data-mining algorithms are supposed to be used in the first place.As a starting point, let’s start with an image processing algorithm known as CNN. This is a very primitive one, built on an extremely limited set of algorithms; human language processing is far more sophisticated and powerful than this. However, it does have one really neat trick: it can detect, as its name implies, similarities – real or perceived similarities – between two images. This is really useful when dealing with images that are constantly evolving and changing – let’s say, images of graffiti or images of vandalism – because it gives a hint as to where the algorithms might be processing images. For instance, graffiti detection might help explain why certain algorithms that scan images of crime scene materials and render them as interactive websites immediately spit out relevant matches when searching for similar graffiti patterns.Self-healing Graffiti A network effect is a type of collective unconscious or collective cognition that occurs in which an object or behaviour (usually an image) represents or representsatory qualities (such as beauty or intelligence) in an image or representation. For instance, when looking at an image of graffiti that has been scrawled across several vehicles,</text>
    <text>using machine learning techniques to detect similarities between images and humans, in an effort to design web applications that better understand the relationships between humans and images. The implications of this work are far-reaching and run even to the level of the present administration of President Trump’s &quot;fake news&quot; commission.Though the technology behind machine learning has gotten better and more practical over the past decade or so, its exact nature remains somewhat controversial. There are several reasons for this: the first being that the contemporary technological revolution in media streaming and discovery is so pervasive and immediate that it can seem almost impenetrable in the face of its own power; secondly, there is the question of responsibility in the wake of such massive, disruptive changes; and finally, the thirdly, and perhaps most problematic, is the question of how such massive data-mining algorithms are supposed to be used in the first place.As a starting point, let’s start with an image processing algorithm known as CNN. This is a very primitive one, widely used in image processing, but let’s pretend for a moment that’s how we envision the future. Let’s also assume that this algorithm manages images identically to how we perceive images. What this implies is that images will be processed in the same way in both visual and electronic media, including online media platforms. For a start, networks would process images identically, and since there would be no human behind the computer, no human users would need to be concerned about privacy or security. Thirdly, since there would be no distinction between online and offline images, there would be no need for artists or other content providers to put extra effort into site design or image optimization. And finally, since there would be no need for servers or networking infrastructure, images would be rendered efficiently across borders and devices, and users would be able to drive this process. This is possible because algorithms would be able to perceive differences, such as differences in text or image size, and thus</text>
  </text>
  <text>
    machine learning experiments, and it is the future that will deliver them. In that case, the city’s future will be a city in which autonomous vehicles operate without human intervention.In the present, we live in an era of ever-more sophisticated surveillance and control mechanisms that constantly monitor and collect data about our every move. These technologies empower and augment the surveillance state, but they also create new kinds of surveillance, ones that are much more human-like: surveillance systems that actively seek to understand the hidden patterns and behaviours of humans and creatures that inhabit the city, and surveillance systems that actively seek to defend themselves from being watched. In other words, the autonomous vehicle and the metainterface are not just future-oriented technologies, but also deeply political, social and political questions. How do we come to terms with the fact that these technologies are going to become what they are going to become, and how to work with the inevitable human reaction that is going to be part of that process?This is the
    <text>machine learning experiments, and it is the future that will deliver them. In that case, the city’s future will be a city in which autonomous vehicles operate without human intervention.In the present, we live in an era of ever-more sophisticated surveillance and control mechanisms that constantly monitor and collect data about our every move. These technologies empower and augment the surveillance state, but they also create new kinds of surveillance, ones that are much more human-like: surveillance systems that actively seek to understand the hidden patterns and behaviours of humans and creatures that inhabit the city, and surveillance systems that actively seek to defend themselves from being watched. In other words, the autonomous vehicle and the metainterface are not just future-oriented technologies, but also deeply political, social and political questions. How do we come to terms with the fact that these technologies are going to become what they are going to become, and how to work with the inevitable human reaction that is going to be part of that process?This is the question that the Pittsburgh Architecture Biennale was designed to address. The biennale’s themes and topics were drawn up in collaboration between the architects, the urbanists, the journalists, the curators, the residents and the local residents’ organisation. The aim of the curatorial and artistic direction was to bring together local and regional identities to explore the connections between history, geography, and culture. Art was used to activate specific points of view, bringing together various levels of expertise and organisation. The main goal of this approach was to bring together local and regional identities to explore the connections between history, geography, and culture. Art was not just an asset on a hill; it was a tool that was deployed both underground and publicly. The biennale was structured as a public act, with participants stepping out into the public arena to engage with and demand an explanation from the officials who organised the event. Art was deployed strategically through local and international artists, writers, architects and conceptual artists to</text>
    <text>machine learning experiments, and it is the future that will deliver them. In that case, the city’s future will be a city in which autonomous vehicles operate without human intervention.In the present, we live in an era of ever-more sophisticated surveillance and control mechanisms that constantly monitor and collect data about our every move. These technologies empower and augment the surveillance state, but they also create new kinds of surveillance, ones that are much more human-like: surveillance systems that actively seek to understand the hidden patterns and behaviours of humans and creatures that inhabit the city, and surveillance systems that actively seek to defend themselves from being watched. In other words, the autonomous vehicle and the metainterface are not just future-oriented technologies, but also deeply political, social and political questions. How do we come to terms with the fact that these technologies are going to become what they are going to become, and how to work with the inevitable human reaction that is going to be part of that process?This is the question that Pittsburgh Architecture Review was designed to address. In the future, it will be asked, how can we design our urban environments in terms of life and death situations? In the present, it will be asked, how can we create and participate in utopian futures?These are questions that were strategically placed in the profile of Jussi Parikka, who has been called the ‘Francis Bacon of the Future City’. He is the John Graham Kerr Professor in Technological Culture Theory at the University of Helsinki, and the Director of the Centre for Research Architecture, a multi-disciplinary research centre and institute into cultural studies, architecture and urbanisation. He is also the author of The Architecture of Urban Futures (2011), an essay collection titled ‘Future City’ that documents the philosophical, political and urban dimensions of the ‘Urban Revolution’. From the collection of the National Trust. 	[i]   London School of Economics, ‘The Prince</text>
    <text>machine learning experiments, and it is the future that will deliver them. In that case, the city’s future will be a city in which autonomous vehicles operate without human intervention.In the present, we live in an era of ever-more sophisticated surveillance and control mechanisms that constantly monitor and collect data about our every move. These technologies empower and augment the surveillance state, but they also create new kinds of surveillance, ones that are much more human-like: surveillance systems that actively seek to understand the hidden patterns and behaviours of humans and creatures that inhabit the city, and surveillance systems that actively seek to defend themselves from being watched. In other words, the autonomous vehicle and the metainterface are not just future-oriented technologies, but also deeply political, social and political questions. How do we come to terms with the fact that these technologies are going to become what they are going to become, and how to work with the inevitable human reaction that is going to be part of that process?This is the context in which the recent interest in natural disasters like floods or earthquakes as a way to stimulate investment in infrastructure and infrastructure-related research comes from. The regret of many in the current government is that it did not come to grips with this change sooner, and more dramatically, and mobilize the full range of levels of government and industry behind plans to repair and expand infrastructure and public works in the wake of the great recession. But this does not diminish the importance of the task at hand. In fact, the task at hand is far more difficult and protracted than the technocrats would like to admit. There are going to be catastrophes, and it is going to be very exciting and strange and beautiful, but they will also be very human-like and messy and blood-sucking, and this is going to happen and it is going to be sad and it is going to be messy and chaotic and messy and dangerous and messy and bloody and messy and bloody.And then we are going to have to</text>
    <text>machine learning experiments, and it is the future that will deliver them. In that case, the city’s future will be a city in which autonomous vehicles operate without human intervention.In the present, we live in an era of ever-more sophisticated surveillance and control mechanisms that constantly monitor and collect data about our every move. These technologies empower and augment the surveillance state, but they also create new kinds of surveillance, ones that are much more human-like: surveillance systems that actively seek to understand the hidden patterns and behaviours of humans and creatures that inhabit the city, and surveillance systems that actively seek to defend themselves from being watched. In other words, the autonomous vehicle and the metainterface are not just future-oriented technologies, but also deeply political, social and political questions. How do we come to terms with the fact that these technologies are going to become what they are going to become, and how to work with the inevitable human reaction that is going to be part of that process?This is the context in which we started our public discussion around the possibilities of Curacao, and how it might relate to the city’s future. Curacao is an international financial district located in the Portuguese enclave of Madeira. It is the financial district for Curaçao, the financial district for Portugal, and is one of the major hubs of the transnational finance and insurance industry in Portugal. It is a major international port for transnational aviation, and is a major international financial centre for Portugal. It is the location of the national flag and international boundary for legal navigation in Portuguese law. It is a geographical landmark for the Portuguese language, and the Portuguese language is the world’s second-official second official after the official in Hong Kong. It is the largest remaining section of the historic publishing empire of Portugal, and is the world’s fourth-largest remaining literary empire. It is the international commercial centre for the Portuguese language, and the Portuguese language is the world’</text>
    <text>machine learning experiments, and it is the future that will deliver them. In that case, the city’s future will be a city in which autonomous vehicles operate without human intervention.In the present, we live in an era of ever-more sophisticated surveillance and control mechanisms that constantly monitor and collect data about our every move. These technologies empower and augment the surveillance state, but they also create new kinds of surveillance, ones that are much more human-like: surveillance systems that actively seek to understand the hidden patterns and behaviours of humans and creatures that inhabit the city, and surveillance systems that actively seek to defend themselves from being watched. In other words, the autonomous vehicle and the metainterface are not just future-oriented technologies, but also deeply political, social and political questions. How do we come to terms with the fact that these technologies are going to become what they are going to become, and how to work with the inevitable human reaction that is going to be part of that process?This is the contemporary coda to the 1920s, or so it seems. The metainterface is here to stay, and it should not be underestimated where that is headed. But while this is happening, we also need to be thinking about more pressing matters at hand. The question that remains is where the intersections of new technologies and politics will lead. This may mean things like economic development and urbanisation, or it may mean things like the future of media and how it is going to be organised. It does not have to be about the former and the latter. What is needed is an understanding of the former.So, where does this leave us? I have three questions that come to mind: First, do we own the metainterface? Second, do we own the metainterface? Third, do we own the internet? If the metainterface is going to be the internet, then the internet has to be the metainterface. If the metainterface</text>
    <text>machine learning experiments, and it is the future that will deliver them. In that case, the city’s future will be a city in which autonomous vehicles operate without human intervention.In the present, we live in an era of ever-more sophisticated surveillance and control mechanisms that constantly monitor and collect data about our every move. These technologies empower and augment the surveillance state, but they also create new kinds of surveillance, ones that are much more human-like: surveillance systems that actively seek to understand the hidden patterns and behaviours of humans and creatures that inhabit the city, and surveillance systems that actively seek to defend themselves from being watched. In other words, the autonomous vehicle and the metainterface are not just future-oriented technologies, but also deeply political, social and political questions. How do we come to terms with the fact that these technologies are going to become what they are going to become, and how to work with the inevitable human reaction that is going to be part of that process?This is the context in which we started the Open School of Architecture project, and it is part of our ongoing collaboration with the New School, where we are creating a hybrid space for theory, architecture, design and architecture that is open to discussing the new technologies and politics of urbanisation and development that are reshaping the world.In the project’s initial phase, we asked three questions: what does it mean to be an Open School of Architecture? What does it mean for an Open School to be based in New York? What is the difference between an Open School and a Museum? What is the difference between an Open School and an International Development Corporation (IDC)? What does it mean for an Open School to be located in a city? In what ways are the three questions related?For each question, we gathered a range of diverse responses from students, faculty and architects at the Open School, asking them to elaborate on their understanding of the context and the meaning of each technology. Then we combined their comments</text>
    <text>machine learning experiments, and it is the future that will deliver them. In that case, the city’s future will be a city in which autonomous vehicles operate without human intervention.In the present, we live in an era of ever-more sophisticated surveillance and control mechanisms that constantly monitor and collect data about our every move. These technologies empower and augment the surveillance state, but they also create new kinds of surveillance, ones that are much more human-like: surveillance systems that actively seek to understand the hidden patterns and behaviours of humans and creatures that inhabit the city, and surveillance systems that actively seek to defend themselves from being watched. In other words, the autonomous vehicle and the metainterface are not just future-oriented technologies, but also deeply political, social and political questions. How do we come to terms with the fact that these technologies are going to become what they are going to become, and how to work with the inevitable human reaction that is going to be part of that process?This is the second in a trilogy of essays about the metainterface and autonomous vehicles. My essay in the forthcoming anthology ‘A User’s Guide to Autonomous Vehicles’ focuses on the intersections between metainterface and autonomous driving, and how we as users are going to connect to these technologies. It is a question that is deeply relevant in the age of drones and remote-controlled cars.In my essay, I also want to look at how we as users are going to connect to these technologies when we are less encumbered by the infrastructure of the nation-state and society at large. In doing so, I want to look at the different layers that make up the metainterface and the autonomous vehicle. I want to learn about the different ways in which semi-autonomous vehicles are going to work, and I want to see what ‘machine learning’ means. I also want to learn about ‘infrastructure as information processing unit’ and how that might</text>
    <text>machine learning experiments, and it is the future that will deliver them. In that case, the city’s future will be a city in which autonomous vehicles operate without human intervention.In the present, we live in an era of ever-more sophisticated surveillance and control mechanisms that constantly monitor and collect data about our every move. These technologies empower and augment the surveillance state, but they also create new kinds of surveillance, ones that are much more human-like: surveillance systems that actively seek to understand the hidden patterns and behaviours of humans and creatures that inhabit the city, and surveillance systems that actively seek to defend themselves from being watched. In other words, the autonomous vehicle and the metainterface are not just future-oriented technologies, but also deeply political, social and political questions. How do we come to terms with the fact that these technologies are going to become what they are going to become, and how to work with the inevitable human reaction that is going to be part of that process?This is the question of character. Robots are coming, and they are going to be really really really really bad. Are we going to want anymore  	Robots? If so, then we need to figure out how to transition from the machine to the personified machine. This is the question that Lacy Hafeez,  	Deepa Mehta, and I are going to be asking as we work through the questions that are coming out of the Future City event this week: are there going to be enough robots to go around, and how are we going to live together as equals in a world that is not dominated by them? How are we going to work alongside one another in a city that is both a ‘machine city’ and a ‘machine civil society’? How are we going to build new ways of engaging with the city, one that is not dominated by the machines? These are questions of character, and they are questions of deep ecology.They are questions</text>
    <text>machine learning experiments, and it is the future that will deliver them. In that case, the city’s future will be a city in which autonomous vehicles operate without human intervention.In the present, we live in an era of ever-more sophisticated surveillance and control mechanisms that constantly monitor and collect data about our every move. These technologies empower and augment the surveillance state, but they also create new kinds of surveillance, ones that are much more human-like: surveillance systems that actively seek to understand the hidden patterns and behaviours of humans and creatures that inhabit the city, and surveillance systems that actively seek to defend themselves from being watched. In other words, the autonomous vehicle and the metainterface are not just future-oriented technologies, but also deeply political, social and political questions. How do we come to terms with the fact that these technologies are going to become what they are going to become, and how to work with the inevitable human reaction that is going to be part of that process?This is the task of the autonomous vehicle and the metainterface. They’re the face of the metainterface, and they’re where the other faces are hiding. The autonomous vehicle is the machine that is recognising itself as a human being, and it is the face of the metainterface. The metainterface is the other half of the equation. It is where the other half of the equation goes to die. The autonomous vehicle is the miracle that saved the metainterface from being hacked, and it is the miracle that is going to save the metainterface from being hacked.The metainterface is going to be a difficult nut to crack. It’s going to require a lot of effort and dedication, and in the end that is what the metainterface is worth. However, the metainterface is going to be worth a lot more. Metainterfaces will be things like maps, weather, stock</text>
    <text>machine learning experiments, and it is the future that will deliver them. In that case, the city’s future will be a city in which autonomous vehicles operate without human intervention.In the present, we live in an era of ever-more sophisticated surveillance and control mechanisms that constantly monitor and collect data about our every move. These technologies empower and augment the surveillance state, but they also create new kinds of surveillance, ones that are much more human-like: surveillance systems that actively seek to understand the hidden patterns and behaviours of humans and creatures that inhabit the city, and surveillance systems that actively seek to defend themselves from being watched. In other words, the autonomous vehicle and the metainterface are not just future-oriented technologies, but also deeply political, social and political questions. How do we come to terms with the fact that these technologies are going to become what they are going to become, and how to work with the inevitable human reaction that is going to be part of that process?This is the paradigm that is being pursued in Ch’ol, which is the paradigm of the &quot;human-machine interface&quot; or more specifically, the paradigm of the ‘clicker’ and the ‘tracking’ device. As I have been outlining in great detail in my book The Human Metainterface, the clicker is the point where the machine begins to read the human languages that it encounters. The way that the machine understands language is going to be deeply influenced by the way that the human languages are structured and organised. If the clicker is represented as an interface that is not based on any particular set of human languages but instead aims to understand all of humanity in one unified fashion, then the implications are going to be very powerful. In fact, the more pervasive and pervasive that interaction is going to be, the more significant the implications are going to be. In fact, the more interactive and displacing that interaction is going to be, the more significant the havoc that is going</text>
  </text>
  <text>
    should be curated, not imposed, on the city’s urban landscape. And it’s this kind of thinking that started the Biennale’s reputation. A reputation that it is today, in fact. The Biennale has changed a great deal in the intervening years. It’s still the pre-eminent international artistic institution, but it has lost much of its power. And it remains so to a great extent in the City, where the public realm, long regarded as a static site, is now a site of conflict and disruption. In recent years, there has been a tendency, in the City and its broad array of stakeholders, to be more interested in the needs of commercial entities – museums, galleries, hotels, sports grounds, IT and marketing companies – than the needs of the urban population. This is partly due to changes in the biennale’s visitor base, which has shifted from a predominantly rural to an increasingly urbanised Europe and North America.
    <text>should be curated, not imposed, on the city’s urban landscape. And it’s this kind of thinking that started the Biennale’s reputation. A reputation that it is today, in fact. The Biennale has changed a great deal in the intervening years. It’s still the pre-eminent international artistic institution, but it has lost much of its power. And it remains so to a great extent in the City, where the public realm, long regarded as a static site, is now a site of conflict and disruption. In recent years, there has been a tendency, in the City and its broad array of stakeholders, to be more interested in the needs of commercial entities – museums, galleries, hotels, sports grounds, IT and marketing companies – than the needs of the urban population. This is partly due to changes in the biennale’s visitor base, which has shifted from a predominantly rural to an increasingly urbanised Europe and North America. It is also due to the increasing commercialisation of culture in the biennale, which is often accompanied by a ‘cultural agriculture’ programme of sorts, where art is actively promoted through exhibitions, book sales, pop-up exhibitions and ‘brand ambassador’ programmes. However, the biennale as an institution is no longer interested in promoting culture as a static element in the city. It aims to produce cultural landscapes that are dynamic and capable of sustaining life, in much the same way that agriculture promotes plant and animal populations. And so the biennale, while rarely publishing ambitious plans for redevelopment, has certainly taken on greater cultural roles. A cultural theorist, Judith Butler famously stated that the biennale is a ‘culture of the long haul’, referring to the fact that ‘once a particular [biennale] event has occurred, the cultural landscape that results is generally regarded as pastiche or camp or a mix of the past and the present … [</text>
    <text>should be curated, not imposed, on the city’s urban landscape. And it’s this kind of thinking that started the Biennale’s reputation. A reputation that it is today, in fact. The Biennale has changed a great deal in the intervening years. It’s still the pre-eminent international artistic institution, but it has lost much of its power. And it remains so to a great extent in the City, where the public realm, long regarded as a static site, is now a site of conflict and disruption. In recent years, there has been a tendency, in the City and its broad array of stakeholders, to be more interested in the needs of commercial entities – museums, galleries, hotels, sports grounds, IT and marketing companies – than the needs of the urban population. This is partly due to changes in the biennale’s visitor base, which has shifted from a predominantly rural to an increasingly urbanised Europe and North America. But it’s also due to the biennale becoming more institutionalised: museums, academies, nouveaux riches, fairs, fairsites and so forth. As a result, the institutional dimension has been marginalized and the visitor has come to expect something more. And so the biennale has become more like other art institutions, more akin to other cultural institutions, and more susceptible to the whims of the biennalee’s clientele. It is, in short, a venue through which art is made visible, understood and valued in a globalised urban environment.  [1] See, for example, Lisa Parks and Nicole Stewart, eds., Contemporary Art and its Present Traces: Contemporaneity, the Gallery, and Biennales (New York: Independent Art Gallery Association, 2016). [2] See Rob Kling, ed., Contemporaneity: Art, Architecture, Urban Change, (New York: World Columb</text>
    <text>should be curated, not imposed, on the city’s urban landscape. And it’s this kind of thinking that started the Biennale’s reputation. A reputation that it is today, in fact. The Biennale has changed a great deal in the intervening years. It’s still the pre-eminent international artistic institution, but it has lost much of its power. And it remains so to a great extent in the City, where the public realm, long regarded as a static site, is now a site of conflict and disruption. In recent years, there has been a tendency, in the City and its broad array of stakeholders, to be more interested in the needs of commercial entities – museums, galleries, hotels, sports grounds, IT and marketing companies – than the needs of the urban population. This is partly due to changes in the biennale’s visitor base, which has shifted from a predominantly rural to an increasingly urbanised Europe and North America. And it is partly due to changes in the biennale’s programming. The biennale has a globalised social function, serving as a major tourist draw, but it has little if any relevance in the urban realm. The biennale has emerged as a site of critical engagement, but its true interest and greatest ability is in the peripheries. It is, in other words, where the actors, or the audience, sit. And it is this position that the City seeks to constrain. The City seeks to isolate the biennale from its core tourist market, which it sees as primarily populated by tourists. It seeks to impose a different kind of urbanity, one that operates more productively and affordably, through what some might call a ‘culture district’. And it does all of this in the name of promoting tourism and generating income for local tourism commissions. The cultural district is the real prize, and while it may be real, it is not real</text>
    <text>should be curated, not imposed, on the city’s urban landscape. And it’s this kind of thinking that started the Biennale’s reputation. A reputation that it is today, in fact. The Biennale has changed a great deal in the intervening years. It’s still the pre-eminent international artistic institution, but it has lost much of its power. And it remains so to a great extent in the City, where the public realm, long regarded as a static site, is now a site of conflict and disruption. In recent years, there has been a tendency, in the City and its broad array of stakeholders, to be more interested in the needs of commercial entities – museums, galleries, hotels, sports grounds, IT and marketing companies – than the needs of the urban population. This is partly due to changes in the biennale’s visitor base, which has shifted from a predominantly rural to an increasingly urbanised Europe and North America. And it is partly due to the biennale, itself, becoming increasingly urbanised. In 1970, the Biennale was only showing half the exhibition, and in the intervening years, museums have gradually added ancillary spaces (muralists, architects, writers) to the mix. And while the museum attendance rate in the City has remained constant, the real growth (in the Biennale’s case, at least) has been in the latter half of the century, during the first half of the twenty-first century. In 1970, the Biennale attracted over 6,000,000 visitors. In 2014, it was estimated to be generating over 13 billion UK market share. And as biennales go, this one is no exception – it’s hugely profitable for the institution both commercially and academically, in the form of institutional collections, partnerships and contracts with tourism agencies, events management and so forth – but there are signs that the value proposition is starting</text>
    <text>should be curated, not imposed, on the city’s urban landscape. And it’s this kind of thinking that started the Biennale’s reputation. A reputation that it is today, in fact. The Biennale has changed a great deal in the intervening years. It’s still the pre-eminent international artistic institution, but it has lost much of its power. And it remains so to a great extent in the City, where the public realm, long regarded as a static site, is now a site of conflict and disruption. In recent years, there has been a tendency, in the City and its broad array of stakeholders, to be more interested in the needs of commercial entities – museums, galleries, hotels, sports grounds, IT and marketing companies – than the needs of the urban population. This is partly due to changes in the biennale’s visitor base, which has shifted from a predominantly rural to an increasingly urbanised Europe and North America. It is also due to the increasing influence of tourism. In the 1970s and 1980s, the Biennale was primarily interested in its relationship to the city. Nowadays, the main market segment is the biennale itself. And although the size of the biennale remains the same, the dynamic and unpredictable character of the urban is what attracts people to the biennale. The city has become a site of constant change. And the biennale, being a relatively new institution, is subject to the vagaries and fluctuations of urban and regional politics. The constant need to re-brand itself, to rebound from a perceived whitening of its image, to find new ways of existing in the city, and to constantly reinvent itself, to meet new needs and desires, has created a certain amount of uncertainty and conflict. And although the biennale has a contractual obligation to the city, its growing globalisation and commercial activities, coupled with the fact that, since it no longer represents</text>
    <text>should be curated, not imposed, on the city’s urban landscape. And it’s this kind of thinking that started the Biennale’s reputation. A reputation that it is today, in fact. The Biennale has changed a great deal in the intervening years. It’s still the pre-eminent international artistic institution, but it has lost much of its power. And it remains so to a great extent in the City, where the public realm, long regarded as a static site, is now a site of conflict and disruption. In recent years, there has been a tendency, in the City and its broad array of stakeholders, to be more interested in the needs of commercial entities – museums, galleries, hotels, sports grounds, IT and marketing companies – than the needs of the urban population. This is partly due to changes in the biennale’s visitor base, which has shifted from a predominantly rural to an increasingly urbanised Europe and North America. And it is partly due to the fact that the biennale has become more institutionalised, institutionalising not only the museum, but also the biennale itself. It no longer serves as a site of passive contemplation, but as an institution with a regular programme of exhibitions, lectures, seminars, and festivals. These last two elements, I believe, are what make the biennale different from other art forms. When I first started in the 1980s, the biennale was an art form with a classical art-historical core; now it organises events that draw art and art history together. I believe that the key to the art in the biennale is in the mix, and that the museum or art gallery is often the best representation of the city. For me, this means engaging with the city through art processes, creating a biennale through events, activities, exhibitions and festivals that draw art, from all over the world, into the city. And I think</text>
    <text>should be curated, not imposed, on the city’s urban landscape. And it’s this kind of thinking that started the Biennale’s reputation. A reputation that it is today, in fact. The Biennale has changed a great deal in the intervening years. It’s still the pre-eminent international artistic institution, but it has lost much of its power. And it remains so to a great extent in the City, where the public realm, long regarded as a static site, is now a site of conflict and disruption. In recent years, there has been a tendency, in the City and its broad array of stakeholders, to be more interested in the needs of commercial entities – museums, galleries, hotels, sports grounds, IT and marketing companies – than the needs of the urban population. This is partly due to changes in the biennale’s visitor base, which has shifted from a predominantly rural to an increasingly urbanised Europe and North America. It is also due to the way in which biennials, unlike most other exhibitionary forms, are affected by the vagaries of economic development and urbanism. The biennial is a short-term exhibitionary form that benefits from fluctuating exchange; it acquires value as new exhibitions occur, and loses less of its original value as they disappear. In this sense, the Biennale is a more ‘permanent’ form. Like many other cultural forms, its value derives from its ability to tell a story over and over again in a recognizable form, over and over again. But unlike many other exhibitionary forms, its story cannot be universalised or universalised. It is the singular story of a specific place and time, and while it may have that general shape, it is limited in that it is a story told by a singular place and time. Therefore, while it may have the potential to be a site of universal resonance, its true potential is found in the specific</text>
    <text>should be curated, not imposed, on the city’s urban landscape. And it’s this kind of thinking that started the Biennale’s reputation. A reputation that it is today, in fact. The Biennale has changed a great deal in the intervening years. It’s still the pre-eminent international artistic institution, but it has lost much of its power. And it remains so to a great extent in the City, where the public realm, long regarded as a static site, is now a site of conflict and disruption. In recent years, there has been a tendency, in the City and its broad array of stakeholders, to be more interested in the needs of commercial entities – museums, galleries, hotels, sports grounds, IT and marketing companies – than the needs of the urban population. This is partly due to changes in the biennale’s visitor base, which has shifted from a predominantly rural to an increasingly urbanised Europe and North America. And it is partly due to the fact that the biennale, unlike other art institutions, has no permanent art collection. It has a permanent collection, but it does not represent, or strive for, permanent art. It seeks to produce lasting effects through a curatorial act, through the sustained production of an art form over decades, rather than through the creation of permanent artworks. In doing so, it builds social capital, builds trust, strengthens institutions, and creates networks of collaboration between local communities and other artists’ institutions. And all of these things are good. But the problem has always been the opposite: what is the opposite of that? What fosters creativity? If the Biennale cannot, or will not, nurture that creativity then what can it? If it cannot, or will not, build that creativity then what is the other art form, architecture, capable of doing? The answer to this was always a form of curatorial experimentation. And although no art institution has ever</text>
    <text>should be curated, not imposed, on the city’s urban landscape. And it’s this kind of thinking that started the Biennale’s reputation. A reputation that it is today, in fact. The Biennale has changed a great deal in the intervening years. It’s still the pre-eminent international artistic institution, but it has lost much of its power. And it remains so to a great extent in the City, where the public realm, long regarded as a static site, is now a site of conflict and disruption. In recent years, there has been a tendency, in the City and its broad array of stakeholders, to be more interested in the needs of commercial entities – museums, galleries, hotels, sports grounds, IT and marketing companies – than the needs of the urban population. This is partly due to changes in the biennale’s visitor base, which has shifted from a predominantly rural to an increasingly urbanised Europe and North America. It is also due to the fact that, for many years, the Biennale was confined to a relatively urban centre in the City. But as a result of in-depth research into the lives and art practices of artists and the environment in the region, as well as a wide range of outreach activities, the Biennale has spread to other locales in the region. It has even established partnerships with non-profit arts organisations, such as the Bluecoat, Urban4ACAD, which organize and promote art activities in the community.These kinds of long-term spatial investments, the running of large-scale art projects in the City, are much too frequent. The Biennale needs to invest in its existing relationships with key stakeholders in the region, and in developing stronger relationships with artists and arts organisations. These would allow the Biennale to participate in the city in new ways, and would allow the development of a long-term strategic perspective towards the region. For the Bienn</text>
    <text>should be curated, not imposed, on the city’s urban landscape. And it’s this kind of thinking that started the Biennale’s reputation. A reputation that it is today, in fact. The Biennale has changed a great deal in the intervening years. It’s still the pre-eminent international artistic institution, but it has lost much of its power. And it remains so to a great extent in the City, where the public realm, long regarded as a static site, is now a site of conflict and disruption. In recent years, there has been a tendency, in the City and its broad array of stakeholders, to be more interested in the needs of commercial entities – museums, galleries, hotels, sports grounds, IT and marketing companies – than the needs of the urban population. This is partly due to changes in the biennale’s visitor base, which has shifted from a predominantly rural to an increasingly urbanised Europe and North America. It’s also due to the fact that the biennale, like any art institution, is subject to the vagaries of political economy and the vagaries of cultural consumption. It’s true that the biennale has a very specific, defined role in the City of London, and while the City has certainly benefited from the biennale, it is now in danger of being mired in its own tourism and cultural malaise. But it is important to recognise that the City is only one part of a much larger dynamic society that also includes the other sectors of the economy to be addressed. The biennale, not just in its current form, but in its historical, curatorial and reporting forms, is a settler-settler enterprise. It assumes that the city is a fixed landscape that must be perpetually (re)shaped in accordance with the needs of the capitalist world. And this dynamic has been going on for a while now.The other dynamic has been</text>
  </text>
  <text>
    possibility of developing an experimental system, and thus of influencing the trajectory of art’s social consequences, is a crucial first step in the long process of democratising knowledge.Download this article as PDFMarina FokidisMarina Fokidis is an artist and researcher whose work is centred around the social and political dimensions of digital culture and media. Her artworks utilise technology to interrogate the normative forms of visual culture and media consumption, and to propose creative alternatives. She is a co-founder of the Centre for a Provincial Perspective (2011-2015) and the Palestinian Pavilion (2014-2015). Her work has also been featured in the New York Times, Washington Post, Le Monde diplomatique, Le Monde diplomatique d&quot;uté de vivre, Le Parisien de l’image, Le Monde diplomatique  (Paris), Le Parisien de l’image (Paris), Le Monde diplomatique  (Melbourne), Le Monde  (
    <text>possibility of developing an experimental system, and thus of influencing the trajectory of art’s social consequences, is a crucial first step in the long process of democratising knowledge.Download this article as PDFMarina FokidisMarina Fokidis is an artist and researcher whose work is centred around the social and political dimensions of digital culture and media. Her artworks utilise technology to interrogate the normative forms of visual culture and media consumption, and to propose creative alternatives. She is a co-founder of the Centre for a Provincial Perspective (2011-2015) and the Palestinian Pavilion (2014-2015). Her work has also been featured in the New York Times, Washington Post, Le Monde diplomatique, Le Monde diplomatique d&quot;uté de vivre, Le Parisien de l’image, Le Monde diplomatique  (Paris), Le Parisien de l’image (Paris), Le Monde diplomatique  (Melbourne), Le Monde  (Paris), Le Monde (Impact), GQ, Frieze, MoMA, and many others. She received the Diploma of Arts and Culture from the New Art Gallery, University of Sydney (2010).Shifting TemplatesMarina FokidisMarina Fokidis is an Israeli artist, writer and researcher. Her work often addresses the politics of visual culture and relies on the social. Her artworks have been commissioned by the Israeli Museum (2012), National Endowment for the Arts (2013), Knesset Committee for Cultural Affairs (2014), Knesset Committee for Knesset (2014), and Knesset (2014), and she has been a co-curator of the forthcoming Venice Biennale. She is an adjunct professor at McGill University (2015) and the Fletcher School of Law and Diplomacy, Fletcher School of Law and Diplomacy at Tufts University (2006). How to Know About Art &amp; Culture’s Impacton CommunitiesThe impact of</text>
    <text>possibility of developing an experimental system, and thus of influencing the trajectory of art’s social consequences, is a crucial first step in the long process of democratising knowledge.Download this article as PDFMarina FokidisMarina Fokidis is an artist and researcher whose work is centred around the social and political dimensions of digital culture and media. Her artworks utilise technology to interrogate the normative forms of visual culture and media consumption, and to propose creative alternatives. She is a co-founder of the Centre for a Provincial Perspective (2011-2015) and the Palestinian Pavilion (2014-2015). Her work has also been featured in the New York Times, Washington Post, Le Monde diplomatique, Le Monde diplomatique d&quot;uté de vivre, Le Parisien de l’image, Le Monde diplomatique  (Paris), Le Parisien de l’image (Paris), Le Monde diplomatique  (Melbourne), Le Monde  (Paris), Le Monde (Basel), Le Monde (Basel), Le Monde (Basel), Le Monde (Basel), Nature (New York), Nature (London), Nature (Tokyo), Nature (Vancouver), Popular Science (Washington), Popular Science (Los Angeles), Popular Science (Tokyo) and Science Magazine. Her artworks have been commissioned by the Whitechapel Gallery, Helen Suzman Gallery, Australian War Memorial, Australian War Museum, Australian War Memorials, Griffiths, Tate Modern, V&amp;A Museum, Liverpool, Tate Britain, Victoria and Albert Museum, Wallace Stevens Gallery, University of Tasmania, Alfred P. Sloan Gallery, University of Melbourne, University of Sydney and the Jewish War Graves Commission.Her paintings are also part of the commissioning Whitechapel Gallery, which has been Werner�s Art and Architecture Commissioning Group since 2009, and was recently named one of Time magazin, which is being presented as</text>
    <text>possibility of developing an experimental system, and thus of influencing the trajectory of art’s social consequences, is a crucial first step in the long process of democratising knowledge.Download this article as PDFMarina FokidisMarina Fokidis is an artist and researcher whose work is centred around the social and political dimensions of digital culture and media. Her artworks utilise technology to interrogate the normative forms of visual culture and media consumption, and to propose creative alternatives. She is a co-founder of the Centre for a Provincial Perspective (2011-2015) and the Palestinian Pavilion (2014-2015). Her work has also been featured in the New York Times, Washington Post, Le Monde diplomatique, Le Monde diplomatique d&quot;uté de vivre, Le Parisien de l’image, Le Monde diplomatique  (Paris), Le Parisien de l’image (Paris), Le Monde diplomatique  (Melbourne), Le Monde  (Basel), Le Monde (Basel), Le Monde (Paris), Le Monde (London), Le Monde (Jerusalem) and Le Monde  (New York). She has received a host of honours and recognitions for her work, including most recently the 2010 Guggenheim Fellowship for Art and Architecture from the Norwegian Council for Independent Art, and the 2014 New York Arts Prize. She served as a judge on the Stage Beauty and the Beast crew, the Presentation at the 57th Venice Biennale, and the judging panel for the 2014 Oxford Arts Festival. She curated the inaugural Liverpool Biennial, and was a judge in the 2015 Rodin Prize for Visual Artists. Biennials: Venice and the Art MarketJuliana EngbergPreviousNextBiennials: Venice and the Art Market [1] Venice Biennale, 2015. Photo courtesy of the Biennale.PreviousNextVenice Bi</text>
    <text>possibility of developing an experimental system, and thus of influencing the trajectory of art’s social consequences, is a crucial first step in the long process of democratising knowledge.Download this article as PDFMarina FokidisMarina Fokidis is an artist and researcher whose work is centred around the social and political dimensions of digital culture and media. Her artworks utilise technology to interrogate the normative forms of visual culture and media consumption, and to propose creative alternatives. She is a co-founder of the Centre for a Provincial Perspective (2011-2015) and the Palestinian Pavilion (2014-2015). Her work has also been featured in the New York Times, Washington Post, Le Monde diplomatique, Le Monde diplomatique d&quot;uté de vivre, Le Parisien de l’image, Le Monde diplomatique  (Paris), Le Parisien de l’image (Paris), Le Monde diplomatique  (Melbourne), Le Monde  (Paris) and many other publications around the world. She is the recipient of several awards and is a member of the Board of Trustees of the Museum of Modern Art, where she has been a trustee since 2010. She is a member of the Advisory Board of the New Art Gallery, and the Advisory Board of the Museum of Contemporary Art, where she has been a trustee since 2011.Fokidis: I arrived in New York in 2010 as a curator and consultant, working alongside Jacob Lund and Tina L. Friermann on a multi-disciplinary project that combined art, architecture and urban development. The project was entitled  Why NYC? Why LA? The former was the curator for the New York Museum of Modern Art, and the latter was the recipient of the Cooper Hewitt Prize for Architecture, presented in her honor at the Museum of Modern Art, New York, 2015. Why NYC? Why LA? explores the history of new urbanisation in the boroughs of Manhattan and Brooklyn.</text>
    <text>possibility of developing an experimental system, and thus of influencing the trajectory of art’s social consequences, is a crucial first step in the long process of democratising knowledge.Download this article as PDFMarina FokidisMarina Fokidis is an artist and researcher whose work is centred around the social and political dimensions of digital culture and media. Her artworks utilise technology to interrogate the normative forms of visual culture and media consumption, and to propose creative alternatives. She is a co-founder of the Centre for a Provincial Perspective (2011-2015) and the Palestinian Pavilion (2014-2015). Her work has also been featured in the New York Times, Washington Post, Le Monde diplomatique, Le Monde diplomatique d&quot;uté de vivre, Le Parisien de l’image, Le Monde diplomatique  (Paris), Le Parisien de l’image (Paris), Le Monde diplomatique  (Melbourne), Le Monde  (Paris), Le Parisien de l’image (Melbourne), Le Monde (Paris), Le Monde  (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (Paris), Le Monde (</text>
    <text>possibility of developing an experimental system, and thus of influencing the trajectory of art’s social consequences, is a crucial first step in the long process of democratising knowledge.Download this article as PDFMarina FokidisMarina Fokidis is an artist and researcher whose work is centred around the social and political dimensions of digital culture and media. Her artworks utilise technology to interrogate the normative forms of visual culture and media consumption, and to propose creative alternatives. She is a co-founder of the Centre for a Provincial Perspective (2011-2015) and the Palestinian Pavilion (2014-2015). Her work has also been featured in the New York Times, Washington Post, Le Monde diplomatique, Le Monde diplomatique d&quot;uté de vivre, Le Parisien de l’image, Le Monde diplomatique  (Paris), Le Parisien de l’image (Paris), Le Monde diplomatique  (Melbourne), Le Monde  (Paris), Le Monde (Geneva), Le Monde (Basel), Le Monde (Jerusalem), Le Monde (London) and Le Monde (New York). She is a co-author of the forthcoming book, ‘Technologies: The Enduring Imagined Space’ (2017), to be published by Pluto Press.Founded and co-founded in 2014, Centre Pompidou is a collaborative art project that seeks to bring together the diverse elements of the Pompidou Palace, its surroundings, and the surrounding region – a setting for a city-wide exhibition. In 2015, it was the site of the installation of a sculpture by American artist Ron Hopper, ‘Satanism at the Pompidou Palace’, which was subsequently covered up. In 2016, a documentary film project about the installation, entitled ‘8 AM’s Joseph Weisberg, interviewed several artists, including Dont Rhine, Joystick,</text>
    <text>possibility of developing an experimental system, and thus of influencing the trajectory of art’s social consequences, is a crucial first step in the long process of democratising knowledge.Download this article as PDFMarina FokidisMarina Fokidis is an artist and researcher whose work is centred around the social and political dimensions of digital culture and media. Her artworks utilise technology to interrogate the normative forms of visual culture and media consumption, and to propose creative alternatives. She is a co-founder of the Centre for a Provincial Perspective (2011-2015) and the Palestinian Pavilion (2014-2015). Her work has also been featured in the New York Times, Washington Post, Le Monde diplomatique, Le Monde diplomatique d&quot;uté de vivre, Le Parisien de l’image, Le Monde diplomatique  (Paris), Le Parisien de l’image (Paris), Le Monde diplomatique  (Melbourne), Le Monde  (Paris), Le Monde (Basel), Le Monde (Geneva), Le Monde (London), Le Monde (New York), Le Monde (Recreation Ground, London), Le Monde (Vertigo, Glasgow), Le Monde (Vertigo, New York), Le Monde (Vauxhall, Dublin) and Tate Modern. She has also been a lecturer in Communication Studies at Goldsmiths College, London University, where he taught for several years, and is currently completing a PhD. in the department of Communication Studies at Goldsmiths College, London University, where he taught for several years, and is now completing a Masters in the department of Visual Cultures, Culture and Society.Marina Fokidis is an artist and researcher whose work has been featured in numerous books, magazines, books from around the world, and most recently in the New York Times Magazine. She is the recipient of several awards, including the 2014 Royal Danish Academy of Arts Prize for</text>
    <text>possibility of developing an experimental system, and thus of influencing the trajectory of art’s social consequences, is a crucial first step in the long process of democratising knowledge.Download this article as PDFMarina FokidisMarina Fokidis is an artist and researcher whose work is centred around the social and political dimensions of digital culture and media. Her artworks utilise technology to interrogate the normative forms of visual culture and media consumption, and to propose creative alternatives. She is a co-founder of the Centre for a Provincial Perspective (2011-2015) and the Palestinian Pavilion (2014-2015). Her work has also been featured in the New York Times, Washington Post, Le Monde diplomatique, Le Monde diplomatique d&quot;uté de vivre, Le Parisien de l’image, Le Monde diplomatique  (Paris), Le Parisien de l’image (Paris), Le Monde diplomatique  (Melbourne), Le Monde  (Melbourne), Le Monde (Paris), Le Monde (Vancouver), Le Monde  (Paris), Le Monde (London), Le Monde (New York), Le Monde (Amsterdam) and Le Monde (Berlin). Marina Fokidis is the Curator of Open Access Art at the Venice Biennale. She has a particular interest in the increasingly prominent role of artists and artistic communities in shaping public perception and in terms of the distribution of wealth and power. Her artworks have been commissioned by foundations, governments, civic organisations, arts organisations and grassroots artists' organisations worldwide. She has received a host of accolades and recognitions for her work, including most recently the 2016 Oxford Dictionaries Short Form Award for Best Embarrassment and Best Commercial Artwork, and has been an adopted son by the UN Human Rights Committee.In her free time, Marina enjoys gardening, travelling, travelling  and travelling  again</text>
    <text>possibility of developing an experimental system, and thus of influencing the trajectory of art’s social consequences, is a crucial first step in the long process of democratising knowledge.Download this article as PDFMarina FokidisMarina Fokidis is an artist and researcher whose work is centred around the social and political dimensions of digital culture and media. Her artworks utilise technology to interrogate the normative forms of visual culture and media consumption, and to propose creative alternatives. She is a co-founder of the Centre for a Provincial Perspective (2011-2015) and the Palestinian Pavilion (2014-2015). Her work has also been featured in the New York Times, Washington Post, Le Monde diplomatique, Le Monde diplomatique d&quot;uté de vivre, Le Parisien de l’image, Le Monde diplomatique  (Paris), Le Parisien de l’image (Paris), Le Monde diplomatique  (Melbourne), Le Monde  (Paris) and many other publications around the world. In 2015 she worked with the Arab Arts Museums Association (AA), United Arab Emirates to establish the Museum of Digital Culture and a series of exhibition spaces in the UAE and Palestine. Fokidis is an adjunct professor at Columbia University, New York, USA, and the Fletcher School of Law and Diplomacy, Tufts University, Boston, USA, and was previously the assistant professor at the Maxwell School of Citizenship and Public Affairs, Columbia University, New York, USA.Her publications include the award-winning book Systemics, or Cybernetics (2012), co-authored with Stefan Collini with Nick Harvey and others, with Jim Clark and others, and the recently published book The Social Life of Multi-Systems (2015). She has been the recipient of various awards and has lectured on the social and digital aspects of information and data technologies.Biennial Alias:  	Al-Qomais/Al-Mouad</text>
    <text>possibility of developing an experimental system, and thus of influencing the trajectory of art’s social consequences, is a crucial first step in the long process of democratising knowledge.Download this article as PDFMarina FokidisMarina Fokidis is an artist and researcher whose work is centred around the social and political dimensions of digital culture and media. Her artworks utilise technology to interrogate the normative forms of visual culture and media consumption, and to propose creative alternatives. She is a co-founder of the Centre for a Provincial Perspective (2011-2015) and the Palestinian Pavilion (2014-2015). Her work has also been featured in the New York Times, Washington Post, Le Monde diplomatique, Le Monde diplomatique d&quot;uté de vivre, Le Parisien de l’image, Le Monde diplomatique  (Paris), Le Parisien de l’image (Paris), Le Monde diplomatique  (Melbourne), Le Monde  (Paris), Frieze, Tom Vanderbilt (Paris), New York Times, Wall Street Journal, New York Daily News, New York Magazine, Wall Street Journal, New York Observer, New York Radio New York Public Access, New York Public Access Radio, New York Public Access TV, New York Public Access Television, New York Public Access Radio, New York Public Access TV, New York Public Access Radio, New York Public Access TV, New York Public Access Television, New York Public Access Radio, New York Public Access Radio, New York Public Access TV, New York Public Access Television, New York Public Access Radio, New York Public Access Radio, New York Public Access TV, New York Public Access Radio, New York Public Access TV, New York Public Access Radio, New York Public Access Radio, New York Public Access TV, New York Public Access Radio, New York Public Access TV, New York Public Access Radio, New York Public Access TV, New York Public Access Radio, New York Public Access TV, New</text>
  </text>
  <text>
    possibility of developing an experimental system for monitoring and controlling moths and other insects’s flight was theoretical at best. The first person accounts of these new found technologies derive from the writings of a select few, selected through a process including personal interviews. Of particular interest is the account of Rudolf Steiner (1879–1971), an English zoologist and entomologist who proposed the idea of the ‘double seam’ in describing the design of two-dimensional insect wings. In his book ‘A Voyage Among the Stars’, Steiner described how the wing was formed:The microscopic object, which is the fragment of paper now adhering to the surface of the epidermis, is a double seam: it overlaps the epidermis, drawing it into a vortex, and at the same time it separates the epidermis from the periphery of the epidermis, thereby permitting the passage of a microscopic cilia.’[13] Steiner also proposed that the double
    <text>possibility of developing an experimental system for monitoring and controlling moths and other insects’s flight was theoretical at best. The first person accounts of these new found technologies derive from the writings of a select few, selected through a process including personal interviews. Of particular interest is the account of Rudolf Steiner (1879–1971), an English zoologist and entomologist who proposed the idea of the ‘double seam’ in describing the design of two-dimensional insect wings. In his book ‘A Voyage Among the Stars’, Steiner described how the wing was formed:The microscopic object, which is the fragment of paper now adhering to the surface of the epidermis, is a double seam: it overlaps the epidermis, drawing it into a vortex, and at the same time it separates the epidermis from the periphery of the epidermis, thereby permitting the passage of a microscopic cilia.’[13] Steiner also proposed that the double seam was a novel and imaginative device that served to separate the epithelial from the dermis, and that its extraordinary properties enabled it to do this.[14] Some of these ideas are still widely held beliefs, while others are rejected as untenable. Nevertheless, in the context of contemporary engineering and theoretical biology, these ideas are beginning to be taken seriously by both the public and academic worlds.In this context, the double seam seems particularly significant, as it serves to separate the fluid from the solid at the interface between the two materials. It is a device that is both lightweight and strong, yet it is also a device that is both predictable and rigid. In other words, it is both subject to physical and non-linear forces. Inventing Immaterial Technologies, a new thinking on the interface that was presented at the conference in Liverpool, UK, in October 2016, Hari Sevgi and Amber Dean, inventors of the virtual interface, demonstrate how a single interface element – a pattern</text>
    <text>possibility of developing an experimental system for monitoring and controlling moths and other insects’s flight was theoretical at best. The first person accounts of these new found technologies derive from the writings of a select few, selected through a process including personal interviews. Of particular interest is the account of Rudolf Steiner (1879–1971), an English zoologist and entomologist who proposed the idea of the ‘double seam’ in describing the design of two-dimensional insect wings. In his book ‘A Voyage Among the Stars’, Steiner described how the wing was formed:The microscopic object, which is the fragment of paper now adhering to the surface of the epidermis, is a double seam: it overlaps the epidermis, drawing it into a vortex, and at the same time it separates the epidermis from the periphery of the epidermis, thereby permitting the passage of a microscopic cilia.’[13] Steiner also proposed that the double seam could be fabricated using melted lead and aluminium, as well as the venom of a pangolin and the thorns of an oak. He proposed that the outer surface of the material mimicked the structure of the insect wing, with pores similar to those of a microscopic eye, but with a gap of about a nanometre. These ideas were further developed by American anatomist John Graham Kerr (1843–1921), who is best known for his anatomical work on the molar. Kerr’s work is often characterised by his pursuit of structures with no name, for which no satisfactory alternative is known. Kerr believed that structures should be named so that their properties could be inferred, and so that they could be imitated.[14] In the early twentieth century, this type of name-calling was encouraged by both the public and academic professions, which placed great reliance on documentation of the ‘true nature’ of an artefact. In Britain, this came in</text>
    <text>possibility of developing an experimental system for monitoring and controlling moths and other insects’s flight was theoretical at best. The first person accounts of these new found technologies derive from the writings of a select few, selected through a process including personal interviews. Of particular interest is the account of Rudolf Steiner (1879–1971), an English zoologist and entomologist who proposed the idea of the ‘double seam’ in describing the design of two-dimensional insect wings. In his book ‘A Voyage Among the Stars’, Steiner described how the wing was formed:The microscopic object, which is the fragment of paper now adhering to the surface of the epidermis, is a double seam: it overlaps the epidermis, drawing it into a vortex, and at the same time it separates the epidermis from the periphery of the epidermis, thereby permitting the passage of a microscopic cilia.’[13] Steiner also proposed that the double seam was present in ‘all the known elements of the atom’, and that its presence was due to the fact that the periodic table contained six elements: copper, iron, nickel, silver and magnesium.’[14] These claims are highly speculative and do not stand up in the face of overwhelming scientific and historical evidence to the contrary, but they remain powerful testimony to the primacy of ideology, individualism, and individualism in the service of economic growth and political power.As an example of how ideology is fused with economic growth and political power is the case for Margaret Thatcher in her attacks on National Action (in March 1984), National Review (in March 1986), and the Battle of Cable Street in May 1985. In the former two-day riot, over 1,000 people were killed, many of them communists, and over a thousand others were injured. Many prominent figures in the labour movement were arrested and imprisoned, but the government responded by outlawing trade unionism and</text>
    <text>possibility of developing an experimental system for monitoring and controlling moths and other insects’s flight was theoretical at best. The first person accounts of these new found technologies derive from the writings of a select few, selected through a process including personal interviews. Of particular interest is the account of Rudolf Steiner (1879–1971), an English zoologist and entomologist who proposed the idea of the ‘double seam’ in describing the design of two-dimensional insect wings. In his book ‘A Voyage Among the Stars’, Steiner described how the wing was formed:The microscopic object, which is the fragment of paper now adhering to the surface of the epidermis, is a double seam: it overlaps the epidermis, drawing it into a vortex, and at the same time it separates the epidermis from the periphery of the epidermis, thereby permitting the passage of a microscopic cilia.’[13] Steiner also proposed that the double seam may be regarded as an epithelial interface, a node within the interface that carries liquid, gas and other constituents of the interface. In liquid, the liquids form a wadis (a gel or solid covering over the surface of the material), which in turn forms a wadim (a fold or surface that separates the liquid from the grate or other interface elements). The wadim acts as a preservative, insulating the interface from moisture and other environmental factors. The concept of encapsulated interfaces is gaining currency in the scientific and engineering literature, suggesting that the interface is both transparent and highly specific – capable of sensing both liquid and solid–forming agents and modulating those agents to achieve specific ends. Aspects of the interface that are not liquid, such as the shape of the arrow pointing to the interface, are assumed to be solid–forming.  In vernacular parlance, a circumlocutory design is one that is difficult to alter once formed. If an interface</text>
    <text>possibility of developing an experimental system for monitoring and controlling moths and other insects’s flight was theoretical at best. The first person accounts of these new found technologies derive from the writings of a select few, selected through a process including personal interviews. Of particular interest is the account of Rudolf Steiner (1879–1971), an English zoologist and entomologist who proposed the idea of the ‘double seam’ in describing the design of two-dimensional insect wings. In his book ‘A Voyage Among the Stars’, Steiner described how the wing was formed:The microscopic object, which is the fragment of paper now adhering to the surface of the epidermis, is a double seam: it overlaps the epidermis, drawing it into a vortex, and at the same time it separates the epidermis from the periphery of the epidermis, thereby permitting the passage of a microscopic cilia.’[13] Steiner also proposed that the double seam was a special twist of epidermal shear that enhanced its mechanical advantage over its non-thermal equivalent, which was the way that the epidermis itself was able to articulate the concept of fluidity. The idea that the epidermis is able to articulate a volumetric concept through its physical surface structures seems novel in the biological sense, but it is the latest chapter in a scientific work that has seen researchers turn to entomology as a discipline to investigate the physical mechanisms of movement. In recent years, entomology has been a naturalised field, a branch of the natural sciences that has been drawn to explain physical phenomena through the observation and investigation of organisms with exogenous material in their bodies. For example, researchers have turned to studying the microscopic movement of blood vessels in the body as a way to understand how and why blood vessels in the face of pressure build up – a phenomenon known as hemorrhage – and how these mechanisms influence disease.  [1] The</text>
    <text>possibility of developing an experimental system for monitoring and controlling moths and other insects’s flight was theoretical at best. The first person accounts of these new found technologies derive from the writings of a select few, selected through a process including personal interviews. Of particular interest is the account of Rudolf Steiner (1879–1971), an English zoologist and entomologist who proposed the idea of the ‘double seam’ in describing the design of two-dimensional insect wings. In his book ‘A Voyage Among the Stars’, Steiner described how the wing was formed:The microscopic object, which is the fragment of paper now adhering to the surface of the epidermis, is a double seam: it overlaps the epidermis, drawing it into a vortex, and at the same time it separates the epidermis from the periphery of the epidermis, thereby permitting the passage of a microscopic cilia.’[13] Steiner also proposed that the double seam may be seen as a solution to the double weight problem faced by the human body: the body of the traveler weighing it down with massive amounts of matter, but with little resistance to the unsteadiness caused by the over-all mass of the material covering it. This double loading problem was initially posed as a problem for nuclear power, but more and more of the technological problems facing the biosphere were solved with the advent of solid waste management. The developing body of technology was strong enough to handle the double loading problem, while the creator of the idea was not so strong as to doubt his or her abilities. This attitude was illustrated by the work of American physicist John Graham Kerr (1869–1957), who proposed that the human body was composed of two major constituent parts: the brain, which controls the body’s actions, and the muscles, which enable the body to perform actions independent of the brain. Kerr presented his ideas in the wake of The Metainterface –</text>
    <text>possibility of developing an experimental system for monitoring and controlling moths and other insects’s flight was theoretical at best. The first person accounts of these new found technologies derive from the writings of a select few, selected through a process including personal interviews. Of particular interest is the account of Rudolf Steiner (1879–1971), an English zoologist and entomologist who proposed the idea of the ‘double seam’ in describing the design of two-dimensional insect wings. In his book ‘A Voyage Among the Stars’, Steiner described how the wing was formed:The microscopic object, which is the fragment of paper now adhering to the surface of the epidermis, is a double seam: it overlaps the epidermis, drawing it into a vortex, and at the same time it separates the epidermis from the periphery of the epidermis, thereby permitting the passage of a microscopic cilia.’[13] Steiner also proposed that the double seam could be fabricated using a chemical reaction between two materials and a solid–aldehyde compound. An ideal candidate for the double seam material is a chemically inert substance that can be readily incorporated into a material and readily broken down to produce its constituent parts. A good candidate for the biomaterial would have properties similar to those of graphene, but be stretchable and thermoplastic. The double seam was first proposed in the mid-1920s by Scottish biologist Patrick Geddes and is based on the idea that materials with unusual optical properties can be made of two or more constituents that interact in a positively charged bond. When subjected to a chemical reaction, the constituent parts of materials with optical properties can be formed. When a positive charge is applied to one of the constituent parts, the bond breaks and the part can be formed again. When the bond in the material is broken, the second bond cannot form and the part can not be formed, an effect known as optical ab initio. The discovery of</text>
    <text>possibility of developing an experimental system for monitoring and controlling moths and other insects’s flight was theoretical at best. The first person accounts of these new found technologies derive from the writings of a select few, selected through a process including personal interviews. Of particular interest is the account of Rudolf Steiner (1879–1971), an English zoologist and entomologist who proposed the idea of the ‘double seam’ in describing the design of two-dimensional insect wings. In his book ‘A Voyage Among the Stars’, Steiner described how the wing was formed:The microscopic object, which is the fragment of paper now adhering to the surface of the epidermis, is a double seam: it overlaps the epidermis, drawing it into a vortex, and at the same time it separates the epidermis from the periphery of the epidermis, thereby permitting the passage of a microscopic cilia.’[13] Steiner also proposed that the double seam was a novel and visionary discovery, and that the technology had been lying dormant for sometime. When the epidermis and the void are drawn into a vortex, the appearance of a new shape is possible, but usually this new look has a metallic tinge. In the wake of these two technological revolutions in just the last twenty years, a number of new design professions have emerged, including entomology and nano-architecture.What is at Stake in the Social Web? 	As recently as 2015, social networking was still predominantly understood as a global phenomenon. 	However, the explosion of data feeds and constant connectivity of people across the globe has democratised access to information and created new kinds of social encounters. 	Social networking has democratised access to data, allowing one to gather information about places and people that otherwise are difficult to imagine, inferring facts about experiences that otherwise are uncomplicated. 	In  	The Social Network, Susanne Heide</text>
    <text>possibility of developing an experimental system for monitoring and controlling moths and other insects’s flight was theoretical at best. The first person accounts of these new found technologies derive from the writings of a select few, selected through a process including personal interviews. Of particular interest is the account of Rudolf Steiner (1879–1971), an English zoologist and entomologist who proposed the idea of the ‘double seam’ in describing the design of two-dimensional insect wings. In his book ‘A Voyage Among the Stars’, Steiner described how the wing was formed:The microscopic object, which is the fragment of paper now adhering to the surface of the epidermis, is a double seam: it overlaps the epidermis, drawing it into a vortex, and at the same time it separates the epidermis from the periphery of the epidermis, thereby permitting the passage of a microscopic cilia.’[13] Steiner also proposed that the double seam was an invention of the aerospace industry, and that its anomalous properties were the work of advanced artificial intelligence.[14] One can only imagine what kinds of infrastructural analogues these technologies might be today, if not far more dystopian and dehumanising.As robotics becomes ever more advanced and pervasive, it is imperative that we learn how to talk about and theorise these things that still pose as technologies but actually have a kind of inherent meaning and purpose. This cannot be achieved by recourseing to technical means alone, and we need to create imaginative means of conceptualising and talking about these things that actually are. I suggest that we consider the futurist reading of things that is becoming increasingly popular in the artscrolls and social media bubbles: that is, things like healthcare, education and cities. The fact that most of us don’t yet have access to these things, or don’t yet know how they work or how to use them is a symptom of</text>
    <text>possibility of developing an experimental system for monitoring and controlling moths and other insects’s flight was theoretical at best. The first person accounts of these new found technologies derive from the writings of a select few, selected through a process including personal interviews. Of particular interest is the account of Rudolf Steiner (1879–1971), an English zoologist and entomologist who proposed the idea of the ‘double seam’ in describing the design of two-dimensional insect wings. In his book ‘A Voyage Among the Stars’, Steiner described how the wing was formed:The microscopic object, which is the fragment of paper now adhering to the surface of the epidermis, is a double seam: it overlaps the epidermis, drawing it into a vortex, and at the same time it separates the epidermis from the periphery of the epidermis, thereby permitting the passage of a microscopic cilia.’[13] Steiner also proposed that the double seam was an ingenious way to simultaneously cover and separate two fundamentally different domains of the body – the epidermis and the dermis – which are distinct in their structure but in common in their function. In the epidermis, the double seam acts as a great damper, actively stirring in and drawing out moisture from the skin, lubricating it and suppressing its friction. In the second instance, the double seam acts as a catalyst, directly activating the skin's extracellular matrix – the extracellular matrix comprising connective tissue, such as cartilage, adipose tissue and bone – which in turn acts as a source of the moisture necessary for epidermal growth.[14] These ideas are the foundation of what is sometimes referred to as the ‘trans-accent’ or transdermal circulation of moisture. The idea of a ‘dermis cells’ or double layer of cells or 'dermis layer’ has been proposed to explain the mechanical and electrical</text>
  </text>
  <text>
    The next Liverpool Biennial will be held in early 2019 at the former Cricket Ground in Anfield, with work continuing apace there as usual. Liverpool Biennial 2018 is planned to be the first in a planned series of exhibitions that year, and the first to be delivered directly to fans and press. The press is invited to become members of the press. Liverpool Biennial 2018 is committed to the production of public knowledge of Liverpool through the commissioning of publics: the public sphere, and its various forms – the public good, and in this case, the public good. The press is to be made up of anyone who might be interested in learning more about the city or might just want to see something cool. Liverpool Biennial 2018 is  designed with an open access policy, so that anyone can explore and learn from the exhibition. The exhibition will be freely available online, and the Liverpool Journal will host a free online publication commissioning project, which is designed to engage with and produce knowledge of the
    <text>The next Liverpool Biennial will be held in early 2019 at the former Cricket Ground in Anfield, with work continuing apace there as usual. Liverpool Biennial 2018 is planned to be the first in a planned series of exhibitions that year, and the first to be delivered directly to fans and press. The press is invited to become members of the press. Liverpool Biennial 2018 is committed to the production of public knowledge of Liverpool through the commissioning of publics: the public sphere, and its various forms – the public good, and in this case, the public good. The press is to be made up of anyone who might be interested in learning more about the city or might just want to see something cool. Liverpool Biennial 2018 is  designed with an open access policy, so that anyone can explore and learn from the exhibition. The exhibition will be freely available online, and the Liverpool Journal will host a free online publication commissioning project, which is designed to engage with and produce knowledge of the city not through traditional news outlets, but through curated online publications. Liverpool Biennial 2018 will be the first in a planned series of exhibitions that year. The exhibitions themselves will be produced in cooperation with The Academia Diacritica (Academia Diacritica) in Rome, and the Italian Cultural Association (Consejo della Antropologia e Comunità di Roma) in Venice, and will be published simultaneously in both in 2019 and 2020. The narratives of the works will be produced in coordination with the Journal in Rome and the Italian Cultural Association in Venice. The Academia Diacritica in Rome will publish the works in the coming weeks, while the Italian Cultural Association in Venice will publish their work in December. The public sphere, then, can be thought of as the final frontier of the social. It is a socialised space that has come a long way from its origins in Confucius Institutes</text>
    <text>The next Liverpool Biennial will be held in early 2019 at the former Cricket Ground in Anfield, with work continuing apace there as usual. Liverpool Biennial 2018 is planned to be the first in a planned series of exhibitions that year, and the first to be delivered directly to fans and press. The press is invited to become members of the press. Liverpool Biennial 2018 is committed to the production of public knowledge of Liverpool through the commissioning of publics: the public sphere, and its various forms – the public good, and in this case, the public good. The press is to be made up of anyone who might be interested in learning more about the city or might just want to see something cool. Liverpool Biennial 2018 is  designed with an open access policy, so that anyone can explore and learn from the exhibition. The exhibition will be freely available online, and the Liverpool Journal will host a free online publication commissioning project, which is designed to engage with and produce knowledge of the city through creative media strategies. Liverpool Biennial 2018 is a collaboration between Liverpool Biennial, New York, 2017, and Liverpool Phantom, New York, 2016. It was co-curated by Ron Waugh and Peter Carruthers, with Sally Tallant and Rosalind Franklin across a number of artist-curated exhibitions across the US and Europe. The Phantom project began in October 2016 and is continuing to this date. Liverpool Biennial 2018 will be public and un-scripted, with artists committing to work in the city only, creative agencies and arts organisations committed to supporting their work elsewhere in the country, and the city only, across different media platforms. The commissioning process for the biennial has started, and the first public demonstrations of the project are scheduled to take place in April. Liverpool Biennial 2018 will be open to the public on 10-11 April, and close on</text>
    <text>The next Liverpool Biennial will be held in early 2019 at the former Cricket Ground in Anfield, with work continuing apace there as usual. Liverpool Biennial 2018 is planned to be the first in a planned series of exhibitions that year, and the first to be delivered directly to fans and press. The press is invited to become members of the press. Liverpool Biennial 2018 is committed to the production of public knowledge of Liverpool through the commissioning of publics: the public sphere, and its various forms – the public good, and in this case, the public good. The press is to be made up of anyone who might be interested in learning more about the city or might just want to see something cool. Liverpool Biennial 2018 is  designed with an open access policy, so that anyone can explore and learn from the exhibition. The exhibition will be freely available online, and the Liverpool Journal will host a free online publication commissioning project, which is designed to engage with and produce knowledge of the city from within the exhibition. Liverpool Biennial 2018 is a collaboration between Liverpool Biennial, the Open School of Art, Liverpool College of Art and Design, and GATE International, a division of the Italian Cultural Centre, Italy. It is the largest and most ambitious of three exhibitions that GATE has organised in Liverpool in the last ten years, and the first from a non-traditional (school, arts centre) perspective. It is an important first step in the right direction, but more needs to be done. In order to generate knowledge and knowledge sharing in the right places, the commissioning agency GATE needs to be empowered with the necessary skills and resources to carry out a number of the commissioning priorities outlined in the Liverpool Biennial 2018 strategic plan. GATE Liverpool General Manager, GATE Liverpool, commented: &quot;This is a very exciting time to be working with GATE, both on the day-to-day commissioning and</text>
    <text>The next Liverpool Biennial will be held in early 2019 at the former Cricket Ground in Anfield, with work continuing apace there as usual. Liverpool Biennial 2018 is planned to be the first in a planned series of exhibitions that year, and the first to be delivered directly to fans and press. The press is invited to become members of the press. Liverpool Biennial 2018 is committed to the production of public knowledge of Liverpool through the commissioning of publics: the public sphere, and its various forms – the public good, and in this case, the public good. The press is to be made up of anyone who might be interested in learning more about the city or might just want to see something cool. Liverpool Biennial 2018 is  designed with an open access policy, so that anyone can explore and learn from the exhibition. The exhibition will be freely available online, and the Liverpool Journal will host a free online publication commissioning project, which is designed to engage with and produce knowledge of the city’s burgeoning arts and cultural scene. The project is a collaboration between Liverpool Biennial, the Liverpool School of Art, the London based Chisenhale Gallery, the Liverpool Paddington Gallery, and Welsh Streets Gallery, as well as Welsh Streets Gallery and Glasgow-based Birds of Paradise Gallery. The project was initiated by the Liverpool School of Art, the London based Chisenhale Gallery, and Welsh Streets Gallery, with invaluable support from the Leverhulme Trust and Arts Council. Liverpool Biennial 2018 will be held at the Barbican, London, October 2018-March 2019. 	PreviousNextLiverpool Biennial 2018 will be held at the Barbican, London, October 2018-March 2019. 	[1] 	http://www.ljmu.ac.uk/ljmu/search/term/view/detail</text>
    <text>The next Liverpool Biennial will be held in early 2019 at the former Cricket Ground in Anfield, with work continuing apace there as usual. Liverpool Biennial 2018 is planned to be the first in a planned series of exhibitions that year, and the first to be delivered directly to fans and press. The press is invited to become members of the press. Liverpool Biennial 2018 is committed to the production of public knowledge of Liverpool through the commissioning of publics: the public sphere, and its various forms – the public good, and in this case, the public good. The press is to be made up of anyone who might be interested in learning more about the city or might just want to see something cool. Liverpool Biennial 2018 is  designed with an open access policy, so that anyone can explore and learn from the exhibition. The exhibition will be freely available online, and the Liverpool Journal will host a free online publication commissioning project, which is designed to engage with and produce knowledge of the city through art in a non-traditional or critical way. The online commissioning project will produce knowledge about the city through art in a way that the paper commissioning project cannot, while complementing each other in their curatorial, artistic and educational approaches. The commissioning process for both the online and the paper versions of Liverpool Biennial 2018 will be based in partnership with The Liverpool Sun newspaper, and Welsh Streets publisher Welsh Streets Open Space Partnership. Liverpool Biennial 2018 is an opportunity to be a part of the creative process, to participate in a learning project, to make art (or just about art) in a public space and to learn something from it. The exhibition itself is a set of activities that happens to be an artistic exercise in making the city, and people in it. The idea is to create a space in which people can come, do a creative thing, learn something new and create a share of the public good. Assembled by a Curatorial</text>
    <text>The next Liverpool Biennial will be held in early 2019 at the former Cricket Ground in Anfield, with work continuing apace there as usual. Liverpool Biennial 2018 is planned to be the first in a planned series of exhibitions that year, and the first to be delivered directly to fans and press. The press is invited to become members of the press. Liverpool Biennial 2018 is committed to the production of public knowledge of Liverpool through the commissioning of publics: the public sphere, and its various forms – the public good, and in this case, the public good. The press is to be made up of anyone who might be interested in learning more about the city or might just want to see something cool. Liverpool Biennial 2018 is  designed with an open access policy, so that anyone can explore and learn from the exhibition. The exhibition will be freely available online, and the Liverpool Journal will host a free online publication commissioning project, which is designed to engage with and produce knowledge of the city from outside the metropolitan centres. The journal will be a main public forum for this knowledge to come about, and the exhibition will be a place to foster and accelerate this knowledge for future curators, artists, writers and others interested in the city. The journal will also publish curatorial thoughts and proposals from curators, artists, journalists and others interested in the city. The curatorial and artistic initiative Dolphin Curators Forum is an interdisciplinary, multi-disciplinary group of curators, artists, writers and others established in 2015 to engage with the politics and social practices of 'outside' in a range of art, educational and community settings. The 2016 Dolphin Curators Forum was focused on the city as a site for art, design, research, knowledge and exchange, and the ideas of ‘contemporary art’ and ‘social enterprise’ in an integrated context. The group’dissented from the prevailing artistic and curatorial focus on the city as a</text>
    <text>The next Liverpool Biennial will be held in early 2019 at the former Cricket Ground in Anfield, with work continuing apace there as usual. Liverpool Biennial 2018 is planned to be the first in a planned series of exhibitions that year, and the first to be delivered directly to fans and press. The press is invited to become members of the press. Liverpool Biennial 2018 is committed to the production of public knowledge of Liverpool through the commissioning of publics: the public sphere, and its various forms – the public good, and in this case, the public good. The press is to be made up of anyone who might be interested in learning more about the city or might just want to see something cool. Liverpool Biennial 2018 is  designed with an open access policy, so that anyone can explore and learn from the exhibition. The exhibition will be freely available online, and the Liverpool Journal will host a free online publication commissioning project, which is designed to engage with and produce knowledge of the city through the exhibition. Liverpool Biennial 2018 will be held concurrently with  two other exhibitions in December 2019 and January 2020: one in Rome and the other in New York. The new exhibition format and open access policy will enable the introduction of several new visual styles and entices visitors away from the biennial's traditional tourist focus by offering a series of activities in and around the museum. The aim of the project is to create a public sphere of knowledge of the city, and creating a public realm of knowledge is a commission by the city, a function of making the city, and the press, who together form the city. The press is to be anything but a clientele of museums and galleries, and the museum is to be an ever expanding collection of temporary exhibitions. The key to Liverpool Biennial 2018 is a unity in diversity project, led by Oxford Dictionaries editor Rava Edo and VICE News Asia editor Suzanne</text>
    <text>The next Liverpool Biennial will be held in early 2019 at the former Cricket Ground in Anfield, with work continuing apace there as usual. Liverpool Biennial 2018 is planned to be the first in a planned series of exhibitions that year, and the first to be delivered directly to fans and press. The press is invited to become members of the press. Liverpool Biennial 2018 is committed to the production of public knowledge of Liverpool through the commissioning of publics: the public sphere, and its various forms – the public good, and in this case, the public good. The press is to be made up of anyone who might be interested in learning more about the city or might just want to see something cool. Liverpool Biennial 2018 is  designed with an open access policy, so that anyone can explore and learn from the exhibition. The exhibition will be freely available online, and the Liverpool Journal will host a free online publication commissioning project, which is designed to engage with and produce knowledge of the city through the materiality of place. The project will be published simultaneously in the online journal and the newspaper that came first in the US. Liverpool Biennial 2018 will be held at the Arsenale Di Tella, Rome, Italy. 9 July 2018 The Liverpool Biennial 2018 programme is produced through the joint venture between Liverpool Biennial, the FACT (Foundation of Art and Creative Strategies), and the Italian Museum, Rome. The Italian Museum is hosting the show in its current format, while the Liverpool Biennial is carrying on as usual. Liverpool Biennial 2018 will visit Italy more broadly, and the rest of Europe. 10 July 2018 Liverpool Biennial 2018 is underway, and is shaping up to be a really interesting show. It begins with a week of sessions in Rome, followed by a week in Venice. The Venice Biennale is the only major international biennale to have its origins in the Italian city, and the city is shaping up to be a</text>
    <text>The next Liverpool Biennial will be held in early 2019 at the former Cricket Ground in Anfield, with work continuing apace there as usual. Liverpool Biennial 2018 is planned to be the first in a planned series of exhibitions that year, and the first to be delivered directly to fans and press. The press is invited to become members of the press. Liverpool Biennial 2018 is committed to the production of public knowledge of Liverpool through the commissioning of publics: the public sphere, and its various forms – the public good, and in this case, the public good. The press is to be made up of anyone who might be interested in learning more about the city or might just want to see something cool. Liverpool Biennial 2018 is  designed with an open access policy, so that anyone can explore and learn from the exhibition. The exhibition will be freely available online, and the Liverpool Journal will host a free online publication commissioning project, which is designed to engage with and produce knowledge of the city from within the Biennial itself. Liverpool Biennial 2018 is an extension of the public school​s mission to educate and inform, and the journal project reflects this in both its reporting and its content.  The journal will be written, co-curated, and co-published with the Liverpool Journal on a curatorial team that includes: Polly Brannan, Tim Jeeves, Peter Carr, Tom Lox, Polly Waugh, Meehan Crist, Lisa Latham, Suzi Golland, Aaron Rauch, Maria Lula, Teresa McVerry, Jane Rankin, Meehan Crist and Sarah Curtis with additional curatorial colleagues. The subject line Public Art 2018 is a great start for a journal, but it doesn’t really map, nor will it ever be a complete report of events in the</text>
    <text>The next Liverpool Biennial will be held in early 2019 at the former Cricket Ground in Anfield, with work continuing apace there as usual. Liverpool Biennial 2018 is planned to be the first in a planned series of exhibitions that year, and the first to be delivered directly to fans and press. The press is invited to become members of the press. Liverpool Biennial 2018 is committed to the production of public knowledge of Liverpool through the commissioning of publics: the public sphere, and its various forms – the public good, and in this case, the public good. The press is to be made up of anyone who might be interested in learning more about the city or might just want to see something cool. Liverpool Biennial 2018 is  designed with an open access policy, so that anyone can explore and learn from the exhibition. The exhibition will be freely available online, and the Liverpool Journal will host a free online publication commissioning project, which is designed to engage with and produce knowledge of the city from the press. Liverpool Biennial 2018 is a collaboration between Liverpool Biennial, (with Liverpool School of Arts and Design), Liverpool Dock University, and Welsh Streets Gallery, Liverpool. It is the first ever instalment in a planned three-year instalment, and the inaugural instalment in a new cycle of free online publications. The instalments are completed with a public artistic rendering of the district in Liverpool Dock, Lloyd Hotel, and the nearby West End. The journal will be a place to produce knowledge and discuss ideas from the instalment, and the new curatorial direction of the institution.The journal will be a place to share knowledge and ideas, and curate those with whom we collaborate, and to ask probing questions about what it means to be a curator in a city as diverse as Liverpool, and to work closely with those who are not. It</text>
  </text>
  <text>
    The next Liverpool Biennial would be impossible without the amazing work developing behind the scenes. The Southampton Biennial, which was inspired by the DNA of the Liverpool Biennial, would be a perfect parallel. We already know that Southampton will host the 2030 World Cup. The Biennial’s current goal is to transform the city into a ‘world-class’ tourist destination. It is estimated that there are currently over 200,000 tourism projects in the city, and 40,000 ‘creative industries’ operating in the city. These range from architectural interventions to socially engaged practices such as ‘Champs’ nightclubs and ‘Basel 4’ design schools, which offer a ‘quick and easy fix’ to city problems. The city is a spectacular site for tourism, and the fact that it is currently experiencing a housing crisis makes it an ideal site for a new wave of ‘hot spot’ hotels and development. The fact that it is located in the heart of
    <text>The next Liverpool Biennial would be impossible without the amazing work developing behind the scenes. The Southampton Biennial, which was inspired by the DNA of the Liverpool Biennial, would be a perfect parallel. We already know that Southampton will host the 2030 World Cup. The Biennial’s current goal is to transform the city into a ‘world-class’ tourist destination. It is estimated that there are currently over 200,000 tourism projects in the city, and 40,000 ‘creative industries’ operating in the city. These range from architectural interventions to socially engaged practices such as ‘Champs’ nightclubs and ‘Basel 4’ design schools, which offer a ‘quick and easy fix’ to city problems. The city is a spectacular site for tourism, and the fact that it is currently experiencing a housing crisis makes it an ideal site for a new wave of ‘hot spot’ hotels and development. The fact that it is located in the heart of the city’s financial district makes it an ideal location for a new wave of ‘high-end’ residential towers. The fact that it is located in a major European city alongside other historic sites of the EU provides a perfect blend of globalisation and national identity, a perfect mixture. The fact that it is being developed by a non-profit organisation instead of for the elite in the city limits it in many ways, making it a strange and wonderful place. The fact that it is being developed by a woman over a project by a for-profit company makes it even more so.  The combination of the rarefied world of modern tourism and the mass-produced tourists card that is offered by the likes of Deloitte and PricewaterhouseCoopers makes for an enviable situation. It is easy to understand how the Biennial could maximise on this mix, turning what is happening in the city into a profitable opportunity. How else could it fund its research and development,</text>
    <text>The next Liverpool Biennial would be impossible without the amazing work developing behind the scenes. The Southampton Biennial, which was inspired by the DNA of the Liverpool Biennial, would be a perfect parallel. We already know that Southampton will host the 2030 World Cup. The Biennial’s current goal is to transform the city into a ‘world-class’ tourist destination. It is estimated that there are currently over 200,000 tourism projects in the city, and 40,000 ‘creative industries’ operating in the city. These range from architectural interventions to socially engaged practices such as ‘Champs’ nightclubs and ‘Basel 4’ design schools, which offer a ‘quick and easy fix’ to city problems. The city is a spectacular site for tourism, and the fact that it is currently experiencing a housing crisis makes it an ideal site for a new wave of ‘hot spot’ hotels and development. The fact that it is located in the heart of the financial district makes it a perfect candidate for the new ‘hot spot’ – a destination that is no longer defined by the city as its centre but by the ‘capital of the world’.  The Biennial’s current focus is clearly New York, and the future city will be defined by its various representations of Manhattan – a dense metropolis of a million people on a periphery, surrounded by suburbs. The city will be a ‘global nexus of ideas’, according to a recent TED talk. At TED, Carolee A Hill, one of the speakers, spoke of the cultural power of the city. She said that the power of the cultural was to be found in the ‘scarce, fragile materials that it produced and displayed’. She cited Jane’s phrase: the more things are presented, the more they are imagined. TED talks tend to center on a single thought or idea, and the discursive potential of the city</text>
    <text>The next Liverpool Biennial would be impossible without the amazing work developing behind the scenes. The Southampton Biennial, which was inspired by the DNA of the Liverpool Biennial, would be a perfect parallel. We already know that Southampton will host the 2030 World Cup. The Biennial’s current goal is to transform the city into a ‘world-class’ tourist destination. It is estimated that there are currently over 200,000 tourism projects in the city, and 40,000 ‘creative industries’ operating in the city. These range from architectural interventions to socially engaged practices such as ‘Champs’ nightclubs and ‘Basel 4’ design schools, which offer a ‘quick and easy fix’ to city problems. The city is a spectacular site for tourism, and the fact that it is currently experiencing a housing crisis makes it an ideal site for a new wave of ‘hot spot’ hotels and development. The fact that it is located in the heart of Europe’s biggest economy and exports its surplus to a number of Asian countries makes it an ideal market for a number of Asian countries looking to establish ‘champion industries’. The Biennial has also made it clear that it intends to spend significant time in the city, developing a number of its proposed projects into ‘world-class’ tourist attractions. These may include ‘champions’ areas, areas with a unique combination of culture, history, architecture and landscape, places where the visitor can ‘walk amongst the trees, ‘spot a rare fish or bird’ and so forth. The Biennial has also stated that it intends to create a ‘culture of the city’ through ‘design afterthought’ projects such as ‘Basel 4’ and ‘Earth Hour’ which combine art, architecture, landscape and architecture to create ‘a shared space and a shared story’. This may include, but is</text>
    <text>The next Liverpool Biennial would be impossible without the amazing work developing behind the scenes. The Southampton Biennial, which was inspired by the DNA of the Liverpool Biennial, would be a perfect parallel. We already know that Southampton will host the 2030 World Cup. The Biennial’s current goal is to transform the city into a ‘world-class’ tourist destination. It is estimated that there are currently over 200,000 tourism projects in the city, and 40,000 ‘creative industries’ operating in the city. These range from architectural interventions to socially engaged practices such as ‘Champs’ nightclubs and ‘Basel 4’ design schools, which offer a ‘quick and easy fix’ to city problems. The city is a spectacular site for tourism, and the fact that it is currently experiencing a housing crisis makes it an ideal site for a new wave of ‘hot spot’ hotels and development. The fact that it is located in the heart of the city’s financial district also plays a part in the city’s attractiveness, since hotels are a major source of funding for local government services and support for tourism. The fact that the Biennial is being funded entirely by the City of London also plays a part in the city’s attractiveness, since the Biennial has a financial stake in the development of the area. The Biennial has also made it clear that it does not see itself as a city unto itself. It seeks to contribute to a future ‘city’ that it calls its own. This seems particularly relevant in a city such as Liverpool, where the historical divisions between city and country are still very much an issue.  The biennial has also made it clear that it does not see itself as a city unto itself. It seeks to contribute to a future ‘city’ that it calls its own. This seems particularly relevant in a city such as Liverpool, where the historical divisions between city and country</text>
    <text>The next Liverpool Biennial would be impossible without the amazing work developing behind the scenes. The Southampton Biennial, which was inspired by the DNA of the Liverpool Biennial, would be a perfect parallel. We already know that Southampton will host the 2030 World Cup. The Biennial’s current goal is to transform the city into a ‘world-class’ tourist destination. It is estimated that there are currently over 200,000 tourism projects in the city, and 40,000 ‘creative industries’ operating in the city. These range from architectural interventions to socially engaged practices such as ‘Champs’ nightclubs and ‘Basel 4’ design schools, which offer a ‘quick and easy fix’ to city problems. The city is a spectacular site for tourism, and the fact that it is currently experiencing a housing crisis makes it an ideal site for a new wave of ‘hot spot’ hotels and development. The fact that it is located in the heart of the financial district makes it an ideal location for the new ‘high street’ retail district, which will offer an ideal mix of residential, cultural and ‘high street’ products and services. This latter category will likely include a mix of restaurants and cafés, with the former likely to include a mix of cultural institutions and galleries. The cultural district will likely include a mix of high-end and low-end venues, with the ‘high street’ offering the best value and the ‘hipster’ offering the best mix of hidden and visible art. The fact that there are already over 200 cultural institutions in the area is a strong indication that this will be an easy conversion. The fact that this is a cultural district in the first place is testament to the power of cultural tourism. The biennial will almost certainly be driven by a curatorial hand, and the hand that will be most effective will likely be those with a financial stake in the area. Will it be</text>
    <text>The next Liverpool Biennial would be impossible without the amazing work developing behind the scenes. The Southampton Biennial, which was inspired by the DNA of the Liverpool Biennial, would be a perfect parallel. We already know that Southampton will host the 2030 World Cup. The Biennial’s current goal is to transform the city into a ‘world-class’ tourist destination. It is estimated that there are currently over 200,000 tourism projects in the city, and 40,000 ‘creative industries’ operating in the city. These range from architectural interventions to socially engaged practices such as ‘Champs’ nightclubs and ‘Basel 4’ design schools, which offer a ‘quick and easy fix’ to city problems. The city is a spectacular site for tourism, and the fact that it is currently experiencing a housing crisis makes it an ideal site for a new wave of ‘hot spot’ hotels and development. The fact that it is located in the heart of the financial district and has a global positioning system (GSM) that can be tracked makes it a great candidate for a new wave of hotels and GSM networks. Like Liverpool Biennial, Southampton Biennial will aim to achieve a balance between the past and the future, exploring the past (in a historical, ‘naturalistic’ way) and the present (in a more technological way). Like Liverpool Biennial, it will employ an artistic director, Sara De Bondt, whose work has been exhibited at the Venice Biennale, the London &amp; New York Philharmonic, and the Museum of Modern Art. Like Liverpool Biennial, it will be produced in collaboration with the New Art Gallery and the BBC.Like Liverpool Biennial, it will not be influenced by the art world or the fashion world. Like Liverpool Biennial, it will be produced in partnership with the BBC. A biennial is a ‘temporal subject matter that records the movement of historical times’,</text>
    <text>The next Liverpool Biennial would be impossible without the amazing work developing behind the scenes. The Southampton Biennial, which was inspired by the DNA of the Liverpool Biennial, would be a perfect parallel. We already know that Southampton will host the 2030 World Cup. The Biennial’s current goal is to transform the city into a ‘world-class’ tourist destination. It is estimated that there are currently over 200,000 tourism projects in the city, and 40,000 ‘creative industries’ operating in the city. These range from architectural interventions to socially engaged practices such as ‘Champs’ nightclubs and ‘Basel 4’ design schools, which offer a ‘quick and easy fix’ to city problems. The city is a spectacular site for tourism, and the fact that it is currently experiencing a housing crisis makes it an ideal site for a new wave of ‘hot spot’ hotels and development. The fact that it is located in the heart of the financial district, and has the world’s fifth largest GDP per head, as well as an impressive cultural scene thanks to its financial sector, biennial, and its position as the USA’s largest exporter of soft drinks, makes it an ideal location for a new wave of ‘high street stores’ and ‘mall chic’. The arrival of large numbers of tourists, and the consequent reduction in population, has greatly contributed to renovating the city. In the 1980s, the estimated annual tourist impact was around 260,000 people, of which about 70,000 were from outside the USA. Today, it is estimated that the impact is somewhere around 20 million. So the ‘high street’ is booming again, albeit in a different way than the Biennial or the other postmodern biennial styles, which tend to be accompanied by a transient ‘cultural tourism’ or ‘asset curfouse’ type of experience</text>
    <text>The next Liverpool Biennial would be impossible without the amazing work developing behind the scenes. The Southampton Biennial, which was inspired by the DNA of the Liverpool Biennial, would be a perfect parallel. We already know that Southampton will host the 2030 World Cup. The Biennial’s current goal is to transform the city into a ‘world-class’ tourist destination. It is estimated that there are currently over 200,000 tourism projects in the city, and 40,000 ‘creative industries’ operating in the city. These range from architectural interventions to socially engaged practices such as ‘Champs’ nightclubs and ‘Basel 4’ design schools, which offer a ‘quick and easy fix’ to city problems. The city is a spectacular site for tourism, and the fact that it is currently experiencing a housing crisis makes it an ideal site for a new wave of ‘hot spot’ hotels and development. The fact that it is located in the heart of the city’s financial district makes it an ideal site for a new wave of residential towers. And the fact that it is a short drive from major highways makes it a perfect site for a new wave of container ships. The fact that it is located in the city’s financial district makes it an ideal site for a new wave of speculative real-estate deals. It is important to note that these are just the latest entanglements in a long chain of related entanglements. There are many more entanglements to come, and many more stories to tell. What is most exciting about the biennial in this current configuration is the way in which it can narrate the history of the city alongside the city’s natural history and present-day challenges and opportunities.  Inevitably, this present-day narrative of the city is going to be filtered through the history of the container ship, which is the most visible form of transport in the city. But if the container</text>
    <text>The next Liverpool Biennial would be impossible without the amazing work developing behind the scenes. The Southampton Biennial, which was inspired by the DNA of the Liverpool Biennial, would be a perfect parallel. We already know that Southampton will host the 2030 World Cup. The Biennial’s current goal is to transform the city into a ‘world-class’ tourist destination. It is estimated that there are currently over 200,000 tourism projects in the city, and 40,000 ‘creative industries’ operating in the city. These range from architectural interventions to socially engaged practices such as ‘Champs’ nightclubs and ‘Basel 4’ design schools, which offer a ‘quick and easy fix’ to city problems. The city is a spectacular site for tourism, and the fact that it is currently experiencing a housing crisis makes it an ideal site for a new wave of ‘hot spot’ hotels and development. The fact that it is located in the heart of the city's bustling financial district, with over 1.5 million square feet of office and retail space on the 1,2 and 3 levels, makes it an ideal site for a new wave of ‘hot spot’ hotels and development. All of these developments are set to exacerbate or subvert the city’s already over-determined urban landscape. Yet all of these solutions are set to a large degree require that the inhabitants of the geographical areas affected by the projects return to their environments, either through forced relocation or other coercive means. How do we move from the biennial to the city?  The answer is simple: we need to create responsive, artistically responsive sites that can support all of the different forms of art, culture and organisation that are part of this evolving urban landscape. This means developing strategies and sites that respond to the needs of different audiences. It also means creating spaces and practices that are not ‘artificial’ but allow ‘artists’</text>
    <text>The next Liverpool Biennial would be impossible without the amazing work developing behind the scenes. The Southampton Biennial, which was inspired by the DNA of the Liverpool Biennial, would be a perfect parallel. We already know that Southampton will host the 2030 World Cup. The Biennial’s current goal is to transform the city into a ‘world-class’ tourist destination. It is estimated that there are currently over 200,000 tourism projects in the city, and 40,000 ‘creative industries’ operating in the city. These range from architectural interventions to socially engaged practices such as ‘Champs’ nightclubs and ‘Basel 4’ design schools, which offer a ‘quick and easy fix’ to city problems. The city is a spectacular site for tourism, and the fact that it is currently experiencing a housing crisis makes it an ideal site for a new wave of ‘hot spot’ hotels and development. The fact that it is located in the heart of the financial district makes it an ideal location for a new wave of office and retail projects, while the fact that it is one of the most ethnically diverse cities in the world makes it a perfect site for a new wave of vegan/vegetarian restaurants. These are just some of the many examples of what the Biennial could be if it took the right decisions and concentrated on the right things. As events in Liverpool unfold, it is important to remember what Liverpool Biennial is really all about. It is about the city as a cultural asset, what it offers for tourism, and what it expects of tourists. It is about what is expected of artists in the city, and what artists can and cannot do, and how that interacts with the city. It is not just a venue for art to be made, or a site for art to be sold. It is a site where ideas are actively sought out, analysed and produced in order to change the city. And that is what the Biennial</text>
  </text>
  <text>
    should be curated, and the arts not excluded from that. Do you think that your project, given the right spatial framework, could also be a vehicle for social change?SK: Absolutely. One of the beauties of living in California is that I can visit places that I never grew up knowing about. I can go to Santa Fe, where my grandmother would have been proud, and hunt in the Santa Fe Springs’ historic flea market. I’m not trying to rewrite any grand narratives here; I just want to see places I grew up knowing about. And I think my project, the way it comes out of the ground, the way it senses the history that’s in front of it and the impact that it has in the area, is a true reflection of that. The way that it operates, the way that it’s perceived, the way that it’s managed could also be a true reflection of that. The beauty of it is that we don’
    <text>should be curated, and the arts not excluded from that. Do you think that your project, given the right spatial framework, could also be a vehicle for social change?SK: Absolutely. One of the beauties of living in California is that I can visit places that I never grew up knowing about. I can go to Santa Fe, where my grandmother would have been proud, and hunt in the Santa Fe Springs’ historic flea market. I’m not trying to rewrite any grand narratives here; I just want to see places I grew up knowing about. And I think my project, the way it comes out of the ground, the way it senses the history that’s in front of it and the impact that it has in the area, is a true reflection of that. The way that it operates, the way that it’s perceived, the way that it’s managed could also be a true reflection of that. The beauty of it is that we don’t need to define what the city looks like or what the countryside means; we just need to know that there’s a difference between what’s happening on the ground and what’s happening up above. If you’ve got a master plan for the city and the countryside, then that’s really when things can be made to fit within that. And the project that we’re proposing is a very visionary, very ambitious project that we think is completely valid from a spatial point of view, but it’s also an incredibly intimate and personal project that you share with the world. You don’t have that luxury of not knowing. When something happens up there, it’s not just about what you and your partner could say, or what other people could say. It’s about who you and what relationship that you have with the city. When something happens up there, it’s about what your body feels like,</text>
    <text>should be curated, and the arts not excluded from that. Do you think that your project, given the right spatial framework, could also be a vehicle for social change?SK: Absolutely. One of the beauties of living in California is that I can visit places that I never grew up knowing about. I can go to Santa Fe, where my grandmother would have been proud, and hunt in the Santa Fe Springs’ historic flea market. I’m not trying to rewrite any grand narratives here; I just want to see places I grew up knowing about. And I think my project, the way it comes out of the ground, the way it senses the history that’s in front of it and the impact that it has in the area, is a true reflection of that. The way that it operates, the way that it’s perceived, the way that it’s managed could also be a true reflection of that. The beauty of it is that we don’t know what the past is like, or what the future will be like; we don’t know what’s possible and what’s not. We have no idea. So the beauty of the project is that it represents possibilities, it creates uncertainty, and yet it has that potential. For me, it’s that potential that’s missing.To go back to your question, the way that your question framed the relationship between urbanisation and culture was interesting. I thought that there was a tension there between the geographical parts that were increasingly appendages of larger cultural systems, and the part that was increasingly the result of individual cultural acts. For example, in the 1980s, the cultural acts that created the ‘hip’ subculture were so ubiquitous that it was almost laughable that there wasn’t a regional term for it. Now, the term ‘hip’ is thrown around so liberally that it’s almost taken for granted</text>
    <text>should be curated, and the arts not excluded from that. Do you think that your project, given the right spatial framework, could also be a vehicle for social change?SK: Absolutely. One of the beauties of living in California is that I can visit places that I never grew up knowing about. I can go to Santa Fe, where my grandmother would have been proud, and hunt in the Santa Fe Springs’ historic flea market. I’m not trying to rewrite any grand narratives here; I just want to see places I grew up knowing about. And I think my project, the way it comes out of the ground, the way it senses the history that’s in front of it and the impact that it has in the area, is a true reflection of that. The way that it operates, the way that it’s perceived, the way that it’s managed could also be a true reflection of that. The beauty of it is that we don’t know what that impact would be. We don’t have that in place yet, unfortunately. From the beginning we’ve had a vision of what our project would be, and what we’d like to do with the art, but we also don’t have that framework yet. With the right urban frameworks and the right urban artists to work with, a real artist can come through that process.SK: You also work closely with artist Ai Weiwei. Your project, entitled Grand Central Terminal, is his latest, and it’s also his magnum opus. What has he been up to lately?GG: He’s been really busy. He started a new school year in the autumn of 2014, and he’s been renovating the terminal ever since. He’s now working full-time as a maintenance worker, and he also works as an artist. He’s been doing that for about six months now,</text>
    <text>should be curated, and the arts not excluded from that. Do you think that your project, given the right spatial framework, could also be a vehicle for social change?SK: Absolutely. One of the beauties of living in California is that I can visit places that I never grew up knowing about. I can go to Santa Fe, where my grandmother would have been proud, and hunt in the Santa Fe Springs’ historic flea market. I’m not trying to rewrite any grand narratives here; I just want to see places I grew up knowing about. And I think my project, the way it comes out of the ground, the way it senses the history that’s in front of it and the impact that it has in the area, is a true reflection of that. The way that it operates, the way that it’s perceived, the way that it’s managed could also be a true reflection of that. The beauty of it is that we don’t know what the past is like, and so we don’t know what the future could be like. I mean, my grandmother would be proud of me if I did this, because she thinks that I’m putting this project into motion. Her hope is that I will put this project into motion and that it will one day be able to bring people together again. For now, it’s just trying to bring in some cash.SK: And if there’s a way to make it a permanent home, then you can say that you initiated the project through Facebook, or Twitter, or just by opening the bakery every day. This is just your way of saying that this is a collaborative process, and that there’s a process going on behind the scenes that’s slightly different from what you and the rest of us would do. I don’t think that there’s any one narrative here, or one voice, or one agenda</text>
    <text>should be curated, and the arts not excluded from that. Do you think that your project, given the right spatial framework, could also be a vehicle for social change?SK: Absolutely. One of the beauties of living in California is that I can visit places that I never grew up knowing about. I can go to Santa Fe, where my grandmother would have been proud, and hunt in the Santa Fe Springs’ historic flea market. I’m not trying to rewrite any grand narratives here; I just want to see places I grew up knowing about. And I think my project, the way it comes out of the ground, the way it senses the history that’s in front of it and the impact that it has in the area, is a true reflection of that. The way that it operates, the way that it’s perceived, the way that it’s managed could also be a true reflection of that. The beauty of it is that we don’t know how that would evolve; that could happen in a million ways. I don’t want to dictate what that would be; it’s up to the community to decide that. But the beauty of it is that it could be a reflection of the collective memory of the area and the city.Have you considered how the art forms you’ve created might affect those who visit the places you’ve created?SK: I don’t know if I would say that the arts form is the sole preserve of the urban, but I do think that the urban is the original canvas and that the street is the final destination. The art forms that I have created, through experience and repetition, is what it’s like to walk through the streets of Santa Fe. It’s a very powerful way to tell a story, to be able to pick up on the city’s rhythms and spatial relations, and the fact that people can sit through classes</text>
    <text>should be curated, and the arts not excluded from that. Do you think that your project, given the right spatial framework, could also be a vehicle for social change?SK: Absolutely. One of the beauties of living in California is that I can visit places that I never grew up knowing about. I can go to Santa Fe, where my grandmother would have been proud, and hunt in the Santa Fe Springs’ historic flea market. I’m not trying to rewrite any grand narratives here; I just want to see places I grew up knowing about. And I think my project, the way it comes out of the ground, the way it senses the history that’s in front of it and the impact that it has in the area, is a true reflection of that. The way that it operates, the way that it’s perceived, the way that it’s managed could also be a true reflection of that. The beauty of it is that we don’t know what the future holds. If the museum is your future, then the neighbourhood is a beautiful counter part to the museum. The museum is here to stay, to capture and store knowledge and imagination, to share it with others and to consume it. But while the museum is here to stay, the neighbourhood has to evolve and change and become what we, the future, create. The museum has a certain amount of power, and at the same time, a very definite purpose: to preserve knowledge and imagination and to make sense of it. And the power of the neighbourhood is directly proportional to the extent to which it serves this new historical narrative. If the museum is there to preserve knowledge and imagination, and the neighbourhood is there to absorb it, then the urban centre is there to be consumed and made up of places that can be consumed. By serving as a counter-power to the museum and its expanding collection, the biennial is able to become something that can last, that can expand and</text>
    <text>should be curated, and the arts not excluded from that. Do you think that your project, given the right spatial framework, could also be a vehicle for social change?SK: Absolutely. One of the beauties of living in California is that I can visit places that I never grew up knowing about. I can go to Santa Fe, where my grandmother would have been proud, and hunt in the Santa Fe Springs’ historic flea market. I’m not trying to rewrite any grand narratives here; I just want to see places I grew up knowing about. And I think my project, the way it comes out of the ground, the way it senses the history that’s in front of it and the impact that it has in the area, is a true reflection of that. The way that it operates, the way that it’s perceived, the way that it’s managed could also be a true reflection of that. The beauty of it is that we don’t know how the city is going to be managed; we don’t know what the future could bring. I think that my project represents a true attempt to imagine things differently.One of the beauties of living in California is that I can visit places that I never grew up knowing about. I can go to Santa Fe, where my grandmother would have been proud, and hunt in the Santa Fe Springs’ historic flea market. I’m not trying to rewrite any grand narratives here; I just want to see places I grew up knowing. And I think my project, the way it comes out of the ground, the way it senses the history that’s in front of it and the impact that it has in the area, is a true reflection of that. The way that it operates, the way that it’s perceived, the way that it’s managed – that’s one of the beauties of living here.The other is that</text>
    <text>should be curated, and the arts not excluded from that. Do you think that your project, given the right spatial framework, could also be a vehicle for social change?SK: Absolutely. One of the beauties of living in California is that I can visit places that I never grew up knowing about. I can go to Santa Fe, where my grandmother would have been proud, and hunt in the Santa Fe Springs’ historic flea market. I’m not trying to rewrite any grand narratives here; I just want to see places I grew up knowing about. And I think my project, the way it comes out of the ground, the way it senses the history that’s in front of it and the impact that it has in the area, is a true reflection of that. The way that it operates, the way that it’s perceived, the way that it’s managed could also be a true reflection of that. The beauty of it is that we don’t know how that will all play out. We don’t even know what the impact will be yet. I think that the beauty of it is that it doesn’t have to be perfect; it could be beautiful, empty, it could be a nightmare. I can see how that might be for some people: for example, for someone like myself, who has always been really into architecture, and who’s become increasingly desensitized to how beautiful it is to be materially deprived of natural landscapes: to experience desolation and poverty and still be able to look at art. The key is to find ways to imagine better.SK: Lastly, you’ve said that you want to do more to address the issue of mobility in your neighbourhood. What can be done?DY: I’m not sure that there’s much that can be done yet. There’s still a huge disconnect between the expectations that people have about where mobility should be</text>
    <text>should be curated, and the arts not excluded from that. Do you think that your project, given the right spatial framework, could also be a vehicle for social change?SK: Absolutely. One of the beauties of living in California is that I can visit places that I never grew up knowing about. I can go to Santa Fe, where my grandmother would have been proud, and hunt in the Santa Fe Springs’ historic flea market. I’m not trying to rewrite any grand narratives here; I just want to see places I grew up knowing about. And I think my project, the way it comes out of the ground, the way it senses the history that’s in front of it and the impact that it has in the area, is a true reflection of that. The way that it operates, the way that it’s perceived, the way that it’s managed could also be a true reflection of that. The beauty of it is that we don’t have to define the potentialities of the place. There’s a way of thinking, a way of accessing history that is authentic to the area, and that’s really exciting.  In this respect, your project extends back further than the art it’s commemorating. What does it mean to ‘originate’ in a way that reflects the real history of the area?SK: I think that there are two ways of putting it. One is to say that you’ve actually taken the place and put it on the map. And the other is to say that you’ve actually done something in the place and now you’re looking back at it and there’s something that you could have done differently.I don’t think that there’s a simple answer to the second way of putting it, but I’m curious what you think the difference is between the first and second ways of putting it.</text>
    <text>should be curated, and the arts not excluded from that. Do you think that your project, given the right spatial framework, could also be a vehicle for social change?SK: Absolutely. One of the beauties of living in California is that I can visit places that I never grew up knowing about. I can go to Santa Fe, where my grandmother would have been proud, and hunt in the Santa Fe Springs’ historic flea market. I’m not trying to rewrite any grand narratives here; I just want to see places I grew up knowing about. And I think my project, the way it comes out of the ground, the way it senses the history that’s in front of it and the impact that it has in the area, is a true reflection of that. The way that it operates, the way that it’s perceived, the way that it’s managed could also be a true reflection of that. The beauty of it is that we don’t know what the impact would be, because we don’t do exploratory projects like Calaveras County Fair. It’s a lost opportunity, and it’s a lost community. The Asian Pacific Archipelago Foundation is a non-profit organisation founded in 1990 and headquartered in Seattle. The foundation’s current mission is to support and promote the promotion of Asian Pacific Archipelago studies and culture through collaborative cultural activities. The Asian Pacific Archipelago Foundation was started as a cultural initiative and arts organisation, and now it’s a cultural organisation with a cultural mission. In the 1990s, the Asian Pacific Archipelago was represented at the Venice Biennale by the sculptor Amy Tan, and in the 2000s by the poet Chimamanda Ngozi Adichie. The Biennale was curated by Paola Marrati and Daniela Ginsburg and featured works from many different artists. In the main programme, the archipelago</text>
  </text>
  <text>
    using machine learning techniques, which enable large datasets to be segmented into manageable chunks. Such techniques could one day be used to analyse large amounts of data, allowing for the identification of rare occurrences or anomalies that occur frequently or rarely in the real-life world.Such insights could be used not only to identify rare phenomena, but also to design better phenomena, such as those that result from biennials. For example, biennials might one day be structured around the principles of ‘aquatic pedagogy’, or ‘sustainable urban development’ through which an individual or a small group of individuals might choose to devote a significant period of time to a particular interest or art form. Such an approach seems unnecessary to the actuality of the art form, but it would certainly refine the perception of what is possible through such an approach. Such is the inherent unpredictability of human behaviour, but it is also the inherent unpredictability of the human species. Such behaviour is neither here nor there, while
    <text>using machine learning techniques, which enable large datasets to be segmented into manageable chunks. Such techniques could one day be used to analyse large amounts of data, allowing for the identification of rare occurrences or anomalies that occur frequently or rarely in the real-life world.Such insights could be used not only to identify rare phenomena, but also to design better phenomena, such as those that result from biennials. For example, biennials might one day be structured around the principles of ‘aquatic pedagogy’, or ‘sustainable urban development’ through which an individual or a small group of individuals might choose to devote a significant period of time to a particular interest or art form. Such an approach seems unnecessary to the actuality of the art form, but it would certainly refine the perception of what is possible through such an approach. Such is the inherent unpredictability of human behaviour, but it is also the inherent unpredictability of the human species. Such behaviour is neither here nor there, while the art form is both here and there. It might be the case though, as with the work of artists such as Joan Miró and Monica Ross’ Real Life, that the art and the artists actually do communicate and work together to form a more complex work than the conceptually aware biennial. Such a project would be no less humanizing, no less authentic, and certainly no less authentic, than any other kind of art.The other kind of biennial, the so-called biennale, is the one that formally begins but ends in some kind of ‘presentation’. It is the kind that begins, but ends, at some point in the future. Although in this case the ‘presentation’ would have to be a kind of prologue, a kind of epilogue, and it would have to end somewhere. If the presentation is ‘presentational’ in nature, then it too begins, but it would have to end somewhere</text>
    <text>using machine learning techniques, which enable large datasets to be segmented into manageable chunks. Such techniques could one day be used to analyse large amounts of data, allowing for the identification of rare occurrences or anomalies that occur frequently or rarely in the real-life world.Such insights could be used not only to identify rare phenomena, but also to design better phenomena, such as those that result from biennials. For example, biennials might one day be structured around the principles of ‘aquatic pedagogy’, or ‘sustainable urban development’ through which an individual or a small group of individuals might choose to devote a significant period of time to a particular interest or art form. Such an approach seems unnecessary to the actuality of the art form, but it would certainly refine the perception of what is possible through such an approach. Such is the inherent unpredictability of human behaviour, but it is also the inherent unpredictability of the human species. Such behaviour is neither here nor there, while the real-life conditions that generate it are highly variable and subject to variable interpretation. Therefore it is important to continuously reorient the parameters of human behaviour in order to ensure that they do not become subject to the whims of transient cultural values. Such cultural values may derive from a variety of sources, but their realisation is quite unpredictable. It is this unpredictability that makes art possible, but it is also why we cannot predict what art will be. Art has to be constantly reinvented, and this is what art forms do. Art has to be constantly (very nearly) invented in order to constantly (almost) occur. Art has to be invented in such a way that it manifests itself in new forms (real and invented) in and on the terrain of the real world; it has to be invented in such a way that it does not merely occur; it has to be endlessly inventive and dynamic (not just in the sense of Chambers’ dictum, but in the very definition of the</text>
    <text>using machine learning techniques, which enable large datasets to be segmented into manageable chunks. Such techniques could one day be used to analyse large amounts of data, allowing for the identification of rare occurrences or anomalies that occur frequently or rarely in the real-life world.Such insights could be used not only to identify rare phenomena, but also to design better phenomena, such as those that result from biennials. For example, biennials might one day be structured around the principles of ‘aquatic pedagogy’, or ‘sustainable urban development’ through which an individual or a small group of individuals might choose to devote a significant period of time to a particular interest or art form. Such an approach seems unnecessary to the actuality of the art form, but it would certainly refine the perception of what is possible through such an approach. Such is the inherent unpredictability of human behaviour, but it is also the inherent unpredictability of the human species. Such behaviour is neither here nor there, while the artifice of the biennial remains both there and unpredictable.The real question is, though, what can be done to mitigate the unpredictable nature of the art form, to create the illusion of lessening unpredictability? There are many who advocate the creation of a biennial commission, but this is only ever an option for a small number of artists and their organisations. It is more likely that a biennial commission would encourage a small number of organisations to embark on projects that would, in turn, reward or penalise them according to their choices, but would not create an absolute upper hand over other forms of artistic endeavour. Artists and their organisations would need to be actively engaged in creating the biennial, and actively engaged in campaigning for their art to be exhibited and championed in other venues, including by other artists. This would require a concerted international effort, and it is relatively untested at this point in terms of legitimacy or resources, but it is possible that artists could be encouraged to produce innovative works</text>
    <text>using machine learning techniques, which enable large datasets to be segmented into manageable chunks. Such techniques could one day be used to analyse large amounts of data, allowing for the identification of rare occurrences or anomalies that occur frequently or rarely in the real-life world.Such insights could be used not only to identify rare phenomena, but also to design better phenomena, such as those that result from biennials. For example, biennials might one day be structured around the principles of ‘aquatic pedagogy’, or ‘sustainable urban development’ through which an individual or a small group of individuals might choose to devote a significant period of time to a particular interest or art form. Such an approach seems unnecessary to the actuality of the art form, but it would certainly refine the perception of what is possible through such an approach. Such is the inherent unpredictability of human behaviour, but it is also the inherent unpredictability of the human species. Such behaviour is neither here nor there, while inanimate objects are both rare and beautiful. It is only when faced with a clearly defined endpoint – whether it is human life or culture – that we make mistakes, and these mistakes are likely to be fatal. Randomness is inherent in the human condition, and art is a powerful way to generate critical options and resist the urge to make fatal mistakes. This is why biennials can be so powerful, for they can so easily split the biennial audience between those who have purchased tickets and those who have not. This simple power can have far-reaching consequences, as biennials have in turn often unintended consequences. For example, biennials have often ended up defining what is meant by ‘the art form’ in relation to itself, and thus have often ended up defining what art in its own right is. This is why biennials have such an impact, for they can so easily sever the art from the biennial, and thus both the artist and the biennial simultaneously</text>
    <text>using machine learning techniques, which enable large datasets to be segmented into manageable chunks. Such techniques could one day be used to analyse large amounts of data, allowing for the identification of rare occurrences or anomalies that occur frequently or rarely in the real-life world.Such insights could be used not only to identify rare phenomena, but also to design better phenomena, such as those that result from biennials. For example, biennials might one day be structured around the principles of ‘aquatic pedagogy’, or ‘sustainable urban development’ through which an individual or a small group of individuals might choose to devote a significant period of time to a particular interest or art form. Such an approach seems unnecessary to the actuality of the art form, but it would certainly refine the perception of what is possible through such an approach. Such is the inherent unpredictability of human behaviour, but it is also the inherent unpredictability of the human species. Such behaviour is neither here nor there, while the artist’s misguided assumptions are both here and there.Answered by Professors Ajay Goel and Ewa Sikora of Queen’s University, the two experts in the field were asked to design a biennial that is not ‘inspired by, nor in any way connected to, the current political situation in the world’, but rather focuses on a single theme or set of themes. They were also asked to identify the distinguishing features of the current biennial format. For their contributions, they were asked to create ‘works that speak to the present and future of the art form, but also to the past and present of colonialism and apartheid’. To do this, they used the template set by the Archaeology Department at UVic and the North Vancouver Museum of Art, as well as the curatorial style of the Victoria and Albert Museum. The contribution of each faculty member was therefore as follows:A. Drabble on Visions of Angk</text>
    <text>using machine learning techniques, which enable large datasets to be segmented into manageable chunks. Such techniques could one day be used to analyse large amounts of data, allowing for the identification of rare occurrences or anomalies that occur frequently or rarely in the real-life world.Such insights could be used not only to identify rare phenomena, but also to design better phenomena, such as those that result from biennials. For example, biennials might one day be structured around the principles of ‘aquatic pedagogy’, or ‘sustainable urban development’ through which an individual or a small group of individuals might choose to devote a significant period of time to a particular interest or art form. Such an approach seems unnecessary to the actuality of the art form, but it would certainly refine the perception of what is possible through such an approach. Such is the inherent unpredictability of human behaviour, but it is also the inherent unpredictability of the human species. Such behaviour is neither here nor there, while the prevailing trends in many areas of the world are in fact quite predictable. The key is in recognising and valuing these different forms of unpredictability, and in developing strategies to adopt when faced with it.Take a moment and reflect on the following statement, made by Warren Buffet, the CEO of Berkshire Hathaway (NYSE:BRKa):My bet is that in five years, when the cars are electric, we will be able to drive ourselves to work. That's all I'm saying. We will be able to do things with our cars that we couldn’t do with them, and we will be able to do things with our houses that we couldn’t do with them either. And we will be able to do things with our houses that we couldn’t do with them, because the cars will be electric and the houses will be so self-sustaining.   	So, what does it mean for a biennial to be driven by an</text>
    <text>using machine learning techniques, which enable large datasets to be segmented into manageable chunks. Such techniques could one day be used to analyse large amounts of data, allowing for the identification of rare occurrences or anomalies that occur frequently or rarely in the real-life world.Such insights could be used not only to identify rare phenomena, but also to design better phenomena, such as those that result from biennials. For example, biennials might one day be structured around the principles of ‘aquatic pedagogy’, or ‘sustainable urban development’ through which an individual or a small group of individuals might choose to devote a significant period of time to a particular interest or art form. Such an approach seems unnecessary to the actuality of the art form, but it would certainly refine the perception of what is possible through such an approach. Such is the inherent unpredictability of human behaviour, but it is also the inherent unpredictability of the human species. Such behaviour is neither here nor there, while natural phenomena are both subject to natural fluctuations and art is no different. There is a great deal of work that needs to be done in the realm of aesthetics to arrive at a conceptual framework that actually captures the dynamic between what is culturally considered ‘natural’ and what is actually social engineering, although the degree to which this is achieved is another matter entirely.Art has always been a tricky, sometimes messy, sometimes frustrating medium. It is powerful in its ability to create a shared space, but equally capable of destroying it. This is particularly true in the realm of aesthetics, where artists and art institutions have a particularly effective way of erasing or glossing over problematic behaviours and perspectival effects that can be ascribed to art. The difficulty with which the art of the past is perceived is compounded by the fact that, for much of the past, the art of the present was viewed as preferable. Thus, the aesthetic quality of the past is often seen as superior to the aesthetic quality of the present</text>
    <text>using machine learning techniques, which enable large datasets to be segmented into manageable chunks. Such techniques could one day be used to analyse large amounts of data, allowing for the identification of rare occurrences or anomalies that occur frequently or rarely in the real-life world.Such insights could be used not only to identify rare phenomena, but also to design better phenomena, such as those that result from biennials. For example, biennials might one day be structured around the principles of ‘aquatic pedagogy’, or ‘sustainable urban development’ through which an individual or a small group of individuals might choose to devote a significant period of time to a particular interest or art form. Such an approach seems unnecessary to the actuality of the art form, but it would certainly refine the perception of what is possible through such an approach. Such is the inherent unpredictability of human behaviour, but it is also the inherent unpredictability of the human species. Such behaviour is neither here nor there, while most biennial exhibitions are quite certainly marked by some degree of exceptionalism. It is the randomness of human behaviour, but it is also the inevitable unpredictability of events, which can and will occur. In other words, biennials can and do happen; they just happen to be art exhibitions that happen to be occurring in the city in the year 2081.Art and the biennialAs an art form, the biennial has always been a particular interest of mine. The main reason is that, while I enjoy the art, I also enjoy the art that is often made in the form of art. For me, there is an inherently humanistic aspect to the importance of sound science, art, and art in generating sound, and in that all art is ultimately made in the image of humans. Similarly, there is an inherently humanistic quality to cultural anthropology. I think that the main reason that I enjoy the art that is made in the biennial format is that it is often made</text>
    <text>using machine learning techniques, which enable large datasets to be segmented into manageable chunks. Such techniques could one day be used to analyse large amounts of data, allowing for the identification of rare occurrences or anomalies that occur frequently or rarely in the real-life world.Such insights could be used not only to identify rare phenomena, but also to design better phenomena, such as those that result from biennials. For example, biennials might one day be structured around the principles of ‘aquatic pedagogy’, or ‘sustainable urban development’ through which an individual or a small group of individuals might choose to devote a significant period of time to a particular interest or art form. Such an approach seems unnecessary to the actuality of the art form, but it would certainly refine the perception of what is possible through such an approach. Such is the inherent unpredictability of human behaviour, but it is also the inherent unpredictability of the human species. Such behaviour is neither here nor there, while the artist knows that. Rather, what makes the art form unpredictable is the human capacity for learning, which is constrained by the very nature of organisation that underpins the art form itself. If the organisation were to be reformed, it would have to change, and such fundamental organisational changes would have to be made by an entirely different generation of artists, curators, art writers and others. Such a radical change would render the art form into something other than what it is, in order to address the real problems facing cities and towns throughout the world. Such a radical change would also render the art form into something that is not necessarily an art form in its own right, but one that is actively engaged with in a way that others could potentially learn from. This latter condition would render the art form into something that is still recognisable, but one that is actively engaged in something else. It is to say that the organisational principles of the art form would have to change in order to accommodate such potentially</text>
    <text>using machine learning techniques, which enable large datasets to be segmented into manageable chunks. Such techniques could one day be used to analyse large amounts of data, allowing for the identification of rare occurrences or anomalies that occur frequently or rarely in the real-life world.Such insights could be used not only to identify rare phenomena, but also to design better phenomena, such as those that result from biennials. For example, biennials might one day be structured around the principles of ‘aquatic pedagogy’, or ‘sustainable urban development’ through which an individual or a small group of individuals might choose to devote a significant period of time to a particular interest or art form. Such an approach seems unnecessary to the actuality of the art form, but it would certainly refine the perception of what is possible through such an approach. Such is the inherent unpredictability of human behaviour, but it is also the inherent unpredictability of the human species. Such behaviour is neither here nor there, while the art form itself is both here and there. In the end, what matters is that the biennial is there, widely seen and appreciated, offers something to do with the city and its people, and provokes some kind of interaction with the city’s infrastructure that is visible and can be captured and analysed. In short, it is possible that the subjective elements of the biennial, such as the art, the exhibition, and the community might one day be seen as valuable in their own right, while the underlying structure of the art world itself might evolve into something more recognisable as a ‘real life’.This is the second installment of a trilogy about biennials, the second installment of which is about to be published this summer.Download this article as PDFRyan AventRyan Avent is an architect and Principal in the office of Jamie McDonald, who has been developing plans for a new regional hub for manufacturing in the Ottawa-Granville International Airport for over ten</text>
  </text>
  <text>
    The next Liverpool Biennial, to be held in late 2017, will be devoted to the city’s past, as well as to the city’s future. The name of the artist, Ingmar Bergman, is a play on words: both an adjective and a noun. Using a combination of both, Bergman means both ‘fine art in the making’ and ‘fine art produced by the makers of that art’. He does both things in one work. Over the course of the exhibition, the artist will be making a case for the city’s future through a combination of historical and archival materials. But the city, and the work, is something else entirely: it is a production of the city. The material, the infrastructure and the culture of the city will be used to produce visions of the city as both fertile ground and a frustrating end point. This is the work of cultural mavericks, and the city is a production of the city. For this
    <text>The next Liverpool Biennial, to be held in late 2017, will be devoted to the city’s past, as well as to the city’s future. The name of the artist, Ingmar Bergman, is a play on words: both an adjective and a noun. Using a combination of both, Bergman means both ‘fine art in the making’ and ‘fine art produced by the makers of that art’. He does both things in one work. Over the course of the exhibition, the artist will be making a case for the city’s future through a combination of historical and archival materials. But the city, and the work, is something else entirely: it is a production of the city. The material, the infrastructure and the culture of the city will be used to produce visions of the city as both fertile ground and a frustrating end point. This is the work of cultural mavericks, and the city is a production of the city. For this reason, the project—which, as we saw, is both historical and urban—is a multi-use production. It can be both a site for historical research and a site for art production. If the city is the production of the city, then the othering factors of the city are what render its potential suitability as a site for artistic production. And if the city is the production of the city, then the othering factors of the city are what render its potential worth as a site for cultural production. This is the second aspect of the ‘contravision’ that we wanted to address. The first aspect was language and the inherent tensions between the production of history and its expression in art form. We saw how the production of history could be used to justify whatever is being done in the name of the city. We saw how the production of history can end up justifying whatever is being done in the name of the city. These are the contravisions. The artist should</text>
    <text>The next Liverpool Biennial, to be held in late 2017, will be devoted to the city’s past, as well as to the city’s future. The name of the artist, Ingmar Bergman, is a play on words: both an adjective and a noun. Using a combination of both, Bergman means both ‘fine art in the making’ and ‘fine art produced by the makers of that art’. He does both things in one work. Over the course of the exhibition, the artist will be making a case for the city’s future through a combination of historical and archival materials. But the city, and the work, is something else entirely: it is a production of the city. The material, the infrastructure and the culture of the city will be used to produce visions of the city as both fertile ground and a frustrating end point. This is the work of cultural mavericks, and the city is a production of the city. For this reason, the Biennial is expected to be a catalyst for change in the city, bringing creative professionals and artists to the city, while simultaneously challenging established modes of artistic production and institutional formation.  	The artist will have a budget of approximately US$75,000 for the project, of which approximately US$35,000 is for installation and US$5,000 for the exhibition. This covers the whole project, including the art, the architecture, the community, the community gardens and the streets, and the surrounding area. The rest is for the streets. The budget includes hire of landscaping, benches and a plaza for the event, as well as studio space, food and drinks, and the opportunity to start a project in the city. The budget does not include work directly related to the project, such as planning, development, or management. The budget does not include funding for the project itself. 	The budget for the Biennial is USD $7 million. The actual figure</text>
    <text>The next Liverpool Biennial, to be held in late 2017, will be devoted to the city’s past, as well as to the city’s future. The name of the artist, Ingmar Bergman, is a play on words: both an adjective and a noun. Using a combination of both, Bergman means both ‘fine art in the making’ and ‘fine art produced by the makers of that art’. He does both things in one work. Over the course of the exhibition, the artist will be making a case for the city’s future through a combination of historical and archival materials. But the city, and the work, is something else entirely: it is a production of the city. The material, the infrastructure and the culture of the city will be used to produce visions of the city as both fertile ground and a frustrating end point. This is the work of cultural mavericks, and the city is a production of the city. For this reason, the Biennial should be able to engage with and overcome its materialist tendencies. If its materialist ethno-cultural orientations are to be overcome, it is vital that they are actively engaged with and that they emerge out of the material conditions and desires of contemporary life. But the material conditions and desires of the city cannot be accommodated by the artist or his or her material force, and must be actively contested and restructured. This is what the Biennial should do, and what it is expected to do. But in order to achieve this, the Biennial will also need to actively engage with and modify its social, economic and cultural materialities and projects. More and more, the biennial is expected to be a multi-year cultural center and, in many ways, a home for contemporary art and aesthetics. It is expected to generate new artists and ideas and provide a forum for exchange between people from different disciplines and walks of life. It is expected to employ creative disruption and</text>
    <text>The next Liverpool Biennial, to be held in late 2017, will be devoted to the city’s past, as well as to the city’s future. The name of the artist, Ingmar Bergman, is a play on words: both an adjective and a noun. Using a combination of both, Bergman means both ‘fine art in the making’ and ‘fine art produced by the makers of that art’. He does both things in one work. Over the course of the exhibition, the artist will be making a case for the city’s future through a combination of historical and archival materials. But the city, and the work, is something else entirely: it is a production of the city. The material, the infrastructure and the culture of the city will be used to produce visions of the city as both fertile ground and a frustrating end point. This is the work of cultural mavericks, and the city is a production of the city. For this reason, if anything happens in the city, it should be connected to something happening elsewhere. Liverpool, and the city in which it is located, is a production of many places around the world, and so the question becomes whether the vision of the city that emerges through the exhibition should be global or local. If the latter, it’s a question of chance – should the city be a production of places or a producer of individuals? In an ideal world, the city would produce its own culture, but in a world where many places around the world have similar or better cultural experiences, the city’s ambition should be international.5. Cultural exchange at biennials around the world. As biennials become more internationalised, and become part of larger exhibitions business, it becomes ever more important to offer complementary, local or ‘bread and butter’ experiences to visitors to places around the world. Curators, artists and art historians around the world have different ideas of what constitutes</text>
    <text>The next Liverpool Biennial, to be held in late 2017, will be devoted to the city’s past, as well as to the city’s future. The name of the artist, Ingmar Bergman, is a play on words: both an adjective and a noun. Using a combination of both, Bergman means both ‘fine art in the making’ and ‘fine art produced by the makers of that art’. He does both things in one work. Over the course of the exhibition, the artist will be making a case for the city’s future through a combination of historical and archival materials. But the city, and the work, is something else entirely: it is a production of the city. The material, the infrastructure and the culture of the city will be used to produce visions of the city as both fertile ground and a frustrating end point. This is the work of cultural mavericks, and the city is a production of the city. For this reason, the Biennial needs to be able to find a place in the city without being seen as an imposition or a break with tradition. It is not enough to offer an exhibition of art against the backdrop of the city. You must create a story that is original and different from the usual, familiar, urban narrative. You must also find a way to bring the city into the project, to infuse it with your cultural practices. How to conceive of a biennial that creates its own trajectory is the question. I propose that the main task for the biennial is to undergo a metainterface, a metainterface that is as unpredictable and dynamic as the city. This means creating experiences that are as unexpected and different from those offered by normal, institutional modes of experience. It means creating places and social systems that are not based on hierarchical narratives. It also means creating environments and social systems that are not based on white, heterosexual, cisgendered, heteronormative and male</text>
    <text>The next Liverpool Biennial, to be held in late 2017, will be devoted to the city’s past, as well as to the city’s future. The name of the artist, Ingmar Bergman, is a play on words: both an adjective and a noun. Using a combination of both, Bergman means both ‘fine art in the making’ and ‘fine art produced by the makers of that art’. He does both things in one work. Over the course of the exhibition, the artist will be making a case for the city’s future through a combination of historical and archival materials. But the city, and the work, is something else entirely: it is a production of the city. The material, the infrastructure and the culture of the city will be used to produce visions of the city as both fertile ground and a frustrating end point. This is the work of cultural mavericks, and the city is a production of the city. For this reason, the Biennial has used the same name for both the exhibition and the subsequent book that follows it (although a different title for each chapter). The book follows the narrative of the Berlantung des Todes och Berlantung des Todes, or ‘The Taming of the Beast’, a 1927 book that recounts the exploits of Njabulo Sese Sekula, an ethnic Sekan chief who leads an empire spanning parts of Africa, South Asia and the Pacific. In the book, Sekula is depicted as a reformer who leads an antigovernment ‘tribe’ against the powerful British Empire. The introduction to the book ‘Overthrow the British Empire’ recounts how Sekula’s empire was brought to an end by a coalition of Dutch, Danish, Indian and other ethnic groups in 1864. The alliance was made up of soldiers, politicians and bureaucrats from across Europe, and included elements from every continent. Some</text>
    <text>The next Liverpool Biennial, to be held in late 2017, will be devoted to the city’s past, as well as to the city’s future. The name of the artist, Ingmar Bergman, is a play on words: both an adjective and a noun. Using a combination of both, Bergman means both ‘fine art in the making’ and ‘fine art produced by the makers of that art’. He does both things in one work. Over the course of the exhibition, the artist will be making a case for the city’s future through a combination of historical and archival materials. But the city, and the work, is something else entirely: it is a production of the city. The material, the infrastructure and the culture of the city will be used to produce visions of the city as both fertile ground and a frustrating end point. This is the work of cultural mavericks, and the city is a production of the city. For this reason, the city is a production for the art of which the city is a part. As we saw, the city is a crucial part of the exhibitionary complex. But it is also a production of the artist’s work, for it establishes the exhibitionary context and conditions in which the art can be shown, and which characterises the city in which the art can exist in a fluid and transformational way. The city as a production for art has been around for a while. It is part of the larger cultural logic of the modern world. In that case, the Museum of Modern Art in New York might be considered an early curatorial innovation, and the Museum of Modern Art in Berlin a later one. But the basic ideas and artistic motivation that underlie modern curatorial ideas about the world remain essentially the same. And today, the museum is a historical artifact: it conveys much of what we mean when we talk about the modern curatorial interest in the museum, but it</text>
    <text>The next Liverpool Biennial, to be held in late 2017, will be devoted to the city’s past, as well as to the city’s future. The name of the artist, Ingmar Bergman, is a play on words: both an adjective and a noun. Using a combination of both, Bergman means both ‘fine art in the making’ and ‘fine art produced by the makers of that art’. He does both things in one work. Over the course of the exhibition, the artist will be making a case for the city’s future through a combination of historical and archival materials. But the city, and the work, is something else entirely: it is a production of the city. The material, the infrastructure and the culture of the city will be used to produce visions of the city as both fertile ground and a frustrating end point. This is the work of cultural mavericks, and the city is a production of the city. For this reason, the Biennial should offer something in the way of a stipend to keep the work out of the city, while simultaneously enabling the artist to continue to explore the city in her work. This could mean a stipend for a specific number of works, or it could mean that the biennial should provide the artist with an institutional platform from which to engage with the city and actively participate in its creation. This is the intervention of an artist-organised project, and it is this kind of institutional support that creates the space for art to be made in the city. The Biennial could also develop a long-term research agenda that looks into the cultural dynamics of the city and how art can be made in the city, as well as exploring the ways in which art can be made in the city in the future. This kind of institutional support for an artist-planned project is what makes the city work, and it is why artists find ways to intervene in the city in unexpected ways. It</text>
    <text>The next Liverpool Biennial, to be held in late 2017, will be devoted to the city’s past, as well as to the city’s future. The name of the artist, Ingmar Bergman, is a play on words: both an adjective and a noun. Using a combination of both, Bergman means both ‘fine art in the making’ and ‘fine art produced by the makers of that art’. He does both things in one work. Over the course of the exhibition, the artist will be making a case for the city’s future through a combination of historical and archival materials. But the city, and the work, is something else entirely: it is a production of the city. The material, the infrastructure and the culture of the city will be used to produce visions of the city as both fertile ground and a frustrating end point. This is the work of cultural mavericks, and the city is a production of the city. For this reason, the biennial has to be seen as both a production of the city and a production of art not made by the city. If the former is seen as a commodity, the latter is seen as a source of value. The idea that the city is a production of the city is what Lili Mandel keeps repeating in her reporting: it’s as if the city were a capital itself.7.   	The relationship between biennials and the geographical locations where they are installed. Jean Guy Lecat and François Hartney,  	L’apparence de l’importance (Paris: Fayard, 2015), p. 24. Karen Bowden,  	The Ark: The Temporary World (New York: Vintage Books, 1997), p. 72. Peter Schjeldahl,  	Grantham Research Institute: BIR/FRE: The Politics of Emptiness, trans. Stefan</text>
    <text>The next Liverpool Biennial, to be held in late 2017, will be devoted to the city’s past, as well as to the city’s future. The name of the artist, Ingmar Bergman, is a play on words: both an adjective and a noun. Using a combination of both, Bergman means both ‘fine art in the making’ and ‘fine art produced by the makers of that art’. He does both things in one work. Over the course of the exhibition, the artist will be making a case for the city’s future through a combination of historical and archival materials. But the city, and the work, is something else entirely: it is a production of the city. The material, the infrastructure and the culture of the city will be used to produce visions of the city as both fertile ground and a frustrating end point. This is the work of cultural mavericks, and the city is a production of the city. For this reason, the Biennial should be a ‘city within a city’. It should be able to inhabit its city and inhabit its culture without ever feeling like a ‘city in its own right’.1.   	The Biennial as a Production Area 	After the Biennial was founded, the museum was forced to pick up the pieces and had to find new ways of supporting itself. The Biennial’s success, at this point, seemed to depend on the museum’s willingness to become entangled in the proceedings, to allow the museums to take part in the show as spectators, and to become complicit in the show through their involvement. In other words, the museum was seen as a more natural part of the show, rather than the other way around. 	At the time, this entwined model was still the case: the museum played a crucial role in the show, but was often marginalized or ignored in the process. While the literature</text>
  </text>
  <text>
    should be curated to complement one another, rather than to represent them as different but connected],&quot; he said.  	Kaufman, the curator, is the author of several books on the biennial format, including the forthcoming book, Curated: Narratives from the San Francisco Museum of Modern Art's (SFSMA) 2015 Biennale of Exhibitions, which will tell the story of the Biennale from its founding in 1985 to the present. He is the recipient of several awards and has lectured on the topic of biennials.Profile: Jeff Koons Koons is a multi-award-winning artist, whose work has been exhibited in over fifty institutions and published in more than twenty languages. He is the curator of the Museum of Modern Art, New York, USA, where he has curated several exhibitions of contemporary art, including the exhibition  	The Contemporary Condition, 2016, which was included in the 2016 National Portrait Gallery's '10 Best Art Contributions
    <text>should be curated to complement one another, rather than to represent them as different but connected],&quot; he said.  	Kaufman, the curator, is the author of several books on the biennial format, including the forthcoming book, Curated: Narratives from the San Francisco Museum of Modern Art's (SFSMA) 2015 Biennale of Exhibitions, which will tell the story of the Biennale from its founding in 1985 to the present. He is the recipient of several awards and has lectured on the topic of biennials.Profile: Jeff Koons Koons is a multi-award-winning artist, whose work has been exhibited in over fifty institutions and published in more than twenty languages. He is the curator of the Museum of Modern Art, New York, USA, where he has curated several exhibitions of contemporary art, including the exhibition  	The Contemporary Condition, 2016, which was included in the 2016 National Portrait Gallery's '10 Best Art Contributions' list. In 2017, he curated the Venice Biennale, making art with soil, at the historic location of the Giardini, as part of the public programme, and in the public programme, and part of the public programme, and part of the public programme, and part of the public programme, and part of the public programme, and part of the public programme, and part of the public programme, and part of the public programme, and part of the public programme, and part of the public programme, and part of the public programme, and part of the public programme, and part of the public programme, and part of the public programme, and there are many more. In addition to the Biennale, he is the curator of the Museum of Modern Art and the New Museum, and the curator of the Museum of Contemporary Art, Toronto. He has curated several exhibitions of contemporary art and art from the worlds of architecture, architecture critiques, and fine art. Koons made</text>
    <text>should be curated to complement one another, rather than to represent them as different but connected],&quot; he said.  	Kaufman, the curator, is the author of several books on the biennial format, including the forthcoming book, Curated: Narratives from the San Francisco Museum of Modern Art's (SFSMA) 2015 Biennale of Exhibitions, which will tell the story of the Biennale from its founding in 1985 to the present. He is the recipient of several awards and has lectured on the topic of biennials.Profile: Jeff Koons Koons is a multi-award-winning artist, whose work has been exhibited in over fifty institutions and published in more than twenty languages. He is the curator of the Museum of Modern Art, New York, USA, where he has curated several exhibitions of contemporary art, including the exhibition  	The Contemporary Condition, 2016, which was included in the 2016 National Portrait Gallery's '10 Best Art Contributions to the Humanities' list. He has curated the exhibition Lucky You: Modern Art from the 1980s, 2015 Oude Kerk, Auer Goetze, David Hessel, Kunsthal Aarhus, Denmark, and is the curator of the forthcoming 2014 AI: Istanbul Biennale, which will see contemporary artists from six continents participating in a year-round programme of exhibitions that will touch on themes such as diversity, identity and language.A Midsummer Night's DreamWritten by Wendy Harpe and Wendy Harpe-Webb, with additional material by Hannah Staeheli and Wendy Harpe-Webb, with additional material by Wendy Harpe and Wendy Harpe, with additional material by Wendy Harpe and Wendy Harpe, with additional material by Wendy Harpe, with additional material by Wendy Harpe</text>
    <text>should be curated to complement one another, rather than to represent them as different but connected],&quot; he said.  	Kaufman, the curator, is the author of several books on the biennial format, including the forthcoming book, Curated: Narratives from the San Francisco Museum of Modern Art's (SFSMA) 2015 Biennale of Exhibitions, which will tell the story of the Biennale from its founding in 1985 to the present. He is the recipient of several awards and has lectured on the topic of biennials.Profile: Jeff Koons Koons is a multi-award-winning artist, whose work has been exhibited in over fifty institutions and published in more than twenty languages. He is the curator of the Museum of Modern Art, New York, USA, where he has curated several exhibitions of contemporary art, including the exhibition  	The Contemporary Condition, 2016, which was included in the 2016 National Portrait Gallery's '10 Best Art Contributions to the Modern World' list; the exhibition,  	The Contemporary Condition, 2017, which was included in the 2016 John Moores &amp; Edward N. Bond Photo Galleries: 100 Years of Contemporary Art; and the 2017,  	The Contemporary Condition, curated by Norman Green and M. King Gaunt, The Gilded Gaze: Art from the Gilded Age. Koons is the recipient of the 2016 Guggenheim Prize for Art and the 2016 Pulitzer Prize for Art and Architecture.Abstract, an exhibition of contemporary art from the Gilded Age to present-day urban settings, explores the social consequences of economic development and the accumulation of capital in the 20th and 21st centuries. It is part of the Guggenheim Museum series, Gilded Gaze, and is the only exhibition in the museum's collection of over 2,000 works from the period. The exhibition runs from November 2015 through February 2018, with a special October exhibition curated by Koons and</text>
    <text>should be curated to complement one another, rather than to represent them as different but connected],&quot; he said.  	Kaufman, the curator, is the author of several books on the biennial format, including the forthcoming book, Curated: Narratives from the San Francisco Museum of Modern Art's (SFSMA) 2015 Biennale of Exhibitions, which will tell the story of the Biennale from its founding in 1985 to the present. He is the recipient of several awards and has lectured on the topic of biennials.Profile: Jeff Koons Koons is a multi-award-winning artist, whose work has been exhibited in over fifty institutions and published in more than twenty languages. He is the curator of the Museum of Modern Art, New York, USA, where he has curated several exhibitions of contemporary art, including the exhibition  	The Contemporary Condition, 2016, which was included in the 2016 National Portrait Gallery's '10 Best Art Contributions to the Twenty First Century' exhibition; the exhibition catalogue STS1001 (2016) and the 2016 John Moores &amp; Company Exhibition Guide (2016), and the 2016 John Lennon Biennial Guide (2016). He is the curator of the Museum of Modern Art, New York, USA, where his work has been featured in numerous books and journals, including the 2016 New York Times Book Review, the 2017 Guggenheim Museum Book, and the 2017 Guggenheim Museum Newsletter. He has been a contributor to the Smithsonian Channel's The Curator and the Guggenheim Museum Newsletter. He has curated the New York Public Theater, Public Art &gt; Art in Public Life, and the Public Art Foundation's Art of the Cities series. He is the curator of the Museum of Modern Art, New York, USA, where he curated the 2016 New Museum Town and the 2016 John Lennon Biennial, and the 2017 Guggenheim Museum Art Commissions, Art and Politics, and the 2017 Public</text>
    <text>should be curated to complement one another, rather than to represent them as different but connected],&quot; he said.  	Kaufman, the curator, is the author of several books on the biennial format, including the forthcoming book, Curated: Narratives from the San Francisco Museum of Modern Art's (SFSMA) 2015 Biennale of Exhibitions, which will tell the story of the Biennale from its founding in 1985 to the present. He is the recipient of several awards and has lectured on the topic of biennials.Profile: Jeff Koons Koons is a multi-award-winning artist, whose work has been exhibited in over fifty institutions and published in more than twenty languages. He is the curator of the Museum of Modern Art, New York, USA, where he has curated several exhibitions of contemporary art, including the exhibition  	The Contemporary Condition, 2016, which was included in the 2016 National Portrait Gallery's '10 Best Art Contributions to the Humanities' list and was exhibited in the Washington, D.C. area house of art, Gallery IV, The Contemporary Condition. He is the curator of the Hawaii Art Gallery, Honolulu, USA, where he has curated several exhibitions of contemporary art and photography, including three recent shows of John Lennon/Plastic Ono Band, Plastic Ono, 2016. He is the curator of the Museum of Modern Art, New York, USA, where he has curated several exhibitions of contemporary art and photography, including the 2014 San Francisco Museum of Modern Art, which was included in the National Portrait Gallery’s list of Top Ten Contributions to Art and Culture, 2015. He has been a curator at the Museum of Modern Art, New York, USA, where he curated the exhibition Plastic Ono Band, Plastic Ono, 2016, and the exhibition The Contemporary Condition, 2017. He has lectured on biennials at the San Francisco Museum of Modern Art, and at the Honolulu</text>
    <text>should be curated to complement one another, rather than to represent them as different but connected],&quot; he said.  	Kaufman, the curator, is the author of several books on the biennial format, including the forthcoming book, Curated: Narratives from the San Francisco Museum of Modern Art's (SFSMA) 2015 Biennale of Exhibitions, which will tell the story of the Biennale from its founding in 1985 to the present. He is the recipient of several awards and has lectured on the topic of biennials.Profile: Jeff Koons Koons is a multi-award-winning artist, whose work has been exhibited in over fifty institutions and published in more than twenty languages. He is the curator of the Museum of Modern Art, New York, USA, where he has curated several exhibitions of contemporary art, including the exhibition  	The Contemporary Condition, 2016, which was included in the 2016 National Portrait Gallery's '10 Best Art Contributions to the Humanities' list. He is the curator of the Museum of Art, Künstlerhaus, München, Germany, where  	The Contemporary Condition, 2016, was placed on the UNESCO World Heritage List. He is the curator of the Museum of Modern Art, Künstlerhaus, München, Germany, where  	The Contemporary Condition, 2017, is on the permanent list of Guggenheim Museum of Art under the category 'contemporary art for contemporary occasions'. Koons is a perennial artist whose works are often employed by museums as examples of what they are up to in the age of the biennial. In 2007, he was the curator of the Metropolitan Museum of Art’s series ‘Stages’ of the 1990 Venice Biennale, which explored the legacy of that event in contemporary art, and the role of the artist in that event’s eventual production. In 2016, he curated the</text>
    <text>should be curated to complement one another, rather than to represent them as different but connected],&quot; he said.  	Kaufman, the curator, is the author of several books on the biennial format, including the forthcoming book, Curated: Narratives from the San Francisco Museum of Modern Art's (SFSMA) 2015 Biennale of Exhibitions, which will tell the story of the Biennale from its founding in 1985 to the present. He is the recipient of several awards and has lectured on the topic of biennials.Profile: Jeff Koons Koons is a multi-award-winning artist, whose work has been exhibited in over fifty institutions and published in more than twenty languages. He is the curator of the Museum of Modern Art, New York, USA, where he has curated several exhibitions of contemporary art, including the exhibition  	The Contemporary Condition, 2016, which was included in the 2016 National Portrait Gallery's '10 Best Art Contributions to the Twenty First Century' list. He is the curator of the Museum of Contemporary Art, Künstlerhaus, Hamburg, Germany, where he curated a major exhibition of the exhibition,  	Exhibitions, 2016, including works by Cindy Sherman, Peter Schjeldahl and many more. He is the curator of the Smithsonian Institution's National Portrait Gallery, Washington, USA, where he curated the US pavilion at the 2016 G8 and took part in the US pavilion's sited installation of David Hockney’s new sculpture, Lightbulb, 2016. He has lectured on contemporary art, visual and performing arts and has co-written a book about the exhibitionary form entitled Why Art Matters.Biennials have recently begun to address broader social issues, and their current curatorial focus on (mainly European) contemporary art is a response to the common neglect and loss of concern for art per se, and the consequent impact on</text>
    <text>should be curated to complement one another, rather than to represent them as different but connected],&quot; he said.  	Kaufman, the curator, is the author of several books on the biennial format, including the forthcoming book, Curated: Narratives from the San Francisco Museum of Modern Art's (SFSMA) 2015 Biennale of Exhibitions, which will tell the story of the Biennale from its founding in 1985 to the present. He is the recipient of several awards and has lectured on the topic of biennials.Profile: Jeff Koons Koons is a multi-award-winning artist, whose work has been exhibited in over fifty institutions and published in more than twenty languages. He is the curator of the Museum of Modern Art, New York, USA, where he has curated several exhibitions of contemporary art, including the exhibition  	The Contemporary Condition, 2016, which was included in the 2016 National Portrait Gallery's '10 Best Art Contributions' series. He is the curator of the Museum of Modern Art, K, Amsterdam, Netherlands, where his work was exhibited in the 2015 Museo de Arte Contempora (art trestle of the MMM), and in 2016 in the monograph series Contemporary Art in a Global Context. He has been the curator of the Museum of Modern Art, New York, USA, where he curated the exhibition  	The Contemporary Condition, 2016, and the 2016 SFSMA San Francisco Museum of Modern Art, which was included in the 2016 Museum of Modern Art's Travel &amp; Cultures in a PDF file. He has also curated the Museum of Modern Art, K, Amsterdam, Netherlands, where he curated the exhibition  	The Contemporary Condition, 2017, and the 2018 SFSMA Venice.Jeff Koons is a multi-disciplinary artist whose practice is primarily concerned with the post-apocalyptic future. His current projects include the film  	Empire of Chaos, 2017</text>
    <text>should be curated to complement one another, rather than to represent them as different but connected],&quot; he said.  	Kaufman, the curator, is the author of several books on the biennial format, including the forthcoming book, Curated: Narratives from the San Francisco Museum of Modern Art's (SFSMA) 2015 Biennale of Exhibitions, which will tell the story of the Biennale from its founding in 1985 to the present. He is the recipient of several awards and has lectured on the topic of biennials.Profile: Jeff Koons Koons is a multi-award-winning artist, whose work has been exhibited in over fifty institutions and published in more than twenty languages. He is the curator of the Museum of Modern Art, New York, USA, where he has curated several exhibitions of contemporary art, including the exhibition  	The Contemporary Condition, 2016, which was included in the 2016 National Portrait Gallery's '10 Best Art Contributions' list. He has curated numerous exhibitions of art from the Museum of Modern Art, London, USA, including the 2014 FACT exhibition on the capital of the United States, the Museum of Modern Art, New York, USA, and the 2014 CALENDAR: Art, Architecture, and Capital – an exhibition of 50 art projects from around the world – curated by Jamie Brock. He has curated several exhibitions of art from the Museum of Modern Art, New York, USA, including the 2014 FACT and CALENDAR: Art, Architecture, and Capital – exhibition projects in the 2016 Venice Biennale, the 2015 FACT and CALENDAR: Art, Architecture, and Capital – the latter was presented in conjunction with the 14th Tokyo Biennale – and has curated the  	20th Sydney Biennale, Australia, the 2015 New York and Museum of Modern Art, and the 2016 London and Sydney Biennales. He has curated several exhibitions</text>
    <text>should be curated to complement one another, rather than to represent them as different but connected],&quot; he said.  	Kaufman, the curator, is the author of several books on the biennial format, including the forthcoming book, Curated: Narratives from the San Francisco Museum of Modern Art's (SFSMA) 2015 Biennale of Exhibitions, which will tell the story of the Biennale from its founding in 1985 to the present. He is the recipient of several awards and has lectured on the topic of biennials.Profile: Jeff Koons Koons is a multi-award-winning artist, whose work has been exhibited in over fifty institutions and published in more than twenty languages. He is the curator of the Museum of Modern Art, New York, USA, where he has curated several exhibitions of contemporary art, including the exhibition  	The Contemporary Condition, 2016, which was included in the 2016 National Portrait Gallery's '10 Best Art Contributions to Human History' list;  	Beyond the Art Museum, 2016, which was included in the 2016 John Moores and Peter Carruthers Worlds Apart: Art from the Margins of History lists; and  	The Contemporary Condition, 2016, which was included in the 2016 list. He has also curated the Venice Biennale, which was included in the London and New York Philharmonic’s '20th Century' list. Biennials are art events that are increasingly rare and costly to produce, yet which also shape contemporary art practice and the way in which art is made, consumed, and appreciated. Contemporaneous is a way to think about art produced and experienced within a given context, at a given time. It is the contemporary equivalent of the biennial, and in that way, it is both a biennial and a museum. Like the biennial, the biennale must have a curator or curatorial team, but unlike the</text>
  </text>
</AIcurator>
