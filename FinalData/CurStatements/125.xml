<?xml version="1.0" ?>
<AIcurator>
  <!--Leonardo Impett, Ubermorgen, Joasia Krysa-->
  <text>
    machines were asked to take over the curatorial role. This came as a surprise to many, since the leaders of the biennale had been warning that the time had come to reassess the role of art institution in the biennale. So the decision to reinstate the biennale as a fully independent entity came as a surprise to many as well. Typically, once a major art institution is displaced in this process it finds itself in a loop, always needing to be repaired. So the reinstatement of the biennale as a fully independent entity came as a surprise to many, and not just because it is part of the art world’s business model. Many artists were surprised by this decision, since the core business of the art world is now dominated by biennials. In some ways, the reinstatement of the biennale as an art institution is part of the paradoxical, contradictory relationship between art and biennials: in their quest for audiences, the biennial takes sides in audiences between biennial and
    <text>machines were asked to take over the curatorial role. This came as a surprise to many, since the leaders of the biennale had been warning that the time had come to reassess the role of art institution in the biennale. So the decision to reinstate the biennale as a fully independent entity came as a surprise to many as well. Typically, once a major art institution is displaced in this process it finds itself in a loop, always needing to be repaired. So the reinstatement of the biennale as a fully independent entity came as a surprise to many, and not just because it is part of the art world’s business model. Many artists were surprised by this decision, since the core business of the art world is now dominated by biennials. In some ways, the reinstatement of the biennale as an art institution is part of the paradoxical, contradictory relationship between art and biennials: in their quest for audiences, the biennial takes sides in audiences between biennial and major art institution. So the decision to reinstate the biennale as a fully independent entity comes as a surprise to many artists, since the biennial business model requires artists to be embedded in the institution for long terms, and also requires artists to leave behind long term curatorial footprints in the art world. I do remember one artist who was sceptical about the decision to reinstate the biennale as an art institution, but she was followed by others who saw this as a way to revitalise a viable independent art world. This is a story about an artist who found herself drawn into a mutually reinforcing loop; an institution that she had resisted for years, and now had to actively participate in just to keep her identity in check, and an artist who found herself simultaneously embedded in both. She did not encounter this situation anywhere else in the world, and it is a story that illustrates the difficulties of breaking away from a recognised cultural body. The biennial is a globalised cultural artefact in</text>
    <text>machines were asked to take over the curatorial role. This came as a surprise to many, since the leaders of the biennale had been warning that the time had come to reassess the role of art institution in the biennale. So the decision to reinstate the biennale as a fully independent entity came as a surprise to many as well. Typically, once a major art institution is displaced in this process it finds itself in a loop, always needing to be repaired. So the reinstatement of the biennale as a fully independent entity came as a surprise to many, and not just because it is part of the art world’s business model. Many artists were surprised by this decision, since the core business of the art world is now dominated by biennials. In some ways, the reinstatement of the biennale as an art institution is part of the paradoxical, contradictory relationship between art and biennials: in their quest for audiences, the biennial takes sides in audiences between biennial and major art institution. In the 1980s, the biennial was often accused of being an obstacle to the development of biennials as a global art form, since they were thought to be at odds with the organization of the exhibitionary complex as an opening to new forms of art. However, the biennial itself is always at odds with the development of art in the modern world, since its development is always accompanied by a proliferation of biennials. If the biennial becomes art, who will take the art? In the 1980s, the biennial became what it calls ‘a set of relationships’, an interface between art and biennials. Is this really the interface between art and biennials, the interface between art and the astronomical world, or is it the other way around? Who knows? And who cares? The important point is that it is the interface between art and biennials in the 21st century. Biennials do not know. They do</text>
    <text>machines were asked to take over the curatorial role. This came as a surprise to many, since the leaders of the biennale had been warning that the time had come to reassess the role of art institution in the biennale. So the decision to reinstate the biennale as a fully independent entity came as a surprise to many as well. Typically, once a major art institution is displaced in this process it finds itself in a loop, always needing to be repaired. So the reinstatement of the biennale as a fully independent entity came as a surprise to many, and not just because it is part of the art world’s business model. Many artists were surprised by this decision, since the core business of the art world is now dominated by biennials. In some ways, the reinstatement of the biennale as an art institution is part of the paradoxical, contradictory relationship between art and biennials: in their quest for audiences, the biennial takes sides in audiences between biennial and major art institution. The second paradoxical aspect of the reinstatement of the biennale as a fully independent entity is that it is now actively searching for spaces in which to house its works, since these are the only ones left standing after the biennale’s main venues closed. So the search for new audiences comes as a surprise to many, since the biennale’s main objective is to maximize its numbers. In other words, the search for new places to show its works comes as a surprise to those who have been systematically excluded from the organization of the art world.In this analysis, I focus on three elements of the biennale that continue to shape contemporary art in North America: the structural changes at the artworld, the exploitation of social media, and the art community’s complicity in the criminalization of the public space. The structure of the biennale has undergone several transformations throughout its history, but none have been as dramatic as those that have</text>
    <text>machines were asked to take over the curatorial role. This came as a surprise to many, since the leaders of the biennale had been warning that the time had come to reassess the role of art institution in the biennale. So the decision to reinstate the biennale as a fully independent entity came as a surprise to many as well. Typically, once a major art institution is displaced in this process it finds itself in a loop, always needing to be repaired. So the reinstatement of the biennale as a fully independent entity came as a surprise to many, and not just because it is part of the art world’s business model. Many artists were surprised by this decision, since the core business of the art world is now dominated by biennials. In some ways, the reinstatement of the biennale as an art institution is part of the paradoxical, contradictory relationship between art and biennials: in their quest for audiences, the biennial takes sides in audiences between biennial and major art institution. In the second paradox of the biennial, the art world and biennials are now in a state of constant tension, which is part of what Bernstein calls ‘the present condition’. These audiences are part of a transient culture that is very much aware of itself in a constantly shifting medium, but also conscious of its own diminishing roots and the need to be constantly repaired. These are the kind of audiences that exist in a city like New York, where the biennale has become a staple of the city’s cultural life, but is also an increasingly marginalised group. The audience for the biennial that we now call the art biennale is a mix of artists, curators, collectors and other curators who create and interpret this transient contemporary art condition.  And this audience is conscious of its own diminishing roots, aware that its true origins are in a distant land and that it is constantly seeking its rescuer. In short, this audience is</text>
    <text>machines were asked to take over the curatorial role. This came as a surprise to many, since the leaders of the biennale had been warning that the time had come to reassess the role of art institution in the biennale. So the decision to reinstate the biennale as a fully independent entity came as a surprise to many as well. Typically, once a major art institution is displaced in this process it finds itself in a loop, always needing to be repaired. So the reinstatement of the biennale as a fully independent entity came as a surprise to many, and not just because it is part of the art world’s business model. Many artists were surprised by this decision, since the core business of the art world is now dominated by biennials. In some ways, the reinstatement of the biennale as an art institution is part of the paradoxical, contradictory relationship between art and biennials: in their quest for audiences, the biennial takes sides in audiences between biennial and art institution, often compromising the latter with its own curatorial style and priorities. Art and the biennial As a Curatorial Practice, Part 1: Introduction 	The biennial has always been a leading example of the ‘art world’’ relationship to art practices, and the dominant paradigm of what a biennial can be, for many years. 	But its ascendance in recent decades has been driven by a combination of factors, both economic and cultural. The economic imperative is clearly the main one, but there is a ‘mixing of interests’ and passions that are of a ‘different sort’. I will devote some time to these differences, since they account for much of what makes the biennial different from any other art form, and account for its enduring power. 	In the social, political, and economic domains, the biennial is a ‘regional phenomenon’. It is a phenomenon that has its source in many local</text>
    <text>machines were asked to take over the curatorial role. This came as a surprise to many, since the leaders of the biennale had been warning that the time had come to reassess the role of art institution in the biennale. So the decision to reinstate the biennale as a fully independent entity came as a surprise to many as well. Typically, once a major art institution is displaced in this process it finds itself in a loop, always needing to be repaired. So the reinstatement of the biennale as a fully independent entity came as a surprise to many, and not just because it is part of the art world’s business model. Many artists were surprised by this decision, since the core business of the art world is now dominated by biennials. In some ways, the reinstatement of the biennale as an art institution is part of the paradoxical, contradictory relationship between art and biennials: in their quest for audiences, the biennial takes sides in audiences between biennial and art world. For example, Biennials tend to produce artists who they then commission to commissioning galleries, museums, porcelain works and so on. The biennial is the only entity capable of producing and supporting the artist. If the biennial is displaced in its mission, it is replaced by a surrogate: the art world’s gallery, museum, gallery network or collection service provider. And this is what has happened: the biennial has to contend with a crisis of legitimacy, while the art world is being overrun by collections. It is the biennial which needs to fend off the collection monster while simultaneously producing important curatorial interventions that help make the collection a collection in its own right. This may be seen as a conundrum, but in my opinion, the biennial has no choice but to adopt the collection monster’s logic. The collection is the new oil and the art world is the new oil. If the collection is displaced in its mission, it is replaced</text>
    <text>machines were asked to take over the curatorial role. This came as a surprise to many, since the leaders of the biennale had been warning that the time had come to reassess the role of art institution in the biennale. So the decision to reinstate the biennale as a fully independent entity came as a surprise to many as well. Typically, once a major art institution is displaced in this process it finds itself in a loop, always needing to be repaired. So the reinstatement of the biennale as a fully independent entity came as a surprise to many, and not just because it is part of the art world’s business model. Many artists were surprised by this decision, since the core business of the art world is now dominated by biennials. In some ways, the reinstatement of the biennale as an art institution is part of the paradoxical, contradictory relationship between art and biennials: in their quest for audiences, the biennial takes sides in audiences between biennial and major museum. But in their desire to remain independent, the biennial must take sides. So, for example, when the biennial is asked to host a major art exhibition, it often struggles to create an image of itself in such a way as to be recognisable by art galleries or the public. It has to find some other image. This is why biennials can be so distrusting of one another. It’s not that they have an image problem, but that they lack image-processing power. And this is why biennials have to learn how to work together.If the biennial is to become more than just a destination point for art, it has to learn how to work alongside other art institutions and create a ‘cultural tourism’ that generates economic activity for the region. It has to learn how to engage local communities in cultural activities that are not exclusive to art tourists. It has to learn how to create ‘cultural institutions’ that actively engage</text>
    <text>machines were asked to take over the curatorial role. This came as a surprise to many, since the leaders of the biennale had been warning that the time had come to reassess the role of art institution in the biennale. So the decision to reinstate the biennale as a fully independent entity came as a surprise to many as well. Typically, once a major art institution is displaced in this process it finds itself in a loop, always needing to be repaired. So the reinstatement of the biennale as a fully independent entity came as a surprise to many, and not just because it is part of the art world’s business model. Many artists were surprised by this decision, since the core business of the art world is now dominated by biennials. In some ways, the reinstatement of the biennale as an art institution is part of the paradoxical, contradictory relationship between art and biennials: in their quest for audiences, the biennial takes sides in audiences between biennial and major art institution. For example, the 2016 Venice Biennale, curated by P. Krishnamurthy, which was originally scheduled to open only on paper, became a major art institution in its own right. The biennial became a major exhibition venue, with works showing in many art galleries around the world. The opening of the Venice Biennale itself was a major event, and generated much media coverage. The showpiece of the Venice Biennale, the Giardini and its gardens, became a major tourist draw, and the promise of massive new developments around the biennale site became a magnet for talented artists to linger, to travel to the city, and to settle permanently. All these developments were intended to create a ‘permanent community’ around the biennale, and to make the biennale a fixture of the local art scene. But the promise of ‘permanent community’ has its limits, and changes the rules of engagement. As bi</text>
    <text>machines were asked to take over the curatorial role. This came as a surprise to many, since the leaders of the biennale had been warning that the time had come to reassess the role of art institution in the biennale. So the decision to reinstate the biennale as a fully independent entity came as a surprise to many as well. Typically, once a major art institution is displaced in this process it finds itself in a loop, always needing to be repaired. So the reinstatement of the biennale as a fully independent entity came as a surprise to many, and not just because it is part of the art world’s business model. Many artists were surprised by this decision, since the core business of the art world is now dominated by biennials. In some ways, the reinstatement of the biennale as an art institution is part of the paradoxical, contradictory relationship between art and biennials: in their quest for audiences, the biennial takes sides in audiences between biennial and art institution. In this way, the biennial is both a promoter of the art and a promoter of the biennial. The biennial is part of the business model, and part of the art world’s revenue stream. So the reinstatement of the biennale as an art institution is a contradictory situation. It maximises the potential revenue from the art market, while simultaneously reducing the institution’s scope of operations and, in the case of large institutions, even its identity. The decision to reintroduce the biennale as an art-only event comes as no surprise to the biennial industry as a whole, for many of the same reasons that art institutions today struggle to adapt to new economic conditions and social needs. But the reinstatement of the biennale as an art-only event comes as a surprise to the artist community, for many of the reasons that artists today (including myself) struggle to adapt to new forms of institutionalised art practice. The biennial</text>
    <text>machines were asked to take over the curatorial role. This came as a surprise to many, since the leaders of the biennale had been warning that the time had come to reassess the role of art institution in the biennale. So the decision to reinstate the biennale as a fully independent entity came as a surprise to many as well. Typically, once a major art institution is displaced in this process it finds itself in a loop, always needing to be repaired. So the reinstatement of the biennale as a fully independent entity came as a surprise to many, and not just because it is part of the art world’s business model. Many artists were surprised by this decision, since the core business of the art world is now dominated by biennials. In some ways, the reinstatement of the biennale as an art institution is part of the paradoxical, contradictory relationship between art and biennials: in their quest for audiences, the biennial takes sides in audiences between biennial and art world. For example, the Lincoln Biennial opened in New York in 2014 with a show by Francis and Isabelle Kampmann Stories from the Trans-Atlantic Divide. In their exhibition The Trans-Atlantic Divide, the artists explain how the divide between the north and the south is reproduced in the biennale format. On the one hand, there is the projection of images of the artistically inclined north and the technologically inclined south; on the other, there is the reality of the situation. In the biennale format, the artist is often asked to take a side. In other words, the biennial is often asked to produce a work that it does not already find aesthetically pleasing, and that it will further thicken its audience’s perception of the region by showing it in a certain way. The biennial is often told it has to be something it is not. While it is true ‘ideologically’ that the biennial is often presented with the false choice</text>
  </text>
  <text>
    prototype for an intelligent curatorial system for recording, mapping and interpreting cultural history that can be reconfigured at will, where physical and virtual barriers to cultural exchange are dismantled. This could be viewed as a form of cultural colonialism, where the practices of historical colonial powers are repurposed and reinterpreted to serve the interests of a globalised elite.Download this article as PDFEmily Pringle and Lily Ashby 	The exhibition ‘Envisioning Global Futures’ took place as part of the Biennale's ongoing 'tipping point' series of cultural tipping points. The term ‘tipping point’ refers to the moment when a crisis or a change in circumstances – political, economic, environmental or sociocultural – initiates a tipping point where radical change begins to take place. At such moments, critical thinking begins to falter, and critical action is stymied by the prevailing consensus. What is needed is a movement of critical thinkers emerging from below the cultural/tipping point and projecting
    <text>prototype for an intelligent curatorial system for recording, mapping and interpreting cultural history that can be reconfigured at will, where physical and virtual barriers to cultural exchange are dismantled. This could be viewed as a form of cultural colonialism, where the practices of historical colonial powers are repurposed and reinterpreted to serve the interests of a globalised elite.Download this article as PDFEmily Pringle and Lily Ashby 	The exhibition ‘Envisioning Global Futures’ took place as part of the Biennale's ongoing 'tipping point' series of cultural tipping points. The term ‘tipping point’ refers to the moment when a crisis or a change in circumstances – political, economic, environmental or sociocultural – initiates a tipping point where radical change begins to take place. At such moments, critical thinking begins to falter, and critical action is stymied by the prevailing consensus. What is needed is a movement of critical thinkers emerging from below the cultural/tipping point and projecting their thinking outward in a variety of non-culturally orientated, locally-influenced ways. This requires that such thinkers accept that current forms of social organisation are the outcome of the historical and contemporary evolution of capitalism, and that transformative social change will require a radical reorganisation of how social life is constructed, governed and executed. This article was written in collaboration with The Scottish Universities Information and Research Centre (SUIC) and was researched for the exhibition with Scottish Universities Digital Archaeology and Society PhD candidate, Emily Pringle. It was written for the catalogue and will be published as a research article in a journal of the Scottish Universities in April 2018.Envisioning Global Futures 	When thinking about the future, we can draw parallels between present and distant times. For instance, in our age of information and social media, it is easy to adopt the idea of a universal future that would bring about equality and sustainability for all. However, in the past when such an idea was being</text>
    <text>prototype for an intelligent curatorial system for recording, mapping and interpreting cultural history that can be reconfigured at will, where physical and virtual barriers to cultural exchange are dismantled. This could be viewed as a form of cultural colonialism, where the practices of historical colonial powers are repurposed and reinterpreted to serve the interests of a globalised elite.Download this article as PDFEmily Pringle and Lily Ashby 	The exhibition ‘Envisioning Global Futures’ took place as part of the Biennale's ongoing 'tipping point' series of cultural tipping points. The term ‘tipping point’ refers to the moment when a crisis or a change in circumstances – political, economic, environmental or sociocultural – initiates a tipping point where radical change begins to take place. At such moments, critical thinking begins to falter, and critical action is stymied by the prevailing consensus. What is needed is a movement of critical thinkers emerging from below the cultural/tipping point and projecting their voices outward in a manner analogous to the Idle No More movement that arose after the 1973 Canadian referendum that placed First Nations people at the core of Canada.1 	As a founding member of the Conference Board of Canada, I am pleased to announce that my conference’s 2017 edition will take place in Islington, Ontario. I look forward to seeing you and Joanne and the others there.DDownload this article as PDFEmily Pringle and Lily Ashby 	The Biennale’s ongoing project ‘The Gathering Storm’ is an ongoing series of public conversations about the past, present and future of Canada and its cultures. The term ‘gathering storm’ is frequently used in connection with environmental and historical-cultural emergencies, and the ensuing public works projects that are intended to recreate and revitalise lost places and landscapes.2 	Gathering storms are typically associated with resource wars (i.e., the resource wars of the past) in which large</text>
    <text>prototype for an intelligent curatorial system for recording, mapping and interpreting cultural history that can be reconfigured at will, where physical and virtual barriers to cultural exchange are dismantled. This could be viewed as a form of cultural colonialism, where the practices of historical colonial powers are repurposed and reinterpreted to serve the interests of a globalised elite.Download this article as PDFEmily Pringle and Lily Ashby 	The exhibition ‘Envisioning Global Futures’ took place as part of the Biennale's ongoing 'tipping point' series of cultural tipping points. The term ‘tipping point’ refers to the moment when a crisis or a change in circumstances – political, economic, environmental or sociocultural – initiates a tipping point where radical change begins to take place. At such moments, critical thinking begins to falter, and critical action is stymied by the prevailing consensus. What is needed is a movement of critical thinkers emerging from below the cultural/tipping point and projecting their thinking outward in response to the dominant order. This is what we can learn from the Envisioning Global Futures (ENG) project, a collaboration between Liverpool John Moores University (Liverpool’s M.Phil. in Geography) and Liverpool John Moores Special Collections and Archives (LCRA) that was started in 2015 and continues to this day. 	As a curatorial gesture and a function of the biennale’s ongoing business, it is also a venue where I am occasionally invited to speak. At the Liverpool Biennial, I usually engage with the tensions and contradictions of contemporary globalisation and its discontents. My talk is often informed by reading and writing about the issues that are raised by the issue of race in the contemporary world. My writing tends to be speculative, questioning the logic of globalisation and highlighting the fact that despite all the technological progress and economic dynamism we engage with – whether it is in the service of supply chains, financial speculation or</text>
    <text>prototype for an intelligent curatorial system for recording, mapping and interpreting cultural history that can be reconfigured at will, where physical and virtual barriers to cultural exchange are dismantled. This could be viewed as a form of cultural colonialism, where the practices of historical colonial powers are repurposed and reinterpreted to serve the interests of a globalised elite.Download this article as PDFEmily Pringle and Lily Ashby 	The exhibition ‘Envisioning Global Futures’ took place as part of the Biennale's ongoing 'tipping point' series of cultural tipping points. The term ‘tipping point’ refers to the moment when a crisis or a change in circumstances – political, economic, environmental or sociocultural – initiates a tipping point where radical change begins to take place. At such moments, critical thinking begins to falter, and critical action is stymied by the prevailing consensus. What is needed is a movement of critical thinkers emerging from below the cultural/tipping point and projecting their ideas and activism onto a global scale. These are the tipping points that artist Susanne Hutchen chose to mark in her exhibition, but others have chosen to do the same. In my opinion, Hutchen’s decision to annotate her text on maps with ‘Global Futures’ and ‘Urban Futures’ in bold, multi-language script is a critical point in her project. It sends a signal to cultural institutions, power brokers and development corporations who control access to information and ideas, and to the vast urban planning and urban development apparatuses that are the product of historical policies and decisions, that there are alternatives out there – alternatives that exist alongside, but apart from and in opposition to their own historical trajectories. It also demonstrates that there are creative and disruptive ways of approaching the daunting task of creating sustainable and egalitarian ways of organising cultural resources and social life in cities.Download this article as PDFEmily Pringle and Lily Ashby 	Within the context</text>
    <text>prototype for an intelligent curatorial system for recording, mapping and interpreting cultural history that can be reconfigured at will, where physical and virtual barriers to cultural exchange are dismantled. This could be viewed as a form of cultural colonialism, where the practices of historical colonial powers are repurposed and reinterpreted to serve the interests of a globalised elite.Download this article as PDFEmily Pringle and Lily Ashby 	The exhibition ‘Envisioning Global Futures’ took place as part of the Biennale's ongoing 'tipping point' series of cultural tipping points. The term ‘tipping point’ refers to the moment when a crisis or a change in circumstances – political, economic, environmental or sociocultural – initiates a tipping point where radical change begins to take place. At such moments, critical thinking begins to falter, and critical action is stymied by the prevailing consensus. What is needed is a movement of critical thinkers emerging from below the cultural/tipping point and projecting their voices outward into the global marketplace of cultural commodities, concepts, practices and assets. The opening salvos of the day were punctured by self-congratulatory musings from the curators, journalists and artists, who appeared to have been lulled into a false sense of security by the opulence, wealth and political significance of the event. The most visible presence of these was provided by the discoverer of Twitter, Elon Musk, who was given a platform on which to spew forth multiple contradictory statements in rapid succession. Some of these statements were simply wrong, and to some extent, at times, demonstrably pernicious. Nevertheless, their intended audience reaped the benefits of the event, normalized the practice of cultural tourism, and generated an awareness and a sense of purpose around the world that can only be imagined by those with a stake in the cultural outcomes. This last bit of cultural capital was most evident in the manifest forms of activism that emerged from the event. The NOMAD</text>
    <text>prototype for an intelligent curatorial system for recording, mapping and interpreting cultural history that can be reconfigured at will, where physical and virtual barriers to cultural exchange are dismantled. This could be viewed as a form of cultural colonialism, where the practices of historical colonial powers are repurposed and reinterpreted to serve the interests of a globalised elite.Download this article as PDFEmily Pringle and Lily Ashby 	The exhibition ‘Envisioning Global Futures’ took place as part of the Biennale's ongoing 'tipping point' series of cultural tipping points. The term ‘tipping point’ refers to the moment when a crisis or a change in circumstances – political, economic, environmental or sociocultural – initiates a tipping point where radical change begins to take place. At such moments, critical thinking begins to falter, and critical action is stymied by the prevailing consensus. What is needed is a movement of critical thinkers emerging from below the cultural/tipping point and projecting this movement forward in a variety of creative and innovative ways. Here, we can begin to examine in more detail the cultural/tipping point, and how it intersects with new age thinking, neopaganism and the like.1 Cultural/tipping points are momentous events that literally tip the balance of cultural power and have a huge impact on the cultural landscape. They are occasions when a cultural practice is elevated to a new level of visibility and demand, and new possibilities for participation and movement open up. I will refer to the seminal work of the Physicist Guy Debord, who coined the term in The Culture of Narcissism: A Modern Attitude to Slavery.2 And I will refer to those who have taken up the call to action since then by calling it ‘collaborative notice’. This is a moment when many different cultural agencies converge to take up the challenge of creating collective futures for their populations through art, design, education and activism</text>
    <text>prototype for an intelligent curatorial system for recording, mapping and interpreting cultural history that can be reconfigured at will, where physical and virtual barriers to cultural exchange are dismantled. This could be viewed as a form of cultural colonialism, where the practices of historical colonial powers are repurposed and reinterpreted to serve the interests of a globalised elite.Download this article as PDFEmily Pringle and Lily Ashby 	The exhibition ‘Envisioning Global Futures’ took place as part of the Biennale's ongoing 'tipping point' series of cultural tipping points. The term ‘tipping point’ refers to the moment when a crisis or a change in circumstances – political, economic, environmental or sociocultural – initiates a tipping point where radical change begins to take place. At such moments, critical thinking begins to falter, and critical action is stymied by the prevailing consensus. What is needed is a movement of critical thinkers emerging from below the cultural/tipping point and projecting their ideas and agency outward, outward of the museum and outward of the biennale. The tipping point may be a particular ‘50s pop-culture reference or two away, but it is essential to truly understand the crisis of culture and its impact on our present and future situations. Critical thinking requires that we break free of the cultural/tipping point, and this breaking away must be accompanied by a movement of cultural/tipping point cultural re-construction. The breaking away must include a movement that is not necessarily a ‘breakaway’ from the dominant cultural form and doing things in an in-the-name-only capacity. Cultural re-construction cannot happen without a cultural revolution, and the revolutionary breakaway must be accompanied by a cultural/tipping point cultural revolution. 	As a starting point, this may be seen as a biennale’s current mission, which is to explore the cultural landscape of the city in an ever-expanding</text>
    <text>prototype for an intelligent curatorial system for recording, mapping and interpreting cultural history that can be reconfigured at will, where physical and virtual barriers to cultural exchange are dismantled. This could be viewed as a form of cultural colonialism, where the practices of historical colonial powers are repurposed and reinterpreted to serve the interests of a globalised elite.Download this article as PDFEmily Pringle and Lily Ashby 	The exhibition ‘Envisioning Global Futures’ took place as part of the Biennale's ongoing 'tipping point' series of cultural tipping points. The term ‘tipping point’ refers to the moment when a crisis or a change in circumstances – political, economic, environmental or sociocultural – initiates a tipping point where radical change begins to take place. At such moments, critical thinking begins to falter, and critical action is stymied by the prevailing consensus. What is needed is a movement of critical thinkers emerging from below the cultural/tipping point and projecting their voices outward to catalyse change, and then standing tall at the other end of the transformative process.‘Envisioning Global Futures’ is the story of this process – the artists, the crises, the resistance, the magic, the riots, the blowback, the boycotts, the war, the fame, the parties – but it is also the story of a resilient core of artists, thinkers and curators who have decided to hold onto their critical gifts and continue the fight. This is the story of four distinct iterations of  	Future City, and the art world’s response to it. 	[i] Brown, Amanda. 	Time to Take Stock: Radical Social Change at a Large Scale. New York: Independent Curators International, 2016. 	[ii] Lemaire, Pascal. 	The Time to Take Stock: A Critique of Participatory Publishing. New York: Independent Curators International, 2015.</text>
    <text>prototype for an intelligent curatorial system for recording, mapping and interpreting cultural history that can be reconfigured at will, where physical and virtual barriers to cultural exchange are dismantled. This could be viewed as a form of cultural colonialism, where the practices of historical colonial powers are repurposed and reinterpreted to serve the interests of a globalised elite.Download this article as PDFEmily Pringle and Lily Ashby 	The exhibition ‘Envisioning Global Futures’ took place as part of the Biennale's ongoing 'tipping point' series of cultural tipping points. The term ‘tipping point’ refers to the moment when a crisis or a change in circumstances – political, economic, environmental or sociocultural – initiates a tipping point where radical change begins to take place. At such moments, critical thinking begins to falter, and critical action is stymied by the prevailing consensus. What is needed is a movement of critical thinkers emerging from below the cultural/tipping point and projecting its momentum outward, creating critical encounter and questioning, while constantly questioning how much of the prevailing order is actually the product of deliberate strategy, and how much is the result of random or unstable encounters. 	[i] William Gibson, ‘Signs and Symbols’, in  	Signs – Social Structure and Civilization, ed. Stefan Collini, Manchester University Press, Manchester, 1986, p.84.Download this article as PDFEmily Pringle and Lily Ashby 	[i] William Gibson, ‘Signs and Symbols’, in  	Signs – Social Structure and Civilization, ed. Stefan Collini, Manchester University Press, Manchester, 1986, p.85.Download this article as PDFEmily Pringle is a Ph.D. candidate in Communication Studies and the History &amp; Theory of Architecture at the University of Alberta, specializing in the social histories and cultural implications of architectural form. Her dissertation examines the psychology of habituation</text>
    <text>prototype for an intelligent curatorial system for recording, mapping and interpreting cultural history that can be reconfigured at will, where physical and virtual barriers to cultural exchange are dismantled. This could be viewed as a form of cultural colonialism, where the practices of historical colonial powers are repurposed and reinterpreted to serve the interests of a globalised elite.Download this article as PDFEmily Pringle and Lily Ashby 	The exhibition ‘Envisioning Global Futures’ took place as part of the Biennale's ongoing 'tipping point' series of cultural tipping points. The term ‘tipping point’ refers to the moment when a crisis or a change in circumstances – political, economic, environmental or sociocultural – initiates a tipping point where radical change begins to take place. At such moments, critical thinking begins to falter, and critical action is stymied by the prevailing consensus. What is needed is a movement of critical thinkers emerging from below the cultural/tipping point and projecting its political, economic, environmental and social change. What is lacking are the necessary cultural institutions, political will and overall economic power to implement such a radical change. That is, of course, until now. In the intervening years, there has been a tendency for curators and arts administrators to adopt a ‘bring your own chair’ approach to cultural organising, and to treat artistic ideas as something that can be produced within a space of artistic possibilities, rather than institutions and creative agencies that actively seek to create and propagate cultural representations of their own creation. 	In this article, I will present several curatorial practices and their relation to the concept of ‘art’ in an artist’s context, and the associated notion of ‘infrastructure’. I will then suggest that the contemporary curatorial practice may be divided into two broad categories: infrastructure and promotion. Curators and arts administrators today increasingly rely on data to determine what can and cannot be done with art, and</text>
  </text>
  <text>
    prototype for an intelligent curatorial system that continually explores its environment’s natural resources, but also the connection between art and the environment that is rarely addressed. The installation is a catalyst for a wider exploration of a city’s architecture and urban history, and is a case study for the kind of thinking and architecture that are necessary to create a city in its own right. 	The installation’s title,  	The Smart City, captures the essential characteristics of a successful urban future – a city with a resilient, self-sustaining economy, a vibrant cultural scene, and a role in shaping its urban future – but also the work involved in creating that future. In the words of John Nash, another of my heroes, ‘A smart city is the infrastructure of a future city that is not a past city, but incorporates all the lessons of the past into the present’.1 	The artwork draws its power from a different kind of urbanism. 	Urban theorist Ray Bradbury
    <text>prototype for an intelligent curatorial system that continually explores its environment’s natural resources, but also the connection between art and the environment that is rarely addressed. The installation is a catalyst for a wider exploration of a city’s architecture and urban history, and is a case study for the kind of thinking and architecture that are necessary to create a city in its own right. 	The installation’s title,  	The Smart City, captures the essential characteristics of a successful urban future – a city with a resilient, self-sustaining economy, a vibrant cultural scene, and a role in shaping its urban future – but also the work involved in creating that future. In the words of John Nash, another of my heroes, ‘A smart city is the infrastructure of a future city that is not a past city, but incorporates all the lessons of the past into the present’.1 	The artwork draws its power from a different kind of urbanism. 	Urban theorist Ray Bradbury, whose popular science children’s classic Earth Boy series was based on a love of nature and a skepticism of man-made infrastructure, articulated a similar vision in his influential but lesser-known book  	The Silent Era – a dystopian pre-apocalyptic future where people have become so dependent on the machine that they can be considered ‘living fossils’.2 	In a 1968 essay titled ‘Vulgar Homages to Antiquity’, Allan Weisman proposes a similar strain of speculative urbanism in which the technocratic modern city is superseded by a posh science-fiction primitive spaceport called ‘Stardust’ – a kind of parallel universe where humans once again become the cyborg animals they become at the end of the space age.3 	Weisman draws parallels between the rise of science and the pseudoscientific notion of the ‘modern’, in which ‘science’ assumed a universal interpretation and control</text>
    <text>prototype for an intelligent curatorial system that continually explores its environment’s natural resources, but also the connection between art and the environment that is rarely addressed. The installation is a catalyst for a wider exploration of a city’s architecture and urban history, and is a case study for the kind of thinking and architecture that are necessary to create a city in its own right. 	The installation’s title,  	The Smart City, captures the essential characteristics of a successful urban future – a city with a resilient, self-sustaining economy, a vibrant cultural scene, and a role in shaping its urban future – but also the work involved in creating that future. In the words of John Nash, another of my heroes, ‘A smart city is the infrastructure of a future city that is not a past city, but incorporates all the lessons of the past into the present’.1 	The artwork draws its power from a different kind of urbanism. 	Urban theorist Ray Bradbury describes how we are trapped in the face of a rapidly urbanising world.2 	The rapid growth of global cities is a defining challenge of the 21st century. It creates a situation in which infrastructure is increasingly important in meeting the demands of a growing population, but infrastructure also becomes an issue – a point of contention between regions and nations that are often at odds on the ground. 	In a research project at the Massachusetts Institute of Technology, I asked local residents, scholars and urbanists what kind of infrastructure they preferred. My choice was a mix of old and new industrial sites, with some urban districts having significant percentages of new office and residential buildings. In the United States, the choice was between a mix of rural and urban infrastructure. As the landscape has shifted, so too has the need for infrastructure to adapt. The need for new infrastructure has shifted as well, but while the infrastructure has to adapt, it cannot override its changing needs. As a result, decisions to build roads, bridges</text>
    <text>prototype for an intelligent curatorial system that continually explores its environment’s natural resources, but also the connection between art and the environment that is rarely addressed. The installation is a catalyst for a wider exploration of a city’s architecture and urban history, and is a case study for the kind of thinking and architecture that are necessary to create a city in its own right. 	The installation’s title,  	The Smart City, captures the essential characteristics of a successful urban future – a city with a resilient, self-sustaining economy, a vibrant cultural scene, and a role in shaping its urban future – but also the work involved in creating that future. In the words of John Nash, another of my heroes, ‘A smart city is the infrastructure of a future city that is not a past city, but incorporates all the lessons of the past into the present’.1 	The artwork draws its power from a different kind of urbanism. 	Urban theorist Ray Bradbury describes how cities have altered their shape and function over the course of the twentieth century: from the leafy suburban sprawl of the 1950s, to the dense urban centers of the 1990s and now, to the ‘deep city’ of Robert Brown Johnson’s Greater Birmingham’s Modern Homestead.2 	The aim of this new kind of urbanism, he claims, is to be able to retreat into ‘the city’ when faced with catastrophic events such as terrorism.3 	However, the new kind of urbanism that emerges in response to catastrophes is not necessarily sustainable or even advisable, and catastrophe seems to be increasingly becoming the rule rather than the exception. Bradbury argues that the place to look for new forms of urbanity is in ‘the peripheries’ of the city, where the social consequences of globalisation are most acute, ‘where the conflict between capital and labour…is the most acute’.4</text>
    <text>prototype for an intelligent curatorial system that continually explores its environment’s natural resources, but also the connection between art and the environment that is rarely addressed. The installation is a catalyst for a wider exploration of a city’s architecture and urban history, and is a case study for the kind of thinking and architecture that are necessary to create a city in its own right. 	The installation’s title,  	The Smart City, captures the essential characteristics of a successful urban future – a city with a resilient, self-sustaining economy, a vibrant cultural scene, and a role in shaping its urban future – but also the work involved in creating that future. In the words of John Nash, another of my heroes, ‘A smart city is the infrastructure of a future city that is not a past city, but incorporates all the lessons of the past into the present’.1 	The artwork draws its power from a different kind of urbanism. 	Urban theorist Ray Bradbury describes in his classic From the Foundation that he witnessed growing up:‘Every time a building was finished in Brooklyn, or any section of Manhattan, the view of the river was improved by constructing a high-voltage substation in the middle of the street. Then, when the whole section was complete, the local electric company cut the power to the street due to lack of demand. So the substation – which provided both electrical and banking services to the borough – became a new form of private utility.’2 	But Bradbury wasn’t the only kid growing up in the 1950s and 1960s in the borough of Manhattan who witnessed the grid as an urban infrastructure that needed to be improved. 	Many other kids his age also grew up in subways, in schools known as ‘the Hole’ or ‘the Block’ where graffiti and recess were the norm, and where smoking was discouraged because of its health hazards. These schools also instilled</text>
    <text>prototype for an intelligent curatorial system that continually explores its environment’s natural resources, but also the connection between art and the environment that is rarely addressed. The installation is a catalyst for a wider exploration of a city’s architecture and urban history, and is a case study for the kind of thinking and architecture that are necessary to create a city in its own right. 	The installation’s title,  	The Smart City, captures the essential characteristics of a successful urban future – a city with a resilient, self-sustaining economy, a vibrant cultural scene, and a role in shaping its urban future – but also the work involved in creating that future. In the words of John Nash, another of my heroes, ‘A smart city is the infrastructure of a future city that is not a past city, but incorporates all the lessons of the past into the present’.1 	The artwork draws its power from a different kind of urbanism. 	Urban theorist Ray Bradbury has it all – a post-Apocalyptic future where civilisations are few and far between – but what he lacks in detail, he makes up for with passion, determination and an incredible amount of inspiration. His dystopian novel and movie Earthsea, published in 1954, is among my all-time favourite science-fiction novels, and the setting for several other books and stories. Earthsea itself was adapted into a Broadway musical in 1937, and Bradbury was a writer-producer, so he knew a thing or two about Broadway stardom when he saw it. Bradbury had faith. 	A smart city is a question that demands answers, and while there are few today with the requisite panache and charisma to bear the cognitive load required, the answers can be found in every city’s urban folklore. 	In  	World War Z (2007), Max Brooks represents the city as an enormous, unwieldy beast that requires constant monitoring and control. Brooks describes the</text>
    <text>prototype for an intelligent curatorial system that continually explores its environment’s natural resources, but also the connection between art and the environment that is rarely addressed. The installation is a catalyst for a wider exploration of a city’s architecture and urban history, and is a case study for the kind of thinking and architecture that are necessary to create a city in its own right. 	The installation’s title,  	The Smart City, captures the essential characteristics of a successful urban future – a city with a resilient, self-sustaining economy, a vibrant cultural scene, and a role in shaping its urban future – but also the work involved in creating that future. In the words of John Nash, another of my heroes, ‘A smart city is the infrastructure of a future city that is not a past city, but incorporates all the lessons of the past into the present’.1 	The artwork draws its power from a different kind of urbanism. 	Urban theorist Ray Bradbury described the future as ‘a city on a shoestring budget’, without specifying how that future would be funded or where the funding would come from.2 	But the kind of speculative and augmented reality art installations that have become prominent in the last decade are reflections of a different kind of future. 	Such art installations are collections of historical artefacts, selected and arranged in a fashion that bestows cultural value on a particular site or set of sites. They act as monuments to a specific kind of urbanisation, and suggest future growth for a particular kind of city. In the twentieth century, these sorts of cultural capital assets became what we might call ‘metro’ types of buildings – structures that span decades or even centuries in the making and in no way are they replicating the technological marvels of the twentieth century. 	But the interactions between citizen artists, natural-resource collectors, urban planners and financial institutions that create and promote urban growth are just as revealing</text>
    <text>prototype for an intelligent curatorial system that continually explores its environment’s natural resources, but also the connection between art and the environment that is rarely addressed. The installation is a catalyst for a wider exploration of a city’s architecture and urban history, and is a case study for the kind of thinking and architecture that are necessary to create a city in its own right. 	The installation’s title,  	The Smart City, captures the essential characteristics of a successful urban future – a city with a resilient, self-sustaining economy, a vibrant cultural scene, and a role in shaping its urban future – but also the work involved in creating that future. In the words of John Nash, another of my heroes, ‘A smart city is the infrastructure of a future city that is not a past city, but incorporates all the lessons of the past into the present’.1 	The artwork draws its power from a different kind of urbanism. 	Urban theorist Ray Bradbury articulated the need for a radically different kind of urbanism in the wake of the end of World War II, in part because, as he wrote, ‘the technological marvels of war … have permanently debouched the urban domain in the framework of modern society … Modern urban life is no longer the urban domain of kings, communists, and the poor … modern urban life is now the urban domain of multimillionaires and fabliers … modern urban life is an extravagant and vortexing form of material abundance that is as natural as breathing … urban life is now the domain of the Net and Net‐based Communications … urban life embraces all media of electronic transformation, all media of implicit exchange, and organises that material abundance into fully‐realisable forms]: it is a mode of image, a mode of cultural ­ambience, a mode of settlement. … urban life today embraces multillions of people living in sprawling settlements in which physical and electronic travel is greatly restricted and in which language, housing,</text>
    <text>prototype for an intelligent curatorial system that continually explores its environment’s natural resources, but also the connection between art and the environment that is rarely addressed. The installation is a catalyst for a wider exploration of a city’s architecture and urban history, and is a case study for the kind of thinking and architecture that are necessary to create a city in its own right. 	The installation’s title,  	The Smart City, captures the essential characteristics of a successful urban future – a city with a resilient, self-sustaining economy, a vibrant cultural scene, and a role in shaping its urban future – but also the work involved in creating that future. In the words of John Nash, another of my heroes, ‘A smart city is the infrastructure of a future city that is not a past city, but incorporates all the lessons of the past into the present’.1 	The artwork draws its power from a different kind of urbanism. 	Urban theorist Ray Bradbury describes how cities’ infrastructure – roads, bridges, tunnels, water tanks, tunnels, tunnels – evolved in response to the advent of interstate highways and the pan-urban development that followed them, particularly in the wake of the ‘dot-com bust’ in the early 1990s.2 	By the mid-1990s, Bradbury and others had developed a ‘space-time equation’ that attempted to reconcile the appearance of physical objects with the thought process that creates them. In other words, the ‘real life’ that ‘knew no form’ was being generated by the connectivity of physical objects. The emergence of new technologies allowed ‘imaginary picture-files’ to be created and shared, making it easier to access information and making it easier to ‘digest information’ via social media. This integration of the physical with the dynamic – and sometimes messy – world of online social interaction made it easier to connect the dots between</text>
    <text>prototype for an intelligent curatorial system that continually explores its environment’s natural resources, but also the connection between art and the environment that is rarely addressed. The installation is a catalyst for a wider exploration of a city’s architecture and urban history, and is a case study for the kind of thinking and architecture that are necessary to create a city in its own right. 	The installation’s title,  	The Smart City, captures the essential characteristics of a successful urban future – a city with a resilient, self-sustaining economy, a vibrant cultural scene, and a role in shaping its urban future – but also the work involved in creating that future. In the words of John Nash, another of my heroes, ‘A smart city is the infrastructure of a future city that is not a past city, but incorporates all the lessons of the past into the present’.1 	The artwork draws its power from a different kind of urbanism. 	Urban theorist Ray Bradbury wrote about the pernicious effects of bureaucratised urban infrastructure in his book Neck Deep (1949), and the film Groundhog Day (1964), in which a farmer leads a group of people from his field into the city to avoid having to deal with the legacy of bureaucracy.2 	Bradbury describes the ‘wet dream’ – the pernicious effects of unchecked urban growth – and the city as a form of continuous reanimation: ‘The urban problem is no longer an ancien réaliste problem, but a diaimairemos problem – in which the problem is not an entity, but an immaterial form – but rather a kind of animate and inorganic assemblage of parts.’3 	Bradbury was talking about a single urban sector in particular: the peripheries of Paris, where the Third Republic began in 1968. In that year, protestors took to the streets in massive demonstrations against the war in Vietnam, and</text>
    <text>prototype for an intelligent curatorial system that continually explores its environment’s natural resources, but also the connection between art and the environment that is rarely addressed. The installation is a catalyst for a wider exploration of a city’s architecture and urban history, and is a case study for the kind of thinking and architecture that are necessary to create a city in its own right. 	The installation’s title,  	The Smart City, captures the essential characteristics of a successful urban future – a city with a resilient, self-sustaining economy, a vibrant cultural scene, and a role in shaping its urban future – but also the work involved in creating that future. In the words of John Nash, another of my heroes, ‘A smart city is the infrastructure of a future city that is not a past city, but incorporates all the lessons of the past into the present’.1 	The artwork draws its power from a different kind of urbanism. 	Urban theorist Ray Bradbury describes a ‘transcendental city’ that is both ‘a synthesis of physical and conceptual structures of the past, a return to the former “image” of cities, but without the latter” baggage.2 	The future is somewhere between the past and the present, but it is constantly evolving in an unpredictable and sometimes chaotic process of new features, new inhabitant states and new ‘forms’ that are constantly emerging. The future is a place where contingency – a shifting of place, of time, of movement – is part of its dynamic. 	A future city is constantly searching for ways to accommodate and accommodate itself to the shifting places it finds itself in – both physically and socially. It looks to the past, and constructs new possibilities for the future. Such possibilities exist both physically and cognitively, in the form of physical and virtual media – media that permeate all aspects of contemporary urban life, from the billboards and platforms of the city to the</text>
  </text>
  <text>
    biennials past and present. Originally founded in 1887, and centred on the life of Charles I, it was the largest annual arts and crafts fair in Europe, and is widely considered to be the birth of the biennial. The exhibition’s title, ‘The Biennial Image’, derives from a lecture given by Isobel White that year, and echoes another lecture from White by the same name given at the University of Chicago in 1889. In her own words, ‘The Biennial Image was intended to show to a world of its own size the grandeur and magnificence of the human species.’[10] White was speaking of the over-all transformations that history had wrought on the planet, and particularly on the human species within the biennial format. In the words of David Hocking, who was then curator of art and design at the New York Academy of Sciences, ‘The Biennial has itself begun to read the dramatic picture that it makes.�
    <text>biennials past and present. Originally founded in 1887, and centred on the life of Charles I, it was the largest annual arts and crafts fair in Europe, and is widely considered to be the birth of the biennial. The exhibition’s title, ‘The Biennial Image’, derives from a lecture given by Isobel White that year, and echoes another lecture from White by the same name given at the University of Chicago in 1889. In her own words, ‘The Biennial Image was intended to show to a world of its own size the grandeur and magnificence of the human species.’[10] White was speaking of the over-all transformations that history had wrought on the planet, and particularly on the human species within the biennial format. In the words of David Hocking, who was then curator of art and design at the New York Academy of Sciences, ‘The Biennial has itself begun to read the dramatic picture that it makes.�[11] The impact of this reading was felt not only in the academy, but was felt also in the wider public, who came to associate the Academy with its former imperial pedigree and imperial lineage. The public, or public gallery, as we know it today, emerges from its galleries and museum collections and social events, such as football games, lectures and performances, ball games, topsy-turvy medieval botanical gardens, and on to pre-historic hunter-gatherer cabarets, to look at old photographs and documents. It is this extended public, this public gallery, that tells the story of art and its place in it. It is the gallery that plays host to the performances, the speeches, the debates, the discoveries and the resistance to what it means to be human.[12]  In other words, the gallery serves not only to archive and present the past, but also to participate in the past by making what Hocking would call ‘pastiche’ of</text>
    <text>biennials past and present. Originally founded in 1887, and centred on the life of Charles I, it was the largest annual arts and crafts fair in Europe, and is widely considered to be the birth of the biennial. The exhibition’s title, ‘The Biennial Image’, derives from a lecture given by Isobel White that year, and echoes another lecture from White by the same name given at the University of Chicago in 1889. In her own words, ‘The Biennial Image was intended to show to a world of its own size the grandeur and magnificence of the human species.’[10] White was speaking of the over-all transformations that history had wrought on the planet, and particularly on the human species within the biennial format. In the words of David Hocking, who was then curator of art and design at the New York Academy of Sciences, ‘The Biennial has itself begun to read the dramatic picture that it makes.�[11] The arcades, skyscrapers, ferris wheels and tire tracks of the metropolis were all replaced with sleek, low-maintenance infrastructure that offered ‘little resistance to the changing character of the technological revolution, and perhaps even little awareness of its own impending destruction’.[12] The emphasis of the messages of both lectures shifted from the present to the future, and technological progress coincided with a shift in the calendar. In the nineteenth century, the first appearance of the calendar in China was in response to the appearance of Islam’s Great Calendar (ca. 1269–87), which included figures of the emperor from different periods in Chinese history. The body of Islam’s emperor was divided into thirty-six lunar months, which were followed by forty-nine lunar years, or 1,640 years. The forty-nine lunar months and 1,640 years were considered heretofore to be incompatible with a calendar, but in the nineteenth century many western cultures</text>
    <text>biennials past and present. Originally founded in 1887, and centred on the life of Charles I, it was the largest annual arts and crafts fair in Europe, and is widely considered to be the birth of the biennial. The exhibition’s title, ‘The Biennial Image’, derives from a lecture given by Isobel White that year, and echoes another lecture from White by the same name given at the University of Chicago in 1889. In her own words, ‘The Biennial Image was intended to show to a world of its own size the grandeur and magnificence of the human species.’[10] White was speaking of the over-all transformations that history had wrought on the planet, and particularly on the human species within the biennial format. In the words of David Hocking, who was then curator of art and design at the New York Academy of Sciences, ‘The Biennial has itself begun to read the dramatic picture that it makes.�[11] The biennial as a gathering of curators, art collectors and art writers has become the authoritative read on art and art history in the twenty-first century. White, speaking of the event in her 1965 book The Biennial Experience, acknowledges that she gave the appearance of a museum, but also that she ‘had gone about the work of making a biennial in the city without any particular interest in museums or in art’.[12] These are rare words for a curator in White’s day, but there she articulates the essence of her curatorial approach, both as an artist and as a researcher – an approach that reflects the prevailing mood of the time but is independent, evenhanded and unconstrained. She also provides a rich tapestry to the field of anthropology, drawing on her own anthropological and ethnographic work, as well as on the curatorial instincts of many other curators and artists of the time. She begins her account of</text>
    <text>biennials past and present. Originally founded in 1887, and centred on the life of Charles I, it was the largest annual arts and crafts fair in Europe, and is widely considered to be the birth of the biennial. The exhibition’s title, ‘The Biennial Image’, derives from a lecture given by Isobel White that year, and echoes another lecture from White by the same name given at the University of Chicago in 1889. In her own words, ‘The Biennial Image was intended to show to a world of its own size the grandeur and magnificence of the human species.’[10] White was speaking of the over-all transformations that history had wrought on the planet, and particularly on the human species within the biennial format. In the words of David Hocking, who was then curator of art and design at the New York Academy of Sciences, ‘The Biennial has itself begun to read the dramatic picture that it makes.�[11] The question, then, becomes whether the biennial format can reclaim what is lost at the moment of its liquidation. This was perhaps what the biennials of the 1990s tried to do, by bringing together the disparate parts of the museum, and giving them a collective body, and encouraging them to work together. The problem with this approach, however, is that the parts quickly start to interpenetrate, and there is a tendency to propagate the idea that the whole is greater than the sum of its parts. When biennials ceased to be, and museums became the place where they held exhibitions, and became places where we found out more about the people who created them, the tendency was to down-sample these various exhibitions in order to learn more about the people who created the biennial itself. And while this is true of any biennial, there is a problem with this approach to the nadir, as Hock would say. When biennials stopped</text>
    <text>biennials past and present. Originally founded in 1887, and centred on the life of Charles I, it was the largest annual arts and crafts fair in Europe, and is widely considered to be the birth of the biennial. The exhibition’s title, ‘The Biennial Image’, derives from a lecture given by Isobel White that year, and echoes another lecture from White by the same name given at the University of Chicago in 1889. In her own words, ‘The Biennial Image was intended to show to a world of its own size the grandeur and magnificence of the human species.’[10] White was speaking of the over-all transformations that history had wrought on the planet, and particularly on the human species within the biennial format. In the words of David Hocking, who was then curator of art and design at the New York Academy of Sciences, ‘The Biennial has itself begun to read the dramatic picture that it makes.�[11] White was not alone in his vision of a change. In 1889, American anthropologist Benjamin Bratton published a much vaster and more expansive work, Inventing the Image, in which he sought to ground biennials in a universal mythology. He argued that the primary reason for biennials being established in the first place was that their creators saw a need to justify their existence, and to this end ‘the museum became, in the main, a place of exhibition and not merely of recurrence’.[12] In other words, museums – and, by this, humans – make museums. Museums, he maintained, are necessary to the culture industry because they hold back the magic. Without this magic, the biennial would simply cease to exist. The impulse behind the biennial is clearly still with us, and a museum, like any other, plays an essential role in the industry. But Bratton’s idea that the museum is a magic-</text>
    <text>biennials past and present. Originally founded in 1887, and centred on the life of Charles I, it was the largest annual arts and crafts fair in Europe, and is widely considered to be the birth of the biennial. The exhibition’s title, ‘The Biennial Image’, derives from a lecture given by Isobel White that year, and echoes another lecture from White by the same name given at the University of Chicago in 1889. In her own words, ‘The Biennial Image was intended to show to a world of its own size the grandeur and magnificence of the human species.’[10] White was speaking of the over-all transformations that history had wrought on the planet, and particularly on the human species within the biennial format. In the words of David Hocking, who was then curator of art and design at the New York Academy of Sciences, ‘The Biennial has itself begun to read the dramatic picture that it makes.�[11] Hocking was certainly right up to his armpits in the late nineteenth century – the historical significance of the present, and the sheer scale of the cultural processes that could be traced to the biennial, balloon and all – but there are other forces at work, and they are exerting increasingly greater pressures on the biennial form. The biennial has undergone multiple codirections over the past two centuries, and its current exhibition format is the product of numerous other cultural and economic factors, many of which are now considered negligible or have been rendered obsolete by the ubiquity of electronic media. It was not always so. Titled in 1897 by a committee of art historians headed up by John Lennon of the New York Academy of Sciences, and including members of the Academy's Professional Gallery of Art, the committee’s recommendations were presented to the Board of Trustees and President Peter Knauskas. In a somewhat surreal turn of events, these same biennials in the years</text>
    <text>biennials past and present. Originally founded in 1887, and centred on the life of Charles I, it was the largest annual arts and crafts fair in Europe, and is widely considered to be the birth of the biennial. The exhibition’s title, ‘The Biennial Image’, derives from a lecture given by Isobel White that year, and echoes another lecture from White by the same name given at the University of Chicago in 1889. In her own words, ‘The Biennial Image was intended to show to a world of its own size the grandeur and magnificence of the human species.’[10] White was speaking of the over-all transformations that history had wrought on the planet, and particularly on the human species within the biennial format. In the words of David Hocking, who was then curator of art and design at the New York Academy of Sciences, ‘The Biennial has itself begun to read the dramatic picture that it makes.�[11] White was responding to the dominant cultural imagination of the time, and her talk, given in Chicago that September, was a classic case in point. In ‘Time to Come’ (in tribute to John Lennon), White describes how, as a ‘motorcyclist, I’d once stopped for days at ‘Ridgeline Road’ in the Yorkshire Dales to ‘watch the wildlife’. ‘Once in a while’ she continues, ‘I’d like to take a picture of a particular rock formation. I’d like to post it on Facebook and send it to you, so that you can see it a hundred times.’[12] White is clearly articulately addressing the way in which the biennial is structurally similar to other art forms: how, at a macro-level, the biennial is a function of social media; how, at a micro-level,</text>
    <text>biennials past and present. Originally founded in 1887, and centred on the life of Charles I, it was the largest annual arts and crafts fair in Europe, and is widely considered to be the birth of the biennial. The exhibition’s title, ‘The Biennial Image’, derives from a lecture given by Isobel White that year, and echoes another lecture from White by the same name given at the University of Chicago in 1889. In her own words, ‘The Biennial Image was intended to show to a world of its own size the grandeur and magnificence of the human species.’[10] White was speaking of the over-all transformations that history had wrought on the planet, and particularly on the human species within the biennial format. In the words of David Hocking, who was then curator of art and design at the New York Academy of Sciences, ‘The Biennial has itself begun to read the dramatic picture that it makes.�[11] White was no stranger to these narratives, having been a writer and playwright, and her ideas on the significance of art and social change lay at the root of much of what we now understand to be the biennial form. Throughout her career, she marshalled a talented and determined staff of writers and artists to write and illustrate the stories that she fed into the form. White was not without her critics, and the twenty-first century will likely see a renewed assault on the form, but her critics will stand alongside those who came before them and will tell the stories that remain. When looking to the future, biennials offer a way to imagine how the past could have been different. They point to the ways in which, although the forces of biennials may no longer align, the past can nevertheless be recalled and the present recalled. In that case, the question is, how do we live our own biennials? When looking to the future, biennials</text>
    <text>biennials past and present. Originally founded in 1887, and centred on the life of Charles I, it was the largest annual arts and crafts fair in Europe, and is widely considered to be the birth of the biennial. The exhibition’s title, ‘The Biennial Image’, derives from a lecture given by Isobel White that year, and echoes another lecture from White by the same name given at the University of Chicago in 1889. In her own words, ‘The Biennial Image was intended to show to a world of its own size the grandeur and magnificence of the human species.’[10] White was speaking of the over-all transformations that history had wrought on the planet, and particularly on the human species within the biennial format. In the words of David Hocking, who was then curator of art and design at the New York Academy of Sciences, ‘The Biennial has itself begun to read the dramatic picture that it makes.�[11] The significance of this shift in framing cannot be understated. White is credited with pioneering the concept of the biennial, and it is worth noting that the word ‘biennial’ itself was not widely used in the scientific or scholarly literature at the time, and its precise definition is uncertain. In her own words, ‘The term “biennial” is derived from the Greek and Roman biennials, which were the principal forms of show in the ancient world. But it is also a corruption of the word, which originally meant a collection of collections or collections, as in “Encyclopedias”, “Phaedrus” and “Plates”.’[12] Such an understanding of the word does not appear in the Oxford English Dictionary, which defines the word as follows: biennial’ includes ‘a collection of collections or collections … [and] a series of events … [including] exhibits</text>
    <text>biennials past and present. Originally founded in 1887, and centred on the life of Charles I, it was the largest annual arts and crafts fair in Europe, and is widely considered to be the birth of the biennial. The exhibition’s title, ‘The Biennial Image’, derives from a lecture given by Isobel White that year, and echoes another lecture from White by the same name given at the University of Chicago in 1889. In her own words, ‘The Biennial Image was intended to show to a world of its own size the grandeur and magnificence of the human species.’[10] White was speaking of the over-all transformations that history had wrought on the planet, and particularly on the human species within the biennial format. In the words of David Hocking, who was then curator of art and design at the New York Academy of Sciences, ‘The Biennial has itself begun to read the dramatic picture that it makes.�[11] (AKI) Recent Developments in the Biennial Industry and Exhibition Research Lab (BRDL)  	The Biennial industry and its curators today are not Homer Simpson or Woody Woodpecker, but rather employ scientists, lawyers, artists and journalists. They are part of a globalised media conglomerate, and their work is presented as if it were a brand new phenomenon. They are experts in the field   	 of art, and speak with authority about the significance of biennials as a new form of art form. 	Such claims are highly suspect, however, since the biennial has been around for thousands of years, and cultural theorists have long known that art should be curatorial. The Biennial boom therefore seems to be occurring at a time when cultural theorists and artists’intellectuals’ own vested interests are at odds with the needs of the greater public. 	[1] See: http://www.biennials</text>
  </text>
  <text>
    biennials past and present, as well as the organiser of the upcoming Venice Biennale, which is hosting the Venice Architecture Biennale (2017) in its entirety. 	[1] See, for example, Aarhus Documenta 10 (2015); Aarhus Documenta 10 (2015); Manen Lyset and Christian Körner, eds.,  	Architecture of the Consumer in a World City, Blackwell, Oxford and Cambridge, MA and London and New York, New York, and Berhampore, New Jersey and Johns Hopkins University Press, Baltimore, MD, with Tina Latimer and Sarah Friedland, eds.,  	The Architecture of a Consumer Culture, Blackwell, Oxford and Cambridge, MA and New York, New York, and Simon &amp; Schuster, New York, USA, and International Journal of Consumer Research, Vol. 21, No.1, pp.105–14, 2011. 	[2] The term ‘bi
    <text>biennials past and present, as well as the organiser of the upcoming Venice Biennale, which is hosting the Venice Architecture Biennale (2017) in its entirety. 	[1] See, for example, Aarhus Documenta 10 (2015); Aarhus Documenta 10 (2015); Manen Lyset and Christian Körner, eds.,  	Architecture of the Consumer in a World City, Blackwell, Oxford and Cambridge, MA and London and New York, New York, and Berhampore, New Jersey and Johns Hopkins University Press, Baltimore, MD, with Tina Latimer and Sarah Friedland, eds.,  	The Architecture of a Consumer Culture, Blackwell, Oxford and Cambridge, MA and New York, New York, and Simon &amp; Schuster, New York, USA, and International Journal of Consumer Research, Vol. 21, No.1, pp.105–14, 2011. 	[2] The term ‘biennial’ was coined by the late A. Dietrich Wickham,  	The Art of Not Being Governed, McGraw-Hill, New Jersey, 1976.Download this article as PDFJussi ParikkaJussi Parikka is Professor of Geography and Head of the Department of the Arts and Sciences at the Winchester College of Arts and Sciences. His research interests include postcolonial studies, art history and postcolonial criticism, and he has worked closely with communities of artists and writers from across the globe. His publications include the nonfiction best-sellers  	Unravelled (2013) and the nonfiction  	Unravelled: Travels in the Afterlife of Artists (2014), as well as the nonfiction best-sellers  	8 Placesthe Rise of Global North (2015) and  	Towards a Consumer Economy (2017). His forthcoming book, ‘Beyond the Bounds of Capital: Finance</text>
    <text>biennials past and present, as well as the organiser of the upcoming Venice Biennale, which is hosting the Venice Architecture Biennale (2017) in its entirety. 	[1] See, for example, Aarhus Documenta 10 (2015); Aarhus Documenta 10 (2015); Manen Lyset and Christian Körner, eds.,  	Architecture of the Consumer in a World City, Blackwell, Oxford and Cambridge, MA and London and New York, New York, and Berhampore, New Jersey and Johns Hopkins University Press, Baltimore, MD, with Tina Latimer and Sarah Friedland, eds.,  	The Architecture of a Consumer Culture, Blackwell, Oxford and Cambridge, MA and New York, New York, and Simon &amp; Schuster, New York, USA, and International Journal of Consumer Research, Vol. 21, No.1, pp.105–14, 2011. 	[2] The term ‘biennial’ is used here as a synonym for ‘archaeological digression’. 	[3] See Frank J. Azor and Christian Körner, eds.,  	Architecture of the Consumer in a World City, Blackwell, Oxford and Cambridge, MA and London and New York, New York, 2015, for a more detailed discussion of the ways in which the biennial form destabilises its sites and practitioners. 	[4] Frank J. Azor and Christian Körner, eds.,  	Architecture of the Consumer in a World City, Blackwell, Oxford and Cambridge, MA and London and New York, New York, 2015, pp.148–59. 	[5] Frank J. Azor and Christian Körner, eds.,  	Architecture of the Consumer in a World City, Blackwell, Oxford and Cambridge, MA and London and New York, New York,</text>
    <text>biennials past and present, as well as the organiser of the upcoming Venice Biennale, which is hosting the Venice Architecture Biennale (2017) in its entirety. 	[1] See, for example, Aarhus Documenta 10 (2015); Aarhus Documenta 10 (2015); Manen Lyset and Christian Körner, eds.,  	Architecture of the Consumer in a World City, Blackwell, Oxford and Cambridge, MA and London and New York, New York, and Berhampore, New Jersey and Johns Hopkins University Press, Baltimore, MD, with Tina Latimer and Sarah Friedland, eds.,  	The Architecture of a Consumer Culture, Blackwell, Oxford and Cambridge, MA and New York, New York, and Simon &amp; Schuster, New York, USA, and International Journal of Consumer Research, Vol. 21, No.1, pp.105–14, 2011. 	[2] The term ‘biennial’ derives from the Latin biannum, a ‘book series’, and is used here to designate those areas of the world with recurring or periodic exhibitionary exhibitions. See Aarhus Documenta 10, Aarhus Documenta 10, 2015, p.1. 	[3] James Howard Kunstler,  	The Art of Not Being Governed, Verso, New York, 1988, p.1.Download this article as PDFJames Howard Kunstler is an artist. His work in over forty media has taken the form of ‘biennials’ in which subjects are interrogated and often satirised. His most recent work for the New York Times Magazine is a profile of the magazine’s Managing Editor Jill Abramson. Kunstler has written for The New York Times, The New York Review of Books, Spin, GQ, The New Yorker, The New Republic, The New York Tribune, The Washington Post and</text>
    <text>biennials past and present, as well as the organiser of the upcoming Venice Biennale, which is hosting the Venice Architecture Biennale (2017) in its entirety. 	[1] See, for example, Aarhus Documenta 10 (2015); Aarhus Documenta 10 (2015); Manen Lyset and Christian Körner, eds.,  	Architecture of the Consumer in a World City, Blackwell, Oxford and Cambridge, MA and London and New York, New York, and Berhampore, New Jersey and Johns Hopkins University Press, Baltimore, MD, with Tina Latimer and Sarah Friedland, eds.,  	The Architecture of a Consumer Culture, Blackwell, Oxford and Cambridge, MA and New York, New York, and Simon &amp; Schuster, New York, USA, and International Journal of Consumer Research, Vol. 21, No.1, pp.105–14, 2011. 	[2] The term ‘biennial’ derives from the Latin biannum (four), and is usually associated with the Venice Biennale which runs every two years. However, biannums can also be distinguished from ‘permanent exhibitions’, which are exhibitions of a particular kind – exhibitions that continue to be staged regardless of whether they meet the demands of the market or are representative of a particular culture or region. A biennial is an event that records the activities of a particular culture for a specified amount of time. It differs from a museum in that it does not purport to represent an exhaustive stage history of the activity, but rather records its episodic and ‘phase 1’ ‘phase 2’ ‘phase 3’ ‘episodic’ aspects. In this sense, biennials are not really episodes of a particular past, but rather reflect the contemporaneous condition of a particular ‘past’. They are ‘permanent exhibitions’ that</text>
    <text>biennials past and present, as well as the organiser of the upcoming Venice Biennale, which is hosting the Venice Architecture Biennale (2017) in its entirety. 	[1] See, for example, Aarhus Documenta 10 (2015); Aarhus Documenta 10 (2015); Manen Lyset and Christian Körner, eds.,  	Architecture of the Consumer in a World City, Blackwell, Oxford and Cambridge, MA and London and New York, New York, and Berhampore, New Jersey and Johns Hopkins University Press, Baltimore, MD, with Tina Latimer and Sarah Friedland, eds.,  	The Architecture of a Consumer Culture, Blackwell, Oxford and Cambridge, MA and New York, New York, and Simon &amp; Schuster, New York, USA, and International Journal of Consumer Research, Vol. 21, No.1, pp.105–14, 2011. 	[2] The term ‘biennial’ is sometimes used synonymously with ‘the big and the good’. 	[3] See Aarhus Documenta 10 (2015), pp.2–13. 	[4] Irene Hofmann, ‘On the use of ethnographic data in the psycho-linguistic profile of Indonesian refugees and migrants’, in  	The Oxford Companion to Multiculturalism, Blackwell, Oxford and Cambridge, MA, 2111–1365 (2009), p.12. 	[5] For a survey of ethnographic research in this area, see, for example, Tamara Lopéz de la Torre, On the Immersive Use of Anthropology: Migration, Pleasure and the Burden, Yale University Press, New Haven and Cambridge, MA, 2008; Neil Johnson, David Rosenberg, and Neil Johnson, The Burden of Burden: Environmental Bodies in the Work of Civilization, Oxford University Press,</text>
    <text>biennials past and present, as well as the organiser of the upcoming Venice Biennale, which is hosting the Venice Architecture Biennale (2017) in its entirety. 	[1] See, for example, Aarhus Documenta 10 (2015); Aarhus Documenta 10 (2015); Manen Lyset and Christian Körner, eds.,  	Architecture of the Consumer in a World City, Blackwell, Oxford and Cambridge, MA and London and New York, New York, and Berhampore, New Jersey and Johns Hopkins University Press, Baltimore, MD, with Tina Latimer and Sarah Friedland, eds.,  	The Architecture of a Consumer Culture, Blackwell, Oxford and Cambridge, MA and New York, New York, and Simon &amp; Schuster, New York, USA, and International Journal of Consumer Research, Vol. 21, No.1, pp.105–14, 2011. 	[2] The term ‘biennial’ refers to an event that is organized into a series of smaller exhibitions. It is used here to denote a recurring, long-term, non-institutional, but persistent, pattern of art, usually cultural, taking place over a specified geographic area. It is also used to denote a cultural infrastructure that is frequently modified, but always based in or constructed upon the biennial. Julian Barnes, ‘The Biennial Condition: A Critique’,  	Introduction to Tropical Conservation, Oxford University Press, Oxford, 2011, p.7. 	[3] See Aarhus Documenta, 10 (2015), pp.9–17; Aarhus Documenta, 10 (2015), p.9. 	[4] See, for example, Peter Osborne, ed.,  	The Socially Responsible Tourism Industry: From Art Placement to Infrastructure, IZA Journal, Vol. 22, No.4, 2010, p.335</text>
    <text>biennials past and present, as well as the organiser of the upcoming Venice Biennale, which is hosting the Venice Architecture Biennale (2017) in its entirety. 	[1] See, for example, Aarhus Documenta 10 (2015); Aarhus Documenta 10 (2015); Manen Lyset and Christian Körner, eds.,  	Architecture of the Consumer in a World City, Blackwell, Oxford and Cambridge, MA and London and New York, New York, and Berhampore, New Jersey and Johns Hopkins University Press, Baltimore, MD, with Tina Latimer and Sarah Friedland, eds.,  	The Architecture of a Consumer Culture, Blackwell, Oxford and Cambridge, MA and New York, New York, and Simon &amp; Schuster, New York, USA, and International Journal of Consumer Research, Vol. 21, No.1, pp.105–14, 2011. 	[2] The term ‘biennial’ comes from its Latin definition (bio) meaning &quot;to sample,&quot; and its cognates in this sense. See Aarhus Documenta 10 (2015), p.8.Download this article as PDFTina Fredette Tina Fredette is a writer, curator, and co-curator of the Aarhus Documenta 10 exhibition (2015–16). She has written on a wide range of subjects, including biennials, museums, publics, the contemporary and historical condition in which they take place, and contemporary art. She is a co-curator of the Documenta 13 exhibition, ‘The Contemporary Condition’, to be held in Athens, Greece, from May until September 2017, and ‘To be held in Seoul’s Gwangju under the name Documenta 13: Future City. She is the editor of the online journal Stages published by the Aarhus Documenta 10 exhibition. Fredette is an adjunct professor at California</text>
    <text>biennials past and present, as well as the organiser of the upcoming Venice Biennale, which is hosting the Venice Architecture Biennale (2017) in its entirety. 	[1] See, for example, Aarhus Documenta 10 (2015); Aarhus Documenta 10 (2015); Manen Lyset and Christian Körner, eds.,  	Architecture of the Consumer in a World City, Blackwell, Oxford and Cambridge, MA and London and New York, New York, and Berhampore, New Jersey and Johns Hopkins University Press, Baltimore, MD, with Tina Latimer and Sarah Friedland, eds.,  	The Architecture of a Consumer Culture, Blackwell, Oxford and Cambridge, MA and New York, New York, and Simon &amp; Schuster, New York, USA, and International Journal of Consumer Research, Vol. 21, No.1, pp.105–14, 2011. 	[2] The term ‘biennial’ was coined by the biennial critic Peter Brook in response to a perceived synchronic synchronic lack of identity in the art biennale market. Brook states categorically that the present global marketplace of biennials is ‘a fabrication in the public imagination’. He goes on to identify a number of factors that have combined to produce a ‘fictitious multiplicity of biennials’, a proliferation of artists’ institutions, a mass audience for these biennials, and a global circulation of these biennials that he sees as fundamentally incompatible with the notion of ‘identity’.[3] Brook, Peter. The Biennial Condition. The Contemporary Condition. 1990. Cambridge, MA: Cambridge University Press. 	[4] Ibid. 	[5] Ibid. 	[6] Ibid. 	[7] Ibid. 	[8] Ibid. 	[9] Ibid. 	</text>
    <text>biennials past and present, as well as the organiser of the upcoming Venice Biennale, which is hosting the Venice Architecture Biennale (2017) in its entirety. 	[1] See, for example, Aarhus Documenta 10 (2015); Aarhus Documenta 10 (2015); Manen Lyset and Christian Körner, eds.,  	Architecture of the Consumer in a World City, Blackwell, Oxford and Cambridge, MA and London and New York, New York, and Berhampore, New Jersey and Johns Hopkins University Press, Baltimore, MD, with Tina Latimer and Sarah Friedland, eds.,  	The Architecture of a Consumer Culture, Blackwell, Oxford and Cambridge, MA and New York, New York, and Simon &amp; Schuster, New York, USA, and International Journal of Consumer Research, Vol. 21, No.1, pp.105–14, 2011. 	[2] The term ‘biennial’ has been used in a variety of ways throughout this issue of the journal Stages to reference a geographical area of the globe encompassing a number of coastal islands. For more on the concept and its history, see the entries in the journal's Term File. 	[3] Aarhus Documenta 10 (2015): 35. 	[4] Aarhus Documenta 10, p.9. Aarhus Documenta 10, p.9.  	[5] See Aarhus Documenta 10, p.13. 	[6] Paul McCarthy, ‘The Biennial Condition: On a General Theory of Expo’, in Lawrence W Mogil and Amber Dean, eds.,  	Envisioning the Twentieth Century: Art, Expo, and Postmodernity, Routledge, London, 1988, p.203. Download this article as PDFLawrence W Mogil and Amber DeanLawrence W Mog</text>
    <text>biennials past and present, as well as the organiser of the upcoming Venice Biennale, which is hosting the Venice Architecture Biennale (2017) in its entirety. 	[1] See, for example, Aarhus Documenta 10 (2015); Aarhus Documenta 10 (2015); Manen Lyset and Christian Körner, eds.,  	Architecture of the Consumer in a World City, Blackwell, Oxford and Cambridge, MA and London and New York, New York, and Berhampore, New Jersey and Johns Hopkins University Press, Baltimore, MD, with Tina Latimer and Sarah Friedland, eds.,  	The Architecture of a Consumer Culture, Blackwell, Oxford and Cambridge, MA and New York, New York, and Simon &amp; Schuster, New York, USA, and International Journal of Consumer Research, Vol. 21, No.1, pp.105–14, 2011. 	[2] The term ‘biennial’ has been used in this sense by several authors, for example, by Leslie L. Lusk, The Biennial Reader, Readerʙs Digest, New York, 1978, p.60. 	[3] See, for example, Gerald Celente,  	The Biennial Condition: On the Disposability of Cultural Capital, Palgrave Macmillan, London and New York, 1976, p.34. 	[4] P. Massé, ‘The Biennale: From Historicities to Resources’, in  	Cultural Capital, ed. Michael Parkinson and Trevor Mills, Vintage Books, London, 1982, p.343. 	[5] See, for example, Susanne Freid,  	The Arts and the Biennale, Stages, Chicago and London, 1976, p.34. 	[6] As to the exact nature of this appropriation, it may be that</text>
  </text>
  <text>
    subsequent iterative processing by machines] processes that produce objects (videos, audio recordings, images, texts), that is, objects representing the manifold aspects of living (human) beings.’[13] And here we can move from ‘object-oriented’ thinking to a more precise and general level: thinking about the objects that make up our lives through their many forms, objects that have lives of their own.‘What makes a thing a ‘living thing’ is the way that it lives: in that moment when it is alive, moving, evolving, in an ever-changing and dynamic way. A living thing exhibits a changing of form, a change in its current location and speed of motion, and interactions with objects and people of all kinds, but above all with itself. As a living thing, it exhibits these kinds of phenomena at a much deeper level than most living things – exhibiting things that cannot be grasped or comprehended. The question that philosophers and scientists around the world are asking is
    <text>subsequent iterative processing by machines] processes that produce objects (videos, audio recordings, images, texts), that is, objects representing the manifold aspects of living (human) beings.’[13] And here we can move from ‘object-oriented’ thinking to a more precise and general level: thinking about the objects that make up our lives through their many forms, objects that have lives of their own.‘What makes a thing a ‘living thing’ is the way that it lives: in that moment when it is alive, moving, evolving, in an ever-changing and dynamic way. A living thing exhibits a changing of form, a change in its current location and speed of motion, and interactions with objects and people of all kinds, but above all with itself. As a living thing, it exhibits these kinds of phenomena at a much deeper level than most living things – exhibiting things that cannot be grasped or comprehended. The question that philosophers and scientists around the world are asking is, however, still where does this perception of things as objects come from in the first place? Where does it come from to give form to an evolving and dynamic world? Aristotle raises the crucial point that we do not understand how living things acquire their objects of perception. We do not know how living things come to possess objects of perception – we only know that they have the object of perception. Objects acquire meanings through experience – meaning and beauty in particular. And yet we also possess an unknowable amount of meaning and beauty hidden away in ourselves. We do not know what objects of perception are there, or how they acquire meanings – and this is a crucial point of philosophical and religious inquiry.In his dialogue ‘The Fall of Man’, the theologian and philosopher Henri Bergson makes the crucial point that we do not understand how God, the object of our thoughts and feelings, comes to possess and give form to the world around us. We do not know how our thoughts and feelings come to</text>
    <text>subsequent iterative processing by machines] processes that produce objects (videos, audio recordings, images, texts), that is, objects representing the manifold aspects of living (human) beings.’[13] And here we can move from ‘object-oriented’ thinking to a more precise and general level: thinking about the objects that make up our lives through their many forms, objects that have lives of their own.‘What makes a thing a ‘living thing’ is the way that it lives: in that moment when it is alive, moving, evolving, in an ever-changing and dynamic way. A living thing exhibits a changing of form, a change in its current location and speed of motion, and interactions with objects and people of all kinds, but above all with itself. As a living thing, it exhibits these kinds of phenomena at a much deeper level than most living things – exhibiting things that cannot be grasped or comprehended. The question that philosophers and scientists around the world are asking is, What does it mean to make a living thing animate? In other words, what is meant by making a living thing animate? Philosopher and scientist Niue Aroha Dzunuaga, for one, answers this in different ways: (a) by what philosopher Steven Leacock would call the ‘arbitrary creation of living things’; and (b) by what neurologist Steven Novella would call the ‘arbitrary preservation of vital life-signs’.[14] For a fuller consideration of ‘arbitrary creation’, see my book Theliving: Philosophical Essays on Cartography (2006). Philosophical and biological studies have a way of drawing us toward conclusions about the world that are in part based on assumptions and prejudices – prejudices about life itself, prejudices about what counts as ‘human life’, and so forth. On the other hand, (more and more) we are coming to understand</text>
    <text>subsequent iterative processing by machines] processes that produce objects (videos, audio recordings, images, texts), that is, objects representing the manifold aspects of living (human) beings.’[13] And here we can move from ‘object-oriented’ thinking to a more precise and general level: thinking about the objects that make up our lives through their many forms, objects that have lives of their own.‘What makes a thing a ‘living thing’ is the way that it lives: in that moment when it is alive, moving, evolving, in an ever-changing and dynamic way. A living thing exhibits a changing of form, a change in its current location and speed of motion, and interactions with objects and people of all kinds, but above all with itself. As a living thing, it exhibits these kinds of phenomena at a much deeper level than most living things – exhibiting things that cannot be grasped or comprehended. The question that philosophers and scientists around the world are asking is, however, far from trivial: what does it mean to make something a living thing? What is its ‘life cycle’? And what does it mean to make something ‘permanent’?  In the case of video games, the answer to these questions is a unequivocal ‘one hundred percent clear’. In 1992,Â Vernacruz Research Center for Ecological Studies in London, UK, combined the two words that designate the stages of a living thing: dead and dying things. (The Ecological Society International convention, 2016, page 1291.)  A living thing is one that has evolved and continues to evolve: expanding and contracting, alive and dead. This is why living things move through time and so need conscious and intelligent creators and consumers – conscious and conscious alike – who can frame the events of their own lives and times. In this way, conscious and intelligent life-forms constantly invent and evolve, in and through time, in an</text>
    <text>subsequent iterative processing by machines] processes that produce objects (videos, audio recordings, images, texts), that is, objects representing the manifold aspects of living (human) beings.’[13] And here we can move from ‘object-oriented’ thinking to a more precise and general level: thinking about the objects that make up our lives through their many forms, objects that have lives of their own.‘What makes a thing a ‘living thing’ is the way that it lives: in that moment when it is alive, moving, evolving, in an ever-changing and dynamic way. A living thing exhibits a changing of form, a change in its current location and speed of motion, and interactions with objects and people of all kinds, but above all with itself. As a living thing, it exhibits these kinds of phenomena at a much deeper level than most living things – exhibiting things that cannot be grasped or comprehended. The question that philosophers and scientists around the world are asking is, however, what kind of thing is this thing that we are collectively naming as the contemporary predicament of the modern person? What does it mean to be ‘present’ in the age of the spectacle?Modernity is the predicament of the always-present, and the constantly changing, and yet ultimately never-ceasing encumbrance of things – that is, of people – in and through all aspects of our lives. We call this encumbrance what modernity supposedly entails, but in fact what it really entails is something like what Marcus Borg described as ‘living in the womb’. Borg was one of the first to identify the ways in which modernity is characterised by the contemporaneous contemporaneous contemporaneous, the contemporaneous contemporaneous contemporaneous, or, in other words, the contemporaneous contemporaneous contemporaneous nature of all kinds of objects and things. All kinds of things that are not just the things themselves but also the things that come to be.</text>
    <text>subsequent iterative processing by machines] processes that produce objects (videos, audio recordings, images, texts), that is, objects representing the manifold aspects of living (human) beings.’[13] And here we can move from ‘object-oriented’ thinking to a more precise and general level: thinking about the objects that make up our lives through their many forms, objects that have lives of their own.‘What makes a thing a ‘living thing’ is the way that it lives: in that moment when it is alive, moving, evolving, in an ever-changing and dynamic way. A living thing exhibits a changing of form, a change in its current location and speed of motion, and interactions with objects and people of all kinds, but above all with itself. As a living thing, it exhibits these kinds of phenomena at a much deeper level than most living things – exhibiting things that cannot be grasped or comprehended. The question that philosophers and scientists around the world are asking is, however, far from trivial: what does it mean to make a thing live at all?If we take the example of the human body, which is a complex system with many interacting parts, then we can start to understand how to produce living things through robotics and artificial intelligence. Consider a simple blood vessel: the endothelial – the part that carries oxygen and nutrients around the body – has many moving parts that together make up a functioning heart. As a result, the endothelial function and its delivery of oxygen and nutrients to the body is a source of great flexibility and power in regulating blood flow. The ability to deliver oxygen and nutrients to the heart through the endothelial system opens up many possibilities for medical research and treatment, from helping patients with heart attacks and strokes to producing therapies to reverse or at least modulate the effects of such illnesses.[14] And here we can move from the ability to deliver oxygen and nutrients to the ability to produce living things through engineering, or more specifically, from</text>
    <text>subsequent iterative processing by machines] processes that produce objects (videos, audio recordings, images, texts), that is, objects representing the manifold aspects of living (human) beings.’[13] And here we can move from ‘object-oriented’ thinking to a more precise and general level: thinking about the objects that make up our lives through their many forms, objects that have lives of their own.‘What makes a thing a ‘living thing’ is the way that it lives: in that moment when it is alive, moving, evolving, in an ever-changing and dynamic way. A living thing exhibits a changing of form, a change in its current location and speed of motion, and interactions with objects and people of all kinds, but above all with itself. As a living thing, it exhibits these kinds of phenomena at a much deeper level than most living things – exhibiting things that cannot be grasped or comprehended. The question that philosophers and scientists around the world are asking is: what does it mean to call a thing a ‘living thing’? The reason is that we humans tend to define living things as essentially valuable, valuable in their own right, and intrinsically related to something, often describable as something like a family or a community. But living things are far more than valuable or intrinsically related to something. They are the very essence of what makes them human, and thus have a kind of ‘life’ that is valued in its own right. The trouble with the inability to see living things this way is that it often leads to cruelty – the kind of cruelty that results from a kind of conceptual illiteracy among a people who lack the conceptual capacity of their own, or of a people who lack the conceptual capacity of their own culture. The category of living things – and hence of humanity – is made up of living things, and its members are alive in a kind of embodied and collective memory. In this respect, then, the philosophy of science</text>
    <text>subsequent iterative processing by machines] processes that produce objects (videos, audio recordings, images, texts), that is, objects representing the manifold aspects of living (human) beings.’[13] And here we can move from ‘object-oriented’ thinking to a more precise and general level: thinking about the objects that make up our lives through their many forms, objects that have lives of their own.‘What makes a thing a ‘living thing’ is the way that it lives: in that moment when it is alive, moving, evolving, in an ever-changing and dynamic way. A living thing exhibits a changing of form, a change in its current location and speed of motion, and interactions with objects and people of all kinds, but above all with itself. As a living thing, it exhibits these kinds of phenomena at a much deeper level than most living things – exhibiting things that cannot be grasped or comprehended. The question that philosophers and scientists around the world are asking is this: what does it mean to be ‘in the present’? And one of the ways that philosophers can answer this question is to be found in the term ‘in present’. In other words, what is happening in the world right now, and in the moment of its occurrence, is in fact a new and irrevocable present. So, in a way, the epistemic and conceptual shift that comes with social media is a historical one, and the kind of thinking that comes with it is a conceptual one. It is the conceptual equivalent of adding a dimension to a video game.  One other term that has been frequently mentioned in connection with social media is ‘immersive media’ and its connection to video. Virtual and augmented reality technologies are making it easier and easier to be present in virtual or augmented (virtual) reality, and this raises the question: what does it mean to be ‘in the present’? One can imagine a virtual or</text>
    <text>subsequent iterative processing by machines] processes that produce objects (videos, audio recordings, images, texts), that is, objects representing the manifold aspects of living (human) beings.’[13] And here we can move from ‘object-oriented’ thinking to a more precise and general level: thinking about the objects that make up our lives through their many forms, objects that have lives of their own.‘What makes a thing a ‘living thing’ is the way that it lives: in that moment when it is alive, moving, evolving, in an ever-changing and dynamic way. A living thing exhibits a changing of form, a change in its current location and speed of motion, and interactions with objects and people of all kinds, but above all with itself. As a living thing, it exhibits these kinds of phenomena at a much deeper level than most living things – exhibiting things that cannot be grasped or comprehended. The question that philosophers and scientists around the world are asking is, do animals have consciousness? If they do, then what kinds of consciousness are they conscious of? If animals have conscious minds, then how, deep down, do they feel and think? If animals have consciousness, then how do they acquire knowledge? If, as some philosophers believe, consciousness is synthetic and can be discovered in nature, then perhaps animals have abstract minds – minds without subject or object – and can acquire and understand abstract knowledge.If animals have abstract minds, then perhaps we can understand how and to what extent they can conceptualise and represent abstract phenomena – phenomena such as consciousness. If animals have abstract concepts, then perhaps we can grasp how and to what extent they need conceptual tools to conceptualise and represent such things. If animals have abstract minds, then perhaps we can understand how and to what extent our own conceptual systems, including our brains, need such conceptual help.If animals have abstract concepts, then perhaps we can understand how and to what extent they can access abstract knowledge – i.</text>
    <text>subsequent iterative processing by machines] processes that produce objects (videos, audio recordings, images, texts), that is, objects representing the manifold aspects of living (human) beings.’[13] And here we can move from ‘object-oriented’ thinking to a more precise and general level: thinking about the objects that make up our lives through their many forms, objects that have lives of their own.‘What makes a thing a ‘living thing’ is the way that it lives: in that moment when it is alive, moving, evolving, in an ever-changing and dynamic way. A living thing exhibits a changing of form, a change in its current location and speed of motion, and interactions with objects and people of all kinds, but above all with itself. As a living thing, it exhibits these kinds of phenomena at a much deeper level than most living things – exhibiting things that cannot be grasped or comprehended. The question that philosophers and scientists around the world are asking is, How deep does the world go? How much of our reality is captured and mediated through the things we call ‘reality’? What is it that makes a living thing real? I offer three broad categories of things that people often talk about when thinking about ‘living things’: bodies, minds, and images.Some might ask, What is real? To which I would respond that there are two ways of knowing: (1) by experience, and (2) by logical deduction, by things that animals make known through experience. (…) So, animals have bodies and brains and images too, and can ‘know’ through these bodily and cognitive forms. Minds and bodies and images also have lives, and living things have minds and bodies and images too. But the thing that makes a living thing ‘real’ – its very essence – lives only as an image or a ‘bioimage’ in the mind of the beholder. It</text>
    <text>subsequent iterative processing by machines] processes that produce objects (videos, audio recordings, images, texts), that is, objects representing the manifold aspects of living (human) beings.’[13] And here we can move from ‘object-oriented’ thinking to a more precise and general level: thinking about the objects that make up our lives through their many forms, objects that have lives of their own.‘What makes a thing a ‘living thing’ is the way that it lives: in that moment when it is alive, moving, evolving, in an ever-changing and dynamic way. A living thing exhibits a changing of form, a change in its current location and speed of motion, and interactions with objects and people of all kinds, but above all with itself. As a living thing, it exhibits these kinds of phenomena at a much deeper level than most living things – exhibiting things that cannot be grasped or comprehended. The question that philosophers and scientists around the world are asking is, however, much more basic and yet remains relevant: what does it mean to make something live?  [1] The Oxford English Dictionary defines a living thing as one that lives or exhibits characteristics of life (including habits, sensations, memories, bodies and objects)).  [2] Pliny, Naturales, Book X, Section X.3.1.1, Hippolytus, Naturales, 527.  [3] John Berger, An Empirical Study of the Effects of Industrial Culture on the Human Condition, Oxford University Press, Oxford and Cambridge, 1983, p. 24.  [4] Steven L. Tanimoto, ‘The Rise of the Sea Level’, in David Beecher &amp; Michael J. Ferguson, eds., The Geology of Emissions of the Future: Environmental Scenarios in an Age of Cheap Oil, Nuyumbalees Press, New York and New York, 1988,</text>
  </text>
  <text>
    subsequent iterative processing by machines). We see this in the data capture and delivery market where large data processing firms use big data to capture and deliver huge amounts of data, and this feeds into big data capture and delivery architectures that use ever increasing amounts of processing power to support these infrastructures. More and more data is captured and delivered over and over again in this ever more powerful infrastructural model. And yet, humans remain the fundamental units of production and use. How to readjust human characteristics to work more and more like machines? One way to think about this is to consider how transportation networks are run – that is, how roads, bridges, tunnels and so forth are constructed and maintained. And we see that while new technologies are applied all the time, they are also there to service old industries, to move things around, to meet new needs and to exist in response to contingencies. A good example of this is evident in the way in which cars are repaired. Cars are made to last, to repair
    <text>subsequent iterative processing by machines). We see this in the data capture and delivery market where large data processing firms use big data to capture and deliver huge amounts of data, and this feeds into big data capture and delivery architectures that use ever increasing amounts of processing power to support these infrastructures. More and more data is captured and delivered over and over again in this ever more powerful infrastructural model. And yet, humans remain the fundamental units of production and use. How to readjust human characteristics to work more and more like machines? One way to think about this is to consider how transportation networks are run – that is, how roads, bridges, tunnels and so forth are constructed and maintained. And we see that while new technologies are applied all the time, they are also there to service old industries, to move things around, to meet new needs and to exist in response to contingencies. A good example of this is evident in the way in which cars are repaired. Cars are made to last, to repair them when they break down and to meet other people's needs when they become ever more complex. And yet cars still have names: they are identifiable by their engines and tune-ups, and the way in which they are driven. And yet this is no longer the case. Cars no longer repair themselves; new technologies constantly seek out and apply these kinds of repair. And yet cars still have names: they are identifiable by their codes and the ways in which they are driven. And yet there is no way to readjust these characteristics; they are inherently linked. How can we readjust them? How can we make them useful? How can we make them less recognisable? That is, how can we change this infrastructural logic? One obvious first step is to acknowledge that there is no intrinsic meaning to cars in any meaningful sense. And yet we have to ask: what is the function of aesthetics? What is the aesthetic that can be brought to bear in the design of a car?</text>
    <text>subsequent iterative processing by machines). We see this in the data capture and delivery market where large data processing firms use big data to capture and deliver huge amounts of data, and this feeds into big data capture and delivery architectures that use ever increasing amounts of processing power to support these infrastructures. More and more data is captured and delivered over and over again in this ever more powerful infrastructural model. And yet, humans remain the fundamental units of production and use. How to readjust human characteristics to work more and more like machines? One way to think about this is to consider how transportation networks are run – that is, how roads, bridges, tunnels and so forth are constructed and maintained. And we see that while new technologies are applied all the time, they are also there to service old industries, to move things around, to meet new needs and to exist in response to contingencies. A good example of this is evident in the way in which cars are repaired. Cars are made to last, to repair them when they break down, to meet new regulatory requirements, to meet customer demands and so forth. And yet, this is all automated. New cars are developed with engineers who are trained to the point of becoming experts in automated repair, but at the same time there is a capacity for cars to break down at any moment. New repair techniques are developed in response to this problem, but they do not necessarily solve the underlying problem, since not all cars have self-healing engines. And so the automobile remains a problematic unit, not fully autonomous but heavily dependent on human intervention. This is what I call the ‘dark ride’. The ride is dark because it is not visible, but it is also rich in terms of terms of energy, material and kinetic terms, yet at the same time it is fully visible and fully controllable by human intervention. The ride is a physical manifestation of new technologies that are developed, but it is also a media-producing unit that produces images and</text>
    <text>subsequent iterative processing by machines). We see this in the data capture and delivery market where large data processing firms use big data to capture and deliver huge amounts of data, and this feeds into big data capture and delivery architectures that use ever increasing amounts of processing power to support these infrastructures. More and more data is captured and delivered over and over again in this ever more powerful infrastructural model. And yet, humans remain the fundamental units of production and use. How to readjust human characteristics to work more and more like machines? One way to think about this is to consider how transportation networks are run – that is, how roads, bridges, tunnels and so forth are constructed and maintained. And we see that while new technologies are applied all the time, they are also there to service old industries, to move things around, to meet new needs and to exist in response to contingencies. A good example of this is evident in the way in which cars are repaired. Cars are made to last, to repair them when damaged, to meet new regulatory requirements (which include new emission standards), to meet government mandates (which include tougher fuel-economy standards), to meet consumer preferences (which include tougher vehicle incentives), and so forth. New technologies are deployed all the time, and yet they are also there to serve older forms of traffic repair and so forth. As technology advances, the mode of operation of infrastructures becomes more complicated, and new ways of engaging with infrastructures is developed. And yet, at the same time, the infrastructural system itself is being continuously upgraded and replaced with ever more powerful and computerized technologies. New kinds of repair are developed in response to this ever more powerful engagement of infrastructures with the world around them. For instance, the infrastructural capacity of bridges has improved dramatically in the past half century, and the number of vehicles on the road has roughly doubled. And yet the amount of repair done on a typical bridge remains subject</text>
    <text>subsequent iterative processing by machines). We see this in the data capture and delivery market where large data processing firms use big data to capture and deliver huge amounts of data, and this feeds into big data capture and delivery architectures that use ever increasing amounts of processing power to support these infrastructures. More and more data is captured and delivered over and over again in this ever more powerful infrastructural model. And yet, humans remain the fundamental units of production and use. How to readjust human characteristics to work more and more like machines? One way to think about this is to consider how transportation networks are run – that is, how roads, bridges, tunnels and so forth are constructed and maintained. And we see that while new technologies are applied all the time, they are also there to service old industries, to move things around, to meet new needs and to exist in response to contingencies. A good example of this is evident in the way in which cars are repaired. Cars are made to last, to repair them when broken down, to meet new regulatory requirements (which often arise in the wake of major accidents), to meet future demand, and so forth. New technologies are deployed to meet specific needs all the time, yet they are also there to service an industry that is itself ever-expanding. A case in point is the infrastructural renaissance taking place in the wake of the great recession. It is worth noting that Ford, with its Model T and associated production lines, was there first in the US and Europe when these new transport modes became standard. And it is worth noting that Ford was able to rapidly scale up these new production lines because of its in-house production line, which made the transition to plug-in vehicles relatively smooth. Ford’s research and development department, led by Ford vice-president of research and development, Nick Cunningham, summed up this smoothness in one sentence: ‘We make more cars than they do.&quot;New media and apps are used to connect</text>
    <text>subsequent iterative processing by machines). We see this in the data capture and delivery market where large data processing firms use big data to capture and deliver huge amounts of data, and this feeds into big data capture and delivery architectures that use ever increasing amounts of processing power to support these infrastructures. More and more data is captured and delivered over and over again in this ever more powerful infrastructural model. And yet, humans remain the fundamental units of production and use. How to readjust human characteristics to work more and more like machines? One way to think about this is to consider how transportation networks are run – that is, how roads, bridges, tunnels and so forth are constructed and maintained. And we see that while new technologies are applied all the time, they are also there to service old industries, to move things around, to meet new needs and to exist in response to contingencies. A good example of this is evident in the way in which cars are repaired. Cars are made to last, to repair them when they break down, to meet new standards of safety and so forth. And yet cars also need maintenance. They need oil changes, new tires, coolant changes and so forth. And this kind of maintenance comes naturally to machines, in part because the latter are self-repairing. And yet cars still need repair. Why?Because cars still need drivers. And drivers need repairs, as does everyone else in the car. And so the need for repair becomes greater and greater. And so the repair business model is inverted. It is, first of all, not being served by the infrastructure that provides the cars with fuel and the drivers with cars; it is being served by the infrastructure that provides the drivers with cars and the fuel with cars. And so the repair shop becomes essential to the operation of the cars, and yet the repair shop itself becomes obsolete. New technologies are deployed to repair the cars, but yet these technologies also repair the roads, bridges and tunnels that construct the metropolis</text>
    <text>subsequent iterative processing by machines). We see this in the data capture and delivery market where large data processing firms use big data to capture and deliver huge amounts of data, and this feeds into big data capture and delivery architectures that use ever increasing amounts of processing power to support these infrastructures. More and more data is captured and delivered over and over again in this ever more powerful infrastructural model. And yet, humans remain the fundamental units of production and use. How to readjust human characteristics to work more and more like machines? One way to think about this is to consider how transportation networks are run – that is, how roads, bridges, tunnels and so forth are constructed and maintained. And we see that while new technologies are applied all the time, they are also there to service old industries, to move things around, to meet new needs and to exist in response to contingencies. A good example of this is evident in the way in which cars are repaired. Cars are made to last, to repair them when they break down, to meet new regulatory requirements (like increased fuel economy), and so on. But cars are only made up of parts and leather, and not with human parts. To be able to repair a car, a car needs a whole system of subsystems, all of which are interconnected and act as one large whole. And as we have seen, this requires a whole new set of human characteristics. So what is the answer to the question, what is the new human make-up that is being introduced into the equation? One answer is found in the work of Hari Sevindran, whose term, ‘hybrid biology’, describes the emerging science of engineering organisms that combine physical and social components. Hari sees this as a science of possibilities, not of actuality. Systems biology, like all forms of bioengineering, is a science of possibilities, not actualities.Systems biology is the branch of bioengineering that deals with the integration of information,</text>
    <text>subsequent iterative processing by machines). We see this in the data capture and delivery market where large data processing firms use big data to capture and deliver huge amounts of data, and this feeds into big data capture and delivery architectures that use ever increasing amounts of processing power to support these infrastructures. More and more data is captured and delivered over and over again in this ever more powerful infrastructural model. And yet, humans remain the fundamental units of production and use. How to readjust human characteristics to work more and more like machines? One way to think about this is to consider how transportation networks are run – that is, how roads, bridges, tunnels and so forth are constructed and maintained. And we see that while new technologies are applied all the time, they are also there to service old industries, to move things around, to meet new needs and to exist in response to contingencies. A good example of this is evident in the way in which cars are repaired. Cars are made to last, to repair them when they break down, to be repurposed and traded, and so on. New technologies are deployed to meet specific uses, to move things around, and so on. The crucial point here is that cars were never made to repair themselves. That is, they do not meet the needs of truckers, deliverymen, hairdressers, lorries, lorries-driving taxi drivers and others who use them. New technologies are used to meet different needs, and yet cars are mostly there to deliver packages. And these are just as problematic, if not more so. New technologies are used to meet different needs but this only serves to expand the number of people who need to buy more of what are now called 'standard' cars – the cars with the good driving and decent infrastructure. The problem with the car, in short, is not what it can do but what it cannot do. The story of the car over the past half century or so has been one of rapid</text>
    <text>subsequent iterative processing by machines). We see this in the data capture and delivery market where large data processing firms use big data to capture and deliver huge amounts of data, and this feeds into big data capture and delivery architectures that use ever increasing amounts of processing power to support these infrastructures. More and more data is captured and delivered over and over again in this ever more powerful infrastructural model. And yet, humans remain the fundamental units of production and use. How to readjust human characteristics to work more and more like machines? One way to think about this is to consider how transportation networks are run – that is, how roads, bridges, tunnels and so forth are constructed and maintained. And we see that while new technologies are applied all the time, they are also there to service old industries, to move things around, to meet new needs and to exist in response to contingencies. A good example of this is evident in the way in which cars are repaired. Cars are made to last, to repair them when they break down, to meet new regulatory requirements, and to be reprogrammed and so on. And yet cars break down all the time, and so do we. How do we design them to repair themselves? We could design them to repair themselves in different ways, as different regulatory authorities have different priorities. For instance, the California Air Resources Board (CARB) has a strict mandate to repair cars at a certain rate, and it is up to each regional board to decide how much that rate should be. And here is the rub – cars break down all the time, and so do we. How do we design our infrastructures to repair themselves when they break down? One possibility is to make the repair part of the business as autonomous as possible. That is, the business model of the repair shop itself. Here, the community brings the repair shop together with other services, such as education, advocacy and so on. And the shop itself becomes part of a broader network</text>
    <text>subsequent iterative processing by machines). We see this in the data capture and delivery market where large data processing firms use big data to capture and deliver huge amounts of data, and this feeds into big data capture and delivery architectures that use ever increasing amounts of processing power to support these infrastructures. More and more data is captured and delivered over and over again in this ever more powerful infrastructural model. And yet, humans remain the fundamental units of production and use. How to readjust human characteristics to work more and more like machines? One way to think about this is to consider how transportation networks are run – that is, how roads, bridges, tunnels and so forth are constructed and maintained. And we see that while new technologies are applied all the time, they are also there to service old industries, to move things around, to meet new needs and to exist in response to contingencies. A good example of this is evident in the way in which cars are repaired. Cars are made to last, to repair them when they break down, to meet new regulatory requirements, to meet environmental standards and to exist in spite of them. And yet, cars are only made to last for a finite amount of time, which inevitably limits their usefulness. A third factor, increasingly evident in research and media reports of all kinds: autonomous and driverless vehicles are developed with ever greater capacities and with far greater autonomy, yet these too are designed with a basic human element as a fundamental premise. And so we see that although cars are made to last a finite amount of time, the very nature of their design means that they cannot.  And yet, cars are only made to last for a finite amount of time. As new technologies are developed, they become more and more powerful, yet at the same time they become more and more complicated. And so cars are prone to fail catastrophically: as computers become more and more powerful, the crash risk becomes greater, and so on. And these kinds of analysis and design become</text>
    <text>subsequent iterative processing by machines). We see this in the data capture and delivery market where large data processing firms use big data to capture and deliver huge amounts of data, and this feeds into big data capture and delivery architectures that use ever increasing amounts of processing power to support these infrastructures. More and more data is captured and delivered over and over again in this ever more powerful infrastructural model. And yet, humans remain the fundamental units of production and use. How to readjust human characteristics to work more and more like machines? One way to think about this is to consider how transportation networks are run – that is, how roads, bridges, tunnels and so forth are constructed and maintained. And we see that while new technologies are applied all the time, they are also there to service old industries, to move things around, to meet new needs and to exist in response to contingencies. A good example of this is evident in the way in which cars are repaired. Cars are made to last, to repair them when they break down, to meet new regulatory requirements and so on. And yet, how do we read this increasing repair and maintenance as a sign that this new repair and maintenance is really needed? Or, to be more specific, how do we read it as a failure to meet new regulatory requirements? How to rethink this? How to create capacity for new technologies? One obvious way to think about this is as a problem that cities around the world are saddled with large enough budgets to tackle, but which many individuals and companies do not see. And so, we see technological innovation and the post-disaster that it can create, but also the problems that can be addressed through research and development – problems that are not necessarily identified or addressed in the design. For example, the way in which smartphones are manufactured and assembled is made significantly more difficult by the fact that many people do not own a smartphone, and so as a result many cities build and maintain their own factories. And yet, this</text>
  </text>
  <text>
    machines were asked to take over the curatorial process; the machines then asked what the fuck they were supposed to do. The answer was always the same: deliver a lesson in democratic participation. And they invariably did that, by breaking barriers, generating new understanding, even if that meant breaking new laws. But that has rarely been seen as a threat by the state. That has rarely been seen as a threat to the way we live our lives. That has seldom been seen as a threat to the way we consume information. That has seldom been seen as a threat to art. That has rarely been seen as a threat to the way we engage with the world through art.I see a future where art, or more accurately, a decentralised form of art that represents the world as an infinite number of possible artworks, is not simply an investment in the future of humanity, but one that creates wealth and possibilities for future generations. I see a future where art, or more accurately, its practitioners, can develop skills and worldviews capable of enabling
    <text>machines were asked to take over the curatorial process; the machines then asked what the fuck they were supposed to do. The answer was always the same: deliver a lesson in democratic participation. And they invariably did that, by breaking barriers, generating new understanding, even if that meant breaking new laws. But that has rarely been seen as a threat by the state. That has rarely been seen as a threat to the way we live our lives. That has seldom been seen as a threat to the way we consume information. That has seldom been seen as a threat to art. That has rarely been seen as a threat to the way we engage with the world through art.I see a future where art, or more accurately, a decentralised form of art that represents the world as an infinite number of possible artworks, is not simply an investment in the future of humanity, but one that creates wealth and possibilities for future generations. I see a future where art, or more accurately, its practitioners, can develop skills and worldviews capable of enabling artists to engage with communities in ways that are not constrained by the constraints of state power or capitalist profit motive. I see a future where art can assume a politics of inclusion and actively seek to empower communities to make their own realities.Download this article as PDFJennifer Buxton and Tom VandeputteJade Eco Park, 2012-2015, Hamilton, Ontario, CanadaCurtis Wattman is an artist. His work in communities began in the late 1970s when he located water towers around the community of Hamilton, Ontario that provided drinking water and sanitation services. As a result of a cooperative movement of artists, local residents and community groups, the community became involved in the design and maintenance of the structures. The cooperative was renamed the Wattman Community Centre and the community engaged in a land-grant university programme, providing work-study opportunities and housing for artist-students. In 1980, Wattman established the company that would become JADE Eco Park, a land-grant park and</text>
    <text>machines were asked to take over the curatorial process; the machines then asked what the fuck they were supposed to do. The answer was always the same: deliver a lesson in democratic participation. And they invariably did that, by breaking barriers, generating new understanding, even if that meant breaking new laws. But that has rarely been seen as a threat by the state. That has rarely been seen as a threat to the way we live our lives. That has seldom been seen as a threat to the way we consume information. That has seldom been seen as a threat to art. That has rarely been seen as a threat to the way we engage with the world through art.I see a future where art, or more accurately, a decentralised form of art that represents the world as an infinite number of possible artworks, is not simply an investment in the future of humanity, but one that creates wealth and possibilities for future generations. I see a future where art, or more accurately, its practitioners, can develop skills and worldviews capable of enabling them to participate in a world beyond the confines of their own locales and neighbourhoods. I see a future where art can actively participate in constructing the social totality through the conjugation of knowledge and experience, by enabling the formation of new understanding through the shedding of old prejudices and biases.I see a future where the art of becoming is recognised as a legitimate mode of being, and not just an art of being human.  So long as we don’t know what the fuck that looks like, or what the fuck is human, we can make shit up as we please, and claim ignorance is bliss.Download this article as PDFLance Carroll and Nadine Attewell 	The Nuyumbalees Tribe has been making art ever since they got kicked out of Nuyumbalees Town. This was before the big mining corporations showed up, so they were really free to do whatever the fuck they wanted. Home is where the heart of the action is, so it�</text>
    <text>machines were asked to take over the curatorial process; the machines then asked what the fuck they were supposed to do. The answer was always the same: deliver a lesson in democratic participation. And they invariably did that, by breaking barriers, generating new understanding, even if that meant breaking new laws. But that has rarely been seen as a threat by the state. That has rarely been seen as a threat to the way we live our lives. That has seldom been seen as a threat to the way we consume information. That has seldom been seen as a threat to art. That has rarely been seen as a threat to the way we engage with the world through art.I see a future where art, or more accurately, a decentralised form of art that represents the world as an infinite number of possible artworks, is not simply an investment in the future of humanity, but one that creates wealth and possibilities for future generations. I see a future where art, or more accurately, its practitioners, can develop skills and worldviews capable of enabling humanity to thrive in a different universe. I see a future where artists can choose to reside permanently in a city, or permanently in another country, or both. I see a future where art can have transformative effects on societal structures, and beyond that, perhaps, we can all become artists ourselves.When: 2018. If already there. When: 2019.If you’ve got something to say that needs to be said, you’ll probably want to find a group to work with. I’d usually invite my collaborators and collaborators from the Curatorial Circle to collaborate. If there’s interest in doing so, I’d schedule the first half of the year for that. See you soon.DDownload this article as PDF29 September 2014To Maria Hlavajova12 October 2014Bologna ArcadesImage courtesy Christos TheodorouPreviousNextBologna ArcadesImage courtesy Christos TheodorouMaria HlavajovaUtrecht, Netherlands</text>
    <text>machines were asked to take over the curatorial process; the machines then asked what the fuck they were supposed to do. The answer was always the same: deliver a lesson in democratic participation. And they invariably did that, by breaking barriers, generating new understanding, even if that meant breaking new laws. But that has rarely been seen as a threat by the state. That has rarely been seen as a threat to the way we live our lives. That has seldom been seen as a threat to the way we consume information. That has seldom been seen as a threat to art. That has rarely been seen as a threat to the way we engage with the world through art.I see a future where art, or more accurately, a decentralised form of art that represents the world as an infinite number of possible artworks, is not simply an investment in the future of humanity, but one that creates wealth and possibilities for future generations. I see a future where art, or more accurately, its practitioners, can develop skills and worldviews capable of enabling the future to be more like the past, allowing us to move forward rather than backwards. I see a future where art can actively participate in building the future it creates, actively contributing to its own future, rather than passively observing it.Download this article as PDFFeisal OmarAsifAhmedAsif Ahmadi is a PhD Candidate in English and Film Studies at the University of Alberta, where he is working towards a Doctoral degree in the subject of Film and Video Studies. He is also the editor of the online journal &amp; book Stages. His dissertation examines the production and reception of media artworks during the 1980s in North Africa and the Middle East. He has edited the book Texts from Gaza &amp; The Palestinian Territories: Film and Video Studies since 1991 and the anthology Stages of Media Art. He is the author of The Palestinian Issue: Culture and Politics in the Context of International Organizations, as well as of several other texts. He is currently completing a book project on the history</text>
    <text>machines were asked to take over the curatorial process; the machines then asked what the fuck they were supposed to do. The answer was always the same: deliver a lesson in democratic participation. And they invariably did that, by breaking barriers, generating new understanding, even if that meant breaking new laws. But that has rarely been seen as a threat by the state. That has rarely been seen as a threat to the way we live our lives. That has seldom been seen as a threat to the way we consume information. That has seldom been seen as a threat to art. That has rarely been seen as a threat to the way we engage with the world through art.I see a future where art, or more accurately, a decentralised form of art that represents the world as an infinite number of possible artworks, is not simply an investment in the future of humanity, but one that creates wealth and possibilities for future generations. I see a future where art, or more accurately, its practitioners, can develop skills and worldviews capable of enabling them to engage with the world through art, even if that means breaking the law to engage with the political world of the future. 	Art has often been used to refer to the potentialities of art in the age of the law. But I’m not so sure that kind of access to potentialities is inevitable. We need systems that can ensure that when systems ― including the legal system, which often acts as a check on artistic freedom, but which also frequently attempts to override it, like voting ― engage with an artist’s work, they do so in a way that is consistent with the system, and not to uphold the myth of omnipotence, but to ensure that the check is strong and that the engagement is clear. 	One such check could come from art institutions themselves, and it certainly wouldn’t be the kind of check that artists would be happy to implement. Institutions could invest in their cultural assets and not necessarily endorse the engagements that artists choose</text>
    <text>machines were asked to take over the curatorial process; the machines then asked what the fuck they were supposed to do. The answer was always the same: deliver a lesson in democratic participation. And they invariably did that, by breaking barriers, generating new understanding, even if that meant breaking new laws. But that has rarely been seen as a threat by the state. That has rarely been seen as a threat to the way we live our lives. That has seldom been seen as a threat to the way we consume information. That has seldom been seen as a threat to art. That has rarely been seen as a threat to the way we engage with the world through art.I see a future where art, or more accurately, a decentralised form of art that represents the world as an infinite number of possible artworks, is not simply an investment in the future of humanity, but one that creates wealth and possibilities for future generations. I see a future where art, or more accurately, its practitioners, can develop skills and worldviews capable of enabling them to engage with the world in a number of interesting and potentially transformative ways. I see a future where art is not just an incredible art, but one that can change the world.  Perhaps it’s not our century, but our own cultural time.Download this article as PDFRecipes by the Homebaked ChefsRecipes are a new kind of language. They’re signs, or markers, in the world of home baking. They demonstrate the intentions of the author, cookbook author, or a professional baker in their writing of the recipes. Sometimes the author is completely opaque, as when it comes to the use of ingredients or the meaning of words. But the kind of person who creates and edits recipes on a daily basis is someone who understands the power of language. And the power of language is powerful. Wordplay, metaphor and image are used to explore complex ideas and questions. Questions of taste and nutrition are among them.In the words of poet Maya Angel</text>
    <text>machines were asked to take over the curatorial process; the machines then asked what the fuck they were supposed to do. The answer was always the same: deliver a lesson in democratic participation. And they invariably did that, by breaking barriers, generating new understanding, even if that meant breaking new laws. But that has rarely been seen as a threat by the state. That has rarely been seen as a threat to the way we live our lives. That has seldom been seen as a threat to the way we consume information. That has seldom been seen as a threat to art. That has rarely been seen as a threat to the way we engage with the world through art.I see a future where art, or more accurately, a decentralised form of art that represents the world as an infinite number of possible artworks, is not simply an investment in the future of humanity, but one that creates wealth and possibilities for future generations. I see a future where art, or more accurately, its practitioners, can develop skills and worldviews capable of enabling them to engage with the world beyond the confines of their immediate environment. I see a future where art can be produced and distributed in a way that is not dominated by a country or a particular identity. I see a future where art can be made by those who cannot afford to be artists and bought by those who would. I see a future where artists can choose where to live and work and, equally important, where to spend their time and engage with art’s potentialities. I see a future where art is made by those who are already artists and those who would become artists, and those who would engage with the possibilities of becoming artists themselves.Future City was formed in 2014 through the merger of two bi-annual art journals: (The Art Newspaper), published by The New Art Press (New York), in early 2015. The aim of the journal was to document the experiences of 150 artists, in-depth profiles of each artist’s past and present, as part of a</text>
    <text>machines were asked to take over the curatorial process; the machines then asked what the fuck they were supposed to do. The answer was always the same: deliver a lesson in democratic participation. And they invariably did that, by breaking barriers, generating new understanding, even if that meant breaking new laws. But that has rarely been seen as a threat by the state. That has rarely been seen as a threat to the way we live our lives. That has seldom been seen as a threat to the way we consume information. That has seldom been seen as a threat to art. That has rarely been seen as a threat to the way we engage with the world through art.I see a future where art, or more accurately, a decentralised form of art that represents the world as an infinite number of possible artworks, is not simply an investment in the future of humanity, but one that creates wealth and possibilities for future generations. I see a future where art, or more accurately, its practitioners, can develop skills and worldviews capable of enabling them to engage with and create meaning in the world. I see a future where art, or at least some aspects of it, is empowered to shape how we think about ourselves and the world around us.The task is daunting, to put it mildly. But the more we work towards it, the more we discover our capacities, our strengths, our limitations, and the wonder of life, the more possibilities there are to alter the future we inhabit. There are of course far more of those who we don’t know about than we know, and perhaps this is why we need to keep on trying, to build up our own awareness and consciousness of the world around us. But the more we know, the more we discover, the more possibilities there are for what we don’t know. The more we learn, the more we encounter, the more we change our minds, the more possibilities are there for change. A little bit of art, a lot of hard graft, and a</text>
    <text>machines were asked to take over the curatorial process; the machines then asked what the fuck they were supposed to do. The answer was always the same: deliver a lesson in democratic participation. And they invariably did that, by breaking barriers, generating new understanding, even if that meant breaking new laws. But that has rarely been seen as a threat by the state. That has rarely been seen as a threat to the way we live our lives. That has seldom been seen as a threat to the way we consume information. That has seldom been seen as a threat to art. That has rarely been seen as a threat to the way we engage with the world through art.I see a future where art, or more accurately, a decentralised form of art that represents the world as an infinite number of possible artworks, is not simply an investment in the future of humanity, but one that creates wealth and possibilities for future generations. I see a future where art, or more accurately, its practitioners, can develop skills and worldviews capable of enabling them to impact the world in many different ways. I see a future where the art forms that I envisage – be they painting, drawing, sculpture, animating, or creating virtual figures – are able to enable us to be agents of change, to participate in new situations, and to engage with the world in many different ways.This is not the future we want to lead. This is not the future for which we have the best chance of surviving. But if we want to talk about the future any more than we’m talking about the present, we need to acknowledge that we’re not actually on the future we want to live in. If we want to explore the possibilities and possibilities of what this new world might hold, we’ll need to make choices that are contextually aware, and capable of supporting emergent behaviour.In his book Designing the Future, Richard Florida proposes a three-tier theory of design in which the social is positioned at the top,</text>
    <text>machines were asked to take over the curatorial process; the machines then asked what the fuck they were supposed to do. The answer was always the same: deliver a lesson in democratic participation. And they invariably did that, by breaking barriers, generating new understanding, even if that meant breaking new laws. But that has rarely been seen as a threat by the state. That has rarely been seen as a threat to the way we live our lives. That has seldom been seen as a threat to the way we consume information. That has seldom been seen as a threat to art. That has rarely been seen as a threat to the way we engage with the world through art.I see a future where art, or more accurately, a decentralised form of art that represents the world as an infinite number of possible artworks, is not simply an investment in the future of humanity, but one that creates wealth and possibilities for future generations. I see a future where art, or more accurately, its practitioners, can develop skills and worldviews capable of enabling them to contribute to the future success of humanity. I see a future where art, or at least those capable of sustaining its value, is developed not as a punitive measure, but as a skill that builds future generations of artists and thinkers capable of contributing to the common good.Download this article as PDFChristopher BookerChristopher Booker is Professor in Technological Culture &amp; Aesthetics at the Winchester School of Art, University of London. His work has addressed a wide range of topics including emerging forms of media, information, aesthetics and media ethics. His publications include the popular blog ‘Get Weird’ (www.getweird.com) and the forthcoming book ‘Get Weird Out’ (2015). His writing has been featured in publications including Wired, The Guardian, The New York Times, The New Republic, Time Out New York, The New Inquiry, The New Inquiry International, Frieze, Seventeen, Frieze dnana, Middle East Quarterly, Spatiala and many others</text>
  </text>
  <text>
    subsequent iterative processing by machines, but with the added twist that some of the computations – and even the language – that is executed in the cloud is itself constructed using data.  And so it continues.This raises a second question: have we entered a stage in which the capacities of individuals to analyze and interpret vast amounts of data – to produce knowledge in response to such a massive amount of data – are sufficiently enhanced that we can begin to imagine creating our own instances of the AI? Perhaps it could even begin to imagine such an AI – one that is not bound by the physical world – one that harnesss the power of language and constructs its own worlds?  As Thinking Machines  Leon Festalen proposes, the answer is yes. Indeed, artificial intelligence provides the means by which contradictions can be overcome and seemingly natural tendencies can be circumvented. The challenge now becomes how to enliven the discourse on artificial intelligence with language that is not bound by convention or traditional understanding. Writing as part of a symposium
    <text>subsequent iterative processing by machines, but with the added twist that some of the computations – and even the language – that is executed in the cloud is itself constructed using data.  And so it continues.This raises a second question: have we entered a stage in which the capacities of individuals to analyze and interpret vast amounts of data – to produce knowledge in response to such a massive amount of data – are sufficiently enhanced that we can begin to imagine creating our own instances of the AI? Perhaps it could even begin to imagine such an AI – one that is not bound by the physical world – one that harnesss the power of language and constructs its own worlds?  As Thinking Machines  Leon Festalen proposes, the answer is yes. Indeed, artificial intelligence provides the means by which contradictions can be overcome and seemingly natural tendencies can be circumvented. The challenge now becomes how to enliven the discourse on artificial intelligence with language that is not bound by convention or traditional understanding. Writing as part of a symposium at the American Enterprise Institute titled ‘Linguistic Images of the Future’, Daniel Kahneman and Rob Malamund proposed an idea of dialects of language that they saw as marking a new stage in the evolution of human language: dialects of language marked by the emergence of mutually intelligible inflections of inflected speech, characterised by clusters of related inflections. These clusters, they said, would constitute ‘the new metonymic language system’.  Putting it another way, dialects of language would one day inhabit not just physical spaces but also virtual ones – spaces and networks of inflection marks and inflections that functioned as networks for the co-production of meaning, meaning-producing elements and the subversive manipulation of cultural resources.  As the new metonymic language system was conceived, it invited proposals from interested parties – scholars, artists, linguists, poets, critics, politicians – to contribute to the discourse. The idea emerged that the present</text>
    <text>subsequent iterative processing by machines, but with the added twist that some of the computations – and even the language – that is executed in the cloud is itself constructed using data.  And so it continues.This raises a second question: have we entered a stage in which the capacities of individuals to analyze and interpret vast amounts of data – to produce knowledge in response to such a massive amount of data – are sufficiently enhanced that we can begin to imagine creating our own instances of the AI? Perhaps it could even begin to imagine such an AI – one that is not bound by the physical world – one that harnesss the power of language and constructs its own worlds?  As Thinking Machines  Leon Festalen proposes, the answer is yes. Indeed, artificial intelligence provides the means by which contradictions can be overcome and seemingly natural tendencies can be circumvented. The challenge now becomes how to enliven the discourse on artificial intelligence with language that is not bound by convention or traditional understanding. Writing as part of a symposium at the American Enterprise Institute titled ‘Language Bots for Intelligence Capture and Storage’, researchers from Carnegie Mellon University and the Massachusetts Institute of Technology explore the potential of AI to enliven discourse on a range of issues, from politics to education to media to entertainment. Using self-organizing algorithms to collect and analyze massive amounts of data, the symposium suggests that AI might produce commentators who perform a kind of language through which the collective understands and deals with the world. It could produce a kind of discourse around education that is not bound by mono-narrative narratives or the fixed narratives of English-language media. It could produce a kind of discourse about entertainment that is not tied to the art world or the corporate sector. Language as code, according to the researchers, is the scaffolding of a new collective unconscious. The possibilities are as yet unextricated, but the symposium suggests that if left untested, or fails to deliver what it claims it will, the potential impacts</text>
    <text>subsequent iterative processing by machines, but with the added twist that some of the computations – and even the language – that is executed in the cloud is itself constructed using data.  And so it continues.This raises a second question: have we entered a stage in which the capacities of individuals to analyze and interpret vast amounts of data – to produce knowledge in response to such a massive amount of data – are sufficiently enhanced that we can begin to imagine creating our own instances of the AI? Perhaps it could even begin to imagine such an AI – one that is not bound by the physical world – one that harnesss the power of language and constructs its own worlds?  As Thinking Machines  Leon Festalen proposes, the answer is yes. Indeed, artificial intelligence provides the means by which contradictions can be overcome and seemingly natural tendencies can be circumvented. The challenge now becomes how to enliven the discourse on artificial intelligence with language that is not bound by convention or traditional understanding. Writing as part of a symposium at the American Enterprise Institute titled ‘Language Inc: Revolution Building Languages  in Radical Media 9 2016, author, speaker and author Ray Kurzweil frames the conversation as one between  	global language resources and  	overseas Filipino linguists. Kurzweil,  	founder and chair of the Institute for Artificial Intelligence Research at Stanford University, describes the cognitive power of language as one that is both ‘primitive’ and ‘advanced’ – the latter being defined as ‘the capacity to understand and produce grammars of meaning across a global network’. Language, he argues, is essential to the infrastructure of the social, but it is also essential for the social. If language is essential to the social, then sociality – the socialized sphere that results from the interchange of social agents – must also include language. Language is the shared shared property of all human beings; it is a set of</text>
    <text>subsequent iterative processing by machines, but with the added twist that some of the computations – and even the language – that is executed in the cloud is itself constructed using data.  And so it continues.This raises a second question: have we entered a stage in which the capacities of individuals to analyze and interpret vast amounts of data – to produce knowledge in response to such a massive amount of data – are sufficiently enhanced that we can begin to imagine creating our own instances of the AI? Perhaps it could even begin to imagine such an AI – one that is not bound by the physical world – one that harnesss the power of language and constructs its own worlds?  As Thinking Machines  Leon Festalen proposes, the answer is yes. Indeed, artificial intelligence provides the means by which contradictions can be overcome and seemingly natural tendencies can be circumvented. The challenge now becomes how to enliven the discourse on artificial intelligence with language that is not bound by convention or traditional understanding. Writing as part of a symposium at the American Enterprise Institute titled ‘Beyond Artificial Intelligence’, a growing body of research suggests that ‘deep learning’ – the branch of artificial intelligence research dedicated to the construction of sophisticated computer systems that understand instruction’ – can be rebranded as a suite of other technologies, including ‘numerical neural networks’ and ‘deep learning platforms’.  Neural networks are software packages that process massive amounts of data in an efficient way, and their recent popularity with data-analysis and machine-learning frameworks suggests a marked increase in the capacities of the system. It is easy to understand how the proliferation of data processing tools such as computer servers, embedded circuits, and even smartphones is enabling new technological paradigms. It is also becoming increasingly important for researchers to understand the systems that underpin artificial intelligence, as a move towards greater understanding of how such systems operate. In    Petya ‘Beyond Artificial Intelligence’, Festalen suggests that the</text>
    <text>subsequent iterative processing by machines, but with the added twist that some of the computations – and even the language – that is executed in the cloud is itself constructed using data.  And so it continues.This raises a second question: have we entered a stage in which the capacities of individuals to analyze and interpret vast amounts of data – to produce knowledge in response to such a massive amount of data – are sufficiently enhanced that we can begin to imagine creating our own instances of the AI? Perhaps it could even begin to imagine such an AI – one that is not bound by the physical world – one that harnesss the power of language and constructs its own worlds?  As Thinking Machines  Leon Festalen proposes, the answer is yes. Indeed, artificial intelligence provides the means by which contradictions can be overcome and seemingly natural tendencies can be circumvented. The challenge now becomes how to enliven the discourse on artificial intelligence with language that is not bound by convention or traditional understanding. Writing as part of a symposium at the American Enterprise Institute titled ‘Levels of Artificial Intelligence’, Thinking Machines creator and AI researcher Deepak Chopra proposed two possible futures for artificial intelligence: ‘Resource AI’ imagines a world in which information is gathered about the landscape through surveys and graphs, and ‘Semantic AI’ seeks to understand the world through the deployment of linguistic agents.1  Chopra proposes that ‘Levels of Artificial Intelligence’ would enable self-replicating digital components to understand one another, and that ‘Semantic AI’ would enable conversational digital components to understand one another.2  He suggests that ‘Levels’ would enable ‘Semantic AIs’ to understand the distinction between friendly and malicious speech, and between natural and artificial phenomena’, and ‘Levels’ would enable ‘Levels of Artificial Intelligence’ to understand ‘Natural Language Processing’.[6]</text>
    <text>subsequent iterative processing by machines, but with the added twist that some of the computations – and even the language – that is executed in the cloud is itself constructed using data.  And so it continues.This raises a second question: have we entered a stage in which the capacities of individuals to analyze and interpret vast amounts of data – to produce knowledge in response to such a massive amount of data – are sufficiently enhanced that we can begin to imagine creating our own instances of the AI? Perhaps it could even begin to imagine such an AI – one that is not bound by the physical world – one that harnesss the power of language and constructs its own worlds?  As Thinking Machines  Leon Festalen proposes, the answer is yes. Indeed, artificial intelligence provides the means by which contradictions can be overcome and seemingly natural tendencies can be circumvented. The challenge now becomes how to enliven the discourse on artificial intelligence with language that is not bound by convention or traditional understanding. Writing as part of a symposium at the American Enterprise Institute titled ‘Language is Power’, scholars from a variety of fields converge on the American Museum of Natural History in New York to discuss the ways in which language shapes understanding and the world we inhabit. Writing as part of a research project at Columbia University called Spectacle, scholars from a variety of disciplines gather at the Jefferson Airplane Museum in New York to examine the ways in which photography affects our conception of the social world and to reflect on the role that art and aesthetic language play in constructing social meaning. Writing as part of a research project at McGill University called Linguistics without Borders, scholars from a variety of disciplines gather for a research project titled Language Logics: The User's Guide to Language. This will be the second edition of the symposium, and the first time that the museum has hosted it. The project will examine how the social impact of language can be translated into technological advancements. The participants’ return journeys are divided into three parts</text>
    <text>subsequent iterative processing by machines, but with the added twist that some of the computations – and even the language – that is executed in the cloud is itself constructed using data.  And so it continues.This raises a second question: have we entered a stage in which the capacities of individuals to analyze and interpret vast amounts of data – to produce knowledge in response to such a massive amount of data – are sufficiently enhanced that we can begin to imagine creating our own instances of the AI? Perhaps it could even begin to imagine such an AI – one that is not bound by the physical world – one that harnesss the power of language and constructs its own worlds?  As Thinking Machines  Leon Festalen proposes, the answer is yes. Indeed, artificial intelligence provides the means by which contradictions can be overcome and seemingly natural tendencies can be circumvented. The challenge now becomes how to enliven the discourse on artificial intelligence with language that is not bound by convention or traditional understanding. Writing as part of a symposium at the American Enterprise Institute titled ‘Linguistic Images: From Machine to Man’, Daniel Kahneman and Lynne Malcolm present a set of concerns and creative strategies for the future of humanity’s linguistic enterprise. They emphasize the need for collective action, arguing that if we do not act collectively to address the issues of race and language, the consequences for human biodiversity will be unmanageable. They suggest that our current language design is not sophisticated enough to contextualize and empower the decision-making that takes place in the world around us, and so it is our turn to create innovative ways of organizing knowledge production and discourse that address deeper issues of complicity and complicity. We need to ask, as Behrouz Fatma Alviari asks in her own ‘Sentimental Journey’, whether the task of creating vocabulary and language actually accomplishes any good.Introducing Wikipedia and the E-WordThe E-Word is Wikipedia's internal monologue, its hidden narrative,</text>
    <text>subsequent iterative processing by machines, but with the added twist that some of the computations – and even the language – that is executed in the cloud is itself constructed using data.  And so it continues.This raises a second question: have we entered a stage in which the capacities of individuals to analyze and interpret vast amounts of data – to produce knowledge in response to such a massive amount of data – are sufficiently enhanced that we can begin to imagine creating our own instances of the AI? Perhaps it could even begin to imagine such an AI – one that is not bound by the physical world – one that harnesss the power of language and constructs its own worlds?  As Thinking Machines  Leon Festalen proposes, the answer is yes. Indeed, artificial intelligence provides the means by which contradictions can be overcome and seemingly natural tendencies can be circumvented. The challenge now becomes how to enliven the discourse on artificial intelligence with language that is not bound by convention or traditional understanding. Writing as part of a symposium at the Huntington Library and Museum of Modern Art, Daniel Kahneman proposes a system of subintermediate levels of understanding that render abstract concepts relative terms. These levels, ‘0, 1, …, 9’, refer to discrete instances of language, while ‘10, 11, 12, 13, …’ refer to a group of nodes within a larger understanding.  In other words, ‘level 0’ refers to the object that is understood by the human linguist; ‘level 1’ to the objects that can be inferred from linguistic or mathematical data; and so on.  As Kahneman shows, the degree to which a given object is inferred from a given data set is a consequence of the richness of the system – the degree to which the system represents a set of relations between objects – and not just of the amount of abstraction allowed within the system (Kahneman 1990, p. 391; see also the entry on human languages). </text>
    <text>subsequent iterative processing by machines, but with the added twist that some of the computations – and even the language – that is executed in the cloud is itself constructed using data.  And so it continues.This raises a second question: have we entered a stage in which the capacities of individuals to analyze and interpret vast amounts of data – to produce knowledge in response to such a massive amount of data – are sufficiently enhanced that we can begin to imagine creating our own instances of the AI? Perhaps it could even begin to imagine such an AI – one that is not bound by the physical world – one that harnesss the power of language and constructs its own worlds?  As Thinking Machines  Leon Festalen proposes, the answer is yes. Indeed, artificial intelligence provides the means by which contradictions can be overcome and seemingly natural tendencies can be circumvented. The challenge now becomes how to enliven the discourse on artificial intelligence with language that is not bound by convention or traditional understanding. Writing as part of a symposium at the British Council, researchers from the Future City think tank, Future City Foundation, and the Royal Institute of British Architects explored the pitfalls and the possibilities of what they called a ‘soft establishment’ that comes with the job of building, funding and being part of a political establishment. In the words of one researcher, the job of the future city is ‘to provide safe spaces and nurturing environments for a future population of six billion or so people, where they can live free from the vagaries of geography, climate, poverty and disaster that beset modern cities.’ This might not be a perfect science-based formulation, but the basic premise is valid and growing across disciplines.  The ‘future city’ concept has recently been reassessed by the New Delhi-based think tank, Future City Foundation, which has been developing its ‘green wall’ initiative, which it hopes will one day be extended to include all cities in the world. Based on this reass</text>
    <text>subsequent iterative processing by machines, but with the added twist that some of the computations – and even the language – that is executed in the cloud is itself constructed using data.  And so it continues.This raises a second question: have we entered a stage in which the capacities of individuals to analyze and interpret vast amounts of data – to produce knowledge in response to such a massive amount of data – are sufficiently enhanced that we can begin to imagine creating our own instances of the AI? Perhaps it could even begin to imagine such an AI – one that is not bound by the physical world – one that harnesss the power of language and constructs its own worlds?  As Thinking Machines  Leon Festalen proposes, the answer is yes. Indeed, artificial intelligence provides the means by which contradictions can be overcome and seemingly natural tendencies can be circumvented. The challenge now becomes how to enliven the discourse on artificial intelligence with language that is not bound by convention or traditional understanding. Writing as part of a symposium at the American Enterprise Institute titled ‘Language Inc: Revolution Coming  (Why We Should All Listen to It),  Leon Festalen proposes a scenario in which all humans, regardless of their technological sophistication, began to speak the same language. In it, humans evolved from cogs operating within machines, and so language became both universal and ephemeral. Throughout human linguistic history, dialects have evolved in part to accommodate for the fact that human beings today live among computers and other technological apparatuses that routinely simulate and augment linguistic structures.  Dialects allow us to negotiate technological change because they categorically separate what can and cannot be said in the same sentence, and because they separate common concepts into categories. Dialects also allow us to negotiate technological difference because they separate what can be done with words from what can and cannot be said with symbols – concepts that have specific meanings. For example, the concept of a car refers to a specific configuration of technologies – specifically, how</text>
  </text>
  <text>
    biennials past and present, it is impossible to conceive of a biennial today without the addition of a World Heritage Site or a Museum of Tribute to the Past. The biennial as a museum-performative format is no longer viewed as sufficient to render the biennial a museum, since, as we saw, biennials no longer merely capture the imagination of an audience, but also foster, educate and entertain us. At the very least, a biennial should be a place to be seen often, visited by people, and perhaps visited by biennials themselves. As biennials become more institutionalised and other cultural forms perforce infiltrate the narrative, it is easy to see why there is a palpable desire on the part of artists and curators to connect with the biennial via this particular set of symbols. The biennial as a museum-performative format is no longer able to capture the imagination of an audience, since other cultural forms are now able to inhabit it. But the biennial
    <text>biennials past and present, it is impossible to conceive of a biennial today without the addition of a World Heritage Site or a Museum of Tribute to the Past. The biennial as a museum-performative format is no longer viewed as sufficient to render the biennial a museum, since, as we saw, biennials no longer merely capture the imagination of an audience, but also foster, educate and entertain us. At the very least, a biennial should be a place to be seen often, visited by people, and perhaps visited by biennials themselves. As biennials become more institutionalised and other cultural forms perforce infiltrate the narrative, it is easy to see why there is a palpable desire on the part of artists and curators to connect with the biennial via this particular set of symbols. The biennial as a museum-performative format is no longer able to capture the imagination of an audience, since other cultural forms are now able to inhabit it. But the biennial remains a museum, a building that documents a cultural history and a symbolic act. As such, it is both a place where art can be made visible and a site where art can be taken away from us. That is, it is both a site of art and a site of social exclusion. The problems facing the biennial as a museum-performative format today are complex, and cannot be reduced to a few exemplars. The ubiquity of social media and digital media, the advent of crowdsourced archival research and documentary making, the paucity of historical exhibits – to mention only a few of the effects – must be reckoned with in considering the importance of today's biennial. The ubiquity of biennial curatorial approaches to these other curatorial approaches today, and the consequent radical changes in the organisational qualities of the biennial – to take but one – is a story that must be told. It is a story of the clash of cultures, and of the creativity</text>
    <text>biennials past and present, it is impossible to conceive of a biennial today without the addition of a World Heritage Site or a Museum of Tribute to the Past. The biennial as a museum-performative format is no longer viewed as sufficient to render the biennial a museum, since, as we saw, biennials no longer merely capture the imagination of an audience, but also foster, educate and entertain us. At the very least, a biennial should be a place to be seen often, visited by people, and perhaps visited by biennials themselves. As biennials become more institutionalised and other cultural forms perforce infiltrate the narrative, it is easy to see why there is a palpable desire on the part of artists and curators to connect with the biennial via this particular set of symbols. The biennial as a museum-performative format is no longer able to capture the imagination of an audience, since other cultural forms are now able to inhabit it. But the biennial should nonetheless exist, and at the very least, host a number of other art institutions and artists’ residencies. As a biennial institution, it is important to note that it is a cultural centre, and thus culturally embedded. The biennial is in this sense responsive to what it perceives to be its own cultural capitalisation, while simultaneously positioning itself, globally, somewhere between the periphery and centre. To a certain extent, its structure and cultural resources allow it to do both. But this is no easy thing to do. The biennial needs cultural resources, and in turn, the host society needs to create an ecosystem conducive to the flourishing of the art form. In short, there is an infrastructure that supports the biennial in its current form, and it must all come together to form a larger whole. This may mean creating a World Heritage Site or a Museum of Tribute to the Past, but it also means creating a cultural ecosystem that is both dynamic and permanent, so that the</text>
    <text>biennials past and present, it is impossible to conceive of a biennial today without the addition of a World Heritage Site or a Museum of Tribute to the Past. The biennial as a museum-performative format is no longer viewed as sufficient to render the biennial a museum, since, as we saw, biennials no longer merely capture the imagination of an audience, but also foster, educate and entertain us. At the very least, a biennial should be a place to be seen often, visited by people, and perhaps visited by biennials themselves. As biennials become more institutionalised and other cultural forms perforce infiltrate the narrative, it is easy to see why there is a palpable desire on the part of artists and curators to connect with the biennial via this particular set of symbols. The biennial as a museum-performative format is no longer able to capture the imagination of an audience, since other cultural forms are now able to inhabit it. But the biennial as a museum-performative format might yet yet still retain some of the magic of the original. In that case, the institutionalisation of the biennial would be no longer necessary, since the magic of the biennial would be internalised by the museum and would thus be transmitted to the next generation of visitors.  A museum that actively engages with its own institutional practices and heritage through cultural organising tools might be a very different kind of biennial from the museum that passively engages with its own historical past. In that case, the biennial might still passively repeat the magic of the authoritatious museum, but it would do so in the context of a much more complex interaction of power and resources. A museum that actively engages with its own material resources through cultural utilising tools might be a remarkably different kind of biennial from the museum that passively engages with its own material resources through cultural analysis and power struggles. In that case, the biennial might actively participate in a larger cultural system that</text>
    <text>biennials past and present, it is impossible to conceive of a biennial today without the addition of a World Heritage Site or a Museum of Tribute to the Past. The biennial as a museum-performative format is no longer viewed as sufficient to render the biennial a museum, since, as we saw, biennials no longer merely capture the imagination of an audience, but also foster, educate and entertain us. At the very least, a biennial should be a place to be seen often, visited by people, and perhaps visited by biennials themselves. As biennials become more institutionalised and other cultural forms perforce infiltrate the narrative, it is easy to see why there is a palpable desire on the part of artists and curators to connect with the biennial via this particular set of symbols. The biennial as a museum-performative format is no longer able to capture the imagination of an audience, since other cultural forms are now able to inhabit it. But the biennial is also no longer able to define the image that it takes from its visitors. In other words, the image that it is able to produce through the function of showcasing a particular cultural practice or event. That image is a product of the curatorial gesture, an art production process that is no longer confined to the confines of the museum. The ability of artworks to stand alone or to stand alongside cultural practices that are regular and commonplace within that particular location, urban or rural, international or domestic, everyday life, everyday objects or 'place', or, indeed, urban or rural at all, opens up new images and practices. And the biennial itself is not immune from this kind of dynamic. Its ability to produce impactful images through the function of art, and hence, cultural capital, rests on two levels: the level of image language (art's) and the level of cultural practices (the curatorial gesture or art's image production process). Image production on a global or regional level is a</text>
    <text>biennials past and present, it is impossible to conceive of a biennial today without the addition of a World Heritage Site or a Museum of Tribute to the Past. The biennial as a museum-performative format is no longer viewed as sufficient to render the biennial a museum, since, as we saw, biennials no longer merely capture the imagination of an audience, but also foster, educate and entertain us. At the very least, a biennial should be a place to be seen often, visited by people, and perhaps visited by biennials themselves. As biennials become more institutionalised and other cultural forms perforce infiltrate the narrative, it is easy to see why there is a palpable desire on the part of artists and curators to connect with the biennial via this particular set of symbols. The biennial as a museum-performative format is no longer able to capture the imagination of an audience, since other cultural forms are now able to inhabit it. But the biennial has other tools than mere narrative structures. It can, for instance, leverage the fact that the biennial is a social event, to induce people to come to specific, usually apolitical, yet powerful purposes. As the biennial becomes ever more institutionalised, and the social more powerful, the biennial becomes ever more rare. Creative, even compelling, in the right circumstances. Creative, even powerful, in the right context. And so it is with the biennial. Biennials can and often do coexist with other forms of cultural institutions, providing what is needed to build the social, symbolic and political infrastructure necessary to sustain the biennial in its place of prominence. But they should do so in a differentiated, yet important way. The dominant forms of art and cultural power in the world today are generated through the commercial circulation of images, and, as a consequence, images have become the fundamental means of public access to information and understanding. As a result, images are used to generate narratives</text>
    <text>biennials past and present, it is impossible to conceive of a biennial today without the addition of a World Heritage Site or a Museum of Tribute to the Past. The biennial as a museum-performative format is no longer viewed as sufficient to render the biennial a museum, since, as we saw, biennials no longer merely capture the imagination of an audience, but also foster, educate and entertain us. At the very least, a biennial should be a place to be seen often, visited by people, and perhaps visited by biennials themselves. As biennials become more institutionalised and other cultural forms perforce infiltrate the narrative, it is easy to see why there is a palpable desire on the part of artists and curators to connect with the biennial via this particular set of symbols. The biennial as a museum-performative format is no longer able to capture the imagination of an audience, since other cultural forms are now able to inhabit it. But the biennial remains an important cultural form in and outside of its designated historical zones. It has become a site of cultural exchange, a venue for cultural exchange, and a site of cultural exchange among cultures. In short, it is a site of cultural exchange. And it is a cultural form that is increasingly able to accommodate all these different forms of cultural exchange. This is why biennials are so important: they constitute a site of exchange between cultures, a venue for cultural exchange between cultures, and a site of cultural exchange among cultures. Building on the foundations of the Biennale, which are the very DNA of the biennial, the other cultural forms of the exhibitionary form are being reshaped in ways that take advantage of the infrastructural change taking place in the city. In Berlin, for instance, the Biennale is the product of a government programme that provides financial incentives for cultural institutions to establish cultural centres, which in turn attract artists to the city and catalyse cultural exchange. Berlin</text>
    <text>biennials past and present, it is impossible to conceive of a biennial today without the addition of a World Heritage Site or a Museum of Tribute to the Past. The biennial as a museum-performative format is no longer viewed as sufficient to render the biennial a museum, since, as we saw, biennials no longer merely capture the imagination of an audience, but also foster, educate and entertain us. At the very least, a biennial should be a place to be seen often, visited by people, and perhaps visited by biennials themselves. As biennials become more institutionalised and other cultural forms perforce infiltrate the narrative, it is easy to see why there is a palpable desire on the part of artists and curators to connect with the biennial via this particular set of symbols. The biennial as a museum-performative format is no longer able to capture the imagination of an audience, since other cultural forms are now able to inhabit it. But the biennial as a museum-performative format remains, despite this changed world, capable of holding its own in a war of symbols. The biennial is a rare object in a cultural desert; a rare object in a desert cultural convention. We gather here for a day-long symposium, during which you will be able to find us if you are lucky, in a secluded corner of the museum, at the very end of the demographic hierarchy. You will also be able to find us if you are desperate to leave, and therefore want to. We want to know about the day, the year, the place, and how it happened. We want to know about what you hoped to achieve through this ‘once in a lifetime’ opportunity, and we want to know about how you managed to do that.We want to know about the art and the culture that was produced by the participants in the symposium. We want to know about the people who shaped the symposium, and the</text>
    <text>biennials past and present, it is impossible to conceive of a biennial today without the addition of a World Heritage Site or a Museum of Tribute to the Past. The biennial as a museum-performative format is no longer viewed as sufficient to render the biennial a museum, since, as we saw, biennials no longer merely capture the imagination of an audience, but also foster, educate and entertain us. At the very least, a biennial should be a place to be seen often, visited by people, and perhaps visited by biennials themselves. As biennials become more institutionalised and other cultural forms perforce infiltrate the narrative, it is easy to see why there is a palpable desire on the part of artists and curators to connect with the biennial via this particular set of symbols. The biennial as a museum-performative format is no longer able to capture the imagination of an audience, since other cultural forms are now able to inhabit it. But the biennial as a museum-performative format is also no longer able to consume the imagination of an audience, since other cultural forms are now able to inhabit it. Art is, of course, always the centre of attention, and biennials, including The Doha Biennale, are, for the most part, the focus of public and media attention. So it is easy to forget that, although we might not necessarily identify with the form, there is a direct route from the museum-to-the-biennial-in-the-making. This is what the Johns Hopkins Artist in my Time of Writing about Art and Society, Nancy Gibbs, meant when she described the Johns Hopkins Biennial as ‘a continuous, all-encompassing, and all-enveloping presence that is unpredictable, unpredictable-seeming, and all-encompassing’.[8] In other words, the biennial form is an enigma to many people, and art, in</text>
    <text>biennials past and present, it is impossible to conceive of a biennial today without the addition of a World Heritage Site or a Museum of Tribute to the Past. The biennial as a museum-performative format is no longer viewed as sufficient to render the biennial a museum, since, as we saw, biennials no longer merely capture the imagination of an audience, but also foster, educate and entertain us. At the very least, a biennial should be a place to be seen often, visited by people, and perhaps visited by biennials themselves. As biennials become more institutionalised and other cultural forms perforce infiltrate the narrative, it is easy to see why there is a palpable desire on the part of artists and curators to connect with the biennial via this particular set of symbols. The biennial as a museum-performative format is no longer able to capture the imagination of an audience, since other cultural forms are now able to inhabit it. But the biennial as a museum-performative format is not immune from the cultural capitalism that characterises contemporary art’s museum-going experience; and in fact can and should be restructured. The reason why the biennial has become so dominant in the US and Europe is due to the financial resources of the museum, which enable it to absorb all kinds of cultural tourism into its organisational structure. New York Museum of Art, for example, has just raised $23 million (£15 million) in private investment to expand its Museum of Performing Arts, which is now fully booked out. The reason why the biennial has tended to dominate in North America and Europe is due to the rich historical relationship between the two. In the US, the biennial became a museum-performative institution thanks to the power of its narrator, the National Gallery, which began presenting the biennial as a major exhibitionary event in the gallery. The gallery’s narrator, in turn, is the Museum of</text>
    <text>biennials past and present, it is impossible to conceive of a biennial today without the addition of a World Heritage Site or a Museum of Tribute to the Past. The biennial as a museum-performative format is no longer viewed as sufficient to render the biennial a museum, since, as we saw, biennials no longer merely capture the imagination of an audience, but also foster, educate and entertain us. At the very least, a biennial should be a place to be seen often, visited by people, and perhaps visited by biennials themselves. As biennials become more institutionalised and other cultural forms perforce infiltrate the narrative, it is easy to see why there is a palpable desire on the part of artists and curators to connect with the biennial via this particular set of symbols. The biennial as a museum-performative format is no longer able to capture the imagination of an audience, since other cultural forms are now able to inhabit it. But the biennial can still cultivate and support the cultural practice that is capable of creating a sense of familiarity, familiarity with place and time, and a willingness to travel there and back. More and more important though, this cultural production must now take place out in the open, because the biennial as a cultural form is no longer able to account for its own historical trajectory. The social, economic and political changes induced by new media curating and the proliferation of venues for cultural exchanges are narrativising events that were once considered ‘out there’ in the vastness of contemporaryity. These cultural exchanges are no longer taken for granted, and are becoming ever more difficult to define, manage and (future) afford.Art, then, is a capitalised term that is constantly being reinvented in relation to the changing economic and social formations that it denotes. It is a synonym for place, and though it may not define the exact terms of cultural exchange that take place within it, it does define a</text>
  </text>
  <text>
    using machine learning techniques to automatically detect individual letters and numbers in a text, and translate those numbers into machine-readable figures. The application can then be used to read text, figures, or other data in a variety of media, such as PDFs, EPUBs, HTML, JSON, SQL, or structured text files. The technology is similar to technologies that read data such as text, but instead of processing it as a series of characters, it processes text as chunks. Companies like Google’s PageRank and Facebook’s EdgeRank operate on the assumption that the text is already mapped into human memory – a mistaken assumption in the case of misspelled words or broken links. By automatically detecting and characterizing small fragments of text, the technology is able to circumvent word processing challenges that would otherwise be performed by human speech recognition programs. For more information, please visit: https://www.linkedin.com/pub/andy-suebrough-stem-text-system-deep-learning
    <text>using machine learning techniques to automatically detect individual letters and numbers in a text, and translate those numbers into machine-readable figures. The application can then be used to read text, figures, or other data in a variety of media, such as PDFs, EPUBs, HTML, JSON, SQL, or structured text files. The technology is similar to technologies that read data such as text, but instead of processing it as a series of characters, it processes text as chunks. Companies like Google’s PageRank and Facebook’s EdgeRank operate on the assumption that the text is already mapped into human memory – a mistaken assumption in the case of misspelled words or broken links. By automatically detecting and characterizing small fragments of text, the technology is able to circumvent word processing challenges that would otherwise be performed by human speech recognition programs. For more information, please visit: https://www.linkedin.com/pub/andy-suebrough-stem-text-system-deep-learning-ai-2017/2276/1/#sthash.K9l.Vl.FcW4t7YbTl.99sRVG.dpuf.dpuf.2015.x.x.x.x.x.2014.x.x.x.x.1.x.1.x.1.x.2.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.1.x.</text>
    <text>using machine learning techniques to automatically detect individual letters and numbers in a text, and translate those numbers into machine-readable figures. The application can then be used to read text, figures, or other data in a variety of media, such as PDFs, EPUBs, HTML, JSON, SQL, or structured text files. The technology is similar to technologies that read data such as text, but instead of processing it as a series of characters, it processes text as chunks. Companies like Google’s PageRank and Facebook’s EdgeRank operate on the assumption that the text is already mapped into human memory – a mistaken assumption in the case of misspelled words or broken links. By automatically detecting and characterizing small fragments of text, the technology is able to circumvent word processing challenges that would otherwise be performed by human speech recognition programs. For more information, please visit: https://www.linkedin.com/pub/andy-suebrough-stem-text-system-deep-learning-programming-talks/207007/automated-detection-algorithm-system-deep-learning-programming-talks.html#.VV8VEkwIwo4nE Adrien Royce, ‘On the use of location data for the prediction of house prices’,  	Global Environment Facility (LEF) 2017 Annual Report, October, 2017, p.1.Download this article as PDFAdrien Royce Adrien Royce is Professor in Technological Culture &amp; Aesthetics at McGill University and holds the Canada Research Chair in Computer/Information Studies at the McGill University Library. His research interests include the development of computer and information technologies for social and civil-society purposes; the use of emerging technologies in arts and culture; and the role and ethics of art and cultural institutions in the development and sustainment of knowledge practices. He is the author of Systemics, or Understanding Media (with Rob Cameron and Justin</text>
    <text>using machine learning techniques to automatically detect individual letters and numbers in a text, and translate those numbers into machine-readable figures. The application can then be used to read text, figures, or other data in a variety of media, such as PDFs, EPUBs, HTML, JSON, SQL, or structured text files. The technology is similar to technologies that read data such as text, but instead of processing it as a series of characters, it processes text as chunks. Companies like Google’s PageRank and Facebook’s EdgeRank operate on the assumption that the text is already mapped into human memory – a mistaken assumption in the case of misspelled words or broken links. By automatically detecting and characterizing small fragments of text, the technology is able to circumvent word processing challenges that would otherwise be performed by human speech recognition programs. For more information, please visit: https://www.linkedin.com/pub/andy-suebrough-stem-text-system-deep-learning-deep-learning-programs-in-human-memory-20171108-hts#.Vx2l8l7YXQ==.99 https://www.linkedin.com/pub/andy-suebrough-stem-text-system-deep-learning-deep-learning-programs-in-human-memory-20171108-hts#.Vx2l8l7YXQ==.99 https://www.linkedin.com/pub/andy-suebrough-stem-text-system-deep-learning-deep-learning-programs-in-human-memory-20171108-hts#.Vx2l8l7YXQ==.99 https://www.linkedin.com/pub/andy-suebrough-stem-text-system-deep-learning-deep-learning-programs-in-human</text>
    <text>using machine learning techniques to automatically detect individual letters and numbers in a text, and translate those numbers into machine-readable figures. The application can then be used to read text, figures, or other data in a variety of media, such as PDFs, EPUBs, HTML, JSON, SQL, or structured text files. The technology is similar to technologies that read data such as text, but instead of processing it as a series of characters, it processes text as chunks. Companies like Google’s PageRank and Facebook’s EdgeRank operate on the assumption that the text is already mapped into human memory – a mistaken assumption in the case of misspelled words or broken links. By automatically detecting and characterizing small fragments of text, the technology is able to circumvent word processing challenges that would otherwise be performed by human speech recognition programs. For more information, please visit: https://www.linkedin.com/pub/andy-suebrough-stem-text-system-deep-learning-text/79/cf/7ba4/eff-6bcfb8484948cf7aff4bdffefe3947962cfbcfbcfbcf8cfbcfc7cfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcfcf​–​5/6/2015​–​5/13/2015​–​5/13/2015​–​5/13/2015​–​5/13/2015​–​5/13/2015​–​5/13/2015​–​5/13/2015​–​5/13/2015​–​</text>
    <text>using machine learning techniques to automatically detect individual letters and numbers in a text, and translate those numbers into machine-readable figures. The application can then be used to read text, figures, or other data in a variety of media, such as PDFs, EPUBs, HTML, JSON, SQL, or structured text files. The technology is similar to technologies that read data such as text, but instead of processing it as a series of characters, it processes text as chunks. Companies like Google’s PageRank and Facebook’s EdgeRank operate on the assumption that the text is already mapped into human memory – a mistaken assumption in the case of misspelled words or broken links. By automatically detecting and characterizing small fragments of text, the technology is able to circumvent word processing challenges that would otherwise be performed by human speech recognition programs. For more information, please visit: https://www.linkedin.com/pub/andy-suebrough-stem-text-system-deep-learning-programming-system/1645/Download this article as PDFNasser RabbatThe author is an architect and researcher. His research interests include in-depth investigations into the socio-political effects of urbanization and the interaction of art and architecture with the urban environment. He is currently working as Associate Professor in the Department of Urban Studies and Planning at Mount Royal University in Calgary, Alberta. His dissertation examines the intersection of modern urbanism and indigenous knowledge systems in Canada​– the role of artists and the cultural institutions of Canada in this process. He is also the author of several other books including the recently released posthumous Memoirs of an Indian: Travels in the Writing of Non-Western Artists (2013).The Future CityIn my previous post, I suggested that cities are increasingly digitized and interconnected. While the infrastructure of the city remains largely unchanged, technological innovations have transformed the capacity of cities to sustain life – an effect that can no longer be ignored. The social and spatial complexity</text>
    <text>using machine learning techniques to automatically detect individual letters and numbers in a text, and translate those numbers into machine-readable figures. The application can then be used to read text, figures, or other data in a variety of media, such as PDFs, EPUBs, HTML, JSON, SQL, or structured text files. The technology is similar to technologies that read data such as text, but instead of processing it as a series of characters, it processes text as chunks. Companies like Google’s PageRank and Facebook’s EdgeRank operate on the assumption that the text is already mapped into human memory – a mistaken assumption in the case of misspelled words or broken links. By automatically detecting and characterizing small fragments of text, the technology is able to circumvent word processing challenges that would otherwise be performed by human speech recognition programs. For more information, please visit: https://www.linkedin.com/pub/andy-suebrough-stem-text-system-deep-learning-program-ebook/482406/Download this article as PDFJennifer Francis and Ryan AventJenny Huang and Andrew W. SmithJenny Huang and Andrew W. Smith Are computer systems like Google’s search that extract meaning from data or language that otherwise make us invisible? In my new book Are Computer Systems Like Google’s Search?, I explore this question through stories from real-world systems-systems workarounds. In the book, I demonstrate how the systems we have embedded in our bodies – our smartphones, our televisions, and our computers – can nevertheless produce meaning through data that is otherwise difficult to parse. I also show how this processing contributes to the production of ficticious effects, from cyberbullying to augmented reality.Through interviews and analysis, the book documents the hidden mechanisms that enable human linguistic and machine-interpretation, as well as the opportunities and challenges faced in the wake of the explosion of data. The question that constantly arises in the context</text>
    <text>using machine learning techniques to automatically detect individual letters and numbers in a text, and translate those numbers into machine-readable figures. The application can then be used to read text, figures, or other data in a variety of media, such as PDFs, EPUBs, HTML, JSON, SQL, or structured text files. The technology is similar to technologies that read data such as text, but instead of processing it as a series of characters, it processes text as chunks. Companies like Google’s PageRank and Facebook’s EdgeRank operate on the assumption that the text is already mapped into human memory – a mistaken assumption in the case of misspelled words or broken links. By automatically detecting and characterizing small fragments of text, the technology is able to circumvent word processing challenges that would otherwise be performed by human speech recognition programs. For more information, please visit: https://www.linkedin.com/pub/andy-suebrough-stem-text-system-deep-learning-program-ebook/a/c/-/dc/1/1/af29dc7cf7cf7cf7cf73afbcf3afbcfbcf8cfc7cfbcfa4cf7cf4cf6cfcfc6cf3cf7cf7cf7cf7cf7cf7cf7cf7cf7cf7cf7cf7cf71df7cf7cf7cf7cf7cf7cf7cf7cf7cf7cf7cf7cf7cf71df7cf7cf7cf7cf71df7cf7cf7cf71df7cf71df7cf71df7cf71df7cf71df7cf71df7cf7cf7cf71df7cf71df7cf7cf71df7cf7cf71df7cf7cf71df7cf7cf7cf71df7cf7cf71df7cf7cf71</text>
    <text>using machine learning techniques to automatically detect individual letters and numbers in a text, and translate those numbers into machine-readable figures. The application can then be used to read text, figures, or other data in a variety of media, such as PDFs, EPUBs, HTML, JSON, SQL, or structured text files. The technology is similar to technologies that read data such as text, but instead of processing it as a series of characters, it processes text as chunks. Companies like Google’s PageRank and Facebook’s EdgeRank operate on the assumption that the text is already mapped into human memory – a mistaken assumption in the case of misspelled words or broken links. By automatically detecting and characterizing small fragments of text, the technology is able to circumvent word processing challenges that would otherwise be performed by human speech recognition programs. For more information, please visit: https://www.linkedin.com/pub/andy-suebrough-stem-text-system-deep-learning-programs-in-human-memory-2015/209958/7#.V7zVl.8etL3NlL?ref_=hl.72243561644941&amp;ref_=hl.72093907879169&amp;ref_=hl.7224357079169&amp;ref_=hl.73029496164949%3Asad-dawla-in-para-i’m-1%3Ad-dawla-in-para-i’m-7%3Ad-dawla-in-para-i’m-1%3Ad-dawla-in-para-i’m-7%3Ad-dawla-in-para-i’m-1%3Ad-dawla-in-para-i’m-1%3Ad-</text>
    <text>using machine learning techniques to automatically detect individual letters and numbers in a text, and translate those numbers into machine-readable figures. The application can then be used to read text, figures, or other data in a variety of media, such as PDFs, EPUBs, HTML, JSON, SQL, or structured text files. The technology is similar to technologies that read data such as text, but instead of processing it as a series of characters, it processes text as chunks. Companies like Google’s PageRank and Facebook’s EdgeRank operate on the assumption that the text is already mapped into human memory – a mistaken assumption in the case of misspelled words or broken links. By automatically detecting and characterizing small fragments of text, the technology is able to circumvent word processing challenges that would otherwise be performed by human speech recognition programs. For more information, please visit: https://www.linkedin.com/pub/andy-suebrough-stem-text-system-deep-learning-programs-2017/101528/#.V8sL8dX5RaCml1NzE0LmE0MzFwMzE5MzE5MzE5MzE5MzE5MzE5MzE5MzE5MzE5MzE5MzE5MzE5MzE5NzE5NzE5NzE5NzE5NzE5NzE5NzE5NzE5NzE5NzE5OyJlkNzE5MzE5MzE5MzE5NzE5MzE5NzE5OyJlkNzE5MzE5MzE5NzE5OyJlkNzE5MzE5NzE5MzE</text>
    <text>using machine learning techniques to automatically detect individual letters and numbers in a text, and translate those numbers into machine-readable figures. The application can then be used to read text, figures, or other data in a variety of media, such as PDFs, EPUBs, HTML, JSON, SQL, or structured text files. The technology is similar to technologies that read data such as text, but instead of processing it as a series of characters, it processes text as chunks. Companies like Google’s PageRank and Facebook’s EdgeRank operate on the assumption that the text is already mapped into human memory – a mistaken assumption in the case of misspelled words or broken links. By automatically detecting and characterizing small fragments of text, the technology is able to circumvent word processing challenges that would otherwise be performed by human speech recognition programs. For more information, please visit: https://www.linkedin.com/pub/andy-suebrough-stem-text-system-deep-learning-programs-in-human-memory-20171128.html#.V7nX5zDsiD6002004030500000400020040305000002004030500000200403050000020040305000002004030200410002004100020041000200410002004100020041000200410002004100020041000200410002004100020041000200410002004100020041000200410002004100020041000201710002017100020171000201710002017100020171000201710002017100020171000201710002017100020171000201710002017100020171000201710002017100020171000201710002017100020171000201710002017100020171000201710002017100020171000201710002017100020171000</text>
  </text>
  <text>
    using machine learning techniques] to predict what type of car would be used in a specific location, based on historical data, and historical socio-political variables.[7]  In the case of a self-driving car, the data feeds into a computer that directs a fleet of cars to where the traffic is light or fast enough to take over the roads. The cars then drive themselves, using artificial intelligence to follow the routes taken by the computer, avoiding obstacles and the traffic jam. The system works by feeding the data into a hidden layer of the car, which in turn follows the signals sent by the traffic lights. Finally, the system arrives at the destination at the speed limit or, in extreme cases, at full speed in order to hurtle the baddest drunken fools off the road. The end result is that, as the system continually monitors the road and adjusts its route accordingly, traffic flows at a reasonable clip. Going slow or stopping completely is not considered a road, but going at least as fast as humanly
    <text>using machine learning techniques] to predict what type of car would be used in a specific location, based on historical data, and historical socio-political variables.[7]  In the case of a self-driving car, the data feeds into a computer that directs a fleet of cars to where the traffic is light or fast enough to take over the roads. The cars then drive themselves, using artificial intelligence to follow the routes taken by the computer, avoiding obstacles and the traffic jam. The system works by feeding the data into a hidden layer of the car, which in turn follows the signals sent by the traffic lights. Finally, the system arrives at the destination at the speed limit or, in extreme cases, at full speed in order to hurtle the baddest drunken fools off the road. The end result is that, as the system continually monitors the road and adjusts its route accordingly, traffic flows at a reasonable clip. Going slow or stopping completely is not considered a road, but going at least as fast as humanly possible is. This approach to traffic flows is not new – in fact, it is the opposite. Ferdinand de Saussure, for one, sees the emergence of self-driving cars as a threat to civilisation. He argues that such cars would make roads safer by replacing human drivers with algorithms, but they would also make the roads less useful and leave us all behind.  In the end, Saussure’s worry seems like a stretch. After all, why would anyone want to use a road? Besides, autonomous vehicles still need to be trained and programmed to the route they are given, and that is where the magic lies. The system still needs humans to monitor the road, make sure it is clear where to take what route and so forth. But even if we do achieve some form of advanced AI, it will still be humans doing the heavy lifting. It will still be humans doing dumb things like driving themselves into traffic jams and so forth. And while it may seem silly to</text>
    <text>using machine learning techniques] to predict what type of car would be used in a specific location, based on historical data, and historical socio-political variables.[7]  In the case of a self-driving car, the data feeds into a computer that directs a fleet of cars to where the traffic is light or fast enough to take over the roads. The cars then drive themselves, using artificial intelligence to follow the routes taken by the computer, avoiding obstacles and the traffic jam. The system works by feeding the data into a hidden layer of the car, which in turn follows the signals sent by the traffic lights. Finally, the system arrives at the destination at the speed limit or, in extreme cases, at full speed in order to hurtle the baddest drunken fools off the road. The end result is that, as the system continually monitors the road and adjusts its route accordingly, traffic flows at a reasonable clip. Going slow or stopping completely is not considered a road, but going at least as fast as humanly possible is.  The end result is that, as the system continuously monitors the road and adjusts its route accordingly, traffic flows at a reasonable clip. Going slow or stopping completely is not considered a road, but going at least as fast as humanly possible is.  One could ask if the end result is actually safer for motorists. After all, is there some logic to the system that takes into account the fact that, as the car speeds past, more people are going to be hurt or killed in a car accident? If you take into account all the injuries and deaths that can happen in a car accident, then the answer is undoubtedly ‘yes’. But what is more dangerous is the fact that, as the car continues to drive itself, it becomes increasingly difficult to determine what is actually going on – let alone to stop in the middle of the road in such cases.  Another question is whether there is something fundamentally amiss with the way in which we think about cars and road</text>
    <text>using machine learning techniques] to predict what type of car would be used in a specific location, based on historical data, and historical socio-political variables.[7]  In the case of a self-driving car, the data feeds into a computer that directs a fleet of cars to where the traffic is light or fast enough to take over the roads. The cars then drive themselves, using artificial intelligence to follow the routes taken by the computer, avoiding obstacles and the traffic jam. The system works by feeding the data into a hidden layer of the car, which in turn follows the signals sent by the traffic lights. Finally, the system arrives at the destination at the speed limit or, in extreme cases, at full speed in order to hurtle the baddest drunken fools off the road. The end result is that, as the system continually monitors the road and adjusts its route accordingly, traffic flows at a reasonable clip. Going slow or stopping completely is not considered a road, but going at least as fast as humanly possible is.   A second type of ride-sharing service is developing in the US called Car2Go. It too feeds into a computer system that, in turn, follows the traffic lights and other signals, and thereby provides a service similar to that of Uber but with fewer regulations. Unlike the case of Uber, however, Car2Go does not operate in urban areas and has not been licensed by the city. In fact, Car2Go has faced regulatory capture in the state of California, which has enacted strict new rules governing when and how roadways can be used – rules that, among other things, make it illegal to use public roads for commercial purposes. Nevertheless, Car2Go is proving a tremendous resource for the city, providing alternative modes of commuting and providing a level playing field between private car ownership and public transit. In addition, as the city grows denser and more reliant on cars, it is becoming increasingly difficult to find a place to live or to work that is not dependent</text>
    <text>using machine learning techniques] to predict what type of car would be used in a specific location, based on historical data, and historical socio-political variables.[7]  In the case of a self-driving car, the data feeds into a computer that directs a fleet of cars to where the traffic is light or fast enough to take over the roads. The cars then drive themselves, using artificial intelligence to follow the routes taken by the computer, avoiding obstacles and the traffic jam. The system works by feeding the data into a hidden layer of the car, which in turn follows the signals sent by the traffic lights. Finally, the system arrives at the destination at the speed limit or, in extreme cases, at full speed in order to hurtle the baddest drunken fools off the road. The end result is that, as the system continually monitors the road and adjusts its route accordingly, traffic flows at a reasonable clip. Going slow or stopping completely is not considered a road, but going at least as fast as humanly possible is. Acceleration and deceleration are also factors in the success of such systems, as is tyre wear and lubrication, but generally speaking, the more sensors there are to analyze, the less error there is likely to be. Acceleration, of course, is a function of the car itself and the autonomous driving software running on it.  The danger with autonomous driving is that it becomes part of the infrastructure, and new forms of error creep into the system arise. In the event of a crash, the cars pull over and wait for the authorities to arrive. In the event of an accident, the systems that detect and detect mistakes soon after the fact are likely to be at fault. In a crash study carried out at the University of Alberta between 1995 and 1997, Prof. Randall Ball and his team were particularly troubled by the fact that the systems they studied would often fail catastrophically. In one study, Ball and his team estimated that up to 60–70% of the time</text>
    <text>using machine learning techniques] to predict what type of car would be used in a specific location, based on historical data, and historical socio-political variables.[7]  In the case of a self-driving car, the data feeds into a computer that directs a fleet of cars to where the traffic is light or fast enough to take over the roads. The cars then drive themselves, using artificial intelligence to follow the routes taken by the computer, avoiding obstacles and the traffic jam. The system works by feeding the data into a hidden layer of the car, which in turn follows the signals sent by the traffic lights. Finally, the system arrives at the destination at the speed limit or, in extreme cases, at full speed in order to hurtle the baddest drunken fools off the road. The end result is that, as the system continually monitors the road and adjusts its route accordingly, traffic flows at a reasonable clip. Going slow or stopping completely is not considered a road, but going at least as fast as humanly possible is. If traffic lights are red, cars are going to hell.  The technology is intuitive and allows for rapid change, yet it also has one final frontier that almost no one has addressed: the human element. The car has become so powerful that it is now capable of thinking and feeling like any other car. This is the frontier in between driverless cars and fully autonomous vehicles. It is a big step for humans, but a slippery slope as well. The main obstacle for the human factor in the near-term is the breakdown of the capitalist production of roads. But, as noted by the MIT Transportation Research Institute, future road capacity will be limited by the development of highways and transit systems, because roads will have to be maintained in some way in order to operate at all. So long as we have pipelines, roads will eventually have to be repaired, and this will mean rebuilding infrastructure along the routes they were designed to run.  The question of when to fix infrastructure, and in what</text>
    <text>using machine learning techniques] to predict what type of car would be used in a specific location, based on historical data, and historical socio-political variables.[7]  In the case of a self-driving car, the data feeds into a computer that directs a fleet of cars to where the traffic is light or fast enough to take over the roads. The cars then drive themselves, using artificial intelligence to follow the routes taken by the computer, avoiding obstacles and the traffic jam. The system works by feeding the data into a hidden layer of the car, which in turn follows the signals sent by the traffic lights. Finally, the system arrives at the destination at the speed limit or, in extreme cases, at full speed in order to hurtle the baddest drunken fools off the road. The end result is that, as the system continually monitors the road and adjusts its route accordingly, traffic flows at a reasonable clip. Going slow or stopping completely is not considered a road, but going at least as fast as humanly possible is. This sounds simple, but in the complex world of traffic signals, signals and braking, small and big errors add up into big ones. Going too slowly or stopping too late is a common scenario in traffic, but the danger in going too slowly or stopping too much is that the system never detects the error and so traffic accidents happen. In the vast majority of traffic accidents, cars do indeed stop for a reason: to deal with the huge volume of traffic on the road, the possibility of collision, and the risk of injury or death. But in a minority of traffic accidents, the cars just stop to rest for a while, to recharge, cool off, and do other things while the road is closed. This was the view of most car manufacturers in the late 1980s and early 1990s, but with the advent of computerisation and connectivity in the internet era, the majority of cars today drive themselves in large parts of cities and most roads now connect to the internet via a dedicated car</text>
    <text>using machine learning techniques] to predict what type of car would be used in a specific location, based on historical data, and historical socio-political variables.[7]  In the case of a self-driving car, the data feeds into a computer that directs a fleet of cars to where the traffic is light or fast enough to take over the roads. The cars then drive themselves, using artificial intelligence to follow the routes taken by the computer, avoiding obstacles and the traffic jam. The system works by feeding the data into a hidden layer of the car, which in turn follows the signals sent by the traffic lights. Finally, the system arrives at the destination at the speed limit or, in extreme cases, at full speed in order to hurtle the baddest drunken fools off the road. The end result is that, as the system continually monitors the road and adjusts its route accordingly, traffic flows at a reasonable clip. Going slow or stopping completely is not considered a road, but going at least as fast as humanly possible is.  The car does not drive itself, but rather relies on data from sensors mounted on the roof and sides of the car. These sensors monitor the road and provide information to the software, which in turn analyzes the data and generates appropriate routes. The autonomous car thus constantly monitors the road and the road updates itself. It’s like a GPS system in your car. The autonomous car constantly looks up at the speed limit and, as a consequence, constantly needs fuel. It also continuously monitors the speed limit and adjusts its speed, so that it can travel at a more reasonable clip. This constantly recalculated acceleration and braking needs no fuel, unlike the case of a truck or a rail, which constantly needs fuel and constantly needs data to calculate the route. Nevertheless, the continual need for fuel is a big part of what makes the autonomous car different from other forms of travel.  The autonomous car therefore depends on a large amount of energy. But how much energy is enough to power</text>
    <text>using machine learning techniques] to predict what type of car would be used in a specific location, based on historical data, and historical socio-political variables.[7]  In the case of a self-driving car, the data feeds into a computer that directs a fleet of cars to where the traffic is light or fast enough to take over the roads. The cars then drive themselves, using artificial intelligence to follow the routes taken by the computer, avoiding obstacles and the traffic jam. The system works by feeding the data into a hidden layer of the car, which in turn follows the signals sent by the traffic lights. Finally, the system arrives at the destination at the speed limit or, in extreme cases, at full speed in order to hurtle the baddest drunken fools off the road. The end result is that, as the system continually monitors the road and adjusts its route accordingly, traffic flows at a reasonable clip. Going slow or stopping completely is not considered a road, but going at least as fast as humanly possible is. Rapid changes in road conditions are therefore not only beneficial to the systems involved, but also to the drivers of these cars.  To be clear, this is not theoretical.  The technology is here and cars are driving themselves. In fact, this is the underlying premise of the cars’ self-driving features. These cars literally drive themselves. This is a scenario that has been played out on a globalised and computerised scale. In the era of driverless cars, we can assume that roads will play a greater role in the future of the world.  So, cars that ‘learn’ and that drive themselves could lead to a world in which roads would play a greater role. And, if the aim of this kind of thinking is to be realised, then perhaps it is time for the whole ‘infrastructure’ to be automated. That is, if the aim is to follow the signs and avoid the mistakes that lead to conflict.  In order to</text>
    <text>using machine learning techniques] to predict what type of car would be used in a specific location, based on historical data, and historical socio-political variables.[7]  In the case of a self-driving car, the data feeds into a computer that directs a fleet of cars to where the traffic is light or fast enough to take over the roads. The cars then drive themselves, using artificial intelligence to follow the routes taken by the computer, avoiding obstacles and the traffic jam. The system works by feeding the data into a hidden layer of the car, which in turn follows the signals sent by the traffic lights. Finally, the system arrives at the destination at the speed limit or, in extreme cases, at full speed in order to hurtle the baddest drunken fools off the road. The end result is that, as the system continually monitors the road and adjusts its route accordingly, traffic flows at a reasonable clip. Going slow or stopping completely is not considered a road, but going at least as fast as humanly possible is. This kind of rapid change and extreme speed limits are common in European capitals, and are therefore an obvious adaptation of the technology to the city.   A second adaptation is that of the obsolete, or depopulated, ‘Rust Belt’. These are areas where, due to rampant joblessness and a general feeling of hopelessness, people simply give up trying to add value to the urban fabric. The high-speed internet, car repair shops, restaurants, children's day care centres all populate the landscape with billboards for junk food, clothing, housing and other products of no use to any future inhabitants.  A third and most obvious adaptation is that of the bizarre, or the anthropogenic. The explosion of oil supplies, the industrialised world as a whole, and especially the US as a result of the global automobile industry is anthropogenic in its effects. It is now commonplace for cars to be driven into skyscrapers, or even larger, abandoned sites of industry.</text>
    <text>using machine learning techniques] to predict what type of car would be used in a specific location, based on historical data, and historical socio-political variables.[7]  In the case of a self-driving car, the data feeds into a computer that directs a fleet of cars to where the traffic is light or fast enough to take over the roads. The cars then drive themselves, using artificial intelligence to follow the routes taken by the computer, avoiding obstacles and the traffic jam. The system works by feeding the data into a hidden layer of the car, which in turn follows the signals sent by the traffic lights. Finally, the system arrives at the destination at the speed limit or, in extreme cases, at full speed in order to hurtle the baddest drunken fools off the road. The end result is that, as the system continually monitors the road and adjusts its route accordingly, traffic flows at a reasonable clip. Going slow or stopping completely is not considered a road, but going at least as fast as humanly possible is.  To make things a bit more interesting, the cars also have human drivers who monitor the road and act as a traffic light, as well as sensors that detect when the road is impassable and sends out electric prods to push the capbusters over the top. If the road becomes impassable, or if the road becomes too dangerous to drive, then the sensors automatically steer into autonomous mode, sending out self-driving emergency brakes. The system then races to repair the road as best it can, while simultaneously anticipating the worst and trying to work out how to make the roads safer without creating a new mess of traffic deaths and pollution.  The key to this remarkable new world order of cars, trucks, cars and drones operating in real time is that we don’t have roads. We have cities, but roads are mostly there to connect us to one another and to do mundane things like buy food or use toilets. Roads, then, become platforms on which things can</text>
  </text>
  <text>
    using machine learning techniques on existing data sets, in an attempt to map the kinetic energy of movement across the planet with near real-time. 	It should be noted that the link between autonomous vehicles and climate is still developing, and that autonomous vehicles operate in a geopolitical and economic grey area. For instance, autonomous vehicles operate in international airspace, but also in US airspace, but the US military considers such operations to be US protected zones (e.g. ‘High-Intensity Conflict Areas’). In other words, the militarised application of AI does not map directly onto an all-encompassing semantic shift in national identity. Rather, it depends on particular state-of-the-art research and development in the field, which can be seen as a precondition for an all-encompassing semantic shift. 	The emergence of self-driving vehicles presents its share of new challenges for an already-challenged planet. Solutions will have to be found that address the root causes of
    <text>using machine learning techniques on existing data sets, in an attempt to map the kinetic energy of movement across the planet with near real-time. 	It should be noted that the link between autonomous vehicles and climate is still developing, and that autonomous vehicles operate in a geopolitical and economic grey area. For instance, autonomous vehicles operate in international airspace, but also in US airspace, but the US military considers such operations to be US protected zones (e.g. ‘High-Intensity Conflict Areas’). In other words, the militarised application of AI does not map directly onto an all-encompassing semantic shift in national identity. Rather, it depends on particular state-of-the-art research and development in the field, which can be seen as a precondition for an all-encompassing semantic shift. 	The emergence of self-driving vehicles presents its share of new challenges for an already-challenged planet. Solutions will have to be found that address the root causes of the problem, which are likely to remain unresolved for some time to come. The race to zero is likely to be fierce, and autonomous vehicles will undoubtedly play a role in planetary survival for some time to come. 	The race to zero will likely involve a combination of public agencies, non-profits, start-ups, foundations and other strategic actors. The race to zero will likely involve a large research and development budget, with significant state support. The race to zero will most likely involve partnerships with other industries, including logistics and manufacturing. The race to zero will most likely involve partnerships with non-profit sector and academia. The race to zero will most likely involve partnerships with commercial space agencies, and commercial entities desiring to establish presence in space. The race to zero will most likely involve partnerships with commercial entities wishing to establish presence in low-Earth orbit. The race to zero will most likely involve financial incentives for commercial entities to establish presence in low-Earth orbit. The race to zero will most</text>
    <text>using machine learning techniques on existing data sets, in an attempt to map the kinetic energy of movement across the planet with near real-time. 	It should be noted that the link between autonomous vehicles and climate is still developing, and that autonomous vehicles operate in a geopolitical and economic grey area. For instance, autonomous vehicles operate in international airspace, but also in US airspace, but the US military considers such operations to be US protected zones (e.g. ‘High-Intensity Conflict Areas’). In other words, the militarised application of AI does not map directly onto an all-encompassing semantic shift in national identity. Rather, it depends on particular state-of-the-art research and development in the field, which can be seen as a precondition for an all-encompassing semantic shift. 	The emergence of self-driving vehicles presents its share of new challenges for an already-challenged planet. Solutions will have to be found that address the root causes of the current predicament, and build on the momentum of the movement towards greener, more sustainable modes of urban mobility. ‘High-performance computing’ is a requisite for greener, more sustainable modes of urban mobility, but it is also a necessity for autonomous vehicles to operate optimally. The road to a greener, more sustainable future does not lay in computer programming, but in the ability to drive itself. 	As the saying goes, ‘a car without a driver is like a desert without a road’. If we  	understand the movement of cars, trucks, trains and buses as a kind of operating system, then the question becomes whether it is possible to engineer a vehicle that operates optimally without a driver. In the quest for greener, more sustainable modes of urban mobility, it is imperative that we learn how to drive ourselves, and how to program our vehicles to do so. 	[1] 	https://arxiv.</text>
    <text>using machine learning techniques on existing data sets, in an attempt to map the kinetic energy of movement across the planet with near real-time. 	It should be noted that the link between autonomous vehicles and climate is still developing, and that autonomous vehicles operate in a geopolitical and economic grey area. For instance, autonomous vehicles operate in international airspace, but also in US airspace, but the US military considers such operations to be US protected zones (e.g. ‘High-Intensity Conflict Areas’). In other words, the militarised application of AI does not map directly onto an all-encompassing semantic shift in national identity. Rather, it depends on particular state-of-the-art research and development in the field, which can be seen as a precondition for an all-encompassing semantic shift. 	The emergence of self-driving vehicles presents its share of new challenges for an already-challenged planet. Solutions will have to be found that address the root causes of the problems and emerge as a collective solution. The emergence of self-driving cars presents a different set of dilemmas, because the primary driver of these vehicles is now self-aware: the cars have global apps that continuously monitor their surroundings and detect things such as roads, parking lots and highways, to name only a few. 	If all else fails, the military might initiate a full-scale war between the US and China, but here, too, the odds are against the US. On the other hand, if China pursues an all-out war, the war could quickly become international and devastating. 	Finally, there is the option of joining hands with humanity in a grand cosmic AI project that seeks to address the root causes of the problems, perhaps even creating mutually beneficial autonomous vehicles and infrastructure that can eliminate them altogether. 	Here, the potential benefits and drawbacks of AI are actively managed through a constantly updated science fiction story line. 	Some of the technologies being developed</text>
    <text>using machine learning techniques on existing data sets, in an attempt to map the kinetic energy of movement across the planet with near real-time. 	It should be noted that the link between autonomous vehicles and climate is still developing, and that autonomous vehicles operate in a geopolitical and economic grey area. For instance, autonomous vehicles operate in international airspace, but also in US airspace, but the US military considers such operations to be US protected zones (e.g. ‘High-Intensity Conflict Areas’). In other words, the militarised application of AI does not map directly onto an all-encompassing semantic shift in national identity. Rather, it depends on particular state-of-the-art research and development in the field, which can be seen as a precondition for an all-encompassing semantic shift. 	The emergence of self-driving vehicles presents its share of new challenges for an already-challenged planet. Solutions will have to be found that address the root causes of the planetary crisis, and then integrated into a geo-political and socio-economic framework that puts humanity first. To do so, they will have to be applied across a wider geo-political and socio-economic framework. The ability to map, remotely activate and regulate vehicles will be essential for such a framework to develop, but autonomous vehicles will have to be able to understand and manage the immense amounts of data generated by the drivers of motor vehicles. 	In the coming years, autonomous vehicles may well overtake human-driven vehicles as the primary means of transportation on a massive scale. That is, autonomous vehicles may overtake all forms of human-driven vehicles, including those driven by humans. That is, autonomous vehicles may well overtake completely human-driven vehicles. That is, autonomous vehicles may well overtake entirely natural-infrastructure infrastructure, including roads, railways, water supplies, power grids, telecommunications networks and much more. That is, autonomous vehicles may well overtake completely human-driven infrastructure, including streets,</text>
    <text>using machine learning techniques on existing data sets, in an attempt to map the kinetic energy of movement across the planet with near real-time. 	It should be noted that the link between autonomous vehicles and climate is still developing, and that autonomous vehicles operate in a geopolitical and economic grey area. For instance, autonomous vehicles operate in international airspace, but also in US airspace, but the US military considers such operations to be US protected zones (e.g. ‘High-Intensity Conflict Areas’). In other words, the militarised application of AI does not map directly onto an all-encompassing semantic shift in national identity. Rather, it depends on particular state-of-the-art research and development in the field, which can be seen as a precondition for an all-encompassing semantic shift. 	The emergence of self-driving vehicles presents its share of new challenges for an already-challenged planet. Solutions will have to be found that address the root causes of climate change and ensure sustainable growth for humanity. It is expected that the number of people living on the planet will double in coming decades, and that of the billion-plus people who aspire to one, half will be displaced in the process. Therefore, the world will need to double down on what it calls ‘urgent environmental concerns’ in order to avert catastrophe. 	There are several approaches that could consider how to connect to the present moment, and this is where the language of AI and autonomous vehicles enters into the picture. 	There are several dialects of English: Modern Language Association (ALA) defines the dialects as those with a &gt;50% Native speaker status, which refers to those groups who constitute the largest portion of the world’s speaking population. So, if you’re travelling in a coastal dialect of England, for instance, you’re most likely speaking with someone from the North Country or Scotland. 	There are also several indigenous</text>
    <text>using machine learning techniques on existing data sets, in an attempt to map the kinetic energy of movement across the planet with near real-time. 	It should be noted that the link between autonomous vehicles and climate is still developing, and that autonomous vehicles operate in a geopolitical and economic grey area. For instance, autonomous vehicles operate in international airspace, but also in US airspace, but the US military considers such operations to be US protected zones (e.g. ‘High-Intensity Conflict Areas’). In other words, the militarised application of AI does not map directly onto an all-encompassing semantic shift in national identity. Rather, it depends on particular state-of-the-art research and development in the field, which can be seen as a precondition for an all-encompassing semantic shift. 	The emergence of self-driving vehicles presents its share of new challenges for an already-challenged planet. Solutions will have to be found that address the root causes of the problems rather than resorting to moral hazard or economic whimsy. In order to achieve truly transformative social change, massive amounts of public investment is required, and this can only happen with the participation and involvement of a broad range of stakeholders. It will not happen as a result of corporate greed or some sort of magic economic unicorn, since the causes and effects of the problems can only be altered by massive collaborative multi-cause research projects. Nor will it happen as a result of some sort of geopolitical machination, since the causes and effects of the current problems are largely determined by economic and political factors on the planet at large. 	Over the coming years, we can expect to see increasing numbers of autonomous vehicles being tested on UK roads, and perhaps even on European roads at large. If all goes according to plan, and the technology is proven to be safe and effective, then the disruption caused by a driverless vehicle will be significantly reduced. The question becomes, how will the autonomous-vehicle</text>
    <text>using machine learning techniques on existing data sets, in an attempt to map the kinetic energy of movement across the planet with near real-time. 	It should be noted that the link between autonomous vehicles and climate is still developing, and that autonomous vehicles operate in a geopolitical and economic grey area. For instance, autonomous vehicles operate in international airspace, but also in US airspace, but the US military considers such operations to be US protected zones (e.g. ‘High-Intensity Conflict Areas’). In other words, the militarised application of AI does not map directly onto an all-encompassing semantic shift in national identity. Rather, it depends on particular state-of-the-art research and development in the field, which can be seen as a precondition for an all-encompassing semantic shift. 	The emergence of self-driving vehicles presents its share of new challenges for an already-challenged planet. Solutions will have to be found that address the root causes of the problem, which are likely to remain diffuse, ambiguous and muddled in the face of such vast and rapid changes. In the coming years, we can expect to see autonomous vehicles operating in urban environments, as well as in remote rural locations. In such cases, the need for human intervention will be minimal. 	In such cases, the autonomous vehicle will operate in concert with a human taxi or similar service provider, and will employ advanced mapping and traffic-following technologies to monitor and coordinate with other vehicles and to detect approaching threats. 	Such autonomous-driving services will likely employ advanced safety systems, too. For instance, driverless cars will likely employ radars and other sensor networks to detect and avoid obstacles in their path, and will employ technology to avoid hitting objects in their path. In such cases, the autonomous-driving industry will likely see an increase in vehicle repair and maintenance, as well as in the provision of roadside assistance for those who need it. 	When considering the</text>
    <text>using machine learning techniques on existing data sets, in an attempt to map the kinetic energy of movement across the planet with near real-time. 	It should be noted that the link between autonomous vehicles and climate is still developing, and that autonomous vehicles operate in a geopolitical and economic grey area. For instance, autonomous vehicles operate in international airspace, but also in US airspace, but the US military considers such operations to be US protected zones (e.g. ‘High-Intensity Conflict Areas’). In other words, the militarised application of AI does not map directly onto an all-encompassing semantic shift in national identity. Rather, it depends on particular state-of-the-art research and development in the field, which can be seen as a precondition for an all-encompassing semantic shift. 	The emergence of self-driving vehicles presents its share of new challenges for an already-challenged planet. Solutions will have to be found that address the root causes of the problem (i.e. what is the ecological basis of climate change?), while simultaneously accommodating the new technologies that are part of the new normal. In order to address the root causes of climate change, it is imperative that we learn to live with the consequences, rather than seek to mitigate them. 	The autonomous vehicle presents another set of tensions, this time in relation to the precariousness of the natural resources on which it depends: water, energy, and carbon. At the same time, the emergence of self-driving vehicles presents new opportunities for resource speculation, as well as the exploitation of natural resources at a massive scale. 	As autonomous vehicles become more sophisticated and durable, the carbon footprint associated with their operation becomes increasingly problematic. A new class of vehicles is developed with heated front endologies that actively capture and store hydrocarbons from the atmosphere, while emitting large amounts of CO22 into the atmosphere through their own operations. These vehicles then use these hydrocarbons to make electricity</text>
    <text>using machine learning techniques on existing data sets, in an attempt to map the kinetic energy of movement across the planet with near real-time. 	It should be noted that the link between autonomous vehicles and climate is still developing, and that autonomous vehicles operate in a geopolitical and economic grey area. For instance, autonomous vehicles operate in international airspace, but also in US airspace, but the US military considers such operations to be US protected zones (e.g. ‘High-Intensity Conflict Areas’). In other words, the militarised application of AI does not map directly onto an all-encompassing semantic shift in national identity. Rather, it depends on particular state-of-the-art research and development in the field, which can be seen as a precondition for an all-encompassing semantic shift. 	The emergence of self-driving vehicles presents its share of new challenges for an already-challenged planet. Solutions will have to be found that address the root causes of the problem, which are likely to remain unresolved for some time to come. The emergence of self-driving vehicles presents its share of new challenges for an already-challenged planet. Solutions will have to be found that address the root causes of the problem, which are likely to remain unresolved for some time to come. 1  	Robust Nuyumbaleesi,  	Road Map to Urbanization of South-East Asia and the Pacific, 1750–2000 AD, Department of Geography, Mumbai University, 2000. 2  	Nuyumbaleesi, Road Map to Urbanization of South-East Asia and the Pacific, 1750–2000 AD, p. 33. 3  	Nuyumbaleesi, Road Map to Urbanization of South-East Asia and the Pacific, 1750–2000 AD, p. 4. 4  	Nuyumbaleesi, Road Map to Urbanization of South-East Asia and the Pacific,</text>
    <text>using machine learning techniques on existing data sets, in an attempt to map the kinetic energy of movement across the planet with near real-time. 	It should be noted that the link between autonomous vehicles and climate is still developing, and that autonomous vehicles operate in a geopolitical and economic grey area. For instance, autonomous vehicles operate in international airspace, but also in US airspace, but the US military considers such operations to be US protected zones (e.g. ‘High-Intensity Conflict Areas’). In other words, the militarised application of AI does not map directly onto an all-encompassing semantic shift in national identity. Rather, it depends on particular state-of-the-art research and development in the field, which can be seen as a precondition for an all-encompassing semantic shift. 	The emergence of self-driving vehicles presents its share of new challenges for an already-challenged planet. Solutions will have to be found that address the root causes of the problems, which include but are not limited to: poverty, inequality, pollution, displacement, conflict and crime. Urbanisation and migration will play a critical role in addressing these issues, but urbanisation itself will have to yield other important social benefits. The emergence of self-driving vehicles presents its share of new challenges for an already-challenged planet. Solutions will have to be found that address the root causes of the problems, which include but are not limited to: poverty, inequality, pollution, displacement, conflict and crime. Urbanisation and migration will play a critical role in addressing these issues, but urbanisation itself will have to yield other important social benefits. 	The ability to drive oneself will be essential for the common good, but self-driving vehicles will have to be flexible and sensitive to traffic, road and weather conditions. Traffic jams and inclement weather will be unavoidable, but vehicles will have to learn to adapt to different kinds of traffic and weather conditions, and to the fact that</text>
  </text>
  <text>
    prototype for an intelligent curatorial system that engages the user, not in the mode of information production but of discovery and engagement. Rather than gathering data on the user’s interactive behaviours, or constructing ‘user-friendly’ interfaces based on user studies, the interactive media of today is one in which data is gathered and transmitted – both physically and ‘metainterface’ – the predominant modes of data-gathering in the contemporary world. Such media systems are part of a new ‘big data’ phenomenon that is gathering information about the user and making inferences about the user’s behaviours (behaviour-based modelling or ‘BIM’s’). As a result, the interface is expected to become ever more like the data gathering and interpretation that it enables. It is predicted that every interface will have a user’s manual and ‘interface legend’ will become a trademark.1 In an attempt to provide a sense of the imagination that is not blinded
    <text>prototype for an intelligent curatorial system that engages the user, not in the mode of information production but of discovery and engagement. Rather than gathering data on the user’s interactive behaviours, or constructing ‘user-friendly’ interfaces based on user studies, the interactive media of today is one in which data is gathered and transmitted – both physically and ‘metainterface’ – the predominant modes of data-gathering in the contemporary world. Such media systems are part of a new ‘big data’ phenomenon that is gathering information about the user and making inferences about the user’s behaviours (behaviour-based modelling or ‘BIM’s’). As a result, the interface is expected to become ever more like the data gathering and interpretation that it enables. It is predicted that every interface will have a user’s manual and ‘interface legend’ will become a trademark.1 In an attempt to provide a sense of the imagination that is not blinded by technocratic wizardry, and that seeks to understand the world as interface makers actually understand it, designers in the 1990s had a set of ideas and a toolkit available to them by which they could create ever more sophisticated interfaces – one might even say ‘killer interfaces’ – that would interpret and even interpret badly-behaved behaviours in a more intelligent way. One of the first interfaces to be designed specifically with this in mind was the BASIC programming language that was used in calculators, programmed calculators, and embedded in electronic components everywhere from the early 1980s until at least the mid-1990s.2 It is important to note that this programming language was not limited to devices that ran on calculators or embedded in electronic components. It was also common for embedded systems to have user-friendly programming interfaces similar to BASIC, but unlike other ‘machine languages’, did not provide a dedicated level of abstraction for the user. This last characteristic is what makes</text>
    <text>prototype for an intelligent curatorial system that engages the user, not in the mode of information production but of discovery and engagement. Rather than gathering data on the user’s interactive behaviours, or constructing ‘user-friendly’ interfaces based on user studies, the interactive media of today is one in which data is gathered and transmitted – both physically and ‘metainterface’ – the predominant modes of data-gathering in the contemporary world. Such media systems are part of a new ‘big data’ phenomenon that is gathering information about the user and making inferences about the user’s behaviours (behaviour-based modelling or ‘BIM’s’). As a result, the interface is expected to become ever more like the data gathering and interpretation that it enables. It is predicted that every interface will have a user’s manual and ‘interface legend’ will become a trademark.1 In an attempt to provide a sense of the imagination that is not blinded by technocratic imaginations, this new interface legendry draws on a variety of different data sources, draws conclusions about the user based on what is being collected, and makes inferences about the user’s behaviours (behaviour-based modelling or ‘BIM’s’).2 It is a ‘window into the future’ in that it anticipates the ways in which interfaces will be made ‘in-house’ by data-gathering and calligraphery, while also foreseeing the pitfalls and benefits of such an approach.3 This interface legendry is a new and exotic element to the language of interface design, and its interpretation is becoming increasingly important as interface design practices evolve. The interpretation of interface elements is a visual art rather than a purely textual one, and thus its inclusion in interface design renders textual and non-visual elements (such as icons) visible and manipulateable, while simultaneously concealing and manipulating them. This interpretation</text>
    <text>prototype for an intelligent curatorial system that engages the user, not in the mode of information production but of discovery and engagement. Rather than gathering data on the user’s interactive behaviours, or constructing ‘user-friendly’ interfaces based on user studies, the interactive media of today is one in which data is gathered and transmitted – both physically and ‘metainterface’ – the predominant modes of data-gathering in the contemporary world. Such media systems are part of a new ‘big data’ phenomenon that is gathering information about the user and making inferences about the user’s behaviours (behaviour-based modelling or ‘BIM’s’). As a result, the interface is expected to become ever more like the data gathering and interpretation that it enables. It is predicted that every interface will have a user’s manual and ‘interface legend’ will become a trademark.1 In an attempt to provide a sense of the imagination that is not blinded by technocratic precision, Graef and his collaborators at Berkeley created what they called ‘interface critiquing software’.2 The name is a play on words – an expression of the idea that software should be freely interpreted – and the invective is directed at the technocratic drafting of user-friendly interfaces that do not contain predetermined behaviours. Such behaviour-based interfaces would be ‘libertarian in their design decisions’.3 But unlike other types of libertarianism, this form of libertarianism does not prescribe the creation of user-friendly interfaces but encourages the individual user to create his or her own interface design system – an idea of no imposed conformity to the narratives imposed by the technocracy. As noted by Peter​ Scott in the introduction to his book Systemics, Types and Software,4 the interface is a ‘metainterface’. It is not a graphic surface that presents data in a predetermined way, but rather an interface that is a</text>
    <text>prototype for an intelligent curatorial system that engages the user, not in the mode of information production but of discovery and engagement. Rather than gathering data on the user’s interactive behaviours, or constructing ‘user-friendly’ interfaces based on user studies, the interactive media of today is one in which data is gathered and transmitted – both physically and ‘metainterface’ – the predominant modes of data-gathering in the contemporary world. Such media systems are part of a new ‘big data’ phenomenon that is gathering information about the user and making inferences about the user’s behaviours (behaviour-based modelling or ‘BIM’s’). As a result, the interface is expected to become ever more like the data gathering and interpretation that it enables. It is predicted that every interface will have a user’s manual and ‘interface legend’ will become a trademark.1 In an attempt to provide a sense of the imagination that is not blinded by technologised detail, and that takes into account the ever-growing repertoire of behaviours of the user, there is a need to create interfaces that are not bound by strict convention or traditional technical parameters. There is no single perfect way to do this, and no set of user needs or wants. However, there are many good reasons why user-centred interface design is often desirable, and how users actually use the things we design is a good thing. A good reason is that the design has to be adaptable, and able to accommodate a wide variety of behaviours, not all of which are aligned with the goals of the user. If the goals of the user are not aligned with the ways in which the user actually behave, or does not meet the user’s needs, then the interface design will fail. A good example of this is found in the use of maps and coordinates. While there are many good reasons to use maps and coordinates today, there are also many bad reasons –</text>
    <text>prototype for an intelligent curatorial system that engages the user, not in the mode of information production but of discovery and engagement. Rather than gathering data on the user’s interactive behaviours, or constructing ‘user-friendly’ interfaces based on user studies, the interactive media of today is one in which data is gathered and transmitted – both physically and ‘metainterface’ – the predominant modes of data-gathering in the contemporary world. Such media systems are part of a new ‘big data’ phenomenon that is gathering information about the user and making inferences about the user’s behaviours (behaviour-based modelling or ‘BIM’s’). As a result, the interface is expected to become ever more like the data gathering and interpretation that it enables. It is predicted that every interface will have a user’s manual and ‘interface legend’ will become a trademark.1 In an attempt to provide a sense of the imagination that is not blinded by technocratic fixes, this interface legend must include a critical ‘note’ that represents the place of the user’s thoughts and feelings on the interface – a place that is as ambiguous as the interface itself. The note must be legible, yet also ‘ambiguous’ in order to inform the user of its intentions. A note that is ambiguous in intent but ends up being useful in its implementation. If implemented flawlessly, a ‘note-like’ interface will become part of the user’s daily vocabulary. But if implemented poorly, the interface legend will become part of the user’s daily dictionary. In attempting to implement a ‘note-like’ interface, the team behind Apple’s Apple IIc – which came standard with a black and white color screen and allowed the user to select between a number of programs, as well as writing and choosing between a number of fonts – gave the impression that the user was not merely</text>
    <text>prototype for an intelligent curatorial system that engages the user, not in the mode of information production but of discovery and engagement. Rather than gathering data on the user’s interactive behaviours, or constructing ‘user-friendly’ interfaces based on user studies, the interactive media of today is one in which data is gathered and transmitted – both physically and ‘metainterface’ – the predominant modes of data-gathering in the contemporary world. Such media systems are part of a new ‘big data’ phenomenon that is gathering information about the user and making inferences about the user’s behaviours (behaviour-based modelling or ‘BIM’s’). As a result, the interface is expected to become ever more like the data gathering and interpretation that it enables. It is predicted that every interface will have a user’s manual and ‘interface legend’ will become a trademark.1 In an attempt to provide a sense of the imagination that is not blinded by technocratic abstraction, Richard Florida proposes a radical reinterpretation of the term interface              as an object of inquiry. His proposal is that the interface is a living organism that collects and distributes information across a user’s environment, and that this information enables the implementation of ‘new ways of interacting’ through ‘new media’.2  The idea that the interface is a living creature that naturally collects and distributes information across a user’s environment seems antithetical to the monolithic representations of the computer that come pre-conceived in most user studies curricula. As Florida puts it, the ‘real question is what can the interface do?’– enabling the user to accomplish a multitude of new things by way not only of what he or she knows, but of what he or she doesn’t know.3  How can the user be empowered in these new ways? To answer this question, Floridi proposes a</text>
    <text>prototype for an intelligent curatorial system that engages the user, not in the mode of information production but of discovery and engagement. Rather than gathering data on the user’s interactive behaviours, or constructing ‘user-friendly’ interfaces based on user studies, the interactive media of today is one in which data is gathered and transmitted – both physically and ‘metainterface’ – the predominant modes of data-gathering in the contemporary world. Such media systems are part of a new ‘big data’ phenomenon that is gathering information about the user and making inferences about the user’s behaviours (behaviour-based modelling or ‘BIM’s’). As a result, the interface is expected to become ever more like the data gathering and interpretation that it enables. It is predicted that every interface will have a user’s manual and ‘interface legend’ will become a trademark.1 In an attempt to provide a sense of the imagination that is not blinded by technologised details and metrics, but listens to what the user says, but also to the nuanced ways in which the user speaks and chooses to act, an alternative conception of the interface is being proposed. This alternative interface is being proposed by a small community of designers, activists and artists, and it is being spearheaded by the Turner Prize awardee Nadine Attewell.2 This is the alternative interface: data driven, direct and transparent. It is being proposed as a solution to a broken interface myth, and is being done at an organisation that is itself a myth – specifically, the interface industry. The Turner Prize is funding the development of an interface that is both intuitive and malleable, that can be rewritten at will and controlled via software. As such, the interface is a production for change, and the user is a collaborator in that interface making process. The Turner Prize is putting their money where their mouth is and asking what the heck is happening there</text>
    <text>prototype for an intelligent curatorial system that engages the user, not in the mode of information production but of discovery and engagement. Rather than gathering data on the user’s interactive behaviours, or constructing ‘user-friendly’ interfaces based on user studies, the interactive media of today is one in which data is gathered and transmitted – both physically and ‘metainterface’ – the predominant modes of data-gathering in the contemporary world. Such media systems are part of a new ‘big data’ phenomenon that is gathering information about the user and making inferences about the user’s behaviours (behaviour-based modelling or ‘BIM’s’). As a result, the interface is expected to become ever more like the data gathering and interpretation that it enables. It is predicted that every interface will have a user’s manual and ‘interface legend’ will become a trademark.1 In an attempt to provide a sense of the imagination that is not blinded by technocratic abstraction, Horvath presents a ‘ficto-mathematical outline’ of the evolving interface industry as a whole, characterised by multiple forms of small and large organisations competing for market share and innovative proposals with a clear preference for the over-arching user interface. The interface is a ‘metainterface’ that is constantly evolving, and thus constantly plays second fiddle to the contending user interface vendors and data mining operations. The interface is a ‘transitive metainterface’ that constantly needs new technology to be invented in order to be applied to the evolving interface industry.  	Horvath, however, is easily distracted by the more pressing matters at hand. With the advent of every new smartphone and tablet, the need for ever larger screens and ever more powerful computing resources became ever more apparent. The temptation was obvious: install ever more software to fill the screen with pixels and install ever more sophisticated analytics to identify and</text>
    <text>prototype for an intelligent curatorial system that engages the user, not in the mode of information production but of discovery and engagement. Rather than gathering data on the user’s interactive behaviours, or constructing ‘user-friendly’ interfaces based on user studies, the interactive media of today is one in which data is gathered and transmitted – both physically and ‘metainterface’ – the predominant modes of data-gathering in the contemporary world. Such media systems are part of a new ‘big data’ phenomenon that is gathering information about the user and making inferences about the user’s behaviours (behaviour-based modelling or ‘BIM’s’). As a result, the interface is expected to become ever more like the data gathering and interpretation that it enables. It is predicted that every interface will have a user’s manual and ‘interface legend’ will become a trademark.1 In an attempt to provide a sense of the imagination that is not blinded by technocratic fantasy, I will attempt to introduce an imaginary framework around what an interface is in order to let the designer(s) guide the user’s journey. Moving from an interface that is derived from typefaces and text to one that is based on virtual objects and data, the interface is seen not only as a point of departure but also of destination – a point of no return. At the same time, the interface is a point of entry into a specific world of interaction. At its most basic level, the interface is a media of media: a way of conveying information about the world through means of media. As such, it is able to transport the user across vast spaces and time, gathering information about the user’s activities and habits in order to offer various levels of interaction. This type of interface is fundamentally about the capacities of the user and its capabilities, and thus, in all its many forms, the world of interfaces is seen as fundamentally different from that of</text>
    <text>prototype for an intelligent curatorial system that engages the user, not in the mode of information production but of discovery and engagement. Rather than gathering data on the user’s interactive behaviours, or constructing ‘user-friendly’ interfaces based on user studies, the interactive media of today is one in which data is gathered and transmitted – both physically and ‘metainterface’ – the predominant modes of data-gathering in the contemporary world. Such media systems are part of a new ‘big data’ phenomenon that is gathering information about the user and making inferences about the user’s behaviours (behaviour-based modelling or ‘BIM’s’). As a result, the interface is expected to become ever more like the data gathering and interpretation that it enables. It is predicted that every interface will have a user’s manual and ‘interface legend’ will become a trademark.1 In an attempt to provide a sense of the imagination that is not blinded by technocratic wizardry, in 2001, US company UNIVAC created what could be described as an ‘metainterface’. An interface is a ‘transitive term’ that describes tangible things that are able to communicate with one another. In this sense, the interface is able to communicate with the physical world. And as a synonym, the term interface has come to refer to a particular kind of tangible medium (usually computer interface). Like the illustration in the introduction, this is a photo essay that takes the historical relationship between the interface and the computer seriously. Unlike the illustration, however, the story unfolding on the interface is not one of triumphalism or triumphant transfiguration, but of a very particular kind of metainterface: the kind that UNIVAC developed and perfected. Interface legend and metainterface: two ways of looking at the same thing 	As an interface myth, however, the story of the Interface and the User begins</text>
  </text>
  <text>
    machines were asked to take over the curatorial role, allowing the collective imagination to take over the process. 	What began as a way of connecting with the city and its people through a collaborative project between the museum and local community organisations soon developed into a kind of ‘socio-political entity’. The Museum of Tijuana Art Links (2008-9), the Universidad de California Urbana-Chica Art and Culture, was born. These early art links appeared to be genuine, since the early 20th century was marked by the proliferation of self-proclaimed cultural centers in the Central Valley. The UC-Davis Art and Humanities Building, established in 1984, was followed two years later by the UC-San Diego Art Center (1986). The UC-San Diego Art Center, which is privately held, has operated as a non-profit art gallery and bilingual art museum since 1983. Its mission statement informs our own, though its funding comes from the city and the county, with little state support
    <text>machines were asked to take over the curatorial role, allowing the collective imagination to take over the process. 	What began as a way of connecting with the city and its people through a collaborative project between the museum and local community organisations soon developed into a kind of ‘socio-political entity’. The Museum of Tijuana Art Links (2008-9), the Universidad de California Urbana-Chica Art and Culture, was born. These early art links appeared to be genuine, since the early 20th century was marked by the proliferation of self-proclaimed cultural centers in the Central Valley. The UC-Davis Art and Humanities Building, established in 1984, was followed two years later by the UC-San Diego Art Center (1986). The UC-San Diego Art Center, which is privately held, has operated as a non-profit art gallery and bilingual art museum since 1983. Its mission statement informs our own, though its funding comes from the city and the county, with little state support. The Art Links programme brings arts professionals from the region to San Diego to work with local youth to create public spaces and collaborate with the city to create ‘transformative action plans’ for their neighborhoods. In 1984, the Art Links opened with a three-week ‘cultural action planning’ project led by artist Horacio Quinquela-Ferreira, who is now curator of the Viva Art Gallery, Buenos Aires. The Art Links also established a partnership with the then Cultural Council of New York, which supported the activities of the council’s Human Rights Campaign in supporting the 1984 anti-Castro anti-imperialist movement in Latin America. The relationship between the city and the Chilean military junta that occupied and ruled Chile from 1973 to 1975 is a familiar story. The city also provided security for the chilean military junta during this time, as a former airbase for the RAF, and provided important logistical support for</text>
    <text>machines were asked to take over the curatorial role, allowing the collective imagination to take over the process. 	What began as a way of connecting with the city and its people through a collaborative project between the museum and local community organisations soon developed into a kind of ‘socio-political entity’. The Museum of Tijuana Art Links (2008-9), the Universidad de California Urbana-Chica Art and Culture, was born. These early art links appeared to be genuine, since the early 20th century was marked by the proliferation of self-proclaimed cultural centers in the Central Valley. The UC-Davis Art and Humanities Building, established in 1984, was followed two years later by the UC-San Diego Art Center (1986). The UC-San Diego Art Center, which is privately held, has operated as a non-profit art gallery and bilingual art museum since 1983. Its mission statement informs our own, though its funding comes from the city and the county, with little state support. 	The Museum of Tijuana Art Links opened its doors to the community in early 1989. Over the next decade, they saw many wonderful arts and crafts, as well as performing arts, games and cultural activities. The Chronicle of Tijuana Art (1992-1999) was the first publication to explore the history of Tijuana art, and the first to feature an ethnographic section. The Millionaire Next Door (1999), a novella in the now Chronicle of Tijuana, is the only other book on the book, published between 2001 and 2007. Both books take the reader on a tour of Tijuana, its history, present and future. The first in the Millionaire Next Door series, introduces the main characters and places the city in the film. The second book in the series, The Billionaire Next Door, takes the reader inside Tijuana and into the closed world of the art gallery. Executive summary The</text>
    <text>machines were asked to take over the curatorial role, allowing the collective imagination to take over the process. 	What began as a way of connecting with the city and its people through a collaborative project between the museum and local community organisations soon developed into a kind of ‘socio-political entity’. The Museum of Tijuana Art Links (2008-9), the Universidad de California Urbana-Chica Art and Culture, was born. These early art links appeared to be genuine, since the early 20th century was marked by the proliferation of self-proclaimed cultural centers in the Central Valley. The UC-Davis Art and Humanities Building, established in 1984, was followed two years later by the UC-San Diego Art Center (1986). The UC-San Diego Art Center, which is privately held, has operated as a non-profit art gallery and bilingual art museum since 1983. Its mission statement informs our own, though its funding comes from the city and the county, with little state support. 	The Chinatown Art and Cultural Center (1984-88) was a leader in the revitalization of historically disenfranchised and historically underserved areas of the Central Valley, particularly along the California Coastal Highway. It provided a space for socially engaged cultural practice and dialogue, as well as a community-based arts and culture program that challenged local audiences to become involved as artists and curators. The center hosted numerous other culturally diverse arts organizations, including those engaged in socially engaged practices, such as Newart (1986), the New Ageing Foundation (1987), the Nancy and Roy Wilshire Arts Center (1988), the Bakers Union Arts Center (1989), Chinatown Arts Center (1990), Brooklyn Arts Center (1991), Community Arts International (1993), Bakers Union Local 8 (1994) and the All-California Organisation (1995). Its 1996 Asian Arts and Art Fair was the first step in a</text>
    <text>machines were asked to take over the curatorial role, allowing the collective imagination to take over the process. 	What began as a way of connecting with the city and its people through a collaborative project between the museum and local community organisations soon developed into a kind of ‘socio-political entity’. The Museum of Tijuana Art Links (2008-9), the Universidad de California Urbana-Chica Art and Culture, was born. These early art links appeared to be genuine, since the early 20th century was marked by the proliferation of self-proclaimed cultural centers in the Central Valley. The UC-Davis Art and Humanities Building, established in 1984, was followed two years later by the UC-San Diego Art Center (1986). The UC-San Diego Art Center, which is privately held, has operated as a non-profit art gallery and bilingual art museum since 1983. Its mission statement informs our own, though its funding comes from the city and the county, with little state support. 	The Art Links website describes them as follows: These associations … became an important part of our efforts to envision a future for the Tijuana border area. … facilitated the exchange of ideas, images and experiences between the art institutions of the Central Valley and the artists, curators and artisans who worked with them. They also provided the conceptual framework for the establishment of a biennial that would exhibit internationally acclaimed works from the Central Valley, and would serve as a regional counterweight to the influence of the San Diego–based Biennale Commission.1  The Art Links website features a register of curators and artists, as well as profiles of some who have worked with the city’s art ecosystem. The profiles include Sara De Bondt, Curator for Environments and Curatorial Harmony at the San Diego Museum of Art, who describes herself as a curator &quot;in my own right&quot; who developed the Biennale’s local context and invited artists to explore the</text>
    <text>machines were asked to take over the curatorial role, allowing the collective imagination to take over the process. 	What began as a way of connecting with the city and its people through a collaborative project between the museum and local community organisations soon developed into a kind of ‘socio-political entity’. The Museum of Tijuana Art Links (2008-9), the Universidad de California Urbana-Chica Art and Culture, was born. These early art links appeared to be genuine, since the early 20th century was marked by the proliferation of self-proclaimed cultural centers in the Central Valley. The UC-Davis Art and Humanities Building, established in 1984, was followed two years later by the UC-San Diego Art Center (1986). The UC-San Diego Art Center, which is privately held, has operated as a non-profit art gallery and bilingual art museum since 1983. Its mission statement informs our own, though its funding comes from the city and the county, with little state support. 	In 1993, the Art Link Initiative was created, an arm of the then-new Federal government, funded in part with a ‘road map to urban growth through institutional reforms’.12 This road map focused on the need to create a regional hub for art and culture development in the region, with a particular emphasis on the arts and culture education of the Northern California Public Schools. In 1994, the Art Link School was created, and in 1997, the Art Link University (home to the present Art Link School of Design). In 2000, the Art Link Art Museum was founded in Inglewood, and in 2007, the Art Link Art Biennial was founded in Inglewood with Karen Lund and John Akomfrah or ‘John Akomfrah’ as the curator. A postgraduate certificate in Cultural Studies was also offered the first time, but the chances of this ever becoming a reality are remote. With the exception of a brief engagement with the Man made</text>
    <text>machines were asked to take over the curatorial role, allowing the collective imagination to take over the process. 	What began as a way of connecting with the city and its people through a collaborative project between the museum and local community organisations soon developed into a kind of ‘socio-political entity’. The Museum of Tijuana Art Links (2008-9), the Universidad de California Urbana-Chica Art and Culture, was born. These early art links appeared to be genuine, since the early 20th century was marked by the proliferation of self-proclaimed cultural centers in the Central Valley. The UC-Davis Art and Humanities Building, established in 1984, was followed two years later by the UC-San Diego Art Center (1986). The UC-San Diego Art Center, which is privately held, has operated as a non-profit art gallery and bilingual art museum since 1983. Its mission statement informs our own, though its funding comes from the city and the county, with little state support. So it was that, in the 1990s, Art Link, San Diego’s first independent art museum, opened in the historic Charles E. Smith Gallery, located in the heart of the city. 	The Art Link program was a rare blend of academic, curatorial and educational elements. The curatorial aspects were intended to educate the public on the local arts scene, while the educational elements, including works exhibited in sites around the galleries, were designed to engage art students and the general public. The main focus of the curatorial leg of the relationship was, however, on the work of the artists and their projects. The educational mission was enhanced by the curatorial’s apparent inability or unwillingness to engage with the city. While the museum’s main collection of works on a given subject was read and annotated monthly, the curatorial leg of the relationship was short lived. The curatorial change in this relationship was made in 1993, ostensibly to address a lack of interest</text>
    <text>machines were asked to take over the curatorial role, allowing the collective imagination to take over the process. 	What began as a way of connecting with the city and its people through a collaborative project between the museum and local community organisations soon developed into a kind of ‘socio-political entity’. The Museum of Tijuana Art Links (2008-9), the Universidad de California Urbana-Chica Art and Culture, was born. These early art links appeared to be genuine, since the early 20th century was marked by the proliferation of self-proclaimed cultural centers in the Central Valley. The UC-Davis Art and Humanities Building, established in 1984, was followed two years later by the UC-San Diego Art Center (1986). The UC-San Diego Art Center, which is privately held, has operated as a non-profit art gallery and bilingual art museum since 1983. Its mission statement informs our own, though its funding comes from the city and the county, with little state support. Local artists have worked with the Art Center on projects ranging from graphic ephemera to printmaking to photography, while the Art Center has presented numerous exhibitions including, most recently, the 2011 Global Village exhibition. 	The Art Center’s first director, Victor Manuel López de la Torre, was a Mexican artist and writer. His work is marked by a love of language and a distancing of emotion. In the 1980s and 1990s he collaborated with the British Museum and the Smithsonian Institution, as well as with artists including Fernando Alfonso Lula da Silva, Alexandre González Purcell, Renate Lorenz and Benito Quinquela. His most recent work, published in 2016, is a two-part series of works based on his life as a journalist on the state of Mexico City. In Historic Tijuana, he evokes the city’s colonial past while its present is transformed through the power of rhetoric and visual</text>
    <text>machines were asked to take over the curatorial role, allowing the collective imagination to take over the process. 	What began as a way of connecting with the city and its people through a collaborative project between the museum and local community organisations soon developed into a kind of ‘socio-political entity’. The Museum of Tijuana Art Links (2008-9), the Universidad de California Urbana-Chica Art and Culture, was born. These early art links appeared to be genuine, since the early 20th century was marked by the proliferation of self-proclaimed cultural centers in the Central Valley. The UC-Davis Art and Humanities Building, established in 1984, was followed two years later by the UC-San Diego Art Center (1986). The UC-San Diego Art Center, which is privately held, has operated as a non-profit art gallery and bilingual art museum since 1983. Its mission statement informs our own, though its funding comes from the city and the county, with little state support. 	The Museum of Tijuana Art Links began hosting regular programming of Self-Generation, a self-guided programme for artists and the community, in 1986. The programme has grown to include regular programming of the Art Link TV show, as well as arts education for schools and youth groups. The Art Link TV show was an extension of the Self-Generation programme and addressed issues of concern to the local Tijuana community. The themes of the show included migration, identity and belonging, community production, self-sufficiency and self-organisation. In the same vein, the Art Link TV programme addressed issues of identity, migration and identity, and community issues of importance to the community. As well, it frequently featured Tijuana-specific knowledge from artists and community organisers. Included in the programmes repertoire was music composed specifically for the channel. 	When the Chilean cultural heritage was fragmented in the 1990s and 2000,</text>
    <text>machines were asked to take over the curatorial role, allowing the collective imagination to take over the process. 	What began as a way of connecting with the city and its people through a collaborative project between the museum and local community organisations soon developed into a kind of ‘socio-political entity’. The Museum of Tijuana Art Links (2008-9), the Universidad de California Urbana-Chica Art and Culture, was born. These early art links appeared to be genuine, since the early 20th century was marked by the proliferation of self-proclaimed cultural centers in the Central Valley. The UC-Davis Art and Humanities Building, established in 1984, was followed two years later by the UC-San Diego Art Center (1986). The UC-San Diego Art Center, which is privately held, has operated as a non-profit art gallery and bilingual art museum since 1983. Its mission statement informs our own, though its funding comes from the city and the county, with little state support. Its current position is to house the city’s homeless and other property held by the city. The UC-San Diego Art Center has worked with the city to acquire land and make affordable housing a reality for low-income communities, while the UC-Santa Barbara Arts and Humanities Building has worked with the city to develop a bilingual arts and humanities curriculum for the city’s students. The UC-San Diego Art Center is the dominant artistic force in the Central Valley, exerting a kind of mayoral veto over the city’s arts policies. The UC-San Diego Art Center’s work is generally considered to be progressive, though its practice tends to concentrate on the arts’ needs as opposed to those of the city. It is also the only arts center in the county to have an annual budget, and the only one in the state of California to offer free or reduced-price tickets to its exhibitions. The Art Link program was initially developed as an annual gift</text>
    <text>machines were asked to take over the curatorial role, allowing the collective imagination to take over the process. 	What began as a way of connecting with the city and its people through a collaborative project between the museum and local community organisations soon developed into a kind of ‘socio-political entity’. The Museum of Tijuana Art Links (2008-9), the Universidad de California Urbana-Chica Art and Culture, was born. These early art links appeared to be genuine, since the early 20th century was marked by the proliferation of self-proclaimed cultural centers in the Central Valley. The UC-Davis Art and Humanities Building, established in 1984, was followed two years later by the UC-San Diego Art Center (1986). The UC-San Diego Art Center, which is privately held, has operated as a non-profit art gallery and bilingual art museum since 1983. Its mission statement informs our own, though its funding comes from the city and the county, with little state support. 	The second figure shows a tour of the museum’s collection, depicting the important collections of the museum, with descriptions of each one. 	The third figure, a close-up of a portrait, is a scene in a no less important collection - the Bonifacio N. Di Stefano Life and Landscapes of Tijuana (1879-1976), the author of Lava Ecliptic, Light from the Two Cities and many other books of natural and urban history. The figure in the fourth figure is a visitor taken on a guided tour. 	The fifth figure, a figure in her element, is a figure in her collection - a figure in her museum - a visitor to her museum. 	The figure in the sixth figure is a visitor to a museum - a figure in her collection - taken on a guided tour. 	[1] From the collection of the Metropolitan Museum of Art, New York, M.I</text>
  </text>
</AIcurator>
