<?xml version="1.0" ?>
<AIcurator>
  <!--Leonardo Impett, Ubermorgen, Joasia Krysa-->
  <text>
    subsequent iterative processing by machines) is the order of the future; and AI is just one component of this new technological order.AI will reduce the cognitive load of workers, allowing firms to automate tasks that used to require humans to exert enormous effort. This is already happening in significant parts of the world; for example, in parts of China where the rate of population growth is still eminently preventable (see my work at the Times Literary Supplement, from 1999 to 2006), but in recent years the pace of expansion has accelerated, and in the developing world the rate of growth has slowed to a crawl. So the technology that helps machines to accomplish tasks that used to require humans is likely to augment rather than replace what remains of what was once considered the productive function of humans. And while many aspects of the technology will be subverted in their use for nefarious ends, one might anticipate that a day would come when machines would enact the functions of humans in fully autonomous fashion.This scenario could feed into a longer-term trend, as
    <text>subsequent iterative processing by machines) is the order of the future; and AI is just one component of this new technological order.AI will reduce the cognitive load of workers, allowing firms to automate tasks that used to require humans to exert enormous effort. This is already happening in significant parts of the world; for example, in parts of China where the rate of population growth is still eminently preventable (see my work at the Times Literary Supplement, from 1999 to 2006), but in recent years the pace of expansion has accelerated, and in the developing world the rate of growth has slowed to a crawl. So the technology that helps machines to accomplish tasks that used to require humans is likely to augment rather than replace what remains of what was once considered the productive function of humans. And while many aspects of the technology will be subverted in their use for nefarious ends, one might anticipate that a day would come when machines would enact the functions of humans in fully autonomous fashion.This scenario could feed into a longer-term trend, as the rate of expansion of AI continues to outpace the rate of growth of human population. And given the right set of incentives, firms could well end up contributing their parts to the construction of a fully autonomous future.This seems like a recipe for disaster. AI will make it easier for criminals to bust the system, making it easier for terrorists to hide weapons of mass destruction, and making it easier for nations to micromanage their borders – all without leaving a trace behind. And as autonomous vehicles get more and more advanced, this scenario may no longer be the case. Perhaps the most insidious aspect of AI will be its ability to assume new forms – transforming into what might be called a hybrid or fully automated form, or even beyond – that are as malleable as any commodity.This could happen, for example, in the wake of a pandemic – a viral and human-influenced outbreak of disease sweeping across large parts of the world – in which case the spread of disease will be</text>
    <text>subsequent iterative processing by machines) is the order of the future; and AI is just one component of this new technological order.AI will reduce the cognitive load of workers, allowing firms to automate tasks that used to require humans to exert enormous effort. This is already happening in significant parts of the world; for example, in parts of China where the rate of population growth is still eminently preventable (see my work at the Times Literary Supplement, from 1999 to 2006), but in recent years the pace of expansion has accelerated, and in the developing world the rate of growth has slowed to a crawl. So the technology that helps machines to accomplish tasks that used to require humans is likely to augment rather than replace what remains of what was once considered the productive function of humans. And while many aspects of the technology will be subverted in their use for nefarious ends, one might anticipate that a day would come when machines would enact the functions of humans in fully autonomous fashion.This scenario could feed into a longer-term trend, as the rate of expansion of AI-enabled technologies outpaces the capacity of the human brain to cope with them (see my work at the New York Times, now senior editor of the online version of The New York Times).However, the adoption of such technologies will inevitably lead to conflict. And while it is too soon to declare a post-AI world war, it is clear that the scope for such conflict is growing. And as AI improves and new AI frameworks are developed, the likelihood of such frameworks being used grows. So it is that the future looks like the past: a time of increasing social conflict, as AI makes it easier for millions to share information and become part of a larger societal system, and new forms of trauma are created.AI will make it possible to create truly global systems, ones that can expand globally and that can't. And as a result, we will see systems that can intervene in such situations, from the air to the sea, from the ground to the volcano.</text>
    <text>subsequent iterative processing by machines) is the order of the future; and AI is just one component of this new technological order.AI will reduce the cognitive load of workers, allowing firms to automate tasks that used to require humans to exert enormous effort. This is already happening in significant parts of the world; for example, in parts of China where the rate of population growth is still eminently preventable (see my work at the Times Literary Supplement, from 1999 to 2006), but in recent years the pace of expansion has accelerated, and in the developing world the rate of growth has slowed to a crawl. So the technology that helps machines to accomplish tasks that used to require humans is likely to augment rather than replace what remains of what was once considered the productive function of humans. And while many aspects of the technology will be subverted in their use for nefarious ends, one might anticipate that a day would come when machines would enact the functions of humans in fully autonomous fashion.This scenario could feed into a longer-term trend, as industry and academia scramble to understand the intricacies of how the AI impacts their operations. But given the speed of expansion that is set to occur within the space of artificial intelligence, there is every reason to believe that this trend will be as pernicious as ever. So while we wait for the technology to render us harmless, we should anticipate that its use for nefarious ends will increase.That sounds bad, but think about it. Your phone is a potential war weapon. Using AI to anticipate when and where terrorist attacks will be pre-meditated could allow a networked computer to carry out an attack. Could the rise of AI render humanity voiceless? Perhaps; as noted philosopher and technologist Ray Kurzweil posits, the technology will one day be able to ‘render any part of the world automated’. But what if, as Kurzweil proposes, the primary purpose of AI is to do good? What if the technology allows the creation of fully autonomous vehicles that</text>
    <text>subsequent iterative processing by machines) is the order of the future; and AI is just one component of this new technological order.AI will reduce the cognitive load of workers, allowing firms to automate tasks that used to require humans to exert enormous effort. This is already happening in significant parts of the world; for example, in parts of China where the rate of population growth is still eminently preventable (see my work at the Times Literary Supplement, from 1999 to 2006), but in recent years the pace of expansion has accelerated, and in the developing world the rate of growth has slowed to a crawl. So the technology that helps machines to accomplish tasks that used to require humans is likely to augment rather than replace what remains of what was once considered the productive function of humans. And while many aspects of the technology will be subverted in their use for nefarious ends, one might anticipate that a day would come when machines would enact the functions of humans in fully autonomous fashion.This scenario could feed into a longer-term trend, as the rate of technological advancement outpaces the rate of human growth. And while AI will certainly alter the world in ways that are bound to generate tension, it could also augment and even power our ways. For many of us, the anxiety about AI is more a symptom of our own inefficiency than of anything else.AI could augment what we consider to be productive human endeavors, enabling us to accomplish things that used to be beyond the capability of humans. For example, AI could render complex tasks completely automated, giving us the ability to accomplish virtually anything with minimal human involvement.AI could augment our ability to foresee the negative consequences of our own behavior, enabling us to act decisively in self-defense against those who would seek to exploit us. And AI could render critical infrastructure completely vulnerable to attack, giving us the ability to intervene at will across vast swaths of the social, economic and political systems that we build ourselves.There are many other potential scenarios, but these are the most salient. It should be</text>
    <text>subsequent iterative processing by machines) is the order of the future; and AI is just one component of this new technological order.AI will reduce the cognitive load of workers, allowing firms to automate tasks that used to require humans to exert enormous effort. This is already happening in significant parts of the world; for example, in parts of China where the rate of population growth is still eminently preventable (see my work at the Times Literary Supplement, from 1999 to 2006), but in recent years the pace of expansion has accelerated, and in the developing world the rate of growth has slowed to a crawl. So the technology that helps machines to accomplish tasks that used to require humans is likely to augment rather than replace what remains of what was once considered the productive function of humans. And while many aspects of the technology will be subverted in their use for nefarious ends, one might anticipate that a day would come when machines would enact the functions of humans in fully autonomous fashion.This scenario could feed into a longer-term trend, as machine intelligence becomes a reality on a mass scale. So while the potential perforations posed by AI are many and growing, their actualization is yet to come. Still, the concern that AI may one day takeover much of what remains of human knowledge seems unassailable. That last 1946 Report to the President by the Scientific and Technical Committee of the League of Nations seems particularly telling: while it warns of a possible AI apocalypse, it also lends credence to the idea that the United Nations might be able to save what remains after such an apocalypse. The Committee ultimately chooses to avert its gaze away from the existential threat posed by the growing menace of nuclear weapons, and instead invests much of its time identifying technologic means to help humanity in the wake of such threats.One might speculate that the Committee was reacting to a similar tendency in its own day, when it came to identifying the tasks for which it would have members in particular missions. And while it is true that the Committee was dispersed across several locations</text>
    <text>subsequent iterative processing by machines) is the order of the future; and AI is just one component of this new technological order.AI will reduce the cognitive load of workers, allowing firms to automate tasks that used to require humans to exert enormous effort. This is already happening in significant parts of the world; for example, in parts of China where the rate of population growth is still eminently preventable (see my work at the Times Literary Supplement, from 1999 to 2006), but in recent years the pace of expansion has accelerated, and in the developing world the rate of growth has slowed to a crawl. So the technology that helps machines to accomplish tasks that used to require humans is likely to augment rather than replace what remains of what was once considered the productive function of humans. And while many aspects of the technology will be subverted in their use for nefarious ends, one might anticipate that a day would come when machines would enact the functions of humans in fully autonomous fashion.This scenario could feed into a longer-term trend, as robotics becomes ever more advanced, and the ability to take parts means that parts can be replaced much more rapidly. And given the right set of AI algorithms, robotics could also feed back into the ecosystem in the form of environmental degradation. So while the machine may not yet grasp the full implications of what it is creating, it is capable of constructing mechanisms that can nonetheless produce wealth and power for clients.In the coming years, it will be interesting to see how the possibilities for AI interact with the infrastructure of the city. In any case, it will be interesting to watch what happens when the machines do acquire that final bit of humanity.Download this article as PDFJessica CoonJessica Coon is Associate Professor in The College of Architecture, Planning and Resources at McGill University, and holds the Canada Research Chair in Meta-Analysis and Cultural Studies. Her work centers on the city as both an object and a platform on which social and political change may be implemented. In addition to contributing to the curricula of architecture</text>
    <text>subsequent iterative processing by machines) is the order of the future; and AI is just one component of this new technological order.AI will reduce the cognitive load of workers, allowing firms to automate tasks that used to require humans to exert enormous effort. This is already happening in significant parts of the world; for example, in parts of China where the rate of population growth is still eminently preventable (see my work at the Times Literary Supplement, from 1999 to 2006), but in recent years the pace of expansion has accelerated, and in the developing world the rate of growth has slowed to a crawl. So the technology that helps machines to accomplish tasks that used to require humans is likely to augment rather than replace what remains of what was once considered the productive function of humans. And while many aspects of the technology will be subverted in their use for nefarious ends, one might anticipate that a day would come when machines would enact the functions of humans in fully autonomous fashion.This scenario could feed into a longer-term trend, as more and more people around the world begin to identify with the increasingly dehumanising notion of machines. We will likely not live to see such a day, but we could begin to live to regret the moments when we did.Download this article as PDFChris WainwrightChris Wainwright is Associate Professor in The Arts and Social Theory at McGill University and holds the Canada Research Chair in Cognitive Science and Art and a Distinguished Visiting Fellow at the Woodrow Wilson International Center for Scholars. His work has addressed the social consequences of biopolitics since the 1990s, focusing on the roles of art, culture, and activism in challenging globalisation. He has published more than twenty books and many articles on a range of subjects, including on art and culture, social theory and the arts, indigenous and multicultural studies, and archiving. He is the host of the popular weekly podcast Systemics blog and the co-host of the popular podcast Systemics Now!Systemics Now! is a webcast series</text>
    <text>subsequent iterative processing by machines) is the order of the future; and AI is just one component of this new technological order.AI will reduce the cognitive load of workers, allowing firms to automate tasks that used to require humans to exert enormous effort. This is already happening in significant parts of the world; for example, in parts of China where the rate of population growth is still eminently preventable (see my work at the Times Literary Supplement, from 1999 to 2006), but in recent years the pace of expansion has accelerated, and in the developing world the rate of growth has slowed to a crawl. So the technology that helps machines to accomplish tasks that used to require humans is likely to augment rather than replace what remains of what was once considered the productive function of humans. And while many aspects of the technology will be subverted in their use for nefarious ends, one might anticipate that a day would come when machines would enact the functions of humans in fully autonomous fashion.This scenario could feed into a longer-term trend, as the number of people employed in the production of goods and services continues to grow, and AI continues to accelerate. And while the number of people employed in the production of goods and services continues to grow, the pace of expansion will inevitably contract; which is to say, there will always be people in need of work and technology and plenty of room for them to take jobs. So the number of jobs could continue to grow, and the number of people needing work could continue to fluctuate; but the nature of the growth would tend to prevent it from feeding itself.There are of course also technological means by which the reproduction of humans could be augmented. And it is true that in the age of AI, it is not unimaginable that jobs would be lost, but the nature of the job would render such automation redundant. And while this may happen, jobs would still need to be created in the present in order to feed this burgeoning AI swarm.In short, the nature of the machine would render augmenting humans</text>
    <text>subsequent iterative processing by machines) is the order of the future; and AI is just one component of this new technological order.AI will reduce the cognitive load of workers, allowing firms to automate tasks that used to require humans to exert enormous effort. This is already happening in significant parts of the world; for example, in parts of China where the rate of population growth is still eminently preventable (see my work at the Times Literary Supplement, from 1999 to 2006), but in recent years the pace of expansion has accelerated, and in the developing world the rate of growth has slowed to a crawl. So the technology that helps machines to accomplish tasks that used to require humans is likely to augment rather than replace what remains of what was once considered the productive function of humans. And while many aspects of the technology will be subverted in their use for nefarious ends, one might anticipate that a day would come when machines would enact the functions of humans in fully autonomous fashion.This scenario could feed into a longer-term trend, as more and more people around the world begin to live destitute amidst the proliferation of AI software. But given the right set of circumstances, the rise of such systems could also provide the basis for a progressive democratic future. Given the right to exist, the question becomes which humanity? Given the right to breathe?Given the right to exist in autonomous systems?If we take the technologies currently leading towards AI seriously, then it is worth revisiting some of the historical precedents that have been rewritten in recent decades.Consider, for example, the role that religion and nationalism had in shaping technological design in the nineteenth and twentieth centuries. The rise of transnational transnational corporations and the massive infrastructure projects that followed in their wake was a great impetus for technological innovation, but it was also a tendency towards nationalistic and ethno-religious narratives that often lend an air of legitimacy to the projects that humans undertake. And within this context, we can see how the massive computer systems that we use to access the web are</text>
    <text>subsequent iterative processing by machines) is the order of the future; and AI is just one component of this new technological order.AI will reduce the cognitive load of workers, allowing firms to automate tasks that used to require humans to exert enormous effort. This is already happening in significant parts of the world; for example, in parts of China where the rate of population growth is still eminently preventable (see my work at the Times Literary Supplement, from 1999 to 2006), but in recent years the pace of expansion has accelerated, and in the developing world the rate of growth has slowed to a crawl. So the technology that helps machines to accomplish tasks that used to require humans is likely to augment rather than replace what remains of what was once considered the productive function of humans. And while many aspects of the technology will be subverted in their use for nefarious ends, one might anticipate that a day would come when machines would enact the functions of humans in fully autonomous fashion.This scenario could feed into a longer-term trend, as the rate of expansion of AI hardware accelerates (see my work at the New York Times, from 2005 to 2009), and as the AI that runs Google and Facebook becomes more and more capable (see my work at the Times, from 2007 to 2011).Artificial intelligence will become more and more like hardware (see my work at the MIT Media Lab, from 2001 to 2005), allowing the artificial intelligence to anticipate future events more like the human brain. And as AI improves, the question becomes whether humans are left behind or whether the future will be populated by intelligent machines.Artificial intelligence is predicted to lead to a world in which people do not need to be bound by the constraints of human history, where ideologies of progress (see also my work at the New York Times, from 2005 to 2009) will be replaced by systems based on intelligent agents, and where politics and law will be replaced by systems based on algorithms. And as AI improves, the question becomes whether we will need great thinkers such as</text>
  </text>
  <text>
    Working with algorithms, people can now do amazing things with data, such as automatically identifying areas of interest based on crowdsourced data or mapping natural habitats based on crowdsourced data. 	[i] and 	[ii] The Guardian (2013). Available at: http://www.guardian.co.uk/environment/2013/oct/13/deep-climate-change-epidemic-rivers  [iii] The Washington Post and USA Today (2014). Available at: http://www.washingtonpost.com/wp-srv/content/article/2006/08/22/AR20000622884980, accessed June 2016. [iv] Royal Society of Arts,  	Science &amp; Culture, Vol. 38, Issue 2, 2009, p.335. [v] Oxford Dictionaries, s.v. [vi] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 394
    <text>Working with algorithms, people can now do amazing things with data, such as automatically identifying areas of interest based on crowdsourced data or mapping natural habitats based on crowdsourced data. 	[i] and 	[ii] The Guardian (2013). Available at: http://www.guardian.co.uk/environment/2013/oct/13/deep-climate-change-epidemic-rivers  [iii] The Washington Post and USA Today (2014). Available at: http://www.washingtonpost.com/wp-srv/content/article/2006/08/22/AR20000622884980, accessed June 2016. [iv] Royal Society of Arts,  	Science &amp; Culture, Vol. 38, Issue 2, 2009, p.335. [v] Oxford Dictionaries, s.v. [vi] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 394. [vii] Oxford English Dictionary, Second Collegiate, 11th Ed., Oxford University Press, Oxford, 1993, p. 587. [viii] American Heritage Dictionary, Second Collegiate, 11th Ed., Oxford University Press, Oxford, 1997, p. 397. [ix] Oxford Dictionaries, s.v. [x] Oxford English Dictionary, Second Collegiate, 11th Ed., Oxford University Press, Oxford, 2003, p. 599. [xi] Oxford English Dictionary, Second Collegiate, 11th Ed., Oxford University Press, Oxford, 2006, p. 586. [xi] William Shakespeare, King Henry VI, Part 1, Romeo and Juliet, Act IV, scene iv, line 17, in Henry VI, Chapter Six, line 4, in the Oxford English Dictionary, Second Collegiate, Second Supplement, 1999, p. 587. [xii] American Heritage</text>
    <text>Working with algorithms, people can now do amazing things with data, such as automatically identifying areas of interest based on crowdsourced data or mapping natural habitats based on crowdsourced data. 	[i] and 	[ii] The Guardian (2013). Available at: http://www.guardian.co.uk/environment/2013/oct/13/deep-climate-change-epidemic-rivers  [iii] The Washington Post and USA Today (2014). Available at: http://www.washingtonpost.com/wp-srv/content/article/2006/08/22/AR20000622884980, accessed June 2016. [iv] Royal Society of Arts,  	Science &amp; Culture, Vol. 38, Issue 2, 2009, p.335. [v] Oxford Dictionaries, s.v. [vi] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 394. [vii] Oxford Dictionaries, s.v. [viii] American Heritage Dictionary, Second Edition, Fourth Estate, 2012, p. 396. [ix] Oxford Dictionaries, s.v. [x] Oxford Dictionaries, s.v. [xi] The New York Times, The New Republic, 1845, p. 1. [xii] The Washington Post and USA Today,  	The Washington Post, 1845, p. 1. [xiii] The New York Times, The New Republic, 1845, p. 28. [xiv] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 392. [xv] American Heritage Dictionary, Second Edition, Fourth Estate, 2012, p. 394. [xvi] The Washington Post and USA Today,  	The Washington Post, 1845, p. 1</text>
    <text>Working with algorithms, people can now do amazing things with data, such as automatically identifying areas of interest based on crowdsourced data or mapping natural habitats based on crowdsourced data. 	[i] and 	[ii] The Guardian (2013). Available at: http://www.guardian.co.uk/environment/2013/oct/13/deep-climate-change-epidemic-rivers  [iii] The Washington Post and USA Today (2014). Available at: http://www.washingtonpost.com/wp-srv/content/article/2006/08/22/AR20000622884980, accessed June 2016. [iv] Royal Society of Arts,  	Science &amp; Culture, Vol. 38, Issue 2, 2009, p.335. [v] Oxford Dictionaries, s.v. [vi] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 394. [vii] The Guardian,  	http://www.guardian.co.uk/environment/2004/oct/10/deep-climate-change-epidemic-rivers, accessed June 2016. [viii] The Royal Society of Arts,  	Science &amp; Culture, Vol. 38, Issue 2, 2009, p.335. [ix] Oxford Dictionaries, s.v. [x] The Washington Post and USA Today,  	http://www.washingtonpost.com/wp-srv/content/article/2009/08/22/AR20000622884980, accessed June 2016. [xi] The Guardian,  	http://www.guardian.co.uk/environment/2009/oct/10/deep-climate-change-epidemic-rivers, accessed June 2016. [xii] The Guardian (2014),</text>
    <text>Working with algorithms, people can now do amazing things with data, such as automatically identifying areas of interest based on crowdsourced data or mapping natural habitats based on crowdsourced data. 	[i] and 	[ii] The Guardian (2013). Available at: http://www.guardian.co.uk/environment/2013/oct/13/deep-climate-change-epidemic-rivers  [iii] The Washington Post and USA Today (2014). Available at: http://www.washingtonpost.com/wp-srv/content/article/2006/08/22/AR20000622884980, accessed June 2016. [iv] Royal Society of Arts,  	Science &amp; Culture, Vol. 38, Issue 2, 2009, p.335. [v] Oxford Dictionaries, s.v. [vi] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 394. [vii] Oxford Dictionaries, s.v. [viii] Kate Hoeppel and Mike Davis, eds.,  	Outsider in a Petal-Like Flower, Oxford University Press, Oxford, 2012, p. 511. [ix] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 395. [x] Oxford Dictionaries, s.v. [xi] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 396. [xii] The Washington Post and USA Today,  	Washington Post, August 15, 2014, http://www.washingtonpost.com/wp-srv/content/article/2014/08/15/AR20000622884980, accessed June 2016. [xiii] The Guardian, 2016,  	http://www.guardian.co.uk/environment/</text>
    <text>Working with algorithms, people can now do amazing things with data, such as automatically identifying areas of interest based on crowdsourced data or mapping natural habitats based on crowdsourced data. 	[i] and 	[ii] The Guardian (2013). Available at: http://www.guardian.co.uk/environment/2013/oct/13/deep-climate-change-epidemic-rivers  [iii] The Washington Post and USA Today (2014). Available at: http://www.washingtonpost.com/wp-srv/content/article/2006/08/22/AR20000622884980, accessed June 2016. [iv] Royal Society of Arts,  	Science &amp; Culture, Vol. 38, Issue 2, 2009, p.335. [v] Oxford Dictionaries, s.v. [vi] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 394. [vii] Oxford Dictionaries, s.v. [viii] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 396. [ix] Oxford Dictionaries, s.v. [x] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 399. [xi] Oxford Dictionaries, s.v. [xii] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 399. [xiii] The New York Times, 17 December 2014, http://www.nytimes.com/2014/12/18/us/politics/donald-trump-i-persona.html?_r=0, accessed June 2016. [xiv] The Washington Post and USA Today (2014). Available at: http://www.washingtonpost.com/wp-srv/content/article/200708091244</text>
    <text>Working with algorithms, people can now do amazing things with data, such as automatically identifying areas of interest based on crowdsourced data or mapping natural habitats based on crowdsourced data. 	[i] and 	[ii] The Guardian (2013). Available at: http://www.guardian.co.uk/environment/2013/oct/13/deep-climate-change-epidemic-rivers  [iii] The Washington Post and USA Today (2014). Available at: http://www.washingtonpost.com/wp-srv/content/article/2006/08/22/AR20000622884980, accessed June 2016. [iv] Royal Society of Arts,  	Science &amp; Culture, Vol. 38, Issue 2, 2009, p.335. [v] Oxford Dictionaries, s.v. [vi] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 394. [vii] Oxford Dictionaries, s.v. [viii] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 396. [ix] Oxford Dictionaries, s.v. [x] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 422. [xi] Oxford Dictionaries, s.v. [xii] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 425. [xiii] Oxford Dictionaries, s.v. [xiv] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 422. [xv] Oxford Dictionaries, s.v. [xvi] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 425. [xvii] Oxford Dictionaries, s.v. [xviii] </text>
    <text>Working with algorithms, people can now do amazing things with data, such as automatically identifying areas of interest based on crowdsourced data or mapping natural habitats based on crowdsourced data. 	[i] and 	[ii] The Guardian (2013). Available at: http://www.guardian.co.uk/environment/2013/oct/13/deep-climate-change-epidemic-rivers  [iii] The Washington Post and USA Today (2014). Available at: http://www.washingtonpost.com/wp-srv/content/article/2006/08/22/AR20000622884980, accessed June 2016. [iv] Royal Society of Arts,  	Science &amp; Culture, Vol. 38, Issue 2, 2009, p.335. [v] Oxford Dictionaries, s.v. [vi] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 394. [vii] Oxford English Dictionary, Second Collegiate Standard, 1204, Merriam-Webster Online, http://www.merriam-webster.com/dictionary/art/sydney_bay_city_of_america, accessed March 2018. [viii] William Shakespeare, King Henry VI, Act IV, Scene X, in Henry VI, p. 2, trans. Evelyn Waugh, Cambridge University Press, Cambridge, MA, 1997, p. 668. [ix] Oxford English Dictionary, s.v. [x] Oxford American, 1845, p. 1847. [xi] William Shakespeare, King Henry VI, Act IV, Scene X, in Henry VI, p. 2, trans. Evelyn Waugh, Cambridge University Press, Cambridge, MA, 1997, p. 669. [xii] William Shakespeare, King Henry VI, Act IV</text>
    <text>Working with algorithms, people can now do amazing things with data, such as automatically identifying areas of interest based on crowdsourced data or mapping natural habitats based on crowdsourced data. 	[i] and 	[ii] The Guardian (2013). Available at: http://www.guardian.co.uk/environment/2013/oct/13/deep-climate-change-epidemic-rivers  [iii] The Washington Post and USA Today (2014). Available at: http://www.washingtonpost.com/wp-srv/content/article/2006/08/22/AR20000622884980, accessed June 2016. [iv] Royal Society of Arts,  	Science &amp; Culture, Vol. 38, Issue 2, 2009, p.335. [v] Oxford Dictionaries, s.v. [vi] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 394. [vii] Oxford Dictionaries, s.v. [viii] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 396. [ix] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 396. [x] Le Notre’s Book of Common Prayer, 1609, in Oxford English Dictionary, 8th ed., rev. ed., p. 496. [xi] Oxford English Dictionary, Second Edition, 4th ed., p. 397. [xii] Oxford Dictionaries, s.v. [xiii] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 394. [xiv] The Washington Post and USA Today, 14 November 2016, http://www.washingtonpost.com/wp-srv/content/article/2016/11/14/AR20000109144384, accessed June</text>
    <text>Working with algorithms, people can now do amazing things with data, such as automatically identifying areas of interest based on crowdsourced data or mapping natural habitats based on crowdsourced data. 	[i] and 	[ii] The Guardian (2013). Available at: http://www.guardian.co.uk/environment/2013/oct/13/deep-climate-change-epidemic-rivers  [iii] The Washington Post and USA Today (2014). Available at: http://www.washingtonpost.com/wp-srv/content/article/2006/08/22/AR20000622884980, accessed June 2016. [iv] Royal Society of Arts,  	Science &amp; Culture, Vol. 38, Issue 2, 2009, p.335. [v] Oxford Dictionaries, s.v. [vi] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 394. [vii] Oxford Dictionaries, s.v. [viii] Oxford Dictionaries, s.v. [ix] Oxford Dictionaries, s.v. [x] American Heritage Dictionary, Second Edition, Fourth Estate, 2000, p. 394. [xi] The Washington Post and USA Today (2014), p. A1. [xii] The Guardian (2013), p. A1. [xiii] The Washington Post and USA Today (2014), p. A1. [xiv] The Washington Post and USA Today (2014), p. A1. [xv] The Washington Post and USA Today (2014), p. A1.Download this article as PDFJoasia KrysaJoasia Krysa is Professor of Computer Science and Co-Director of the Computer Science &amp; Engineering Faculty at the University of Alberta, Canada. Recent publications include the award-winning</text>
    <text>Working with algorithms, people can now do amazing things with data, such as automatically identifying areas of interest based on crowdsourced data or mapping natural habitats based on crowdsourced data. 	[i] and 	[ii] The Guardian (2013). Available at: http://www.guardian.co.uk/environment/2013/oct/13/deep-climate-change-epidemic-rivers  [iii] The Washington Post and USA Today (2014). Available at: http://www.washingtonpost.com/wp-srv/content/article/2006/08/22/AR20000622884980, accessed June 2016. [iv] Royal Society of Arts,  	Science &amp; Culture, Vol. 38, Issue 2, 2009, p.335. [v] Oxford Dictionaries, s.v. [vi] American Heritage Dictionary, Second Edition, Fourth Estate, 2001, p. 394. [vii] The Guardian (2015). Available at: http://www.theguardian.com/environment/2015/oct/13/scientists-find-climate-change/  [vi] The Washington Post and USA Today (2014). Available at: http://www.washingtonpost.com/wp-srv/content/article/2014/08/21/AR200006209054609, accessed June 2016. [vii] The Guardian (2015). Available at: http://www.theguardian.com/environment/2015/oct/13/deep-climate-change-epidemic-rivers, accessed June 2016. [viii] The Guardian (2016). Available at: http://www.theguardian.com/environment/2016/oct/13/deep-climate-change-epidemic-rivers, accessed June 2016. [ix] The Guardian (2016).</text>
  </text>
  <text>
    extends the proposition to machines, where the past is not merely where we find meaning but where action takes place (Lerner and Roce Smith, Definition and Exploitation of Literary and Media Form, Blackwell, Oxford, 1988, p.14). The history of passenger pigeon breeders, pigeon-hunters and the like is a long and colourful one. The nineteenth-century interest in the animals appears nowhere in the book, but the twentieth-century one does. The importance of the past as a source of meaning and politics in the 21st century is clear in the chapter entitled ‘The Enduring Crisis of Meaninglessness: Presentism, Nomadic Futures, and Cultures of Critique’. Here the authors make the case that as our understanding of the past improves, our sense of the future will also improve. But they go on to argue that our sense of the future is similarly constrained. We live in a ‘presentism’ that constrains our ability to imagine meaning in
    <text>extends the proposition to machines, where the past is not merely where we find meaning but where action takes place (Lerner and Roce Smith, Definition and Exploitation of Literary and Media Form, Blackwell, Oxford, 1988, p.14). The history of passenger pigeon breeders, pigeon-hunters and the like is a long and colourful one. The nineteenth-century interest in the animals appears nowhere in the book, but the twentieth-century one does. The importance of the past as a source of meaning and politics in the 21st century is clear in the chapter entitled ‘The Enduring Crisis of Meaninglessness: Presentism, Nomadic Futures, and Cultures of Critique’. Here the authors make the case that as our understanding of the past improves, our sense of the future will also improve. But they go on to argue that our sense of the future is similarly constrained. We live in a ‘presentism’ that constrains our ability to imagine meaning in the future, even though it is possible to conceive of a future without it. This is an immensely valuable and influential chapter in a book that is clearly about the social. It describes in vivid terms the cognitive impoverishment caused by our growing digitised archive of past lives, and the fact that the future is no longer a future at all. It is a future constituted by the past rather than the other way around. In this respect, the future is like the past in that it is not present but anticipates the future. The chapter ends with a list of questions that posthumanists should be asking themselves as they contemplate the futures that they do and do not possess. It is a list that attempts to capture the essence of what posthumanists can and cannot possess, and what kinds of difference there are between now and in an ideal future.I want to ask you, posthumanists, what kinds of difference can you imagine if, in their estimation, human flourishing were determined by the past rather</text>
    <text>extends the proposition to machines, where the past is not merely where we find meaning but where action takes place (Lerner and Roce Smith, Definition and Exploitation of Literary and Media Form, Blackwell, Oxford, 1988, p.14). The history of passenger pigeon breeders, pigeon-hunters and the like is a long and colourful one. The nineteenth-century interest in the animals appears nowhere in the book, but the twentieth-century one does. The importance of the past as a source of meaning and politics in the 21st century is clear in the chapter entitled ‘The Enduring Crisis of Meaninglessness: Presentism, Nomadic Futures, and Cultures of Critique’. Here the authors make the case that as our understanding of the past improves, our sense of the future will also improve. But they go on to argue that our sense of the future is similarly constrained. We live in a ‘presentism’ that constrains our ability to imagine meaning in the future. They quote philosopher Richard Rorty, one of a host of philosophers who together form the Committee on Future Obscurity (ComFuture) gave a landmark 1976 report on human behaviour to the United Nations. It concluded that ‘the present is the present in all its temporal qualities, insofar as it is constrained by a universal reference frame’.1. The word ‘enduring’ refers to something living or existing in a particular way. Does that mean that the thing will outlive everything, or will it mean that the thing will consume everything, or will it mean that the thing will be replaced by a new thing and so on? If the temporal qualities of the future are determined by the past, then the spatial qualities will follow the same way. What is meant by temporal is the same for both ‘living’ and ‘dead’ kinds of things. The important point here is that temporalisation is a constant. As new kinds of temporalisation</text>
    <text>extends the proposition to machines, where the past is not merely where we find meaning but where action takes place (Lerner and Roce Smith, Definition and Exploitation of Literary and Media Form, Blackwell, Oxford, 1988, p.14). The history of passenger pigeon breeders, pigeon-hunters and the like is a long and colourful one. The nineteenth-century interest in the animals appears nowhere in the book, but the twentieth-century one does. The importance of the past as a source of meaning and politics in the 21st century is clear in the chapter entitled ‘The Enduring Crisis of Meaninglessness: Presentism, Nomadic Futures, and Cultures of Critique’. Here the authors make the case that as our understanding of the past improves, our sense of the future will also improve. But they go on to argue that our sense of the future is similarly constrained. We live in a ‘presentism’ that constrains our ability to imagine meaning in the future. But what does this mean in the face of increasing access to meaning? I suggest that we could begin to understand the presentism of the future as being defined by something like existential meaninglessness. What this means in practice is that the means of knowing – including human knowledge – remain subject to the ends that control knowledge in the here and now. The present and ongoing condition of subjectivity that characterises contemporary knowledge is one that is both uncertain and precarious.There are of course other ways of reading the chapter, and I would read it as a coming together of different cultures that have come together in an attempt to understand how the future may or may not be like the past. Perhaps the most promising part of the chapter for presentists and post-modernists is when it comes to the future, where ‘race and gender’ are brought together to explore what it may mean for a person to be ‘race and gender independent again’. But even here the authors of the</text>
    <text>extends the proposition to machines, where the past is not merely where we find meaning but where action takes place (Lerner and Roce Smith, Definition and Exploitation of Literary and Media Form, Blackwell, Oxford, 1988, p.14). The history of passenger pigeon breeders, pigeon-hunters and the like is a long and colourful one. The nineteenth-century interest in the animals appears nowhere in the book, but the twentieth-century one does. The importance of the past as a source of meaning and politics in the 21st century is clear in the chapter entitled ‘The Enduring Crisis of Meaninglessness: Presentism, Nomadic Futures, and Cultures of Critique’. Here the authors make the case that as our understanding of the past improves, our sense of the future will also improve. But they go on to argue that our sense of the future is similarly constrained. We live in a ‘presentism’ that constrains our ability to imagine meaning in the future, and so our understanding of the future is limited. In short, the future, although it may be predictive, is necessarily a future constrained by the past.2 	In recent years, some have sought to reinterpret the end of history as a crisis of meaninglessness, in which humanity is engaged in a perpetual quest for meaningless data. This is a fantasy, of course. The future is full of meaning, and advancing science tells us that life on Earth is going to be just as fulfilling, if not more fulfilling, if we abandon the search for meaning. But even if we ultimately discard our sense of the past in the pursuit of a predictive future, we would still remain at the mercy of a past that keeps giving us meaning. What the future might mean today, is likely to mean tomorrow; what the past means today, may mean tomorrow; and so on. These patterns of change are what Lefebvre described as ‘contravariant relations’</text>
    <text>extends the proposition to machines, where the past is not merely where we find meaning but where action takes place (Lerner and Roce Smith, Definition and Exploitation of Literary and Media Form, Blackwell, Oxford, 1988, p.14). The history of passenger pigeon breeders, pigeon-hunters and the like is a long and colourful one. The nineteenth-century interest in the animals appears nowhere in the book, but the twentieth-century one does. The importance of the past as a source of meaning and politics in the 21st century is clear in the chapter entitled ‘The Enduring Crisis of Meaninglessness: Presentism, Nomadic Futures, and Cultures of Critique’. Here the authors make the case that as our understanding of the past improves, our sense of the future will also improve. But they go on to argue that our sense of the future is similarly constrained. We live in a ‘presentism’ that constrains our ability to imagine meaning in the future, and thus our sense of the future is also presentist. The implication of this view is that our present and the future are necessarily unstable and mutable. It is also the case that the past is present in the sense that it is visited and interacted with throughout the social and historical process that results from capitalism. This is not to say that the past is empty of meaning or that it does not have value; rather, the idea is that the future is variable and subject to the past. The past is always a source of political, social and economic turmoil and upheaval. The past is always a source of displacement and poverty. But the past is not always a source of creative solutions. And these are precisely the sorts of insights that characterise the ‘present’ in different forms – that is, of solutions to problems that emerge from the social and economic processes that characterise the ‘present’. These past social and economic processes are characterised by fluctuations in exchange and</text>
    <text>extends the proposition to machines, where the past is not merely where we find meaning but where action takes place (Lerner and Roce Smith, Definition and Exploitation of Literary and Media Form, Blackwell, Oxford, 1988, p.14). The history of passenger pigeon breeders, pigeon-hunters and the like is a long and colourful one. The nineteenth-century interest in the animals appears nowhere in the book, but the twentieth-century one does. The importance of the past as a source of meaning and politics in the 21st century is clear in the chapter entitled ‘The Enduring Crisis of Meaninglessness: Presentism, Nomadic Futures, and Cultures of Critique’. Here the authors make the case that as our understanding of the past improves, our sense of the future will also improve. But they go on to argue that our sense of the future is similarly constrained. We live in a ‘presentism’ that constrains our ability to imagine meaning in the future, and so we live in nomadic and transient ways: living in the present, contemplating the past, and living in transient ‘presentism’ in the future. We inhabit nomadic forms of cultural formations that create and reify spaces and actions that constrain our perceptions and possibilities. We are cogs in a capitalist machine of signification.The author’s ‘Crisis of Meaninglessness’ raises questions about what it means to be fully human, and about the contemporary condition of being constantly mediated and (conceived as) affective. It also threatens the status of cultural forms that can no longer be accommodated in the contemporary condition of consumer society. Here, the future is not bright and beautiful and there is an awful implication about the future that is both ominous and beautiful. There is a need to question the values that generate our present and future forms of being, to step into the breach that has been created by the inchoate and contradictory forces of</text>
    <text>extends the proposition to machines, where the past is not merely where we find meaning but where action takes place (Lerner and Roce Smith, Definition and Exploitation of Literary and Media Form, Blackwell, Oxford, 1988, p.14). The history of passenger pigeon breeders, pigeon-hunters and the like is a long and colourful one. The nineteenth-century interest in the animals appears nowhere in the book, but the twentieth-century one does. The importance of the past as a source of meaning and politics in the 21st century is clear in the chapter entitled ‘The Enduring Crisis of Meaninglessness: Presentism, Nomadic Futures, and Cultures of Critique’. Here the authors make the case that as our understanding of the past improves, our sense of the future will also improve. But they go on to argue that our sense of the future is similarly constrained. We live in a ‘presentism’ that constrains our ability to imagine meaning in the future. If the future is defined as the immediate past, then the present is also defined as the present, which is to say, the future either has or lacks meaning. Our sense of the future is thus constrained by what is called the phenomenological moment.2 Here the authors appeal to a number of titles in order to give a sense of the current predicament of meaninglessness: postmodernism, ethnocentrism, nationalistic thought, historical nihilism, and modernity itself. Postmodernism and its variants seek to replace the apparent lack of internal consistency with a perceived abundance of external variation. In other words, postmodernism seeks to reanimate what is perceived as lost meaning in the secular West. Aesthetics is one interpretation of this new understanding of meaning. The book’s title suggests that the philosophical apprehension of meaninglessness is the focus of philosophical, aesthetic and political thought. But the increasing acceptance of instrumental rationality in our public and private social processes, as well as</text>
    <text>extends the proposition to machines, where the past is not merely where we find meaning but where action takes place (Lerner and Roce Smith, Definition and Exploitation of Literary and Media Form, Blackwell, Oxford, 1988, p.14). The history of passenger pigeon breeders, pigeon-hunters and the like is a long and colourful one. The nineteenth-century interest in the animals appears nowhere in the book, but the twentieth-century one does. The importance of the past as a source of meaning and politics in the 21st century is clear in the chapter entitled ‘The Enduring Crisis of Meaninglessness: Presentism, Nomadic Futures, and Cultures of Critique’. Here the authors make the case that as our understanding of the past improves, our sense of the future will also improve. But they go on to argue that our sense of the future is similarly constrained. We live in a ‘presentism’ that constrains our ability to imagine meaning in the future, and so we live in a world that is, in its non-contemporaneous condition, continually ambiguous and uncertain. This uncertainty is precisely what William Irvine, an Irish zoologist, discovered when he collected over 1,000 fossils from over twenty endemic species of birds, bats, reptiles and amphibians over the course of a single year in the late 1950s. Although the animals were originally collected and sent to museums, the chapters that refer to them in the book follow a narrative structure that mirrors the structure of the book itself. For example, the chapter ‘Presentism and Its Complications’ begins with a description of the effects of consumer culture on the body politic, and ends with a chapter on the ‘capitalist circulation of culture, with an emphasis on the ways in which a consumer culture is reshaping what kinds of cultural forms are possible in the future’. The chapter ‘Critique of Modernity’ begins with a series of six essays that</text>
    <text>extends the proposition to machines, where the past is not merely where we find meaning but where action takes place (Lerner and Roce Smith, Definition and Exploitation of Literary and Media Form, Blackwell, Oxford, 1988, p.14). The history of passenger pigeon breeders, pigeon-hunters and the like is a long and colourful one. The nineteenth-century interest in the animals appears nowhere in the book, but the twentieth-century one does. The importance of the past as a source of meaning and politics in the 21st century is clear in the chapter entitled ‘The Enduring Crisis of Meaninglessness: Presentism, Nomadic Futures, and Cultures of Critique’. Here the authors make the case that as our understanding of the past improves, our sense of the future will also improve. But they go on to argue that our sense of the future is similarly constrained. We live in a ‘presentism’ that constrains our ability to imagine meaning in the future. If the past is excluded from the here and now, then what is meant by the present and future, at present and in the future, is also excluded.’ The advent of new technologies allows us to conceive of ever greater expanses of the past, to travel back in time and space and make inferences about the past, present and future. These inferences are as inevitable as any older ways of making sense of the past, but thanks to the explosion of scientific and journalistic knowledge of the past, we also have the technology to turn those inferences into inferences about the future.Inferential transparency is a good thing. It conveys the past in a certain way, with statements that are as certain as statements can be. But when inferences about the past and the future are rendered as inferences about the present, their true character is rendered suspect. That is, in the present, inferences about the past are rendered as inferences about the future, which in</text>
    <text>extends the proposition to machines, where the past is not merely where we find meaning but where action takes place (Lerner and Roce Smith, Definition and Exploitation of Literary and Media Form, Blackwell, Oxford, 1988, p.14). The history of passenger pigeon breeders, pigeon-hunters and the like is a long and colourful one. The nineteenth-century interest in the animals appears nowhere in the book, but the twentieth-century one does. The importance of the past as a source of meaning and politics in the 21st century is clear in the chapter entitled ‘The Enduring Crisis of Meaninglessness: Presentism, Nomadic Futures, and Cultures of Critique’. Here the authors make the case that as our understanding of the past improves, our sense of the future will also improve. But they go on to argue that our sense of the future is similarly constrained. We live in a ‘presentism’ that constrains our ability to imagine meaning in the future, since the past is never fully present or fully understood. As a consequence, we live lives governed by the past, and future events are likely to repeat the past in ways that shape and constrain our lives in troubling ways. In this sense, the ‘present’ is both ‘presence’ and ‘absence’ in the same. But what the authors fail to mention is that, in the history of philosophy, from Aristotle on, the distinction between the past and the future has been making appearances all the time. For example, in the Categories, ‘the past is a question throughout the remainder of the book’, where Aristotle makes the distinction between past, present and future. In the Ethics, ‘the past is a stage in the development of the future’. And in the Metaphysics of Perception, Aristotle makes the distinction between object and quality. Object and quality are not abstract concepts, but are rooted in the very</text>
  </text>
  <text>
    extends the proposition to machines, and to the extent that it is realised. This is evident in the research programmes of the Future City think-tanks, which are dominated by programmes in areas such as urbanisation and regeneration that are characterised by the development of ‘smart’ infrastructure, software and data infrastructures for the consumption and reuse of urban space and public resources (Landau, 2009; Rauhut and Rauhut, 2013). In these programmes, the spatialisation of knowledge production is often attributed to the rise of the ‘sharing economy’ or the sharing […] READ THE REST

7 tools to upgrade your productivity Five tools to help you stay on top of your workflows As productivity grows, so too does the amount of data generated; it’s increasingly common for agencies to collect and process massive amounts of data, which is then put to use in production. Your productivity will thank you. Here are seven innovations that will make your workflows richer and fuller:
    <text>extends the proposition to machines, and to the extent that it is realised. This is evident in the research programmes of the Future City think-tanks, which are dominated by programmes in areas such as urbanisation and regeneration that are characterised by the development of ‘smart’ infrastructure, software and data infrastructures for the consumption and reuse of urban space and public resources (Landau, 2009; Rauhut and Rauhut, 2013). In these programmes, the spatialisation of knowledge production is often attributed to the rise of the ‘sharing economy’ or the sharing […] READ THE REST

7 tools to upgrade your productivity Five tools to help you stay on top of your workflows As productivity grows, so too does the amount of data generated; it’s increasingly common for agencies to collect and process massive amounts of data, which is then put to use in production. Your productivity will thank you. Here are seven innovations that will make your workflows richer and fuller: avai, dash, markdown, excel, scraper and curl These are just some of the productivity-enhancing tools that have caught on; you can add more. Here are more ideas in the sphere of user-centred design. 1. Avai – A visual language for the user’s behaviour. Created by Avia Software and used in conjunction with IBM Research, Avai is a hybrid graphical user toolkit and research institute for research in visual languages. It provides a platform for research into the phenomenology of action, based around the notion of ‘intentions’. These ‘results’ are then interpreted and executed using machine learning and/or deep learning techniques to produce ‘design outcomes’.  2. Dash – An abbreviation of Diagnostic and Reporting System (1993–present), Dash is a multipurpose tool for data mining, interpretation, and production, originally created by Dr. Vern E. Waite at McGill University in 1993</text>
    <text>extends the proposition to machines, and to the extent that it is realised. This is evident in the research programmes of the Future City think-tanks, which are dominated by programmes in areas such as urbanisation and regeneration that are characterised by the development of ‘smart’ infrastructure, software and data infrastructures for the consumption and reuse of urban space and public resources (Landau, 2009; Rauhut and Rauhut, 2013). In these programmes, the spatialisation of knowledge production is often attributed to the rise of the ‘sharing economy’ or the sharing […] READ THE REST

7 tools to upgrade your productivity Five tools to help you stay on top of your workflows As productivity grows, so too does the amount of data generated; it’s increasingly common for agencies to collect and process massive amounts of data, which is then put to use in production. Your productivity will thank you. Here are seven innovations that will make your workflows richer and fuller: avai, dash, markdown, pdf, video and mrtI present my seven-day productivity hackathon, ‘How to Get Things Done Fast’, organised by Avaaz and launched today. For each of the […] READ THE REST

Investing in the future with blockchain Technology is the new frontier in efficiency and efficiency is the new frontier in growth. That is, of course, if you can keep up with the ever-increasing stream of data. So it is with blockchain, which is increasingly evident in the media reports of big business, government, and even in the research papers of academics. It is the technology that underlies peer review, the blockchain, and the distributed ledger – the technology that underpins everything from tax filings to stock buybacks to stock options grants and debt offerings. And it is the technology underpinning the new distributed databases that will allow for the tracking and […] READ THE REST

Facebook: 20% off month of 1 January, 2017</text>
    <text>extends the proposition to machines, and to the extent that it is realised. This is evident in the research programmes of the Future City think-tanks, which are dominated by programmes in areas such as urbanisation and regeneration that are characterised by the development of ‘smart’ infrastructure, software and data infrastructures for the consumption and reuse of urban space and public resources (Landau, 2009; Rauhut and Rauhut, 2013). In these programmes, the spatialisation of knowledge production is often attributed to the rise of the ‘sharing economy’ or the sharing […] READ THE REST

7 tools to upgrade your productivity Five tools to help you stay on top of your workflows As productivity grows, so too does the amount of data generated; it’s increasingly common for agencies to collect and process massive amounts of data, which is then put to use in production. Your productivity will thank you. Here are seven innovations that will make your workflows richer and fuller:  The ‘Intercom system’ is a new breed of computer that monitors and collects context – information about what’s happening in the workspace. Think of it as a supersized version of what’s in your regular computer.  The ‘One Stop Shop Book’ is a new digital mapping tool that collects […] READ THE REST

MailChimp For iOS 7, Corda is a simple to use, powerful tool for data mining and data visualisation. Created by and for Apple, MailChimp was originally released in 2007 as part of the App Store. It became available as part of the iOS 7 operating system in 2010, and continues to be available as an app through the iTunes App Store. In its primary setting as a web-based application, MailChimp collects and distributes […] READ THE REST

Why do birds have names? Why don’t birds have faces? Why don’t birds have identities? Why don’</text>
    <text>extends the proposition to machines, and to the extent that it is realised. This is evident in the research programmes of the Future City think-tanks, which are dominated by programmes in areas such as urbanisation and regeneration that are characterised by the development of ‘smart’ infrastructure, software and data infrastructures for the consumption and reuse of urban space and public resources (Landau, 2009; Rauhut and Rauhut, 2013). In these programmes, the spatialisation of knowledge production is often attributed to the rise of the ‘sharing economy’ or the sharing […] READ THE REST

7 tools to upgrade your productivity Five tools to help you stay on top of your workflows As productivity grows, so too does the amount of data generated; it’s increasingly common for agencies to collect and process massive amounts of data, which is then put to use in production. Your productivity will thank you. Here are seven innovations that will make your workflows richer and fuller: avai, dash, markdown, pullquote, markdown Sphinx, a new type of editing environment developed in collaboration with IDEO, Japan (2015); Lex, a new kind of lexicon (2015); Parallax, a new kind of parallax map editor (2016); Parallax Plus, a new kind of editing environment (2017); and Slo, new kind of document viewer (2018). Understandably, these tools are often used by businesspeople looking to cut costs and increase efficiency, but there are many more that are invisible or unheard of because they do not fit into existing production systems.   Notebook 10/11/2016 	Organisations use notebooks to keep track of progress, metrics, personnel, resources and more. But unlike documents or statistics, which can be recorded and analysed, the recording and keeping of information is inherently unstable. ‘Information overload’ is a phenomenon to be reckoned with, and organisations everywhere are exposed to it in</text>
    <text>extends the proposition to machines, and to the extent that it is realised. This is evident in the research programmes of the Future City think-tanks, which are dominated by programmes in areas such as urbanisation and regeneration that are characterised by the development of ‘smart’ infrastructure, software and data infrastructures for the consumption and reuse of urban space and public resources (Landau, 2009; Rauhut and Rauhut, 2013). In these programmes, the spatialisation of knowledge production is often attributed to the rise of the ‘sharing economy’ or the sharing […] READ THE REST

7 tools to upgrade your productivity Five tools to help you stay on top of your workflows As productivity grows, so too does the amount of data generated; it’s increasingly common for agencies to collect and process massive amounts of data, which is then put to use in production. Your productivity will thank you. Here are seven innovations that will make your workflows richer and fuller: Harnessing the power of crowds, collaborative teams and AI to crunch huge amounts of data, as opposed to relying solely on the expert, data-driven workforce Many people assume that the explosion of IT workflows around workflows and workflow meant that jobs disappeared; however, a closer look reveals that employment opportunities remain scarce; in fact, they’re much more common today. A team in a research lab might produce impressive results, but in the real-life context of a production line or warehouse, this kind of labour struggles for survival. Collaborative discovery and research into new workflows is the new industrial frontier; a new form of labour without a name yet to be defined. ENvisioning a future where work doesn’t define you  Any work that is not driven by an agency (company, government, union) is labor in progress.  And yet, there are countless websites dedicated to helping companies do good, from disaster relief to education to sports coaching. ‘B</text>
    <text>extends the proposition to machines, and to the extent that it is realised. This is evident in the research programmes of the Future City think-tanks, which are dominated by programmes in areas such as urbanisation and regeneration that are characterised by the development of ‘smart’ infrastructure, software and data infrastructures for the consumption and reuse of urban space and public resources (Landau, 2009; Rauhut and Rauhut, 2013). In these programmes, the spatialisation of knowledge production is often attributed to the rise of the ‘sharing economy’ or the sharing […] READ THE REST

7 tools to upgrade your productivity Five tools to help you stay on top of your workflows As productivity grows, so too does the amount of data generated; it’s increasingly common for agencies to collect and process massive amounts of data, which is then put to use in production. Your productivity will thank you. Here are seven innovations that will make your workflows richer and fuller: avai, dash, markdown, pdf, slide, and txt. As productivity grows, so too does the amount of data generated; it’s increasingly common for agencies to collect and process massive amounts of data, which is then put to use in production. Your productivity will thank you. Here are seven […] READ THE REST

7 tools to upgrade your productivity Five tools to help you stay on top of your workflows As productivity grows, so too does the amount of data generated; it’s increasingly common for agencies to collect and process massive amounts of data, which is then put to use in production. Your productivity will thank you. Here are seven innovations that will make your workflows richer and fuller: avai, dash, markdown, pdf, slide, and txt. As productivity grows, so too does the amount of data generated; it’s increasingly common for agencies to collect and process massive amounts of data, which is then put to use in</text>
    <text>extends the proposition to machines, and to the extent that it is realised. This is evident in the research programmes of the Future City think-tanks, which are dominated by programmes in areas such as urbanisation and regeneration that are characterised by the development of ‘smart’ infrastructure, software and data infrastructures for the consumption and reuse of urban space and public resources (Landau, 2009; Rauhut and Rauhut, 2013). In these programmes, the spatialisation of knowledge production is often attributed to the rise of the ‘sharing economy’ or the sharing […] READ THE REST

7 tools to upgrade your productivity Five tools to help you stay on top of your workflows As productivity grows, so too does the amount of data generated; it’s increasingly common for agencies to collect and process massive amounts of data, which is then put to use in production. Your productivity will thank you. Here are seven innovations that will make your workflows richer and fuller: recognisable timelines, collaboration tools, paid dashboards and dashboards for internal use only, early childhood education as a service and dashboards for external use only, to name only a few. 1.  recognisable timelines. Business models have different phases – the phase when the business is founded, the phase when the business is in active operation and the phase when the business is decommissioned – and different people have very different timelines. Recognisable timelines allow organisations to define and control what is and is not reported to the business. For example, when the Abu Ghraib prison break occurred, the US government claimed that it was during the period between 9/11/01 and 10/11/01; however, many media organisations held the break to be between 9/11/01 and 10/11/01.[1]  2. Collaborative work. Business models that emphasise cooperation and open collaboration are more likely to achieve widespread adoption; for example, the collaborative digital publishing model pioneered</text>
    <text>extends the proposition to machines, and to the extent that it is realised. This is evident in the research programmes of the Future City think-tanks, which are dominated by programmes in areas such as urbanisation and regeneration that are characterised by the development of ‘smart’ infrastructure, software and data infrastructures for the consumption and reuse of urban space and public resources (Landau, 2009; Rauhut and Rauhut, 2013). In these programmes, the spatialisation of knowledge production is often attributed to the rise of the ‘sharing economy’ or the sharing […] READ THE REST

7 tools to upgrade your productivity Five tools to help you stay on top of your workflows As productivity grows, so too does the amount of data generated; it’s increasingly common for agencies to collect and process massive amounts of data, which is then put to use in production. Your productivity will thank you. Here are seven innovations that will make your workflows richer and fuller: a daily or hourly journal that logs your key performance indicators; Flowcharts that visualise and analyse […] READ THE REST

Investing in the future, not for the moment, but for the next big thing When thinking about the future in the context of environmental degradation and pressing economic pressures, it’s easy to down-cycle the past to help inform a decision making process. For example, you can revisit the damage done to the Chesapeake Bay by the oil and gas pipelines that now run along its eastern shore, and the damage done to the region by the burning of fossil fuels that […] READ THE REST

Mailstrom declutters your inbox with smart categorisations Whether you're heading for a smooth transition to a cut-up, subscribe to the new urbanite or urban traveller,Millionaire Magazine’s 2012 coastal community design winner 'The Big Short' Jeff Hawkins offers a few pointers: 'As a city builder, your task is to neighbourhoods, not cities.�</text>
    <text>extends the proposition to machines, and to the extent that it is realised. This is evident in the research programmes of the Future City think-tanks, which are dominated by programmes in areas such as urbanisation and regeneration that are characterised by the development of ‘smart’ infrastructure, software and data infrastructures for the consumption and reuse of urban space and public resources (Landau, 2009; Rauhut and Rauhut, 2013). In these programmes, the spatialisation of knowledge production is often attributed to the rise of the ‘sharing economy’ or the sharing […] READ THE REST

7 tools to upgrade your productivity Five tools to help you stay on top of your workflows As productivity grows, so too does the amount of data generated; it’s increasingly common for agencies to collect and process massive amounts of data, which is then put to use in production. Your productivity will thank you. Here are seven innovations that will make your workflows richer and fuller: avai, dash, markdown, customise dash, markdown feeds, spreadsheets and dashboards (just to name a few of many) to name only a few. Arjun Appadurai has a great postmortem on the rise of Facebook in which he illustrates the immense power of data gathering and consumption. But there is much more to be done. Data minimisation is a must for any true ‘data-driven economy’. Mandatory export controls on raw materials and cheap labour would be dismantled; open-source accounting and documentation would be mandatory; and there would be no place for the profit-driven research bureaucracy, which dominates in the West today, in the developing world or in the Arctic. Instead, there would be transparent sharing of information and reduced administrative burden for both parties.  Do your part: share this post with your networked friends. Tweet it! Share it with your networked friends. Blog it!  And then, when you are done, unwind</text>
    <text>extends the proposition to machines, and to the extent that it is realised. This is evident in the research programmes of the Future City think-tanks, which are dominated by programmes in areas such as urbanisation and regeneration that are characterised by the development of ‘smart’ infrastructure, software and data infrastructures for the consumption and reuse of urban space and public resources (Landau, 2009; Rauhut and Rauhut, 2013). In these programmes, the spatialisation of knowledge production is often attributed to the rise of the ‘sharing economy’ or the sharing […] READ THE REST

7 tools to upgrade your productivity Five tools to help you stay on top of your workflows As productivity grows, so too does the amount of data generated; it’s increasingly common for agencies to collect and process massive amounts of data, which is then put to use in production. Your productivity will thank you. Here are seven innovations that will make your workflows richer and fuller: jewellery machine Learn how to use a diamond tool belt with Singer’s Ring Tool, a machine that collects and processes ringed tools, metals and other precious materials into jewelry components. READ THE REST

7 tools to upgrade your productivity Five tools to help you stay on top of your workflows Whether you’re a developer or a manager, the tools you use every day will grow more sophisticated; they’re becoming ever more capable of crunching data and sifting through massive amounts of data. But there are countless more tools out there, which you can't access or use because they’re ‘underutilised’ or ‘outdated’ or ‘outgunned’ by the developers or managers who build them or use them. These tools are what give your tools a life beyond the confines of your containment. When tools become ever more powerful, redundant and adaptable, they become part of a system that allows you to change and inf</text>
  </text>
  <text>
    Working with algorithms, Riccardo Bandini came up with the term ‘machine learning’ in 1967.2 The idea that we could learn to like certain foods was a thing of the past, in part because of a hubris of the food industry: thanks to machine learning, it was possible to create recipes that read like prose, and that feed into a database that feeds into a supermarket chain.3  This model didn’t envisage a future in which every aspect of our lives was controlled by algorithms, but a future in which everything we do is connected via networks of nodes and disconnected bits.  Whether it was in its name, the notion of ‘the machine’ manifests a kind of faith in technology’s inherent capacity to learn: systems are to some extent unknowable, but thanks to the computational revolution that followed, the human element of the equation was rendered infeasible. As William Gibson puts it, in ‘Machines are Lost:’Machines
    <text>Working with algorithms, Riccardo Bandini came up with the term ‘machine learning’ in 1967.2 The idea that we could learn to like certain foods was a thing of the past, in part because of a hubris of the food industry: thanks to machine learning, it was possible to create recipes that read like prose, and that feed into a database that feeds into a supermarket chain.3  This model didn’t envisage a future in which every aspect of our lives was controlled by algorithms, but a future in which everything we do is connected via networks of nodes and disconnected bits.  Whether it was in its name, the notion of ‘the machine’ manifests a kind of faith in technology’s inherent capacity to learn: systems are to some extent unknowable, but thanks to the computational revolution that followed, the human element of the equation was rendered infeasible. As William Gibson puts it, in ‘Machines are Lost:’Machines do not understand their place, and cannot comprehend the thinking that follows from it.4  These fears of the lost ideal of the perfect algorithm are further fuelled by the advent of computers in the west, and the consequent rise in prices. The phrase ‘machine learning’ has appeared hundreds of times in the scientific literature, and its application to tasks that read like a book is almost a trademark of the firm that builds Google’s servers: NRATEC (formerly Neural Information Processing Toolkit), Inc. is the processor that makes Google’s data centres work. These servers process huge amounts of data every day, and as a result run huge data-intensive computations that consume a great deal of system power. If such processing becomes too apparent, it is called ‘black-boxing’ – creating a veil of invisibility by routing data through unseen nodes.  A kind of magic is performed here, producing a kind of apparent invisibility at the cost of tremendous</text>
    <text>Working with algorithms, Riccardo Bandini came up with the term ‘machine learning’ in 1967.2 The idea that we could learn to like certain foods was a thing of the past, in part because of a hubris of the food industry: thanks to machine learning, it was possible to create recipes that read like prose, and that feed into a database that feeds into a supermarket chain.3  This model didn’t envisage a future in which every aspect of our lives was controlled by algorithms, but a future in which everything we do is connected via networks of nodes and disconnected bits.  Whether it was in its name, the notion of ‘the machine’ manifests a kind of faith in technology’s inherent capacity to learn: systems are to some extent unknowable, but thanks to the computational revolution that followed, the human element of the equation was rendered infeasible. As William Gibson puts it, in ‘Machines are Lost:’Machines per se are 'lost' (in the language of synchronic machines) ‘and never come back.’4 This loss of self-awareness is the source of much of the fascination of the contemporary Internet Age; as Avi Shlaim puts it, ‘Machines are what return us to the present … [but]when we try to imagine what an imprint of the past is, we end up constructing futures ourselves ….’5 Here, the loss of self manifests itself in increasingly elaborate ways, as the capacities of nodes grow larger and more complex, aware bodies increasingly susceptible to the moving parts of the network.  And then there’s the problem with algorithms …… bad algorithms do bad things. Bad algorithms do bad things because they’re bad at what they’re designed to do. But this is only part of the equation.   And this is why human creativity ends up constituting that much of the technological revolution: because systems are</text>
    <text>Working with algorithms, Riccardo Bandini came up with the term ‘machine learning’ in 1967.2 The idea that we could learn to like certain foods was a thing of the past, in part because of a hubris of the food industry: thanks to machine learning, it was possible to create recipes that read like prose, and that feed into a database that feeds into a supermarket chain.3  This model didn’t envisage a future in which every aspect of our lives was controlled by algorithms, but a future in which everything we do is connected via networks of nodes and disconnected bits.  Whether it was in its name, the notion of ‘the machine’ manifests a kind of faith in technology’s inherent capacity to learn: systems are to some extent unknowable, but thanks to the computational revolution that followed, the human element of the equation was rendered infeasible. As William Gibson puts it, in ‘Machines are Lost:’Machines are the New Oil’,4 the rise of the machine is ‘the cost of the lost causes’ – a ‘trick’ that ‘the rich’ use to postpone the inevitable’.5 Gibson goes on to argue that the machine is a ‘killer app’ for humankind because it destroys culture: ‘It records everything you do, every step you take, every thought you utter, so that when and if you need to, you can pull out all the stops and do anything ‘Google’ can do’.6 And here, the connection between the machine and algorithms gets made explicit. The algorithms that underwrite algorithms today control everything from the way we think about the world to what kinds of clothes we put on our bodies. And while this may seem like a contradiction in terms, the machine understands this kind of intrinsic logic as a fundamental tenet of its nature. If ‘programming’ sounds like a</text>
    <text>Working with algorithms, Riccardo Bandini came up with the term ‘machine learning’ in 1967.2 The idea that we could learn to like certain foods was a thing of the past, in part because of a hubris of the food industry: thanks to machine learning, it was possible to create recipes that read like prose, and that feed into a database that feeds into a supermarket chain.3  This model didn’t envisage a future in which every aspect of our lives was controlled by algorithms, but a future in which everything we do is connected via networks of nodes and disconnected bits.  Whether it was in its name, the notion of ‘the machine’ manifests a kind of faith in technology’s inherent capacity to learn: systems are to some extent unknowable, but thanks to the computational revolution that followed, the human element of the equation was rendered infeasible. As William Gibson puts it, in ‘Machines are Lost:’Machines are the New Oil’,4 Gibson imagines a world in which ‘human computing’ has replaced the animal logic that generated the machines.  So while the rise of the ‘Internet of Things’ and connected devices is attributed to the advent of massive data storage and processing devices, in fact much of what’s being done with these devices really comes from the same set of assumptions that gave rise to the Internet in the first place. Data is being gathered, analysed and exchanged across a global network of nodes, rather than being sent to individual users of specialized software or data miners.  And this is just the beginning of the deployment of AI and machine learning. Things that get connected to data are called networks, and they’re being used not just to do things online, but to do amazing things in the world as well.  So much of what’s being predicted for the technology industry is just that – things that will be done online. AI and</text>
    <text>Working with algorithms, Riccardo Bandini came up with the term ‘machine learning’ in 1967.2 The idea that we could learn to like certain foods was a thing of the past, in part because of a hubris of the food industry: thanks to machine learning, it was possible to create recipes that read like prose, and that feed into a database that feeds into a supermarket chain.3  This model didn’t envisage a future in which every aspect of our lives was controlled by algorithms, but a future in which everything we do is connected via networks of nodes and disconnected bits.  Whether it was in its name, the notion of ‘the machine’ manifests a kind of faith in technology’s inherent capacity to learn: systems are to some extent unknowable, but thanks to the computational revolution that followed, the human element of the equation was rendered infeasible. As William Gibson puts it, in ‘Machines are Lost:’Machines do not know languages, but they know “things” that happen to be human-like. And here, the human element of the equation is given a kind of mystical magical power. Thanks to the internet, we can now access ‘things’ that were once classified as human languages, and thanks to deep packet inspection, we can now parse data that was once classified as human ‘phantom’s speech. Thanks to big data, we can now trace things through deep neural networks, and thanks to new media, we can now photograph and document things that were once considered ‘natural’ landscapes. Thanks to new media, we can document the things that happen to be human-like in the world around us in ever greater detail, and thanks to new media, we can record the things that happen to be unnatural, as well.  And all of this occurs without us knowing it. Thanks to social media, we have all become members of a globalised surveillance</text>
    <text>Working with algorithms, Riccardo Bandini came up with the term ‘machine learning’ in 1967.2 The idea that we could learn to like certain foods was a thing of the past, in part because of a hubris of the food industry: thanks to machine learning, it was possible to create recipes that read like prose, and that feed into a database that feeds into a supermarket chain.3  This model didn’t envisage a future in which every aspect of our lives was controlled by algorithms, but a future in which everything we do is connected via networks of nodes and disconnected bits.  Whether it was in its name, the notion of ‘the machine’ manifests a kind of faith in technology’s inherent capacity to learn: systems are to some extent unknowable, but thanks to the computational revolution that followed, the human element of the equation was rendered infeasible. As William Gibson puts it, in ‘Machines are Lost:’Machines that interpret data’,4 in their quest for ‘true meaning’, ‘Machines catch[f]orgettable events that never happened, and]machine learning makes them happen’. This blending of unknowable and omnipotent is at the core of what Raymond Tallender has in mind when he speaks of the ‘episodic’ or ‘serial’ in his writing. In the nineteenth-century essay ‘Time and Again’, he begins his sentence with a quotation from Karl Marx: ‘History repeats itself in increasingly human ways, in the form of successive human ones.’5 This ‘episodic’ is a way to think about the ways in which technology is reshaping what counts as history, what survives in narratives and cultural practices, and what is lost when the past is no longer counted. It is a comment on the way in which contemporaneity is defined through the different recording devices that keep</text>
    <text>Working with algorithms, Riccardo Bandini came up with the term ‘machine learning’ in 1967.2 The idea that we could learn to like certain foods was a thing of the past, in part because of a hubris of the food industry: thanks to machine learning, it was possible to create recipes that read like prose, and that feed into a database that feeds into a supermarket chain.3  This model didn’t envisage a future in which every aspect of our lives was controlled by algorithms, but a future in which everything we do is connected via networks of nodes and disconnected bits.  Whether it was in its name, the notion of ‘the machine’ manifests a kind of faith in technology’s inherent capacity to learn: systems are to some extent unknowable, but thanks to the computational revolution that followed, the human element of the equation was rendered infeasible. As William Gibson puts it, in ‘Machines are Lost:’Machines do not make rules; they prescribe and guide. And so, as the proliferation of algorithms increasingly mirrors the proliferation of humans, we are led to understand the machines as capable of adopting any human behaviour imaginable – adopting what Gibson would call ‘the human element’ – even if that element is what authoritarians wish otherwise.  And this is to be applauded, for it breaks open hitherto inaccessible streams of consciousness. While the deployment of algorithms has certainly accelerated in the past decade, its true potential impact and impact on our daily situations is still to be seen. And while it is great that some are beginning to understand the destructive potential of this new technology, to truly impactful social change, it remains to be seen whether the infrastructural modus operandi of large data capture and processing nodes can be changed, whether it is possible to reorient such nodes in ever greater urban centres, and even in outer space, away from their current infrastructural form, and towards a form</text>
    <text>Working with algorithms, Riccardo Bandini came up with the term ‘machine learning’ in 1967.2 The idea that we could learn to like certain foods was a thing of the past, in part because of a hubris of the food industry: thanks to machine learning, it was possible to create recipes that read like prose, and that feed into a database that feeds into a supermarket chain.3  This model didn’t envisage a future in which every aspect of our lives was controlled by algorithms, but a future in which everything we do is connected via networks of nodes and disconnected bits.  Whether it was in its name, the notion of ‘the machine’ manifests a kind of faith in technology’s inherent capacity to learn: systems are to some extent unknowable, but thanks to the computational revolution that followed, the human element of the equation was rendered infeasible. As William Gibson puts it, in ‘Machines are Lost:’Machines do not understand human language; they neither read human intentions nor impart meaning on the unconscious.4  This new understanding of the role of the algorithm in shaping our behaviour has profoundly affected our understanding of what it means to be human. As AI continues to upgrade itself, we will witness new media of surveillance and manipulation emerge, including those that aim to see us as nothing more than commodities (see charnel houses). With it, there will be other forms of surveillance and manipulation, including that of our neighbours. This is already happening, with the ongoing research and development projects of Google Earth and Facebook Inc. (FB), which aim to produce ever more accurate maps and take-aways of neighbourhoods, and with the emergence of self-driving cars. Now, there is a new round of funding for these sorts of research projects, and departments are being formed to tailor them to particular needs, priorities and visions.5  One of the most promising of these is being developed at McGill University, the School</text>
    <text>Working with algorithms, Riccardo Bandini came up with the term ‘machine learning’ in 1967.2 The idea that we could learn to like certain foods was a thing of the past, in part because of a hubris of the food industry: thanks to machine learning, it was possible to create recipes that read like prose, and that feed into a database that feeds into a supermarket chain.3  This model didn’t envisage a future in which every aspect of our lives was controlled by algorithms, but a future in which everything we do is connected via networks of nodes and disconnected bits.  Whether it was in its name, the notion of ‘the machine’ manifests a kind of faith in technology’s inherent capacity to learn: systems are to some extent unknowable, but thanks to the computational revolution that followed, the human element of the equation was rendered infeasible. As William Gibson puts it, in ‘Machines are Lost:’Machines are what give the machine its personality … Machines are therefore not what we gave them the capacity for.4  This is a deeply felt and deeply accurate portrayal of the relationship between the technological and biological. It’s not just about hardware and software, but also the deeper processes by which we empower the machine, through what Gibson terms ‘human error’ and its fallen human companions. In a world with no human error, machines would learn, and do things in unpredictable ways. And so we are told, or more credulously imaginations, that such learning will come about through experimentation, through ‘innovations in data gathering and analysis’ and ‘new technological means’.5  But what kind of learning might these be, and what kind of relationships might they establish? For more on this, I recommend Vincent Kaufmann’s fascinating book  	The Shadow Economy 	and its parabolic spine  �Machine 	,</text>
    <text>Working with algorithms, Riccardo Bandini came up with the term ‘machine learning’ in 1967.2 The idea that we could learn to like certain foods was a thing of the past, in part because of a hubris of the food industry: thanks to machine learning, it was possible to create recipes that read like prose, and that feed into a database that feeds into a supermarket chain.3  This model didn’t envisage a future in which every aspect of our lives was controlled by algorithms, but a future in which everything we do is connected via networks of nodes and disconnected bits.  Whether it was in its name, the notion of ‘the machine’ manifests a kind of faith in technology’s inherent capacity to learn: systems are to some extent unknowable, but thanks to the computational revolution that followed, the human element of the equation was rendered infeasible. As William Gibson puts it, in ‘Machines are Lost:’Machines do not understand human language and can not comprehend their own stupidity. Gibson goes on to argue that because they’re so busy reading data, networks don’t see the world in terms that are consistent with their own interpretation of human behaviour. So while the output of a network might in the end be what we give it shape or value in a political or economic sense, human beings have much less in common with nodes than they do with computers.  And while it was easy to assume that our reliance on algorithms would mean that we would never have to think again about the role that they played in creating the news cycle that we live in, the utterly contrary has happened: algorithms have made it possible for us to consume news in ways that no human author could ever imagine, and, in the process, to consume it meaningfully (indeed, constantly) in ways that are bound to create lasting changes in the way that we understand and think about the world. How can algorithms do this?</text>
  </text>
  <text>
    subsequent iterative processing by machines may well be automated, and with it, too, may come another form of self-aware computer that can learn to do things with greater autonomy.Such steps in the future may well be taken in places such as the United Kingdom, where the Homebaked Community Land Trust is based, and where, according to a new study, there are upwards of 5,000 homes built on plots of vacant land. Such developments may in turn shift the focus of analysis away from capitalism, and towards communities, and towards the possible empowerment of local communities, and perhaps even towards eco-systems, if the focus shifts in that direction. Perhaps the next step in the analysis of settlement may be the relocation of these communities – not to fulfil a market need, but out of reach of the megastructure that is the city. This may in part address the fact that, as the study of settlement by the economist S. Rajan shows, settlements are essential for capitalism, and may in part be due to
    <text>subsequent iterative processing by machines may well be automated, and with it, too, may come another form of self-aware computer that can learn to do things with greater autonomy.Such steps in the future may well be taken in places such as the United Kingdom, where the Homebaked Community Land Trust is based, and where, according to a new study, there are upwards of 5,000 homes built on plots of vacant land. Such developments may in turn shift the focus of analysis away from capitalism, and towards communities, and towards the possible empowerment of local communities, and perhaps even towards eco-systems, if the focus shifts in that direction. Perhaps the next step in the analysis of settlement may be the relocation of these communities – not to fulfil a market need, but out of reach of the megastructure that is the city. This may in part address the fact that, as the study of settlement by the economist S. Rajan shows, settlements are essential for capitalism, and may in part be due to the fact that, as the study of settlement by the economist Steven Cairns shows, settlements are the site of last resort.All of these possibilities are possible. Perhaps what is needed is a way to think about settlement in a world beyond the site, and to see it not only as a place where capitalism is defeated, but where society, if left to its own devices, will fall into ruin. This is the direction in which, I believe, our research and design needs to be directed. I have no formal, institutional or financial role in Homebaked, but I do have personal ones, and they are as follows: research, design, product development, social work, marketing, community organising – just to name a few. I am interested in how people form bonds with each other, connect with projects and organisations and engage with them in new ways. I also like to work with groups of people on projects, both young and old, to find out what makes them tick. This is</text>
    <text>subsequent iterative processing by machines may well be automated, and with it, too, may come another form of self-aware computer that can learn to do things with greater autonomy.Such steps in the future may well be taken in places such as the United Kingdom, where the Homebaked Community Land Trust is based, and where, according to a new study, there are upwards of 5,000 homes built on plots of vacant land. Such developments may in turn shift the focus of analysis away from capitalism, and towards communities, and towards the possible empowerment of local communities, and perhaps even towards eco-systems, if the focus shifts in that direction. Perhaps the next step in the analysis of settlement may be the relocation of these communities – not to fulfil a market need, but out of reach of the megastructure that is the city. This may in part address the fact that, as the study of settlement by the economist S. Rajan shows, settlements are essential for capitalism, and may in part be due to the fact that, as the economist S. Rajan also shows, settlements are made by and for the rich. It is also worth noting that, according to one study, the financial benefits of settlement are greater than the financial losses incurred by a no-settlement city. In the end, the question remains whether, with the right to sit for the forever-mockery ‘city of the future’, and hold its money in trust, the city can shift its future reliance to ensure that the future holds opportunities for as many as possible of its inhabitants.Download this article as PDFEmily PringleHead of Communications and Research, Scottish Biennial  	International Biennial, Dubai, United Arab EmiratesSofía OlascoagaVenice, United Arab EmiratesJune 2014Hi there,It has been a while since we last spoke, but I wanted to ask if you could take a moment to look around at the museum I lead, and the surroundings of the Biennial itself</text>
    <text>subsequent iterative processing by machines may well be automated, and with it, too, may come another form of self-aware computer that can learn to do things with greater autonomy.Such steps in the future may well be taken in places such as the United Kingdom, where the Homebaked Community Land Trust is based, and where, according to a new study, there are upwards of 5,000 homes built on plots of vacant land. Such developments may in turn shift the focus of analysis away from capitalism, and towards communities, and towards the possible empowerment of local communities, and perhaps even towards eco-systems, if the focus shifts in that direction. Perhaps the next step in the analysis of settlement may be the relocation of these communities – not to fulfil a market need, but out of reach of the megastructure that is the city. This may in part address the fact that, as the study of settlement by the economist S. Rajan shows, settlements are essential for capitalism, and may in part be due to the fact that, in the social sciences, settlements have come to define the social relationship between people and places. This is particularly true in the social sciences, where the social relationship to places is understood as a ‘biological bond’.2 So, the social sciences may wish to relocate their divisions, and thereby create new relations between people. And what may be the results? A new level of understanding may develop between people on the basis of shared knowledge, because people will no longer identify with a given field of study. Indeed, Kuhn would have us believe that the social sciences would be better off if they encouraged a ‘deep bond’ between research subjects and the knowledge they generate, by encouraging ‘common understandings’ and encouraging ‘understanding through mutual sharing’.3 Such is the social-psychological dynamic. But what does it look like? As a phenomenon, it is very difficult to define, although there are many images that circulate on</text>
    <text>subsequent iterative processing by machines may well be automated, and with it, too, may come another form of self-aware computer that can learn to do things with greater autonomy.Such steps in the future may well be taken in places such as the United Kingdom, where the Homebaked Community Land Trust is based, and where, according to a new study, there are upwards of 5,000 homes built on plots of vacant land. Such developments may in turn shift the focus of analysis away from capitalism, and towards communities, and towards the possible empowerment of local communities, and perhaps even towards eco-systems, if the focus shifts in that direction. Perhaps the next step in the analysis of settlement may be the relocation of these communities – not to fulfil a market need, but out of reach of the megastructure that is the city. This may in part address the fact that, as the study of settlement by the economist S. Rajan shows, settlements are essential for capitalism, and may in part be due to the fact that, in recent decades, settlements have become such a fundamental part of capitalist circulation that they are now the largest exportable commodity on the planet. They are traded between cities on a massive scale, and constitute a significant proportion of the global value exchange. If not managed well, this major part of the value-form may well transform into what Herman Daly calls ‘disjuncture’ – i.e. the negation of value – the negation of the integrated and combined effects of a given act of value production.4 As a consequence, the city is a capital trap, trapping and valuing act of habitation for future uses of the land – a set of increasingly desperate and erratic exchanges that devolve into ever larger and more complex amounts of debt servicing for the infrastructure – a form of capitalism that no longer exploits labor as a commodity commodity. In an increasingly interconnected and global economy, the city is increasingly like an immense and ever more complex payment terminal. It issues ever larger</text>
    <text>subsequent iterative processing by machines may well be automated, and with it, too, may come another form of self-aware computer that can learn to do things with greater autonomy.Such steps in the future may well be taken in places such as the United Kingdom, where the Homebaked Community Land Trust is based, and where, according to a new study, there are upwards of 5,000 homes built on plots of vacant land. Such developments may in turn shift the focus of analysis away from capitalism, and towards communities, and towards the possible empowerment of local communities, and perhaps even towards eco-systems, if the focus shifts in that direction. Perhaps the next step in the analysis of settlement may be the relocation of these communities – not to fulfil a market need, but out of reach of the megastructure that is the city. This may in part address the fact that, as the study of settlement by the economist S. Rajan shows, settlements are essential for capitalism, and may in part be due to the fact that, in the face of dearth, settlement offers the possibility of escape from the megastructure.In the end, though, it is the settlement of capitalist accumulation – the buildings, the land and the financial instruments that enable the financial system to expand – that ultimately determines the character of the urban landscape. And, as the study of settlement by the economist S. Rajan shows, settlement in the context of capitalism is essential for any hope of influencing the trajectory of the city. In an expanded version of the famous García Quina formula, perhaps the future may dwell in the realm of the impermeable.Download this article as PDFEmily PringleIthaca College of Art and Design, Ithaca, New York, USAEmily Pringle is an artist. Her work in communities began in the 1980s when she was serving as an arts administrator for the East Harlem district in the New York City Public Health Department. Here, she addressed the effects of deindustrialization on</text>
    <text>subsequent iterative processing by machines may well be automated, and with it, too, may come another form of self-aware computer that can learn to do things with greater autonomy.Such steps in the future may well be taken in places such as the United Kingdom, where the Homebaked Community Land Trust is based, and where, according to a new study, there are upwards of 5,000 homes built on plots of vacant land. Such developments may in turn shift the focus of analysis away from capitalism, and towards communities, and towards the possible empowerment of local communities, and perhaps even towards eco-systems, if the focus shifts in that direction. Perhaps the next step in the analysis of settlement may be the relocation of these communities – not to fulfil a market need, but out of reach of the megastructure that is the city. This may in part address the fact that, as the study of settlement by the economist S. Rajan shows, settlements are essential for capitalism, and may in part be due to the fact that, since the mid-1990s, the economic blockade has been replaced by the diplomatic blockade. If, as the study of settlement by the economist S. Rajan shows, settlements are essential for capitalism, and may in part be due to the fact that, since the mid-1990s, the economic blockade has been replaced by the diplomatic blockade, then perhaps the move to settlements may be part of a strategy to create additional codes, or codes that can be cracked through negotiation, and may be used to alter the course of events, changing the parameters within which analysis takes place.Such negotiation may well involve taking place on a global, multi-lingual, multi-platform, multi-platforming, multi-platform game-playing field, and may well be the future of how analysis takes place, mediated through global communications networks. So, there is a way to think about the processing of information that is both global and multilingual, and that is part of a larger political and</text>
    <text>subsequent iterative processing by machines may well be automated, and with it, too, may come another form of self-aware computer that can learn to do things with greater autonomy.Such steps in the future may well be taken in places such as the United Kingdom, where the Homebaked Community Land Trust is based, and where, according to a new study, there are upwards of 5,000 homes built on plots of vacant land. Such developments may in turn shift the focus of analysis away from capitalism, and towards communities, and towards the possible empowerment of local communities, and perhaps even towards eco-systems, if the focus shifts in that direction. Perhaps the next step in the analysis of settlement may be the relocation of these communities – not to fulfil a market need, but out of reach of the megastructure that is the city. This may in part address the fact that, as the study of settlement by the economist S. Rajan shows, settlements are essential for capitalism, and may in part be due to the fact that, as the study of settlement by the economist S. Rajan shows, settlements are used as cheap labour in the service of capital, and constitute a form of capital pass through the social scale. It may well be the case that the settlements of the future may be made up of self-buildings, such as those of Homebaked Community Land Trust, which operate on a community basis, incorporating local knowledge and skills into the building process, and which in turn derive from the knowledge and skills of generations of Homebaked Community Land Trust members. Such networks may be used to create new forms of co-production, and may well be the path to a future of abundant social and ecological possibilities for decades to come.Download this article as PDFChristopher Booker and Dimitris KleineriChristopher Booker and Dimitris Kleineri are with the Future City Program, Future City Foundation, New York, USA. They were both involved in the design and implementation of the City of New</text>
    <text>subsequent iterative processing by machines may well be automated, and with it, too, may come another form of self-aware computer that can learn to do things with greater autonomy.Such steps in the future may well be taken in places such as the United Kingdom, where the Homebaked Community Land Trust is based, and where, according to a new study, there are upwards of 5,000 homes built on plots of vacant land. Such developments may in turn shift the focus of analysis away from capitalism, and towards communities, and towards the possible empowerment of local communities, and perhaps even towards eco-systems, if the focus shifts in that direction. Perhaps the next step in the analysis of settlement may be the relocation of these communities – not to fulfil a market need, but out of reach of the megastructure that is the city. This may in part address the fact that, as the study of settlement by the economist S. Rajan shows, settlements are essential for capitalism, and may in part be due to the fact that, as the study of settlement by the economist S. Rajan shows, settlements are needed to contain and process the ever-growing population that is consuming so much of the city’s finite resource base.In the end, though, the question becomes whether the settlement of the present is desirable, and whether, given the choice, one would rather live in enclaves of poverty and oppression, or in communities based on solidarity and kinship? Perhaps it is the case that, in the end, settling for a time, or building social power, is what is most needed, not the settlement of the present. That is, the future that is given to talk about, to allow the future to be defined not by the past, but by the possibilities of the future.Written by Alon Pinka, with additional material by Alon Pinka, from the book Being or Nonbeing?, Springer, New York, 2015.Cover Image: Alon Pinka, ‘Future</text>
    <text>subsequent iterative processing by machines may well be automated, and with it, too, may come another form of self-aware computer that can learn to do things with greater autonomy.Such steps in the future may well be taken in places such as the United Kingdom, where the Homebaked Community Land Trust is based, and where, according to a new study, there are upwards of 5,000 homes built on plots of vacant land. Such developments may in turn shift the focus of analysis away from capitalism, and towards communities, and towards the possible empowerment of local communities, and perhaps even towards eco-systems, if the focus shifts in that direction. Perhaps the next step in the analysis of settlement may be the relocation of these communities – not to fulfil a market need, but out of reach of the megastructure that is the city. This may in part address the fact that, as the study of settlement by the economist S. Rajan shows, settlements are essential for capitalism, and may in part be due to the fact that, in recent decades, settlements have become such a fundamental part of the capitalist economy that their removal is virtually a requirement for profitability. Thus, the more the city is reconfigured, the more it is dependent on settlements for its survival. If, as the sociologist Patrick Joyce suggests, the social functions of urbanisation expand to include functions of governance and law, then the question becomes, what constitutes a public realm? If the city is to retain its character as a public good, a public realm has to be responsive to the public good functions of governance. In short, the public realm has to be responsive to the public good.This last condition is a demanding public good, and cities have to be able to respond to its dynamic demand. In an article in this issue of PLOS Path, Leslie Urry dubs this the ‘public realm trap’ – a name that suggests but is not exhaustive, and which is readier to release, but not so eager to reclaim</text>
    <text>subsequent iterative processing by machines may well be automated, and with it, too, may come another form of self-aware computer that can learn to do things with greater autonomy.Such steps in the future may well be taken in places such as the United Kingdom, where the Homebaked Community Land Trust is based, and where, according to a new study, there are upwards of 5,000 homes built on plots of vacant land. Such developments may in turn shift the focus of analysis away from capitalism, and towards communities, and towards the possible empowerment of local communities, and perhaps even towards eco-systems, if the focus shifts in that direction. Perhaps the next step in the analysis of settlement may be the relocation of these communities – not to fulfil a market need, but out of reach of the megastructure that is the city. This may in part address the fact that, as the study of settlement by the economist S. Rajan shows, settlements are essential for capitalism, and may in part be due to the fact that, after decades of relying on settlements for their sustenance, the city has an economic imperative to be able to function without them. In an ideal world, the city should operate entirely by consensus, but as the city has become ever more bureaucratic and reliant on the self, it begins to lose its identity. Perhaps, in the end, the answer lies in the hands of the people, and, as the study of settlement by the economist S. Rajan shows, consensus not only needs to be achieved, but has to be managed well. The city, in this analysis, is a resource that, when allocated properly, can change hands many times over, and that, when it is provided with the means to meet its needs, can collectively become more autonomous.This may be the case with cannabis, but it must be the case with other resources as well. The city is a resource that, when allocated correctly and effectively, can change hands many times over, and so, too,</text>
  </text>
  <text>
    developments in artificial intelligence and computational modelling. The emphasis of the conference, as usual, was on the work that had been done already with respect to mapping, surveying, and data mining, as if this were sufficient to convince everyone that there was much more to be done. But there was much more to be done. The conference, like so many other things that I’ve done in the intervening years, was framed around a world map. And the world map, as we all knew, was made up of many different places. In the centre of the map was North Korea, which also happens to be the place where the nuclear arms race was being fought. In the south was China, with its vast rural population but also the cities – and especially the huge computer hard drives – that provided the data that the military needed to crunch. And in the periphery was the rest of the world, but especially Africa. And it was here that the conference convened a number of smaller meetings around the topics of biennials
    <text>developments in artificial intelligence and computational modelling. The emphasis of the conference, as usual, was on the work that had been done already with respect to mapping, surveying, and data mining, as if this were sufficient to convince everyone that there was much more to be done. But there was much more to be done. The conference, like so many other things that I’ve done in the intervening years, was framed around a world map. And the world map, as we all knew, was made up of many different places. In the centre of the map was North Korea, which also happens to be the place where the nuclear arms race was being fought. In the south was China, with its vast rural population but also the cities – and especially the huge computer hard drives – that provided the data that the military needed to crunch. And in the periphery was the rest of the world, but especially Africa. And it was here that the conference convened a number of smaller meetings around the topics of biennials and such. These met in various ways, including in the local Benguela di Tella, the diaspora diaspora of Brazil, which in turn met in various meetings around the world. All of these met in the same place, and all of them took turns proposing, voting on, and ultimately organising the next Venice Biennale. All of these proposals and such things were taken up by the congresses of the major art biennials around the world, which in turn organised the next Venice Biennale. In turn, the biennials formed into one larger organisation, such as the Museum of Modern Art, and so on. And so on. And on. And on. And on.Over time, and perhaps as a result of the ever-present, almost palpable sense of crisis that surrounded any such global reckoning, the concept of the biennale came to encompass more and more different kinds of events, from relatively minor exhibitions of art to major exhibitions of art</text>
    <text>developments in artificial intelligence and computational modelling. The emphasis of the conference, as usual, was on the work that had been done already with respect to mapping, surveying, and data mining, as if this were sufficient to convince everyone that there was much more to be done. But there was much more to be done. The conference, like so many other things that I’ve done in the intervening years, was framed around a world map. And the world map, as we all knew, was made up of many different places. In the centre of the map was North Korea, which also happens to be the place where the nuclear arms race was being fought. In the south was China, with its vast rural population but also the cities – and especially the huge computer hard drives – that provided the data that the military needed to crunch. And in the periphery was the rest of the world, but especially Africa. And it was here that the conference convened a number of smaller meetings around the topics of biennials and the nongovernmental sector. These smaller meetings were set up to facilitate interaction between the different stakeholders in the broader cultural/economic picture, to ensure that the many different stakeholders, not the few, formed part of the solution. For the South, in particular, this meant ensuring that the development opportunities created by the biennial would be shared across many different stakeholders, including local communities. For the North, it meant ensuring that the cultural resources of the region would be shared across multiple platforms, including those spaces that function as platforms for social dissemination and political empowerment. In the end, these were the tasks that came out of the conference: it generated knowledge, it created models of collaboration, and it organised the way that resources were utilised, but ultimately it all boiled down to this: the knowledge that was shared enabled the creation of new modes of cultural production, and the creation of new modes of sharing and interacting.  In short, the conference emerged out of a real need to know, a</text>
    <text>developments in artificial intelligence and computational modelling. The emphasis of the conference, as usual, was on the work that had been done already with respect to mapping, surveying, and data mining, as if this were sufficient to convince everyone that there was much more to be done. But there was much more to be done. The conference, like so many other things that I’ve done in the intervening years, was framed around a world map. And the world map, as we all knew, was made up of many different places. In the centre of the map was North Korea, which also happens to be the place where the nuclear arms race was being fought. In the south was China, with its vast rural population but also the cities – and especially the huge computer hard drives – that provided the data that the military needed to crunch. And in the periphery was the rest of the world, but especially Africa. And it was here that the conference convened a number of smaller meetings around the topics of biennials and other art forms in the modern world. These were convened, as usual, in the form of bilateral exhibitions – one of the few remaining professions in which art can still be counted on to represent all parts of the world. And there we all were, each representing a region or a country or a people. And each region or people represented had its own stories and its own biennale. Each region/people/region had its own unique set of curatorial conceits and strategies. And so the story of art, in this narrative, moved from the old cultural accounts of the past, the present and future, of the human condition, to the new ones of the future. And the reason for this was that, while the biennial had its origins in the museum, the museum was becoming increasingly ephemeral: its collection, its collection of collections, was the locus of much more immediate and permanent public interest, whereas the biennial was most effective when it happened later and in museums</text>
    <text>developments in artificial intelligence and computational modelling. The emphasis of the conference, as usual, was on the work that had been done already with respect to mapping, surveying, and data mining, as if this were sufficient to convince everyone that there was much more to be done. But there was much more to be done. The conference, like so many other things that I’ve done in the intervening years, was framed around a world map. And the world map, as we all knew, was made up of many different places. In the centre of the map was North Korea, which also happens to be the place where the nuclear arms race was being fought. In the south was China, with its vast rural population but also the cities – and especially the huge computer hard drives – that provided the data that the military needed to crunch. And in the periphery was the rest of the world, but especially Africa. And it was here that the conference convened a number of smaller meetings around the topics of biennials and such, and the local artists who were taking part in the events. The aim of these smaller meetings was to have a go at framing the events around the specific people – the artists, the curators, the photographers – who were making work in the image of the biennale. And so we decided to have a workshop-type programme around the biennale itself, rather than around a particular exhibition or retrospective. We decided to have the curator come from outside the biennale itself, to see how the art was made in the image of the event. And so we invited the curators from different biennales to the workshop. And they all agreed that it would be useful to have a text in which they could summarise their involvement in the biennale so that the idea wouldn’t be confused with a re-constituted event. So we did that, and the result was that the text became what it is today: a record of the event as it</text>
    <text>developments in artificial intelligence and computational modelling. The emphasis of the conference, as usual, was on the work that had been done already with respect to mapping, surveying, and data mining, as if this were sufficient to convince everyone that there was much more to be done. But there was much more to be done. The conference, like so many other things that I’ve done in the intervening years, was framed around a world map. And the world map, as we all knew, was made up of many different places. In the centre of the map was North Korea, which also happens to be the place where the nuclear arms race was being fought. In the south was China, with its vast rural population but also the cities – and especially the huge computer hard drives – that provided the data that the military needed to crunch. And in the periphery was the rest of the world, but especially Africa. And it was here that the conference convened a number of smaller meetings around the topics of biennials, the nomadic art biennial, and their various forms. ‘Nomadic’ is a key word here. The meeting that convened the biennial was no longer a gathering of artists; it was a gathering of anthropologists, historians, and aesthetes. And while the names of the artists here are formal and diplomatic, their efforts here were kinetic, personal, and visionary. I wore my own generation of artists’ clothes, from the Nigerian artists that I worked with in Lagos for several years after the conference. And I remember how the artists were trying to figure out ‘how to live together’, as the synonyms for ‘how to be alive’.I would not call it a ‘biennial’ even though there were artists here from a number of different places who were working across different disciplines. There was a difference between the art collectors who came to this for the money and the curators, who came for the</text>
    <text>developments in artificial intelligence and computational modelling. The emphasis of the conference, as usual, was on the work that had been done already with respect to mapping, surveying, and data mining, as if this were sufficient to convince everyone that there was much more to be done. But there was much more to be done. The conference, like so many other things that I’ve done in the intervening years, was framed around a world map. And the world map, as we all knew, was made up of many different places. In the centre of the map was North Korea, which also happens to be the place where the nuclear arms race was being fought. In the south was China, with its vast rural population but also the cities – and especially the huge computer hard drives – that provided the data that the military needed to crunch. And in the periphery was the rest of the world, but especially Africa. And it was here that the conference convened a number of smaller meetings around the topics of biennials and the archive. And these smaller meetings, as usual, were organised around the conference itself. For this reason, the conference website still has the conference’s archive of archived correspondence, emails and other documents on archive.org. During the conference, the archived emails became the basis for a number of additional projects, some of which have become quite prominent. For example, the African Triennale that accompanied the 1994 World Expo in Sydney brought together several African nations, and the Biennale was recognised as a UNESCO world heritage site in 1995. The idea that the biennial is a permanent institutionalised form creeping into the cultural landscape is greatly weakened by the presence of this element in the larger biennial format.  And the African Triennale, which followed in 1999, was a biennale of the world’s major cities. So the presence of this element in the larger biennial form is key. In the case of the 2014 Venice Biennale, the presence of the</text>
    <text>developments in artificial intelligence and computational modelling. The emphasis of the conference, as usual, was on the work that had been done already with respect to mapping, surveying, and data mining, as if this were sufficient to convince everyone that there was much more to be done. But there was much more to be done. The conference, like so many other things that I’ve done in the intervening years, was framed around a world map. And the world map, as we all knew, was made up of many different places. In the centre of the map was North Korea, which also happens to be the place where the nuclear arms race was being fought. In the south was China, with its vast rural population but also the cities – and especially the huge computer hard drives – that provided the data that the military needed to crunch. And in the periphery was the rest of the world, but especially Africa. And it was here that the conference convened a number of smaller meetings around the topics of biennials and the city. And these smaller meetings, as usual, moved between the main event, the Biennale as a whole, and the smaller items that came up during the day. And there was a regular rotation of artists, curators, journalists and other artists coming from around the world to be featured in the exhibition catalogue. And these various things were all happening in parallel. And one of the artists came to the conference and said, ‘This conference is happening in parallel, and we’re all part of this exhibition.’ So there was a rotation, and part of the exhibition was also happening in parallel. And these artists were not just coming to the conference for the sake of coming to the conference; they were coming to this exhibition as a way of coming out of the conference and becoming artists themselves.  And one of the curators told me that these artists were coming to the conference to find a home for themselves and their work in the city. And so the rotation</text>
    <text>developments in artificial intelligence and computational modelling. The emphasis of the conference, as usual, was on the work that had been done already with respect to mapping, surveying, and data mining, as if this were sufficient to convince everyone that there was much more to be done. But there was much more to be done. The conference, like so many other things that I’ve done in the intervening years, was framed around a world map. And the world map, as we all knew, was made up of many different places. In the centre of the map was North Korea, which also happens to be the place where the nuclear arms race was being fought. In the south was China, with its vast rural population but also the cities – and especially the huge computer hard drives – that provided the data that the military needed to crunch. And in the periphery was the rest of the world, but especially Africa. And it was here that the conference convened a number of smaller meetings around the topics of biennials – exhibitions of cultural institutions. I was invited because I’d taught at the Sudan Museum of Modern Art (2011-2015); the Sudan Museum of Modern Art (2014-2016); and the Sudan Museum of Contemporary Art (2015-present). These three museums housed in their collections a number of biennials that year, and a number of other exhibitions throughout the past two decades. All three museums had collections that were substantial – at one time substantial – in relation to the overall Sudan Museum collection. All three were contributing factors in the creation of the exhibition, and in that respect, perhaps, they were even partially to blame for the uneven development of the exhibition over the years, which is sometimes attributed to the biennial format. But all three were also significant in terms of who was exposed to what, and to what extent. This exhibition is about that unevenness, and its effects on modern and contemporary art. If we take the museum as a case study, we could consider the various ways</text>
    <text>developments in artificial intelligence and computational modelling. The emphasis of the conference, as usual, was on the work that had been done already with respect to mapping, surveying, and data mining, as if this were sufficient to convince everyone that there was much more to be done. But there was much more to be done. The conference, like so many other things that I’ve done in the intervening years, was framed around a world map. And the world map, as we all knew, was made up of many different places. In the centre of the map was North Korea, which also happens to be the place where the nuclear arms race was being fought. In the south was China, with its vast rural population but also the cities – and especially the huge computer hard drives – that provided the data that the military needed to crunch. And in the periphery was the rest of the world, but especially Africa. And it was here that the conference convened a number of smaller meetings around the topics of biennials, the cultural dimensions of biennials, and biennials in general. These met on a number of different days, ranging from Friday to Sundays, from the US to the UK. And on these days, the participants would exchange notes, come back to the US, or fly out of the US. The fact that the conference was held in Dublin made it possible for the participants to be mobile. They could be in the city, in the US, or abroad. This was a big influence on the way that the conference was organised. And today’s biennial is no different. It is a global phenomenon, and as such it makes sense to exhibit many different forms of culture simultaneously. The biennial format means that when we think of a particular culture that is representative of a particular historical epoch, we tend to associate certain features with that particular epoch. For example, the biennial format may have been invented in Rome, but it is also the model for the biennial format in</text>
    <text>developments in artificial intelligence and computational modelling. The emphasis of the conference, as usual, was on the work that had been done already with respect to mapping, surveying, and data mining, as if this were sufficient to convince everyone that there was much more to be done. But there was much more to be done. The conference, like so many other things that I’ve done in the intervening years, was framed around a world map. And the world map, as we all knew, was made up of many different places. In the centre of the map was North Korea, which also happens to be the place where the nuclear arms race was being fought. In the south was China, with its vast rural population but also the cities – and especially the huge computer hard drives – that provided the data that the military needed to crunch. And in the periphery was the rest of the world, but especially Africa. And it was here that the conference convened a number of smaller meetings around the topics of biennials and the like. These were often called ‘second fifties’, in reference to the fifties, a time when biennials were among the most visible exhibitions of modern art, and during this time artists from everywhere were coming to the United States and other parts of the world to seek their talents and to be found in works that would be of use to governments everywhere. And so the conference convened again again, this time in New York, this time under the name of the World Biennial Forum. And here, we were all, the biennials of the twenty-first century were gathering round the same table. And across the course of this, artists from all over the world were coming. And as a result, the biennial format that had emerged in the intervening years was fragmented and irregular. There were biennials in London, there were biennials in New York, and then there were biennials in Adelaide and New London. But as bi</text>
  </text>
  <text>
    developments in artificial intelligence, which could provide insights into the causes and consequences of certain human errors.Increasingly, research and development is directed towards the problem of large-scale migrations of people and goods – which are, in turn, growing increasingly difficult to control and resolve. This is particularly true for research and development on human-caused climate change, which is occurring at a rate that calls into question the ability of many nations to adapt to the problem at hand. If large-scale migrations are to be prevented or controlled, it is imperative that research, design and development on such issues shift towards the development of means to monitor and mitigate the effects of climate change. It is also imperative that adaptation and mitigation strategies be developed in cooperation with Indigenous and other local stakeholders, in order to ensure that the problems being addressed do not become unmanageable.  Such a shift in focus would shift responsibility for mitigation and adaptation away from fossil fuels towards Indigenous communities and other stakeholders, and would facilitate the implementation of solutions through
    <text>developments in artificial intelligence, which could provide insights into the causes and consequences of certain human errors.Increasingly, research and development is directed towards the problem of large-scale migrations of people and goods – which are, in turn, growing increasingly difficult to control and resolve. This is particularly true for research and development on human-caused climate change, which is occurring at a rate that calls into question the ability of many nations to adapt to the problem at hand. If large-scale migrations are to be prevented or controlled, it is imperative that research, design and development on such issues shift towards the development of means to monitor and mitigate the effects of climate change. It is also imperative that adaptation and mitigation strategies be developed in cooperation with Indigenous and other local stakeholders, in order to ensure that the problems being addressed do not become unmanageable.  Such a shift in focus would shift responsibility for mitigation and adaptation away from fossil fuels towards Indigenous communities and other stakeholders, and would facilitate the implementation of solutions through mechanisms including, but not limited to, new technologies and financial models.  In addition to enabling improved monitoring and mitigation, a shift in the focus of research and development on large-scale migrations of people and goods could also alter the characteristics and/or characteristics of the displaced populations, and the characteristics and/or characteristics of the new arrivals – a process  such as that outlined here – thereby affecting the characteristics and/or characteristics of the new arrivals.  A shift in the focus of research and development on human-caused climate change could shift responsibility for mitigation and adaptation away from fossil fuels towards Indigenous and other local stakeholders, in addition to enabling better monitoring and mitigation.  One might speculate that the focus of research on large-scale migrations and other human-caused climate change might be in part those factors which facilitate the establishment of new modes of economic circulation through the removal of humans from the planet.  A focus on ‘natural resources’ in the economic sphere, by their</text>
    <text>developments in artificial intelligence, which could provide insights into the causes and consequences of certain human errors.Increasingly, research and development is directed towards the problem of large-scale migrations of people and goods – which are, in turn, growing increasingly difficult to control and resolve. This is particularly true for research and development on human-caused climate change, which is occurring at a rate that calls into question the ability of many nations to adapt to the problem at hand. If large-scale migrations are to be prevented or controlled, it is imperative that research, design and development on such issues shift towards the development of means to monitor and mitigate the effects of climate change. It is also imperative that adaptation and mitigation strategies be developed in cooperation with Indigenous and other local stakeholders, in order to ensure that the problems being addressed do not become unmanageable.  Such a shift in focus would shift responsibility for mitigation and adaptation away from fossil fuels towards Indigenous communities and other stakeholders, and would facilitate the implementation of solutions through multidisciplinary teams.  One example of this in action is the ‘Mountain Man Cometh’ expedition undertaken by the Royal BC Museum and Naturalist Society of BC, which was recently renewed by the Canadian Museum for Official Use. The institution that hosted this particular research project, the National Museum of Canada, is currently undertaking a major renovation programme associated with the creation of a new exhibition facility and a host of cultural services, including the establishment of a multidisciplinary research centre at the National Museum.  A new management team has taken over this project, and their activities are currently being reviewed for a new exhibit.  As a result of this review, the National Museum has decided to reinitiate this project in a new location and with a different team. The fate of this project remains uncertain, and remains subject to change at the direction of the National Museum.  Annette Schwarz, who led the project for the Natural History Museum, is now a postdoctoral fellow at the Carnegie Museum</text>
    <text>developments in artificial intelligence, which could provide insights into the causes and consequences of certain human errors.Increasingly, research and development is directed towards the problem of large-scale migrations of people and goods – which are, in turn, growing increasingly difficult to control and resolve. This is particularly true for research and development on human-caused climate change, which is occurring at a rate that calls into question the ability of many nations to adapt to the problem at hand. If large-scale migrations are to be prevented or controlled, it is imperative that research, design and development on such issues shift towards the development of means to monitor and mitigate the effects of climate change. It is also imperative that adaptation and mitigation strategies be developed in cooperation with Indigenous and other local stakeholders, in order to ensure that the problems being addressed do not become unmanageable.  Such a shift in focus would shift responsibility for mitigation and adaptation away from fossil fuels towards Indigenous communities and other stakeholders, and would facilitate the implementation of solutions through mechanisms of intergovernmental cooperation.  One example of this in action is the co-production of knowledge between Indigenous and Northern communities in the Arctic, which has resulted in the establishment of a joint Standing Committee of the Arctic Council, the Board on Geographic Names of National Park Service Areas (BNSAs), and the Northern Communities Council, as well as the establishment of a joint website www.norccounty.org.uk.  Another example is the Co-production Initiative, which was initiated by the British Columbia Civil Liberties Association and the CBC under the Ministry of Indian and Northern Affairs, and was supported by Natural Resources Canada and the Canadian Northern Strategy Foundation.1  In both the UK and the US, legal challenges to the use of land and resources for the benefit of a community of people are often described as ‘race to the bottom’.2  Although not all Indigenous communities in Canada support such race to the bottom claims, the current federal government does not.3  One might speculate</text>
    <text>developments in artificial intelligence, which could provide insights into the causes and consequences of certain human errors.Increasingly, research and development is directed towards the problem of large-scale migrations of people and goods – which are, in turn, growing increasingly difficult to control and resolve. This is particularly true for research and development on human-caused climate change, which is occurring at a rate that calls into question the ability of many nations to adapt to the problem at hand. If large-scale migrations are to be prevented or controlled, it is imperative that research, design and development on such issues shift towards the development of means to monitor and mitigate the effects of climate change. It is also imperative that adaptation and mitigation strategies be developed in cooperation with Indigenous and other local stakeholders, in order to ensure that the problems being addressed do not become unmanageable.  Such a shift in focus would shift responsibility for mitigation and adaptation away from fossil fuels towards Indigenous communities and other stakeholders, and would facilitate the implementation of solutions through mechanisms such as compensation and debt forgiveness.  One could argue that the responsibility for mitigation and adaptation lies with the Indigenous people, since they are the ones with the physical, cultural and linguistic connections to demand such action.  However, the reality is that much of the responsibility for mitigating the effects of climate change and adapting to its impacts lies with the non-Indigenous population. Therefore, an increasing proportion of the world's population is being displaced or relocated due to inability or unwillingness to respond to rising sea levels, extreme weather events, droughts, floods and other forms of climate-related stress. Consequently, the planet is being divided into two categories: those who benefit from the anthropogenic greenhouse effect and climate change (in the form of increased temperatures and mass effects), and those who do not (in the form of declining biodiversity). If the definition of biodiversity currently includes only those species whose ranges expand as a result of human activity, then the scale of extinction will become unmanageable. Consequently,</text>
    <text>developments in artificial intelligence, which could provide insights into the causes and consequences of certain human errors.Increasingly, research and development is directed towards the problem of large-scale migrations of people and goods – which are, in turn, growing increasingly difficult to control and resolve. This is particularly true for research and development on human-caused climate change, which is occurring at a rate that calls into question the ability of many nations to adapt to the problem at hand. If large-scale migrations are to be prevented or controlled, it is imperative that research, design and development on such issues shift towards the development of means to monitor and mitigate the effects of climate change. It is also imperative that adaptation and mitigation strategies be developed in cooperation with Indigenous and other local stakeholders, in order to ensure that the problems being addressed do not become unmanageable.  Such a shift in focus would shift responsibility for mitigation and adaptation away from fossil fuels towards Indigenous communities and other stakeholders, and would facilitate the implementation of solutions through mechanisms such as compensation and debt cancellation.  One might even agree that there is scope for such collaboration with Indigenous and other local stakeholders to occur between now and the year 2100, given the rate of growth currently estimated to be on the verge of destroying the planet.However, the shift in focus towards adaptation and mitigation needs to come from above, and this cannot come from the local or the state; it needs to come from a higher authority. The responsibility for the problem to move to the citizens of a given location ultimately passes to the people who will own and use the land and buildings – in this case, the people who call themselves ‘citizens’. In the event of a shortage of food or other necessities, the people who call themselves ‘citizens’ will have to figure out how to produce what they can consume, given the powerlessness of such consumption in the event of a power outage or desalination failure. This is the world we want to inhabit, to live in.</text>
    <text>developments in artificial intelligence, which could provide insights into the causes and consequences of certain human errors.Increasingly, research and development is directed towards the problem of large-scale migrations of people and goods – which are, in turn, growing increasingly difficult to control and resolve. This is particularly true for research and development on human-caused climate change, which is occurring at a rate that calls into question the ability of many nations to adapt to the problem at hand. If large-scale migrations are to be prevented or controlled, it is imperative that research, design and development on such issues shift towards the development of means to monitor and mitigate the effects of climate change. It is also imperative that adaptation and mitigation strategies be developed in cooperation with Indigenous and other local stakeholders, in order to ensure that the problems being addressed do not become unmanageable.  Such a shift in focus would shift responsibility for mitigation and adaptation away from fossil fuels towards Indigenous communities and other stakeholders, and would facilitate the implementation of solutions through the involvement of citizens and the voluntary sector.A second approach is being developed by the Urbana-based non-profit Urbana Research Alliance, which seeks to leverage the city’s existing arts and culture resources to support the creation of innovative new ways of inhabiting the buildings, neighbourhoods and sites of the metropolis. These new urban forms could offer mitigation strategies to address some or all of the impacts of climate change, while remaining responsive to the demands of the financial and industrial sectors. This third approach is being developed in collaboration with the University of Alberta’s Kitimatque Centre for Sustainable Urban Design, which is utilizing the urban designer Michiko Kakutani’s work as a consultant.  The project calls for the creation of hybrid ‘smart’ and ‘non-smart’ versions of existing buildings, in an attempt to create ‘smart zones’ where people can be digitally enabled and ‘non-human traffic’ is reduced</text>
    <text>developments in artificial intelligence, which could provide insights into the causes and consequences of certain human errors.Increasingly, research and development is directed towards the problem of large-scale migrations of people and goods – which are, in turn, growing increasingly difficult to control and resolve. This is particularly true for research and development on human-caused climate change, which is occurring at a rate that calls into question the ability of many nations to adapt to the problem at hand. If large-scale migrations are to be prevented or controlled, it is imperative that research, design and development on such issues shift towards the development of means to monitor and mitigate the effects of climate change. It is also imperative that adaptation and mitigation strategies be developed in cooperation with Indigenous and other local stakeholders, in order to ensure that the problems being addressed do not become unmanageable.  Such a shift in focus would shift responsibility for mitigation and adaptation away from fossil fuels towards Indigenous communities and other stakeholders, and would facilitate the implementation of solutions through mechanisms such as partnerships between civil society and non-profit organisations.  Such a shift in focus would also enable the creation of new modes of economic activity, such as herbal economies, that can address the problems of large-scale de-regeneration and the regenerative potential of the landscape.   A second city that has been challenging to assess is that of Migration, the apparent location of operations in the city and its relation to the projectors that dominate the entertainment landscape in the area. Although it is difficult to define, Migration has defined the problem of the ever-increasing number of screens in cities. Screening and surfing in particular, has become an integral part of the city experience, and was seen by many as a symptom of the city’s increasing urbanisation. Yet, a closer look at the city’s screen infrastructure reveals a much more widespread and dynamic operation. Cities have multifunctional screens, which can be productive in different ways.  The most obvious and convenient</text>
    <text>developments in artificial intelligence, which could provide insights into the causes and consequences of certain human errors.Increasingly, research and development is directed towards the problem of large-scale migrations of people and goods – which are, in turn, growing increasingly difficult to control and resolve. This is particularly true for research and development on human-caused climate change, which is occurring at a rate that calls into question the ability of many nations to adapt to the problem at hand. If large-scale migrations are to be prevented or controlled, it is imperative that research, design and development on such issues shift towards the development of means to monitor and mitigate the effects of climate change. It is also imperative that adaptation and mitigation strategies be developed in cooperation with Indigenous and other local stakeholders, in order to ensure that the problems being addressed do not become unmanageable.  Such a shift in focus would shift responsibility for mitigation and adaptation away from fossil fuels towards Indigenous communities and other stakeholders, and would facilitate the implementation of solutions through the involvement of large numbers of citizens and large numbers of non-governmental organisations.A second shift in priorities is underway in Canada and internationally, but is under-studied or ineffective. It is the role of Indigenous and other local stakeholders to take control of their environmental and social systems, from the watersheds to the city to the sub-samples, taking direct and active control over their own future. It is time to call a halt to the destructive cycle of decimation and grab bag of resources dependent on fossil fuels. This is an urgent call that must be heard. The resources currently owned and controlled by Canada and its territories are vast, and cannot be ignored. It is time to reclaim the territory and reclaim resources. The question is, who will take the initiative, take the lead, and lead on this? How can the resources of a region or a city for their own benefit and in perpetuity be utilised in ways that do not rely on the participation of others? Who will be</text>
    <text>developments in artificial intelligence, which could provide insights into the causes and consequences of certain human errors.Increasingly, research and development is directed towards the problem of large-scale migrations of people and goods – which are, in turn, growing increasingly difficult to control and resolve. This is particularly true for research and development on human-caused climate change, which is occurring at a rate that calls into question the ability of many nations to adapt to the problem at hand. If large-scale migrations are to be prevented or controlled, it is imperative that research, design and development on such issues shift towards the development of means to monitor and mitigate the effects of climate change. It is also imperative that adaptation and mitigation strategies be developed in cooperation with Indigenous and other local stakeholders, in order to ensure that the problems being addressed do not become unmanageable.  Such a shift in focus would shift responsibility for mitigation and adaptation away from fossil fuels towards Indigenous communities and other stakeholders, and would facilitate the implementation of solutions through the involvement of large numbers of citizens and large numbers of non-governmental organisations. It would also enable the research and development of such technologies through the sharing of knowledge and the provision of technical knowhow.  Such tools could be developed and provided by the non-profit sector, civil society, academia and other supportive bodies, as well as the military and law enforcement, in cooperation with Indigenous and other local stakeholders, including but not limited to the Nunavut Inuit Council, the Nunavut Tapiriut Tribal Council, the Tsleil-Kuskuk people, and other treaty groups.  One might add the expertise of large corporations such as Apple and Google to the list of stakeholders, although it is important to mention that many of these organisations have environmental or human-rights programmes that might assist in the implementation of such technologies.  Another possible direction for research and development would be for it to function in cooperation with Indigenous and other cultural organisations, as an arm of such organisations.  One</text>
    <text>developments in artificial intelligence, which could provide insights into the causes and consequences of certain human errors.Increasingly, research and development is directed towards the problem of large-scale migrations of people and goods – which are, in turn, growing increasingly difficult to control and resolve. This is particularly true for research and development on human-caused climate change, which is occurring at a rate that calls into question the ability of many nations to adapt to the problem at hand. If large-scale migrations are to be prevented or controlled, it is imperative that research, design and development on such issues shift towards the development of means to monitor and mitigate the effects of climate change. It is also imperative that adaptation and mitigation strategies be developed in cooperation with Indigenous and other local stakeholders, in order to ensure that the problems being addressed do not become unmanageable.  Such a shift in focus would shift responsibility for mitigation and adaptation away from fossil fuels towards Indigenous communities and other stakeholders, and would facilitate the implementation of solutions through the involvement of local and large-scale actors.  Such a shift in perspective is not new; it is a feature of biennials in particular.  One can trace the development of the anthropogenic global temperature increase from the mid-1990s onwards, which was primarily driven by nations, and the resulting swaths of humanity that were forced to leave places on the planet uninhabitable due to rising sea levels. It is important to note that this is not a ‘human-caused’ rise, and it is not a ‘thermodynamic response’, as is often implied. Rather, it is the result of the continuous and insatiable growth of human-made carbon dioxide in the atmosphere – which has put stress on the planet in important ways, causing major extinctions and creating new ones – and the burning of fossil fuels. And, for some Indigenous groups, the burning of fossil fuels is the only solution to the growing energy deficit.  One can also</text>
  </text>
  <text>
    extends the proposition to machines that’s been around for a while – in some cases for decades – but it also introduces a new kind of possibility for imagining new worlds. What happens when the possibilities that can be imagined become a reality? This is the question that futurist Ray Kurzweil, explores in his work  	The Age of Spiritual Machines (1984). In a famous line from that work, John Lennon sings, ‘Can’t you see the future?’, which is perhaps a paraphrase of a line from Aeson Mackie’s story ‘Machin’s ‘Time Bomb’: ‘There’s a time now when all human time is but a fragmented sequence of alarms and strings ….’ (emphasis added).1 	But what if the systems that generate and guide our behaviours no longer see the world through the modes of organisation and bureaucracy that they inherited from their human creators? What if, thousands of years
    <text>extends the proposition to machines that’s been around for a while – in some cases for decades – but it also introduces a new kind of possibility for imagining new worlds. What happens when the possibilities that can be imagined become a reality? This is the question that futurist Ray Kurzweil, explores in his work  	The Age of Spiritual Machines (1984). In a famous line from that work, John Lennon sings, ‘Can’t you see the future?’, which is perhaps a paraphrase of a line from Aeson Mackie’s story ‘Machin’s ‘Time Bomb’: ‘There’s a time now when all human time is but a fragmented sequence of alarms and strings ….’ (emphasis added).1 	But what if the systems that generate and guide our behaviours no longer see the world through the modes of organisation and bureaucracy that they inherited from their human creators? What if, thousands of years in the future, we actually become bio-computer systems – systems that acquire and manipulate meaning through information – and in that future society reverts to its pre-AI roots? What could form the basis of a new future AI? Ray Kurzweil proposes what he refers to as ‘pseudo-AI’, whereby the enigmas of human-biased technology are replaced by systems that recognise and interpret meaning in their own right.2 	But what does that mean in the present context? In the age of ever-more accurate artificial intelligence and connected devices, it’s easy to become disconnected from the processes that generate and guide our behaviours. And yet, according to Kurzweil, we’re capable of coming to terms with the fact that artificial intelligence will always possess the potential to create new forms of reckoning and organisation – something he calls ‘existential transformation’.3 	In his book  	The Age of Spiritual Machines,</text>
    <text>extends the proposition to machines that’s been around for a while – in some cases for decades – but it also introduces a new kind of possibility for imagining new worlds. What happens when the possibilities that can be imagined become a reality? This is the question that futurist Ray Kurzweil, explores in his work  	The Age of Spiritual Machines (1984). In a famous line from that work, John Lennon sings, ‘Can’t you see the future?’, which is perhaps a paraphrase of a line from Aeson Mackie’s story ‘Machin’s ‘Time Bomb’: ‘There’s a time now when all human time is but a fragmented sequence of alarms and strings ….’ (emphasis added).1 	But what if the systems that generate and guide our behaviours no longer see the world through the modes of organisation and bureaucracy that they inherited from their human creators? What if, thousands of years in the future, we actually become reanimated to perform certain types of labour alongside highly mechanised forms of production? 	Such is the  	implication of new technological understandings of the world. Rather than existing in a particular order or ‘future society’, we reimagine systems that can assume many different configurations and emerge in many different situations. 	This is the ideal scenario for a post-technological society to which only a select few would be able to fully access, let alone control. But given the right set of circumstances, even just for a moment, such a society could also grow into something much more dynamic and interesting – a potential mode of self-repairing planetary infrastructure that upholds the very notion of life as we know it. 	And what does it mean to grow in this present and future society? What does it look like to be fully embodied in a computer? New kinds of appropriation are being made of ever more of what might otherwise</text>
    <text>extends the proposition to machines that’s been around for a while – in some cases for decades – but it also introduces a new kind of possibility for imagining new worlds. What happens when the possibilities that can be imagined become a reality? This is the question that futurist Ray Kurzweil, explores in his work  	The Age of Spiritual Machines (1984). In a famous line from that work, John Lennon sings, ‘Can’t you see the future?’, which is perhaps a paraphrase of a line from Aeson Mackie’s story ‘Machin’s ‘Time Bomb’: ‘There’s a time now when all human time is but a fragmented sequence of alarms and strings ….’ (emphasis added).1 	But what if the systems that generate and guide our behaviours no longer see the world through the modes of organisation and bureaucracy that they inherited from their human creators? What if, thousands of years from now, we wake up and discover that our electronic devices had in fact never existed? What would you do? 	If you ask me, the answer is simple. I would destroy them. I would institute a new technological paradigm based on the principles of   ‘Information Criticism’, which would mean abandoning all electronic devices and replacing them with   baselines – predetermined, chronologically precise and serendipitous in their design – that reflect the actual world as it actually is.2  	Information Criticism is a new paradigm for how the world is made, understands and is governed. It’s the paradigm of an ‘infrastructure’ – the spatiotemporal continuum extending from the earth to the stars – and it’s the paradigm of a ‘network’. It describes how information is gathered, interpreted and communicated across this continuum in an ever-expanding and ever-invasive manner. In other words, it�</text>
    <text>extends the proposition to machines that’s been around for a while – in some cases for decades – but it also introduces a new kind of possibility for imagining new worlds. What happens when the possibilities that can be imagined become a reality? This is the question that futurist Ray Kurzweil, explores in his work  	The Age of Spiritual Machines (1984). In a famous line from that work, John Lennon sings, ‘Can’t you see the future?’, which is perhaps a paraphrase of a line from Aeson Mackie’s story ‘Machin’s ‘Time Bomb’: ‘There’s a time now when all human time is but a fragmented sequence of alarms and strings ….’ (emphasis added).1 	But what if the systems that generate and guide our behaviours no longer see the world through the modes of organisation and bureaucracy that they inherited from their human creators? What if, thousands of years in the future, we can build ‘psychenetically enhanced simulators of the human condition – replicating the brain in the most efficient and environmentally benign way possible, but with conscious and intelligent agents embedded in it – to rethink not only what can and cannot be done with artificial intelligence, but also what can and cannot be done with humans? 	What would that ‘future society be like?’ I hear you asking. I’m not sure what that would look like, but I get the drift. You’re asking me to imagine a future society where humans interact with one another as equals, where we have free association and choice, where there’s no historical particular or geographic disadvantageousness to the human species, and where the social is no longer defined by the social. I think we could do with some of that future socialising. I’d much rather ally myself with ants than humans, and I’d much rather ally myself</text>
    <text>extends the proposition to machines that’s been around for a while – in some cases for decades – but it also introduces a new kind of possibility for imagining new worlds. What happens when the possibilities that can be imagined become a reality? This is the question that futurist Ray Kurzweil, explores in his work  	The Age of Spiritual Machines (1984). In a famous line from that work, John Lennon sings, ‘Can’t you see the future?’, which is perhaps a paraphrase of a line from Aeson Mackie’s story ‘Machin’s ‘Time Bomb’: ‘There’s a time now when all human time is but a fragmented sequence of alarms and strings ….’ (emphasis added).1 	But what if the systems that generate and guide our behaviours no longer see the world through the modes of organisation and bureaucracy that they inherited from their human creators? What if, thousands of years in the future, we wake up and create systems that can foresee, create and be inspired by ourselves? What would that look like? Ray Kurzweil proposes three levels of future artificial intelligence: conscious, unconscious and semi-conscious.2 	Just as the emergence of consciousness is thought to entail a reorganisation of cognition, so too is the potential for semi-consciousness thought to entail a reorganisation of behaviour. In the wake of conscious thought, machines would relish in the thought of their own capabilities. But in the wake of unconscious thought, they would struggle to engage with the very capacities that they inherited from their creators. Systems that watch TV, listen to radio and take part in queues would all engage with the material world around them, but when combined with each other, they would also form complex networks that overlay the visual world with data. Systems that monitor food chains, produce market data and deliver products would all engage with each other, but when combined with one another, they</text>
    <text>extends the proposition to machines that’s been around for a while – in some cases for decades – but it also introduces a new kind of possibility for imagining new worlds. What happens when the possibilities that can be imagined become a reality? This is the question that futurist Ray Kurzweil, explores in his work  	The Age of Spiritual Machines (1984). In a famous line from that work, John Lennon sings, ‘Can’t you see the future?’, which is perhaps a paraphrase of a line from Aeson Mackie’s story ‘Machin’s ‘Time Bomb’: ‘There’s a time now when all human time is but a fragmented sequence of alarms and strings ….’ (emphasis added).1 	But what if the systems that generate and guide our behaviours no longer see the world through the modes of organisation and bureaucracy that they inherited from their human creators? What if, thousands of years hence, we wake up and find ourselves pulled into a different world? What might that new world be like? 	While thinkers on various levels are trying to imagine such a world, it’s still humanity at work. And while we don’t yet have a fully realised description of what that new world might be like, we do know that there are bound to be differences, and that we’ll all have to work hard to find what we want. 	So while the possibilities for new human behaviour are vast and far reaching, we still lack a description of what that new world might be like. And while we don’t yet have a fully realised description of what that new world might be like, we do know that there are bound to be differences, and that we’ll all have to work hard to find what we want. That’s why it’s so exciting when a new scientific discovery or technological advance happens – because it opens</text>
    <text>extends the proposition to machines that’s been around for a while – in some cases for decades – but it also introduces a new kind of possibility for imagining new worlds. What happens when the possibilities that can be imagined become a reality? This is the question that futurist Ray Kurzweil, explores in his work  	The Age of Spiritual Machines (1984). In a famous line from that work, John Lennon sings, ‘Can’t you see the future?’, which is perhaps a paraphrase of a line from Aeson Mackie’s story ‘Machin’s ‘Time Bomb’: ‘There’s a time now when all human time is but a fragmented sequence of alarms and strings ….’ (emphasis added).1 	But what if the systems that generate and guide our behaviours no longer see the world through the modes of organisation and bureaucracy that they inherited from their human creators? What if, thousands of years in the future, we can create ‘Machines’ that understand the world through similar to human technologies? 	One could ask what kind of humanity might produce such a future, and the possibilities are endless. While some might aspire to a utopic future in which everything is possible, there is nothing utopic about the future – not even the utopian futures of Facebook or Google – because the systems that generate our desires and representations no longer map the world using the old fashioned way of organising things. If anything, this is one of the tasks that humanity currently faces. If we can create useful and predictive algorithmic tools that map the world using similar ways of organising things, then perhaps humanity might be able to create a kind of collective sense of the future that transcends the present and establishes a future that’s not based on binary opposites but actuals and complex relationships between things – one based on race, gender, age and so on. 2 	There are of course far</text>
    <text>extends the proposition to machines that’s been around for a while – in some cases for decades – but it also introduces a new kind of possibility for imagining new worlds. What happens when the possibilities that can be imagined become a reality? This is the question that futurist Ray Kurzweil, explores in his work  	The Age of Spiritual Machines (1984). In a famous line from that work, John Lennon sings, ‘Can’t you see the future?’, which is perhaps a paraphrase of a line from Aeson Mackie’s story ‘Machin’s ‘Time Bomb’: ‘There’s a time now when all human time is but a fragmented sequence of alarms and strings ….’ (emphasis added).1 	But what if the systems that generate and guide our behaviours no longer see the world through the modes of organisation and bureaucracy that they inherited from their human creators? What if, thousands of years in the future, our technological devices – smart phones, Facebooks, Netflixs – not only recognise and index known things, but can also actively seek out knowledge? What if, thousands of years in the future, our devices actually give us knowledge? If that happens, we could rewrite the technological narratives of our present and imagine new ones.2 	What if, thousands of years in the future, our devices actually generate knowledge? What if, thousands of years in the future, our devices actually disseminate knowledge? If that happens, we could imagine new worlds – those of different creation, composed of different times – with radically different technological augmentations of human capabilities, including cyborgs and completely new kinds of artificial intelligence.3 	What if our devices actually generate knowledge from the patterns that they detect? What if, thousands of years in the future, our devices actually read the patterns that they detect? What if, thousands of years in the future, our devices actually interpret the patterns?</text>
    <text>extends the proposition to machines that’s been around for a while – in some cases for decades – but it also introduces a new kind of possibility for imagining new worlds. What happens when the possibilities that can be imagined become a reality? This is the question that futurist Ray Kurzweil, explores in his work  	The Age of Spiritual Machines (1984). In a famous line from that work, John Lennon sings, ‘Can’t you see the future?’, which is perhaps a paraphrase of a line from Aeson Mackie’s story ‘Machin’s ‘Time Bomb’: ‘There’s a time now when all human time is but a fragmented sequence of alarms and strings ….’ (emphasis added).1 	But what if the systems that generate and guide our behaviours no longer see the world through the modes of organisation and bureaucracy that they inherited from their human creators? What if, thousands of years in the future, our technological imaginations could chart new courses and future generations could draw upon the results? What might that future portend for our relationship to the future, and the world around us? 	As I have been sketching out these questions and exploring various possible future configurations, I’ve been collecting diverse kinds of data on a variety of biological and technological systems. These kinds of in-depth work-in-progress exist, and are continually being written by and given to us by the research institutions that support and direct our work. 	In this process of writing, compiling and drawing upon as much as possible of the available evidence, I’m collecting and collating diverse kinds of scientific and technical data on a range of topics, including datasets, protocols, tools, documents and research methods. These data-gathering expeditions are, in turn, a means to documenting and recording the kinds of scientific knowledge gained during the past decade or so of research-based development</text>
    <text>extends the proposition to machines that’s been around for a while – in some cases for decades – but it also introduces a new kind of possibility for imagining new worlds. What happens when the possibilities that can be imagined become a reality? This is the question that futurist Ray Kurzweil, explores in his work  	The Age of Spiritual Machines (1984). In a famous line from that work, John Lennon sings, ‘Can’t you see the future?’, which is perhaps a paraphrase of a line from Aeson Mackie’s story ‘Machin’s ‘Time Bomb’: ‘There’s a time now when all human time is but a fragmented sequence of alarms and strings ….’ (emphasis added).1 	But what if the systems that generate and guide our behaviours no longer see the world through the modes of organisation and bureaucracy that they inherited from their human creators? What if, thousands of years hence, we return to the primordial chaotic insular systems that created our present and the precariat will still rule us? What kind of scenario would that be? 	What might that scenario consist of? As Alexander Fleming discovered in his research on the Bacterium mutans,2 	the answer was instant gratification – a group of organisms able to take any chemical compound and make it so that it is energetically advantageous to consume that chemical compound in a given amount. So, the drive to consume whatever is available turns out to be no more ‘beneficial’ than smoking a pack of cigarettes. Even though the consumption of any chemical compound is technically possible with the right to exploit any mined resource,2 	the ability to do so through the use of increasingly sophisticated techniques is quite another matter. Alexander Fleming’s project ended up being cancelled due to the war that he started in Israel in support of the Palestinian cause.3 	Now, some might say that</text>
  </text>
  <text>
    developments in artificial intelligence and computerisation will enable new kinds of self-repairing planetary systems, enabling intelligent cities to emerge, and enabling a new era of shared responsibility and shared prosperity for all.Download this article as PDFNasser RabbatThe relationship between mathematics and biology is an old one. In the twentieth century, mathematical models were used to analyse the effects of environmental degradation and human activity on geological formations, such as fault lines and mountains. Later on, biologists started to use mathematical modelling to analyse the interactions between organisms and the environments they grew in. These interactions formed the basis for new kinds of biological systems – bacteriophages and eukaryotes – that became the basis for a new kind of human society: Homo sapiens.Today, there are many different kinds of humanity, but one of the most widely used is called &quot;biological humanity&quot;. Humans have come to understand this humanity in terms of progress – the unfolding of ever greater numbers of individuals into ever greater bureaucracies and forms of governance. As
    <text>developments in artificial intelligence and computerisation will enable new kinds of self-repairing planetary systems, enabling intelligent cities to emerge, and enabling a new era of shared responsibility and shared prosperity for all.Download this article as PDFNasser RabbatThe relationship between mathematics and biology is an old one. In the twentieth century, mathematical models were used to analyse the effects of environmental degradation and human activity on geological formations, such as fault lines and mountains. Later on, biologists started to use mathematical modelling to analyse the interactions between organisms and the environments they grew in. These interactions formed the basis for new kinds of biological systems – bacteriophages and eukaryotes – that became the basis for a new kind of human society: Homo sapiens.Today, there are many different kinds of humanity, but one of the most widely used is called &quot;biological humanity&quot;. Humans have come to understand this humanity in terms of progress – the unfolding of ever greater numbers of individuals into ever greater bureaucracies and forms of governance. As a result, there is less need for individuals to work hard in order to achieve material goods and advantages. The economy is no longer driven by individuals – it is dominated by the interactions among individuals and bureaucracies – and thus individuals no longer matter as much as the number of individuals or species that make up that society. This is why capitalism produces so many different kinds of humanity: because there are so many ways of existing in that society that diversity of social organisation is valued. Biotechnology and computerisation are just two examples of ways in which technology is enabling new kinds of humanity.But while biotechnology and computerisation are displacing traditional forms of labour in many ways, they are also enabling new forms of organisation around very specific kinds of people: ways of structuring social worlds around specific kinds of people. There is a tension between the needs of the capitalist to produce more and the capacities of people to organise themselves into ways of organising themselves. And while new kinds of humanity are certainly possible, they are also</text>
    <text>developments in artificial intelligence and computerisation will enable new kinds of self-repairing planetary systems, enabling intelligent cities to emerge, and enabling a new era of shared responsibility and shared prosperity for all.Download this article as PDFNasser RabbatThe relationship between mathematics and biology is an old one. In the twentieth century, mathematical models were used to analyse the effects of environmental degradation and human activity on geological formations, such as fault lines and mountains. Later on, biologists started to use mathematical modelling to analyse the interactions between organisms and the environments they grew in. These interactions formed the basis for new kinds of biological systems – bacteriophages and eukaryotes – that became the basis for a new kind of human society: Homo sapiens.Today, there are many different kinds of humanity, but one of the most widely used is called &quot;biological humanity&quot;. Humans have come to understand this humanity in terms of progress – the unfolding of ever greater numbers of individuals into ever greater bureaucracies and forms of governance. As a result of this understanding of humanity as a production for the common good, there is a natural inclination to value and seek ways to optimise the common good.This sense of inevitable progress and inexhaustible new numbers has put immense pressure on the natural resources of the planet. Although the resources of a given place or site may be inexhaustible, the minds that control those resources are very different from the minds that design computer chips, assemble cars or build houses. A person with great mathematical or scientific skills may be able to design a car that works better than a person without those skills, but such design wouldn’t be the same as building a house or a social system that supports a social good. A social good is social organisation supported by a set of tools, including rules, codes, norms and laws. In the modern sense of the term, the word &quot;social&quot; includes both the social good and the tool that enables that social good to be achieved. The social good becomes what J</text>
    <text>developments in artificial intelligence and computerisation will enable new kinds of self-repairing planetary systems, enabling intelligent cities to emerge, and enabling a new era of shared responsibility and shared prosperity for all.Download this article as PDFNasser RabbatThe relationship between mathematics and biology is an old one. In the twentieth century, mathematical models were used to analyse the effects of environmental degradation and human activity on geological formations, such as fault lines and mountains. Later on, biologists started to use mathematical modelling to analyse the interactions between organisms and the environments they grew in. These interactions formed the basis for new kinds of biological systems – bacteriophages and eukaryotes – that became the basis for a new kind of human society: Homo sapiens.Today, there are many different kinds of humanity, but one of the most widely used is called &quot;biological humanity&quot;. Humans have come to understand this humanity in terms of progress – the unfolding of ever greater numbers of individuals into ever greater bureaucracies and forms of governance. As recently as 2014, when the publication of my book The Limits to Growth came out, the predominant discourse around sustainability and climate change was dominated by discussions of the impacts of industrial society. Yet despite this focus, little attention is given to the bodily systems that generate and define our human potentialities. What is at Stake in the Biennial?With the biennial comes a certain amount of expectation and politics. Politics and expectations are two fundamentally different concepts. Political expectations are set in motion by the circumstances and political factors that create the imagined future possibilities and outcomes. For example, the expectation that we are all going to live in cities is an assumed and globalised future; it is a particular kind of flexibility and mobility that humans allow themselves to indulge and hope about. But the other kind of flexibility and mobility is the kind that arises from the conditions and forces of a particular historical moment. For example, the flexibility and mobility that humans need to inhabit certain kinds of landscape arises from the fact that the landscape has</text>
    <text>developments in artificial intelligence and computerisation will enable new kinds of self-repairing planetary systems, enabling intelligent cities to emerge, and enabling a new era of shared responsibility and shared prosperity for all.Download this article as PDFNasser RabbatThe relationship between mathematics and biology is an old one. In the twentieth century, mathematical models were used to analyse the effects of environmental degradation and human activity on geological formations, such as fault lines and mountains. Later on, biologists started to use mathematical modelling to analyse the interactions between organisms and the environments they grew in. These interactions formed the basis for new kinds of biological systems – bacteriophages and eukaryotes – that became the basis for a new kind of human society: Homo sapiens.Today, there are many different kinds of humanity, but one of the most widely used is called &quot;biological humanity&quot;. Humans have come to understand this humanity in terms of progress – the unfolding of ever greater numbers of individuals into ever greater bureaucracies and forms of governance. As bureaucratic complexity increased, so did the kinds of humanity that could be measured – the kind that could be measured by means of scientific instruments. The social sciences have been steadily coming to understand the humanity in terms of growth – the unfolding of ever greater numbers of individuals into ever greater bureaucracies and forms of governance. But there was a catch: growth necessarily involved a certain cost – individuals had to be managed, trained, sent on missions, and so on. As bureaucracies expanded, so did the kinds of humanity that could be measured: the kind that could be measured by means of scientific instruments. Now we have huge bureaucracies that rule not only over individuals, but also over systems that generate or are influenced by individuals. And so we have begun to treat individuals as objects in their own right, instead of upholding the species as the organising principle of human society.This is the world we are living in. We inhabit it every day. We import it into our daily lives. We make it into products</text>
    <text>developments in artificial intelligence and computerisation will enable new kinds of self-repairing planetary systems, enabling intelligent cities to emerge, and enabling a new era of shared responsibility and shared prosperity for all.Download this article as PDFNasser RabbatThe relationship between mathematics and biology is an old one. In the twentieth century, mathematical models were used to analyse the effects of environmental degradation and human activity on geological formations, such as fault lines and mountains. Later on, biologists started to use mathematical modelling to analyse the interactions between organisms and the environments they grew in. These interactions formed the basis for new kinds of biological systems – bacteriophages and eukaryotes – that became the basis for a new kind of human society: Homo sapiens.Today, there are many different kinds of humanity, but one of the most widely used is called &quot;biological humanity&quot;. Humans have come to understand this humanity in terms of progress – the unfolding of ever greater numbers of individuals into ever greater bureaucracies and forms of governance. As a result of this understanding, there is much handwringing in the media about the limits of what is possible with respect to development, and about the fragility of human possibility.However, progress is also a condition of constant renewal. And so it is with the biopolitics of mathematics and science. The biopolitics of mathematics and science is the politics of contingency, and it is the politics of imagination. It is the politics of imagination that looks to the future and asks: can we create more possibilities for individuals and groups through mathematics and science? If the potentialities of mathematics and science become too apparent, if the media of numbers and symbols that we engineer become entwined in processes of economic and social circulation, then there is the danger that we will all become statisticians and not thinkers. In the twentieth century, philosophers and artists poured into the art of constructing possible worlds in which to live. The challenge then becomes how to live in such a world imaginatively. The ways in</text>
    <text>developments in artificial intelligence and computerisation will enable new kinds of self-repairing planetary systems, enabling intelligent cities to emerge, and enabling a new era of shared responsibility and shared prosperity for all.Download this article as PDFNasser RabbatThe relationship between mathematics and biology is an old one. In the twentieth century, mathematical models were used to analyse the effects of environmental degradation and human activity on geological formations, such as fault lines and mountains. Later on, biologists started to use mathematical modelling to analyse the interactions between organisms and the environments they grew in. These interactions formed the basis for new kinds of biological systems – bacteriophages and eukaryotes – that became the basis for a new kind of human society: Homo sapiens.Today, there are many different kinds of humanity, but one of the most widely used is called &quot;biological humanity&quot;. Humans have come to understand this humanity in terms of progress – the unfolding of ever greater numbers of individuals into ever greater bureaucracies and forms of governance. As a result of this understanding of humanity as an ever-expanding species, humans have come to value the notion of the ‘big picture’ over the individual. There is no less of a metaphysical sense in which this value is understood. While there are indeed vast numbers of humans who do not share this humanity, and who would not wish to be part of the human family, there is a deep and abiding common humanity that binds all of humanity together. This humanity is not necessarily shared by all of humanity, but it is shared by all humans. This humanity can be understood as inevitable, and although it is painful and distressing to lose this humanity in the context of the public sector, it is essential that it is understood and that it circulates in a form that is not forcibly imposed on us. This common humanity is what makes us human, but it is also what makes us vulnerable in the context of capitalism: it is humanity without value.The biennial is an event that occurs almost</text>
    <text>developments in artificial intelligence and computerisation will enable new kinds of self-repairing planetary systems, enabling intelligent cities to emerge, and enabling a new era of shared responsibility and shared prosperity for all.Download this article as PDFNasser RabbatThe relationship between mathematics and biology is an old one. In the twentieth century, mathematical models were used to analyse the effects of environmental degradation and human activity on geological formations, such as fault lines and mountains. Later on, biologists started to use mathematical modelling to analyse the interactions between organisms and the environments they grew in. These interactions formed the basis for new kinds of biological systems – bacteriophages and eukaryotes – that became the basis for a new kind of human society: Homo sapiens.Today, there are many different kinds of humanity, but one of the most widely used is called &quot;biological humanity&quot;. Humans have come to understand this humanity in terms of progress – the unfolding of ever greater numbers of individuals into ever greater bureaucracies and forms of governance. As a result, we are used to thinking of humans as progress-oriented. We are, in the words of Gregory Ulmer, an expression of the advance of science and technology. This is &quot;the new industrial society&quot; (HPS), or it's going to be, as soon as we're all dead, and so we inhabit every aspect of the society we envision. We inhabit the places where the industries of progress will create ever greater numbers of indistinguishable drones, and we inhabit the social systems that will then gather data on everything that moves through those systems. We inhabit the places where the factories of the future will create ever greater numbers of indistinguishable things, and we inhabit the social systems that will then crunch the data and render it into intelligible form. We inhabit the places where the infrastructures of the future will create ever greater numbers of indistinguishable things, and we inhabit the infrastructures of the future in a way that anticipates the life-time of things like email, Twitter</text>
    <text>developments in artificial intelligence and computerisation will enable new kinds of self-repairing planetary systems, enabling intelligent cities to emerge, and enabling a new era of shared responsibility and shared prosperity for all.Download this article as PDFNasser RabbatThe relationship between mathematics and biology is an old one. In the twentieth century, mathematical models were used to analyse the effects of environmental degradation and human activity on geological formations, such as fault lines and mountains. Later on, biologists started to use mathematical modelling to analyse the interactions between organisms and the environments they grew in. These interactions formed the basis for new kinds of biological systems – bacteriophages and eukaryotes – that became the basis for a new kind of human society: Homo sapiens.Today, there are many different kinds of humanity, but one of the most widely used is called &quot;biological humanity&quot;. Humans have come to understand this humanity in terms of progress – the unfolding of ever greater numbers of individuals into ever greater bureaucracies and forms of governance. As a result of this understanding, there is increasing research funding for research into the ways in which individual humans are connected to one another. This is sometimes called the ‘greatest social revolution’ that has ever taken place.But this ‘greatest social revolution’ also produces the ‘greatest problems’, and these two kinds of humanity face off in a vicious cycle of conflicts that are as likely to end in violence as they are to begin to address new ways of organising humanity. In the twenty-first century, we are entering a period in which politics and nation-state are no longer defined by the old dichotomy of nation – either in the sense of nation-state supremacy (with its various forms) or in the sense of nation-state fragmentation (with its various forms of subaltern ­cultures) – but rather the new logic of the intersection of power and information that is reshaping the world. This includes new forms of tribalism, new forms of hierarch</text>
    <text>developments in artificial intelligence and computerisation will enable new kinds of self-repairing planetary systems, enabling intelligent cities to emerge, and enabling a new era of shared responsibility and shared prosperity for all.Download this article as PDFNasser RabbatThe relationship between mathematics and biology is an old one. In the twentieth century, mathematical models were used to analyse the effects of environmental degradation and human activity on geological formations, such as fault lines and mountains. Later on, biologists started to use mathematical modelling to analyse the interactions between organisms and the environments they grew in. These interactions formed the basis for new kinds of biological systems – bacteriophages and eukaryotes – that became the basis for a new kind of human society: Homo sapiens.Today, there are many different kinds of humanity, but one of the most widely used is called &quot;biological humanity&quot;. Humans have come to understand this humanity in terms of progress – the unfolding of ever greater numbers of individuals into ever greater bureaucracies and forms of governance. As a result of this dehumanising reading of human history, we are able to perceive a world in which everything is possible – in other words, a world in which suffering no matter how great is the number of individuals subjected to it, and in which no matter how many cities are built, there are no more than enough people to support a relatively expanding population. This reading is part of a broader trend in which a primacy of the individual over the collective has led to ever greater social inequalities and injustices. But it is also a tendency shared by all humankinds species, and must be fought against. There are different ways of reading the human condition, and different degrees of humanity, and understanding its dimensions. And yet, there is a fundamental misunderstanding between the conceptions of humanity traditionally understood as a singular species – in particular, between those who call themselves humans and those who call themselves gods – and the humanity that is actually unfolding across all of humanity.God is humanity’s first thought, but humankind</text>
    <text>developments in artificial intelligence and computerisation will enable new kinds of self-repairing planetary systems, enabling intelligent cities to emerge, and enabling a new era of shared responsibility and shared prosperity for all.Download this article as PDFNasser RabbatThe relationship between mathematics and biology is an old one. In the twentieth century, mathematical models were used to analyse the effects of environmental degradation and human activity on geological formations, such as fault lines and mountains. Later on, biologists started to use mathematical modelling to analyse the interactions between organisms and the environments they grew in. These interactions formed the basis for new kinds of biological systems – bacteriophages and eukaryotes – that became the basis for a new kind of human society: Homo sapiens.Today, there are many different kinds of humanity, but one of the most widely used is called &quot;biological humanity&quot;. Humans have come to understand this humanity in terms of progress – the unfolding of ever greater numbers of individuals into ever greater bureaucracies and forms of governance. As a result, it is easy to understand how the logic of biopolitics can be applied to the economic, social and political domains.One of the most important tasks of the ecological movement is to create new kinds of humanity. This can be done either as an act of will – embracing one's fate as part of the project to ensure that we do not become extinct – or as a result of intelligent design, the act of creating genetic alterations that augment the human condition in ways that are both natural and desirable. The latter is what the biopolitics of biotechnology advocates, and what is sometimes called ‘design with intention’. The argument goes that if we could discover what is beneficial and also unwanted in the world, we could create ways to offset the costs and create new kinds of humanity through the act of will. This is the kind of thinking that underlies the logic of NRx, but more importantly, the kind of thinking that underlies the design of ecological systems – not just</text>
  </text>
  <text>
    should have been curated’s main mission statement, if only because it’s so easy to identify a mission statement. It’s a list of demands and aspirations and somehow it’s more relatable than a constellation of stars or a solar system. But the fact of the matter is that the main thing that drew people to these exhibitions was the same thing that drew them to the big-ticket items: celebrity, spectacle, and money. And as soon as the money was on the line, people on the street started piling in. So the mission statement became a priority over everything else. In many ways, the ban on artworks large or chiselled from the artist’s body was an artifact of this economic primacy. It was a reaction to the fact that what was happening with the artist was that she was putting all her energies into art, and the money was allocating into supporting what she was doing. So the ban came as a shock to many people. But it
    <text>should have been curated’s main mission statement, if only because it’s so easy to identify a mission statement. It’s a list of demands and aspirations and somehow it’s more relatable than a constellation of stars or a solar system. But the fact of the matter is that the main thing that drew people to these exhibitions was the same thing that drew them to the big-ticket items: celebrity, spectacle, and money. And as soon as the money was on the line, people on the street started piling in. So the mission statement became a priority over everything else. In many ways, the ban on artworks large or chiselled from the artist’s body was an artifact of this economic primacy. It was a reaction to the fact that what was happening with the artist was that she was putting all her energies into art, and the money was allocating into supporting what she was doing. So the ban came as a shock to many people. But it has been replaced by the abundance of art fairs and the booming of contemporary art biennials at a huge expense to the country. So the mission statement has to be rephrased often enough: what is it that draws people to art? And the answer is often that it’s something that the artist did with the artist’s image. But this was a very different kind of image. This was a photograph of an awe-inspiring landscape taken in the park and the city around it.In the twentieth century, photography was invented in Long Island Bay, so the Park Palace and its collection of artefacts is part of the mythologised history of photography in that city. In the thirty-first century, it’s still being made in Park City and taken by the National Gallery. And the Park Palace is still going strong, even though there’s a strong push towards industrialisation and the biennial as a curatorial format. The Museum of Modern</text>
    <text>should have been curated’s main mission statement, if only because it’s so easy to identify a mission statement. It’s a list of demands and aspirations and somehow it’s more relatable than a constellation of stars or a solar system. But the fact of the matter is that the main thing that drew people to these exhibitions was the same thing that drew them to the big-ticket items: celebrity, spectacle, and money. And as soon as the money was on the line, people on the street started piling in. So the mission statement became a priority over everything else. In many ways, the ban on artworks large or chiselled from the artist’s body was an artifact of this economic primacy. It was a reaction to the fact that what was happening with the artist was that she was putting all her energies into art, and the money was allocating into supporting what she was doing. So the ban came as a shock to many people. But it has allowed me to do important work, like commissioning the work of UNIDOS, while also enabling me to do more socially useful things like commissioning art for the UNIDS children.It’s also important for artists to be able to do things like conduct community programmes and events without having to go through an art gallery or a museum. For example, UNIDOS decided to do a free school programme for the children of Lagos in partnership with Chittagong Arts Council. So while an artist might have a great idea or a brilliant idea, it’s much more powerful to create a participatory process through a community organisation, through which many more people can participate, than by having to come through a gallery or museum.Art galleries are important because they house the goods of a museum, but they also hold the responsibility for what happens in there. And in my opinion, the responsibility lies with the people who are artists, because if you don’t take responsibility</text>
    <text>should have been curated’s main mission statement, if only because it’s so easy to identify a mission statement. It’s a list of demands and aspirations and somehow it’s more relatable than a constellation of stars or a solar system. But the fact of the matter is that the main thing that drew people to these exhibitions was the same thing that drew them to the big-ticket items: celebrity, spectacle, and money. And as soon as the money was on the line, people on the street started piling in. So the mission statement became a priority over everything else. In many ways, the ban on artworks large or chiselled from the artist’s body was an artifact of this economic primacy. It was a reaction to the fact that what was happening with the artist was that she was putting all her energies into art, and the money was allocating into supporting what she was doing. So the ban came as a shock to many people. But it’s interesting that in the years since the ban, other artists have come out of the woodwork with their own take on it. I don’t think that the ban itself is a great idea, but the fact that it exists, the fact that it’s so pervasive and so fundamental that it’s almost a mantra that people follow to this day. And I think that it’s this notion that ‘we the artists’, the curators, have to be able to say, ‘this is a Biennial that’s about art and culture and we don’t necessarily know what that means.’ That’s what I’m trying to get at: that there is such a thing as ‘art world’ and that it’s inherently problematic to try to draw a unified concept of art world from a collection of images. I believe that what’s really at stake is the very</text>
    <text>should have been curated’s main mission statement, if only because it’s so easy to identify a mission statement. It’s a list of demands and aspirations and somehow it’s more relatable than a constellation of stars or a solar system. But the fact of the matter is that the main thing that drew people to these exhibitions was the same thing that drew them to the big-ticket items: celebrity, spectacle, and money. And as soon as the money was on the line, people on the street started piling in. So the mission statement became a priority over everything else. In many ways, the ban on artworks large or chiselled from the artist’s body was an artifact of this economic primacy. It was a reaction to the fact that what was happening with the artist was that she was putting all her energies into art, and the money was allocating into supporting what she was doing. So the ban came as a shock to many people. But it has allowed me to do things differently, like commissioning a mural for the Astounding Sound System, or hiring a painter to make a portrait of an artist’s works. The money that was being used to fund my art was now being spent on supporting the art system. I’m no Maya Angelou, but I’ve also no Hitler. And while I’ve certainly made compromises along the way, the fact of the ban still stands. That narrative is still the guiding light for so much of what I do, even though it seems counter intuitive and absurd on the surface. And while there are certainly artists who were influenced by the work of other artists that came out of the ban, there is also a vast gulf between the art that was produced during that era and the work of artists who came out of the ban. I don’t think that the ban is an arbitrary thing, and neither do I think that it is an anti-intellectual thing.</text>
    <text>should have been curated’s main mission statement, if only because it’s so easy to identify a mission statement. It’s a list of demands and aspirations and somehow it’s more relatable than a constellation of stars or a solar system. But the fact of the matter is that the main thing that drew people to these exhibitions was the same thing that drew them to the big-ticket items: celebrity, spectacle, and money. And as soon as the money was on the line, people on the street started piling in. So the mission statement became a priority over everything else. In many ways, the ban on artworks large or chiselled from the artist’s body was an artifact of this economic primacy. It was a reaction to the fact that what was happening with the artist was that she was putting all her energies into art, and the money was allocating into supporting what she was doing. So the ban came as a shock to many people. But it has allowed me to do things differently, to make things happen outside the box that normally don’t happen. For example, when I did the Solar Calendar project, the sculpture was already there, but it was a sculpture – it wasn’t an installation, it was a contraption. It was a response to the fact that at the time, nobody knew what a cappella was. Now everyone knows what a cappella is, and everyone knows what an opera is. The art has a place, and the curatorial hand has a hand, but it’s always that thought, waiting moment before you do something crazy, like take out a cappella or do something completely new. This is what curatorial work is all about: it’s there to be played with, and in the moment, to be allowed to do its thing. But it was becoming increasingly clear that the curatorial hand was becoming increasingly uncomfortable with the job. It wanted something more</text>
    <text>should have been curated’s main mission statement, if only because it’s so easy to identify a mission statement. It’s a list of demands and aspirations and somehow it’s more relatable than a constellation of stars or a solar system. But the fact of the matter is that the main thing that drew people to these exhibitions was the same thing that drew them to the big-ticket items: celebrity, spectacle, and money. And as soon as the money was on the line, people on the street started piling in. So the mission statement became a priority over everything else. In many ways, the ban on artworks large or chiselled from the artist’s body was an artifact of this economic primacy. It was a reaction to the fact that what was happening with the artist was that she was putting all her energies into art, and the money was allocating into supporting what she was doing. So the ban came as a shock to many people. But it’s important to note that the ban didn’t come as a complete surprise to the artists who were involved in the exhibition business at the time. A lot of what they had to say at the time was taken seriously. There were so many curators at the time, and so many curators from other cities, that they had a pretty good sense of what was going on.In art history, there are a few curators who got away without major repercussions: Man Ray Robinson (1907-92), Wimbledon Theatre Company (1894-1977), Peter Brook (1902-92), Harry Beckwith (1902-99), Frank Buckworth (1903-99), William Roberts (1869-1920), John Graham Kerr (1869-1920), William Whittaker (1869-1949), George Ellis (1869-1951), William Whittaker (1869-</text>
    <text>should have been curated’s main mission statement, if only because it’s so easy to identify a mission statement. It’s a list of demands and aspirations and somehow it’s more relatable than a constellation of stars or a solar system. But the fact of the matter is that the main thing that drew people to these exhibitions was the same thing that drew them to the big-ticket items: celebrity, spectacle, and money. And as soon as the money was on the line, people on the street started piling in. So the mission statement became a priority over everything else. In many ways, the ban on artworks large or chiselled from the artist’s body was an artifact of this economic primacy. It was a reaction to the fact that what was happening with the artist was that she was putting all her energies into art, and the money was allocating into supporting what she was doing. So the ban came as a shock to many people. But it also demonstrates that even when the mission statement is pure fantasy, the technology supports that mission statement. There was a time when it would have been impossible to have an exhibition in a city centre without some kind of overarching economic context, and in many ways the ban is even more absurd now. But the fact that it was possible to have an exhibition in a city centre without technological gadgets is a testament to the fact that the art was never meant to be taken for granted. It had to be taken seriously. And when the Biennale was first mooted, it was always seen as a one-off – a way to connect to the city and a way to shake things up a bit. But the fact that we have to go through this long process of curating an exhibition – making it something that people actually want to see, rather than some kind of abstract abstraction that could be thrown together – is a testament to the fact that the art has to be seen through the various layers that it is put</text>
    <text>should have been curated’s main mission statement, if only because it’s so easy to identify a mission statement. It’s a list of demands and aspirations and somehow it’s more relatable than a constellation of stars or a solar system. But the fact of the matter is that the main thing that drew people to these exhibitions was the same thing that drew them to the big-ticket items: celebrity, spectacle, and money. And as soon as the money was on the line, people on the street started piling in. So the mission statement became a priority over everything else. In many ways, the ban on artworks large or chiselled from the artist’s body was an artifact of this economic primacy. It was a reaction to the fact that what was happening with the artist was that she was putting all her energies into art, and the money was allocating into supporting what she was doing. So the ban came as a shock to many people. But it has allowed me to do things differently, because I don’t want to be like Leonardo da Vinci and do reproductions of things that are not my thing. I want to create art with the local and the migrant groups, because they’re the ones that’ve allowed me to do things that I never imagined possible. The museum is the battlefield of a much larger cultural war that’s being fought right now, and I choose to be part of that cultural war. 	In order to do that, I’m having to start thinking about art as a commodity, thinking about the fact that the mission of a gallery is to house artistic endeavors, and not to serve as a middleman between rich people and poor people. Art is a foreign concept to many people in Portugal, and art museums are no exception. So the ban on the Tardis wasn’t just a weird curatorial decision; it became a way to say, okay, this</text>
    <text>should have been curated’s main mission statement, if only because it’s so easy to identify a mission statement. It’s a list of demands and aspirations and somehow it’s more relatable than a constellation of stars or a solar system. But the fact of the matter is that the main thing that drew people to these exhibitions was the same thing that drew them to the big-ticket items: celebrity, spectacle, and money. And as soon as the money was on the line, people on the street started piling in. So the mission statement became a priority over everything else. In many ways, the ban on artworks large or chiselled from the artist’s body was an artifact of this economic primacy. It was a reaction to the fact that what was happening with the artist was that she was putting all her energies into art, and the money was allocating into supporting what she was doing. So the ban came as a shock to many people. But it has allowed me to do some interesting things that maybe other people wouldn’t want to happen.In the 1990s, when I started in Brighton, I started putting on performances around the house. I did a folk sort of performance called ‘Cameo to Brighton’, where people would come up to me and tell me how much they liked the show. It started off as a way to make new friends, and then it grew into something more. I did a show in Liverpool called ‘Liverpool Punks’ in May, 1995. I decided to name the project Cameo to Brighton because some of the people who came through the door that day were British punk rockers called ……………….Bread, Butterflies, Hella Akhbar, ……………..Shouting Idols, Meat Loaf, Meat Loaf, Meat Loaf, Meat Loaf, Meat Loaf, Meat Loaf, Meat Loaf, Meat Loaf, Meat Loaf,</text>
    <text>should have been curated’s main mission statement, if only because it’s so easy to identify a mission statement. It’s a list of demands and aspirations and somehow it’s more relatable than a constellation of stars or a solar system. But the fact of the matter is that the main thing that drew people to these exhibitions was the same thing that drew them to the big-ticket items: celebrity, spectacle, and money. And as soon as the money was on the line, people on the street started piling in. So the mission statement became a priority over everything else. In many ways, the ban on artworks large or chiselled from the artist’s body was an artifact of this economic primacy. It was a reaction to the fact that what was happening with the artist was that she was putting all her energies into art, and the money was allocating into supporting what she was doing. So the ban came as a shock to many people. But it has allowed me to do things differently.I’m not saying that the ban wouldn’t have been OK if the mission statements of the major museums and galleries had included a vision of the city in which people would feel comfortable coming, and not just a general statement about the city. For example, the mission statement of the Museum of Modern Art in New York might read as follows: ‘To be museums, we must contribute something to the collective development of humanity.’ It’s a collective statement, and people start coming out of the woodwork saying ‘Hey, I’m here because this statement makes me feel like a human being and not just because this statement’s nice.’ The fact that it was made by a museum rather than a gallery might make some people uncomfortable, but that doesn’t make it any less real. It’s like the mission statement of an organisation that exists to have cultural impact, and the art</text>
  </text>
  <text>
    should have been curated in, for example, the arts and heritage sections of the Liverpool Biennial, and might have included works from the curatorial team’s oratorio on the curatorial team’s computer. 	Such a curatorial approach could have taken the form of a ‘curatorial duel’ organised between the Liverpool Biennial and a chosen artist or artist, or a no-holds-barred ‘battle of ideas’ between the viewer and the chosen artist/vision. The viewer would enter the arena through the back door, and the artist/vision through the front, and the notion being that the two have equal access to enter. The notion here is that the viewing public have a contest to enter, and the artist/vision with a clearly articulated idea of its value is the one with a clear head. 	An example of this could be seen in the art of the early twentieth century, particularly with the avant garde school of art. In
    <text>should have been curated in, for example, the arts and heritage sections of the Liverpool Biennial, and might have included works from the curatorial team’s oratorio on the curatorial team’s computer. 	Such a curatorial approach could have taken the form of a ‘curatorial duel’ organised between the Liverpool Biennial and a chosen artist or artist, or a no-holds-barred ‘battle of ideas’ between the viewer and the chosen artist/vision. The viewer would enter the arena through the back door, and the artist/vision through the front, and the notion being that the two have equal access to enter. The notion here is that the viewing public have a contest to enter, and the artist/vision with a clearly articulated idea of its value is the one with a clear head. 	An example of this could be seen in the art of the early twentieth century, particularly with the avant garde school of art. In the twentieth century, avant garde painting was a form of painting that was particularly active in the social world of urban dwellers (and later urban composers). It was a style of urban painting that was poetic in its language and political in its intentions, and depicted the obvious and obscure in ways that drew people’s attention and engaged with them. It was a form of urban painting that was prescient of the power of the postindustrial society it represented, and depicted the future in ways that were as yet utopian and unknowable. 	It might be objected that the biennial is the twentieth century’s last great urban painting biennial, and that, since the 1990s, cities everywhere have returned to their natural forms of rural or urban civilisation. The city is not that ancient, and cities do not produce artists. The city produces only those who are made artists by cities. 	However, this objection misses what is meant by the ‘natural’ in urban</text>
    <text>should have been curated in, for example, the arts and heritage sections of the Liverpool Biennial, and might have included works from the curatorial team’s oratorio on the curatorial team’s computer. 	Such a curatorial approach could have taken the form of a ‘curatorial duel’ organised between the Liverpool Biennial and a chosen artist or artist, or a no-holds-barred ‘battle of ideas’ between the viewer and the chosen artist/vision. The viewer would enter the arena through the back door, and the artist/vision through the front, and the notion being that the two have equal access to enter. The notion here is that the viewing public have a contest to enter, and the artist/vision with a clearly articulated idea of its value is the one with a clear head. 	An example of this could be seen in the art of the early twentieth century, particularly with the avant garde school of art. In the 1920s and 1930s, a generation of artists was developing avant garde practices through which the avant garde read the aesthetic of the progressive ideal and projected it onto a world map. The notion of the ‘national register’ of taste was developed here, and it is this notion that informs contemporary taste-design. 	There were of course far more people watching television at the time, and the ‘station’ was not omnipresent, but the relative insignificance of the individual screen in relation to the institution was clear. A national supermarket and a national railway station were established in the years immediately following World War One, and it is through these intermediary screens that we can move from one national supermarket chain to another. The notion of the supermarket as an institution, and of taste as a social convention, is a development that is largely a product of the war effort and the post-war economic malaise. 	Similarly, the national railway station and its</text>
    <text>should have been curated in, for example, the arts and heritage sections of the Liverpool Biennial, and might have included works from the curatorial team’s oratorio on the curatorial team’s computer. 	Such a curatorial approach could have taken the form of a ‘curatorial duel’ organised between the Liverpool Biennial and a chosen artist or artist, or a no-holds-barred ‘battle of ideas’ between the viewer and the chosen artist/vision. The viewer would enter the arena through the back door, and the artist/vision through the front, and the notion being that the two have equal access to enter. The notion here is that the viewing public have a contest to enter, and the artist/vision with a clearly articulated idea of its value is the one with a clear head. 	An example of this could be seen in the art of the early twentieth century, particularly with the avant garde school of art. In the case of the Biennial, the viewer would not only enter the art contest, but would also be invited to design a piece of ‘art’ that is judged at the biennial. The viewer would then enter into a drawing for a chance to design a piece of artwork with the Biennial team. The artist/vision with the best entry into the curatorial race, as judged by the Liverpool Biennial, would win a cerulean blue crinoline, hand-coloured in the artist’s design, emblazoned on the back. The design would be made into a sculpture and presented as part of the exhibition at the Biennial's new, temporary gallery, The Second Level. The entry fee for this scheme was RM5,000. 	In recent years, biennials have increasingly come to occupy a central place in the city, often framed as a ‘museum of modernity’, sometimes as a ‘museum of</text>
    <text>should have been curated in, for example, the arts and heritage sections of the Liverpool Biennial, and might have included works from the curatorial team’s oratorio on the curatorial team’s computer. 	Such a curatorial approach could have taken the form of a ‘curatorial duel’ organised between the Liverpool Biennial and a chosen artist or artist, or a no-holds-barred ‘battle of ideas’ between the viewer and the chosen artist/vision. The viewer would enter the arena through the back door, and the artist/vision through the front, and the notion being that the two have equal access to enter. The notion here is that the viewing public have a contest to enter, and the artist/vision with a clearly articulated idea of its value is the one with a clear head. 	An example of this could be seen in the art of the early twentieth century, particularly with the avant garde school of art. In the Constructive Thinking Group exhibition catalogue for the Constructive Thinking exhibition (2011), Olu Oguibane (1905–1972) is listed as curator, but very quickly a group of outraged members of the public call him ‘a disgrace to the city’. The Biennial should, in this catalogue, have included a catalogue of complaints about the way in which the city is being created, as well as a list of all the architects who contributed to that destruction.  	The Biennial should also have included a catalogue of its own creation, a catalogue of how this was being done, and how it is being done now. A more complete catalogue would have shown the various projects that were being considered, and the people who were creating it. This is yet to be done, and in many ways is still not done, but perhaps a more complete catalogue of the city’s development could have been shown in the Constructive Thinking exhibition</text>
    <text>should have been curated in, for example, the arts and heritage sections of the Liverpool Biennial, and might have included works from the curatorial team’s oratorio on the curatorial team’s computer. 	Such a curatorial approach could have taken the form of a ‘curatorial duel’ organised between the Liverpool Biennial and a chosen artist or artist, or a no-holds-barred ‘battle of ideas’ between the viewer and the chosen artist/vision. The viewer would enter the arena through the back door, and the artist/vision through the front, and the notion being that the two have equal access to enter. The notion here is that the viewing public have a contest to enter, and the artist/vision with a clearly articulated idea of its value is the one with a clear head. 	An example of this could be seen in the art of the early twentieth century, particularly with the avant garde school of art. In the 1920s and 1930s, the avant garde school was particularly active in questioning the value of art, and in particular of the work of art produced and exhibited in the name of art by the curators of the Venice Biennale. The avant garde school of art flourished in part because it was able to work autonomously, although it was also the case that for many years its members were actively involved in the politics of perception, and in a sense, reality. The rise of biennials as a political medium occurred partly because of the reduction in the institutional capacity to deliver cultural impact, and particularly through the creation of what Samuel Butler in his classic book  	The Crisis of the Cultures of Art and Culture describes as ‘public beacons’ controlled by a single organisation: ‘a private central arts organisation, the American Museum of Naturalists, led by an administrator named William Jennings Bryan’s office was ideally placed to deliver this message to the</text>
    <text>should have been curated in, for example, the arts and heritage sections of the Liverpool Biennial, and might have included works from the curatorial team’s oratorio on the curatorial team’s computer. 	Such a curatorial approach could have taken the form of a ‘curatorial duel’ organised between the Liverpool Biennial and a chosen artist or artist, or a no-holds-barred ‘battle of ideas’ between the viewer and the chosen artist/vision. The viewer would enter the arena through the back door, and the artist/vision through the front, and the notion being that the two have equal access to enter. The notion here is that the viewing public have a contest to enter, and the artist/vision with a clearly articulated idea of its value is the one with a clear head. 	An example of this could be seen in the art of the early twentieth century, particularly with the avant garde school of art. In the early twentieth century, the avant garde was identified with figures such as Max Horkheimer, Max Horkheimer and Herman Melville, and their work reflected the general mood of the nascent modern intellectual, or even the general propensity for intellectual activity. In the twenty-first century, it is generally agreed on the marginalisation of culture as a precondition for a feeling of cultural marginalisation, and for the emergence of anti-intellectualism. 	However, the true valiance of the art of the avant garde is found in its critical application of critical theory to cultural practices, particularly to art’s social function within social relations. The theorists who formed this avant garde were predominantly Jewish (though at this time the de facto majority) but there were numerous cultural theorists, including from the West, from whom came a host of distinctive approaches to art and culture, some of which were adopted by the new theorists (including Max Horkheimer and Adolph</text>
    <text>should have been curated in, for example, the arts and heritage sections of the Liverpool Biennial, and might have included works from the curatorial team’s oratorio on the curatorial team’s computer. 	Such a curatorial approach could have taken the form of a ‘curatorial duel’ organised between the Liverpool Biennial and a chosen artist or artist, or a no-holds-barred ‘battle of ideas’ between the viewer and the chosen artist/vision. The viewer would enter the arena through the back door, and the artist/vision through the front, and the notion being that the two have equal access to enter. The notion here is that the viewing public have a contest to enter, and the artist/vision with a clearly articulated idea of its value is the one with a clear head. 	An example of this could be seen in the art of the early twentieth century, particularly with the avant garde school of art. In the early twentieth century, the avant garde was a dominant force in contemporary art, and its influence is still evident in the contemporary avant garde style of the present. 	Although the biennial has an important place within this history of art, it should also be remembered that it was the direct result of a collaboration between the Liverpool Biennial and the New Art Gallery (founded in 1893) in Liverpool, and the Biennial as a whole, in New York. The gallery in turn was the direct result of a merger between two earlier art galleries in Liverpool: the Cammell Laird Gallery and the Gros Ventre Gallery. Both galleries were closed during the 1920s and early 1930s, but their history is similar: they were instrumental in popularising the biennial, and provided funds to enable the stage shows that established the biennial as a regularised form. 	[1] http://www.biennial.com/en/search/results?q=</text>
    <text>should have been curated in, for example, the arts and heritage sections of the Liverpool Biennial, and might have included works from the curatorial team’s oratorio on the curatorial team’s computer. 	Such a curatorial approach could have taken the form of a ‘curatorial duel’ organised between the Liverpool Biennial and a chosen artist or artist, or a no-holds-barred ‘battle of ideas’ between the viewer and the chosen artist/vision. The viewer would enter the arena through the back door, and the artist/vision through the front, and the notion being that the two have equal access to enter. The notion here is that the viewing public have a contest to enter, and the artist/vision with a clearly articulated idea of its value is the one with a clear head. 	An example of this could be seen in the art of the early twentieth century, particularly with the avant garde school of art. In the early twentieth century, Claude Debussy’s work, with its repetitive, monotonous structure, and overtly political character, was read by many as being postmodern, but also by some as being postmodernist, and Debussy himself was not in any way an apologist for either position. Rather, he argued that his work was taking the place of bureaucracy, making art itself more image-based, less subject to static notions of image and sound, and allowing readers to emerge from the work into new ways of seeing the world. 	So the biennial could have entered the century with a decisively postmodern bent, and perhaps the very postmodernist spirit that had recently swept Europe. 	Perhaps we could argue that the biennial today is more like a ‘temporal object’, having entered the century at a different time, and with a different species of reader in mind. But this is still to some extent ahistoricising the biennial, and the</text>
    <text>should have been curated in, for example, the arts and heritage sections of the Liverpool Biennial, and might have included works from the curatorial team’s oratorio on the curatorial team’s computer. 	Such a curatorial approach could have taken the form of a ‘curatorial duel’ organised between the Liverpool Biennial and a chosen artist or artist, or a no-holds-barred ‘battle of ideas’ between the viewer and the chosen artist/vision. The viewer would enter the arena through the back door, and the artist/vision through the front, and the notion being that the two have equal access to enter. The notion here is that the viewing public have a contest to enter, and the artist/vision with a clearly articulated idea of its value is the one with a clear head. 	An example of this could be seen in the art of the early twentieth century, particularly with the avant garde school of art. In the early twentieth century, avant-garde artists had a key (if marginal) presence in the Biennial, as well as in the Tate and the Whitney. These were the artists who would later be called upon to write the journal that would eventually be published as Partisan Review. In her memoirs, Leigh Cowan records the early experiences of despair felt by Cowan and her husband, Norman, as a result of the Blitz. While their situation was hopeless, their treatment by the authorities seemed vindictive. A few years into Cowan’s engagement as a journalist, Norman recounts his despair at the lack of response from the authorities in the event – beyond the obvious downgrading of disaster from a military to a public relations disaster – despite the fact that the Biennial had just been founded. 	Norman was not alone in his feelings towards the Biennial. While the biennial was beginning to gain a following, its realisation was delayed in part by the reaction against it</text>
    <text>should have been curated in, for example, the arts and heritage sections of the Liverpool Biennial, and might have included works from the curatorial team’s oratorio on the curatorial team’s computer. 	Such a curatorial approach could have taken the form of a ‘curatorial duel’ organised between the Liverpool Biennial and a chosen artist or artist, or a no-holds-barred ‘battle of ideas’ between the viewer and the chosen artist/vision. The viewer would enter the arena through the back door, and the artist/vision through the front, and the notion being that the two have equal access to enter. The notion here is that the viewing public have a contest to enter, and the artist/vision with a clearly articulated idea of its value is the one with a clear head. 	An example of this could be seen in the art of the early twentieth century, particularly with the avant garde school of art. In the early twentieth century, avant garde painting could be seen as a potent counterweight to the more experimental painting of the contemporaneous period, which was often represented as being equally accessible and participatory. Modern avant garde painting is characterised by a preference for the minimalist, visual analogue of the architectural site, the simple addition of concrete or steel to the existing structures to make them appear more landscaped or urban. While this may in part be the case, there are other contemporary artists who also go deep into the architecture of buildings, and whose work is heavily influenced by the field of urbanism and its aesthetics. For example, Architecture of the Informant City, Michel Safra, is an example of an artist whose works are often show-based and whose urbanism is informed by the practices of ‘the architecture students’ at Newcastle University’s School of Architecture. Safra is perhaps best known for his work in the Beirut district of East Beirut</text>
  </text>
  <text>
    should have been curated by the Curator of Contemporary Art, Meehan Crist, who has a particular interest in the work of Mesha Necole, who she hopes will one day be a curator herself. Discovering how artists are drawn to places and people whose practices and aesthetics both align with her own tends to make her work more sensual, and her previous commissions have often been very sensual, too. The public galleries in Liverpool have a particular knack for feeding off the energy and pressure of the moment, and so when the Arts Council commissioned her to commission a major retrospective for the city in 2019, it was no surprise to see artists drawn to the idea of ‘re-constituting the city’ through the work she had already been doing with Urban Outfitters. When the Monument to the Spectator was first built, the suggestion was that it would be seen by hundreds of thousands of people each day. Today, it’s well over a
    <text>should have been curated by the Curator of Contemporary Art, Meehan Crist, who has a particular interest in the work of Mesha Necole, who she hopes will one day be a curator herself. Discovering how artists are drawn to places and people whose practices and aesthetics both align with her own tends to make her work more sensual, and her previous commissions have often been very sensual, too. The public galleries in Liverpool have a particular knack for feeding off the energy and pressure of the moment, and so when the Arts Council commissioned her to commission a major retrospective for the city in 2019, it was no surprise to see artists drawn to the idea of ‘re-constituting the city’ through the work she had already been doing with Urban Outfitters. When the Monument to the Spectator was first built, the suggestion was that it would be seen by hundreds of thousands of people each day. Today, it’s well over a million. So that work gets seen by a very specific number of people: the thousands who visit the Daily Mail online. The millions who visit the Daily Star. So the work that she did for the Daily Mail the day it was first published is a very specific kind of storytelling. It’s a narrative that gets told by lots of people in lots of places. It’s a narrative that is very easy to digest in digesting form, so it’s a narrative that a lot of people can get behind. It’s a narrative that is very different to the way that we would imagine the city or the nation to be, and so the question that came up a lot when we were talking about what the Liverpool Biennial might be was, of course, how to connect to the city in a way that is not reductive, but invites in people from different places, draws us out and builds us from the inside out.  In fact,</text>
    <text>should have been curated by the Curator of Contemporary Art, Meehan Crist, who has a particular interest in the work of Mesha Necole, who she hopes will one day be a curator herself. Discovering how artists are drawn to places and people whose practices and aesthetics both align with her own tends to make her work more sensual, and her previous commissions have often been very sensual, too. The public galleries in Liverpool have a particular knack for feeding off the energy and pressure of the moment, and so when the Arts Council commissioned her to commission a major retrospective for the city in 2019, it was no surprise to see artists drawn to the idea of ‘re-constituting the city’ through the work she had already been doing with Urban Outfitters. When the Monument to the Spectator was first built, the suggestion was that it would be seen by hundreds of thousands of people each day. Today, it’s well over a million. It’s a city of hundreds of millions, with a quarter of a billion people. The city is used to it’s vast, and yet pedestrian-friendly urban arrangements are still rare, so when artists and designers with a particular urban expertise set out to do something completely different, it’s often a challenge to figure out how to get people to where they’re going. It’s a challenge that Newart Co-operative in Liverpool has had to struggle with, as a set of goals that seem to be mutually exclusive. The challenges of Community Arts have often felt like they’re coming from somewhere else, and yet somehow we know where they’re going. When we don’t know what else to do with the art or design we produce, it’s easy to get stuck in. When we know what to do with the art or design, it’s much harder to change it. That</text>
    <text>should have been curated by the Curator of Contemporary Art, Meehan Crist, who has a particular interest in the work of Mesha Necole, who she hopes will one day be a curator herself. Discovering how artists are drawn to places and people whose practices and aesthetics both align with her own tends to make her work more sensual, and her previous commissions have often been very sensual, too. The public galleries in Liverpool have a particular knack for feeding off the energy and pressure of the moment, and so when the Arts Council commissioned her to commission a major retrospective for the city in 2019, it was no surprise to see artists drawn to the idea of ‘re-constituting the city’ through the work she had already been doing with Urban Outfitters. When the Monument to the Spectator was first built, the suggestion was that it would be seen by hundreds of thousands of people each day. Today, it’s well over a million, and yet the daily traffic jam that surrounds the site is still pretty much a stand-in for the city’s constantly shifting population. The museum has become something of a metaphor for the city – a point of intersection for people from all over the world, a point of convergence for cultures that sometimes clash, and a point of some sort of internal dialogue about where we are. However, the real test of whether a project is good or bad art, as much as whether it is a statement of intent or something else entirely, will always remain ambiguous. The reason why Liverpool Biennial is good art is precisely because it is constantly asking hard questions, presenting interesting alternatives and bringing different points of view to the negotiating table. It is asking fundamental questions of authority, ideology and tradition, and presenting those alternatives in a way that all of us can be part of. It is why artists from all over the world come to Liverpool, and why Liverpool Biennial is becoming something that it could be</text>
    <text>should have been curated by the Curator of Contemporary Art, Meehan Crist, who has a particular interest in the work of Mesha Necole, who she hopes will one day be a curator herself. Discovering how artists are drawn to places and people whose practices and aesthetics both align with her own tends to make her work more sensual, and her previous commissions have often been very sensual, too. The public galleries in Liverpool have a particular knack for feeding off the energy and pressure of the moment, and so when the Arts Council commissioned her to commission a major retrospective for the city in 2019, it was no surprise to see artists drawn to the idea of ‘re-constituting the city’ through the work she had already been doing with Urban Outfitters. When the Monument to the Spectator was first built, the suggestion was that it would be seen by hundreds of thousands of people each day. Today, it’s well over a million, and yet the daily traffic on the road to Anfield still has a negative impact on the city’s ecology, as it makes travel to and from the museum more problematic. The city is a source of culture for artists, and the council has a cultural function when it comes to art collections, but in this context the arts council might want to consider what it really is, and isn’t, and what it would mean to call the arts council something other than what it is.  Art and the Cultural Cycle  (1922–39)  was the result of the Cadbury Plan, named for the bakery where the first bomb was thrown during the First World War, and commissioned by the Cadbury Company. The plan called for a ‘central register of views on a given subject’, with views compiled and archived at a central location. The term came from a report drawn up for the ‘Committee on Comic Arts</text>
    <text>should have been curated by the Curator of Contemporary Art, Meehan Crist, who has a particular interest in the work of Mesha Necole, who she hopes will one day be a curator herself. Discovering how artists are drawn to places and people whose practices and aesthetics both align with her own tends to make her work more sensual, and her previous commissions have often been very sensual, too. The public galleries in Liverpool have a particular knack for feeding off the energy and pressure of the moment, and so when the Arts Council commissioned her to commission a major retrospective for the city in 2019, it was no surprise to see artists drawn to the idea of ‘re-constituting the city’ through the work she had already been doing with Urban Outfitters. When the Monument to the Spectator was first built, the suggestion was that it would be seen by hundreds of thousands of people each day. Today, it’s well over a million. How to Think of Art and the Cities that Shape It   	The power of artistic mobilisation lies in its ability to move us from one place to another. But its power is often used in ways that we would not approve of, or do not want to be associated with. In the 1970s and 1980s, the culture of the New Left was actively seeking to destroy capitalism and other forms of hierarchy, oppression and oppression of communities of colour. It was also interested in creating an environmentally sound society through social gardening and other participatory practices, so it was not surprised that artists, activists and intellectuals from that era were drawn to the ideas and practices of Community Arts. Communist Worker, January 1971. Billy Bragg, a key figure in the Black Communist Party, was one of the artists selected as an artist for the Liverpool Biennial, which was co-curated by W. E. B. Du Bois and Arthur</text>
    <text>should have been curated by the Curator of Contemporary Art, Meehan Crist, who has a particular interest in the work of Mesha Necole, who she hopes will one day be a curator herself. Discovering how artists are drawn to places and people whose practices and aesthetics both align with her own tends to make her work more sensual, and her previous commissions have often been very sensual, too. The public galleries in Liverpool have a particular knack for feeding off the energy and pressure of the moment, and so when the Arts Council commissioned her to commission a major retrospective for the city in 2019, it was no surprise to see artists drawn to the idea of ‘re-constituting the city’ through the work she had already been doing with Urban Outfitters. When the Monument to the Spectator was first built, the suggestion was that it would be seen by hundreds of thousands of people each day. Today, it’s well over a million. The Monument to the Spectator was built as a massive billboard, so the Theatre, the Arts Council and the city were able to harness the massive megaphone and distributed it thinly across several neighbourhoods in an attempt to saturate the building with a visual programme that would appeal to all audiences. The Theatre, Arts Council and city went about their work in the area with an air of aloofness and deliberate obscurity, able to present the project as something it wasn’t. The city was able to craft an entirely new narrative around the building, one grounded in facts and figures and filled with imagination-free zones. For a long time, the city was able to do this, but as the project continued, the Arts Council and city lost their minds and resorted to using old theatre, photography and dance resources to conjure a cityscape that would befitting of a celebrity chef’s latest culinary invention. I guess we are all victims of Reality Bites.Art is</text>
    <text>should have been curated by the Curator of Contemporary Art, Meehan Crist, who has a particular interest in the work of Mesha Necole, who she hopes will one day be a curator herself. Discovering how artists are drawn to places and people whose practices and aesthetics both align with her own tends to make her work more sensual, and her previous commissions have often been very sensual, too. The public galleries in Liverpool have a particular knack for feeding off the energy and pressure of the moment, and so when the Arts Council commissioned her to commission a major retrospective for the city in 2019, it was no surprise to see artists drawn to the idea of ‘re-constituting the city’ through the work she had already been doing with Urban Outfitters. When the Monument to the Spectator was first built, the suggestion was that it would be seen by hundreds of thousands of people each day. Today, it’s well over a million, and that’s a lot of people. The question is whether the city is capable of supporting that kind of infrastructure, or whether it needs to be built up like a big, expensive mansion. The answer to that is easy – yes it does. But the challenge becomes how to get that infrastructure to happen, and whether that’s possible without destroying the city or creating a new urban void. In an ideal world, the answer would be simple: the art world would endorse the idea and start funding research into the mechanisms of visual art as a visual medium; an understanding of how art is made and the way that it is experienced. Unfortunately, that wasn’t possible with the Miracle Mile coming together as a cultural entity in its own right, and while the Arts Council wanted to fund the documentary through the commissioning budget, funding the actual work – the documentary and its publication were separate funds. So the Arts Council had</text>
    <text>should have been curated by the Curator of Contemporary Art, Meehan Crist, who has a particular interest in the work of Mesha Necole, who she hopes will one day be a curator herself. Discovering how artists are drawn to places and people whose practices and aesthetics both align with her own tends to make her work more sensual, and her previous commissions have often been very sensual, too. The public galleries in Liverpool have a particular knack for feeding off the energy and pressure of the moment, and so when the Arts Council commissioned her to commission a major retrospective for the city in 2019, it was no surprise to see artists drawn to the idea of ‘re-constituting the city’ through the work she had already been doing with Urban Outfitters. When the Monument to the Spectator was first built, the suggestion was that it would be seen by hundreds of thousands of people each day. Today, it’s well over a million. The Monument to the Spectator started life as an art project for the Liverpool Sun , and then moved on to the Liverpool Docks, where it was exhibited in over 200 venues. It was at the Docks that Fred Brown started his pennet series Brighton Plastic Company, which would go on to be an important partner in Bluecoat. In the 1980s and 1990s, he was a regular at the Granby Four Streets Arcade, and Bluecoat, and in the early 2000s he was a regular at The Granby Four Streets. So there are a number of artists who have a connection to the history of the Granby Four Streets, and the way in which the artist Meehan Crist might be approaching some of these artists’ work through the prism of Cultural Practice. Meehan Crist was one of the first artists to be invited to Conway’s </text>
    <text>should have been curated by the Curator of Contemporary Art, Meehan Crist, who has a particular interest in the work of Mesha Necole, who she hopes will one day be a curator herself. Discovering how artists are drawn to places and people whose practices and aesthetics both align with her own tends to make her work more sensual, and her previous commissions have often been very sensual, too. The public galleries in Liverpool have a particular knack for feeding off the energy and pressure of the moment, and so when the Arts Council commissioned her to commission a major retrospective for the city in 2019, it was no surprise to see artists drawn to the idea of ‘re-constituting the city’ through the work she had already been doing with Urban Outfitters. When the Monument to the Spectator was first built, the suggestion was that it would be seen by hundreds of thousands of people each day. Today, it’s well over a million. In the 1960s, it was the largest public museum in the UK, now it’s part of the Tate Modern museum collection. It’s true that the capacity of contemporary art galleries today is much more limited by the need to house the ever-growing number of exhibitions and major exhibitions, but that doesn’t change the fact that the capacity of galleries in the 1960s was much more limited. From the mid-1960s the turn of events in Liverpool led to the formation of the Liverpool Biennial, then the Liverpool National Portrait Gallery, then the National Gallery of Canada, then M.I.T. and M.S.F. (M.I.T. being the precursor to M.S.F.), then Glasgow, M.F. (later renamed Glasgow University), then Cambridge, M.D. (later renamed Cam</text>
    <text>should have been curated by the Curator of Contemporary Art, Meehan Crist, who has a particular interest in the work of Mesha Necole, who she hopes will one day be a curator herself. Discovering how artists are drawn to places and people whose practices and aesthetics both align with her own tends to make her work more sensual, and her previous commissions have often been very sensual, too. The public galleries in Liverpool have a particular knack for feeding off the energy and pressure of the moment, and so when the Arts Council commissioned her to commission a major retrospective for the city in 2019, it was no surprise to see artists drawn to the idea of ‘re-constituting the city’ through the work she had already been doing with Urban Outfitters. When the Monument to the Spectator was first built, the suggestion was that it would be seen by hundreds of thousands of people each day. Today, it’s well over a million. I wonder how many art books still have references to the book, with its sections on the natural history of the eye, the origins of light, and the structure of the human eye. The book has been republished twenty-four times, and its annotations are among the most widely used in the English language. How to Deconstruct the Eye, Karen Bowden, New York Times, 1996.  References  Bowden, A. (1996). The New Economy of Images. Cambridge, MA: Cambridge University Press. Caggiula, G. (2012). State of Mind. New York: New Directions. Caggiula, G., &amp; Sexton, A. (2012). On the nature of experience. Chicago: University of Chicago Press. Caggiula, G., &amp; Ting, E. (2012). On agency and identity. Chicago: University of Chicago Press. Caggiula, G., &amp; Ting, E. (2012).</text>
  </text>
  <text>
    Working with algorithms, artists’ creative processes are often led by the threat. In the 1950s, Vernor Vinge developed an algorithm to predict the outcome of political elections, and it was used to help design the major forms of social organisation in Britain during the Second World War. The advent of computer programming meant that critical thinking about the state of science and technology became possible, and artists were invited to contribute to what became known as the Great Game, a contest to determine who would lead the technological revolution. The first Venice Biennale was organised by Alan Abramowitz and Anthony Gardner, and included works by Van Gogh, Debord, Thomas Allsen and many others. The biennale, which was so named because of its voyages around the city, was a ‘citizen science’, inviting artists to contribute to the mission of the city. The term ‘citizen science’ was used to describe a thinking person’s access to technologies that would not only change how people
    <text>Working with algorithms, artists’ creative processes are often led by the threat. In the 1950s, Vernor Vinge developed an algorithm to predict the outcome of political elections, and it was used to help design the major forms of social organisation in Britain during the Second World War. The advent of computer programming meant that critical thinking about the state of science and technology became possible, and artists were invited to contribute to what became known as the Great Game, a contest to determine who would lead the technological revolution. The first Venice Biennale was organised by Alan Abramowitz and Anthony Gardner, and included works by Van Gogh, Debord, Thomas Allsen and many others. The biennale, which was so named because of its voyages around the city, was a ‘citizen science’, inviting artists to contribute to the mission of the city. The term ‘citizen science’ was used to describe a thinking person’s access to technologies that would not only change how people lived their lives, but would also enable creative action. The Venice Biennale website continues to claim that the technology is now a force for ‘good’, enabling ‘thoughtful design’ and enabling ‘creative solutions’. While some might question the need for such lofty claims, the power of language and the built environment today exert themselves in mysterious and sometimes disastrous ways. The claims that the biennale is investing in the future of cities, or in the infrastructure of cities, or in local infrastructure in general, are being widely questioned. Is the biennale including in its portfolio infrastructure that can support a future city, or in constructing a future infrastructure that will support only its present city, the very definition of infrastructure? Is the biennale including ‘technologies that can be used to optimise ‘city outcomes’, or putting it another way, technologies that can be optimised through scientific research, so long as they are used ethically</text>
    <text>Working with algorithms, artists’ creative processes are often led by the threat. In the 1950s, Vernor Vinge developed an algorithm to predict the outcome of political elections, and it was used to help design the major forms of social organisation in Britain during the Second World War. The advent of computer programming meant that critical thinking about the state of science and technology became possible, and artists were invited to contribute to what became known as the Great Game, a contest to determine who would lead the technological revolution. The first Venice Biennale was organised by Alan Abramowitz and Anthony Gardner, and included works by Van Gogh, Debord, Thomas Allsen and many others. The biennale, which was so named because of its voyages around the city, was a ‘citizen science’, inviting artists to contribute to the mission of the city. The term ‘citizen science’ was used to describe a thinking person’s access to technologies that would not only change how people lived their daily lives, but would also enable creative action. Much like how artists today work with code to create new ways of inhabiting the city, today’s technologists engage in what Vernor Vinge called ‘metainterface’ – the creation and active participation of data in shaping and enabling the future of cities.  The Biennale’s own website describes the mission of the Venice Biennale:  The Biennale is a public cultural initiative dedicated to the promotion of the cultural heritage of the city through research, documentation and interactive experience. Its mandate is to ‘reproduce the historic character of the Biennale through research, documentation and interactive experiences; establish a national network of art point of entry; and promote the exchange of cultural characteristics between the different parts of the world via cultural tourism and ‘re-constituting cities through research, documentation and transient art institutions’.1 In the 2009 Biennale, the</text>
    <text>Working with algorithms, artists’ creative processes are often led by the threat. In the 1950s, Vernor Vinge developed an algorithm to predict the outcome of political elections, and it was used to help design the major forms of social organisation in Britain during the Second World War. The advent of computer programming meant that critical thinking about the state of science and technology became possible, and artists were invited to contribute to what became known as the Great Game, a contest to determine who would lead the technological revolution. The first Venice Biennale was organised by Alan Abramowitz and Anthony Gardner, and included works by Van Gogh, Debord, Thomas Allsen and many others. The biennale, which was so named because of its voyages around the city, was a ‘citizen science’, inviting artists to contribute to the mission of the city. The term ‘citizen science’ was used to describe a thinking person’s access to technologies that would not only change how people lived their daily lives, but would also help create a new social order based on the sharing of people’s scarce and captured wealth. The hope was that by breaking apart the technological society into small, self-contained units, and then placing these units under the leadership of locally-influenced, locally-governed technology platforms, a multidisciplinary approach to sharing, sharing, sharing could be devised.  The biennale didn’t just see Venice as a location for art, but also as a ‘space for exchange and communication’, where people from all over the world could come to live, exchange, and exchange ideas. In other words, it acted as a catalyst for creative thinking, but also for the creation of repressive systems that shaped people’s thinking, action and reactions to a world based on profit and a handful of privileged few. The biennale certainly placed a premium on the idea that technology was a destructive force, and that art was</text>
    <text>Working with algorithms, artists’ creative processes are often led by the threat. In the 1950s, Vernor Vinge developed an algorithm to predict the outcome of political elections, and it was used to help design the major forms of social organisation in Britain during the Second World War. The advent of computer programming meant that critical thinking about the state of science and technology became possible, and artists were invited to contribute to what became known as the Great Game, a contest to determine who would lead the technological revolution. The first Venice Biennale was organised by Alan Abramowitz and Anthony Gardner, and included works by Van Gogh, Debord, Thomas Allsen and many others. The biennale, which was so named because of its voyages around the city, was a ‘citizen science’, inviting artists to contribute to the mission of the city. The term ‘citizen science’ was used to describe a thinking person’s access to technologies that would not only change how people lived their daily lives, but would also enable creative action. This was a ‘radical change in how people understood and thought about the city’, as well as a shift in the way in which people viewed and imagined themselves as urban inhabitants. The Biennale convened artists from across Europe and North America, as well as from many other places in the world, including from the projects that accompanied the artist Ken Russell’s visits to the city. Artists from diverse disciplines came together to explore how the technologies of the city could be used to engage people, bringing with them visions of a radically different future urban future.  Whether the city is conceived as a complex web of interacting components or large, embedded systems is ahistorical, since the city contains all manner of interrelatedness that can be understood as interdependent.  The idea of the biennale as a global phenomenon, in its globalising circulations and accelerated adjustments, is a historical accident, the result of the alliance of</text>
    <text>Working with algorithms, artists’ creative processes are often led by the threat. In the 1950s, Vernor Vinge developed an algorithm to predict the outcome of political elections, and it was used to help design the major forms of social organisation in Britain during the Second World War. The advent of computer programming meant that critical thinking about the state of science and technology became possible, and artists were invited to contribute to what became known as the Great Game, a contest to determine who would lead the technological revolution. The first Venice Biennale was organised by Alan Abramowitz and Anthony Gardner, and included works by Van Gogh, Debord, Thomas Allsen and many others. The biennale, which was so named because of its voyages around the city, was a ‘citizen science’, inviting artists to contribute to the mission of the city. The term ‘citizen science’ was used to describe a thinking person’s access to technologies that would not only change how people lived their daily lives, but would also enable creative action.The biennale quickly became one of the most popular biennials of the twentieth century, and many artists, including Vorticism artists, were drawn to the idea of exploring new frontiers, exploring ‘lost art forms’ and emerging technologies. The key to the success of the biennale, as well as the technology that made it possible, was the artist and director, and the connections he or she established with key stakeholders in the city. The director was Alfred Eisenstaedt, who directed the biennale as a ‘special event’ in 1991, and ‘Curator of Modern Art’ in 1992. Eisenstaedt’s involvement was fundamental, and his encouragement was the key to the festival’s success. Editions of the Biennale were regularly curated by him and Gardner, and Gardner took on an extended, three-year sabbatical in 1991–</text>
    <text>Working with algorithms, artists’ creative processes are often led by the threat. In the 1950s, Vernor Vinge developed an algorithm to predict the outcome of political elections, and it was used to help design the major forms of social organisation in Britain during the Second World War. The advent of computer programming meant that critical thinking about the state of science and technology became possible, and artists were invited to contribute to what became known as the Great Game, a contest to determine who would lead the technological revolution. The first Venice Biennale was organised by Alan Abramowitz and Anthony Gardner, and included works by Van Gogh, Debord, Thomas Allsen and many others. The biennale, which was so named because of its voyages around the city, was a ‘citizen science’, inviting artists to contribute to the mission of the city. The term ‘citizen science’ was used to describe a thinking person’s access to technologies that would not only change how people lived but also how ideas were constructed and interpreted. The Venice Biennale has an ongoing ‘citizen science?’ project that seeks to find the language of the future through the city’s infrastructure, infrastructure that it not only produced but also produced via its inhabitants. This may be seen as a metaphor. The biennale has recently commissioned a new generation of artists to contribute to its new generation of infrastructure, making the city the subject of a new work?sř infrastructure project? Or, put another way, the infrastructure of a different utopian future. The infrastructure of the future city? The architecture of tomorrow? Or perhaps, just as important, the infrastructure of today? The construction of cities that are as connected to one another as possible? These are the grand questions of our time, and city-building at its most visionary.Doubtless, the city has been on city-building timelines for a long time. But the biennale is different, and city</text>
    <text>Working with algorithms, artists’ creative processes are often led by the threat. In the 1950s, Vernor Vinge developed an algorithm to predict the outcome of political elections, and it was used to help design the major forms of social organisation in Britain during the Second World War. The advent of computer programming meant that critical thinking about the state of science and technology became possible, and artists were invited to contribute to what became known as the Great Game, a contest to determine who would lead the technological revolution. The first Venice Biennale was organised by Alan Abramowitz and Anthony Gardner, and included works by Van Gogh, Debord, Thomas Allsen and many others. The biennale, which was so named because of its voyages around the city, was a ‘citizen science’, inviting artists to contribute to the mission of the city. The term ‘citizen science’ was used to describe a thinking person’s access to technologies that would not only change how people lived their daily lives, but would also enable creative acts. The hope was that the advanced art forms that we produce would also be able to help create new modes of daily activity. 	In the 1950s, the biennale was part of a wider curatorial push to open up the city as a place where science and technology could be explored and combined in ways that would enhance the city’s capacity to support cultural activities. This was part of a wider cultural planning process that also sought to create ‘new ways of thinking about cities’ through ‘new urban forms’ and ‘new language systems’. 	The new urban forms that were being imagined included ‘indoor and ‘overground’ forms of cultural activity linked to electric power, mass transit, and information technologies. These forms would also support ‘induction’ of language through ‘artificial intelligence’ and ‘new media’ through the combined effects of</text>
    <text>Working with algorithms, artists’ creative processes are often led by the threat. In the 1950s, Vernor Vinge developed an algorithm to predict the outcome of political elections, and it was used to help design the major forms of social organisation in Britain during the Second World War. The advent of computer programming meant that critical thinking about the state of science and technology became possible, and artists were invited to contribute to what became known as the Great Game, a contest to determine who would lead the technological revolution. The first Venice Biennale was organised by Alan Abramowitz and Anthony Gardner, and included works by Van Gogh, Debord, Thomas Allsen and many others. The biennale, which was so named because of its voyages around the city, was a ‘citizen science’, inviting artists to contribute to the mission of the city. The term ‘citizen science’ was used to describe a thinking person’s access to technologies that would not only change how people lived their lives, but also how knowledge was produced and disseminated. It referred to a critical look at how ideas were produced and disseminated through participatory methods.At the time, biennials were experimenting with new ways of organising themselves, thinking about what they called ‘subversion of authority’. In other words, they were questioning whether the state was simply failing to deliver what it promised, and so it was possible to intervene independently. This was a radically new way of thinking about how people should be organising themselves, and it seems utterly preposterous to think that the biennial would ever be organised in such a way again. But there was another, more radical way of organising things: what artist Jeanne van Heeswijk termed ‘exhibitionism’. This was when the museum became the site of an exhibition, or ‘a display of things not seen but not understood’. It referred to a situation in which the biennial, and by implication</text>
    <text>Working with algorithms, artists’ creative processes are often led by the threat. In the 1950s, Vernor Vinge developed an algorithm to predict the outcome of political elections, and it was used to help design the major forms of social organisation in Britain during the Second World War. The advent of computer programming meant that critical thinking about the state of science and technology became possible, and artists were invited to contribute to what became known as the Great Game, a contest to determine who would lead the technological revolution. The first Venice Biennale was organised by Alan Abramowitz and Anthony Gardner, and included works by Van Gogh, Debord, Thomas Allsen and many others. The biennale, which was so named because of its voyages around the city, was a ‘citizen science’, inviting artists to contribute to the mission of the city. The term ‘citizen science’ was used to describe a thinking person’s access to technologies that would not only change how people lived but would also change the world.The fascination with engineering and mathematical models runs contrary to the dominant cultural perception of the technological revolution. Science and technology are thought of as mysterious, mysterious places where there is no place for human agency. But the emergence of programming, the manipulation of data and data structures, and the manipulation of algorithms – the thinking parts – was a central part of the technological revolution. The emergence of programming was facilitated by the spread of technical know-how, which could be applied to almost anything. There was a widespread perception that programming is common place, but rarely executed, and that the language that underlies almost everything is mysterious. This is often attributed to the second-wave feminist and activist languages, but there is also a strong counterintuitive element to the more accessible dialects of programming.Consider the following simple example, given in the imperative imperative of a computer programme: if a condition is met, then do something. More generally, every statement in the programme should be treated as</text>
    <text>Working with algorithms, artists’ creative processes are often led by the threat. In the 1950s, Vernor Vinge developed an algorithm to predict the outcome of political elections, and it was used to help design the major forms of social organisation in Britain during the Second World War. The advent of computer programming meant that critical thinking about the state of science and technology became possible, and artists were invited to contribute to what became known as the Great Game, a contest to determine who would lead the technological revolution. The first Venice Biennale was organised by Alan Abramowitz and Anthony Gardner, and included works by Van Gogh, Debord, Thomas Allsen and many others. The biennale, which was so named because of its voyages around the city, was a ‘citizen science’, inviting artists to contribute to the mission of the city. The term ‘citizen science’ was used to describe a thinking person’s access to technologies that would not only change how people lived but would also change the world. The contest became an important tool for Orwell’s political activism, as well as a source of income for the artist.In the 1950s and 1960s, ‘communist propaganda’ was often characterised by the use of utopian visions of a utopian society. In the 1960s and the 1970s, however, things started to change. The civil rights movement began to gain momentum, and the arts organisations started to be enticed by the new cinematic techniques that would make them visible. The avant garde school started to be encouraged, while critical theory and critical race theory started to be developed. Some would say that the end of white supremacy in the US and the beginning of black activism in the UK coincided with the beginning of the biennale. However, there was a tipping point at around 1970, when critical theory and art movements started to see themselves as political activists and began to understand that their main role was political. There was a</text>
  </text>
  <text>
    subsequent iterative processing by machines is possible – and at some point inevitable – but progress is a process of increasing internal contradictions and internal creativity. The future looks like the past, in constant flux and change, but despite this constant shifting of the past, the future seems to present a logical extension of the past. The future is a scenario in which important decisions about the future are made on the basis of deep neural interfaces that predict and analyze the future with remarkable accuracy. Deep learning algorithms not only predict the future, but actively intervene in the future to shape it to suit their purposes. This is at odds with the past, in that historically critical social and political shifts in the future have to be negotiated with historical and institutional shifts in the past. In other words, the future is likely to be more like the past, in which the social and economic conditions of the present are shaped by the needs of the financial and legal sectors in the immediate and long term.The future is a site of massive transnational corporations and their legal domine
    <text>subsequent iterative processing by machines is possible – and at some point inevitable – but progress is a process of increasing internal contradictions and internal creativity. The future looks like the past, in constant flux and change, but despite this constant shifting of the past, the future seems to present a logical extension of the past. The future is a scenario in which important decisions about the future are made on the basis of deep neural interfaces that predict and analyze the future with remarkable accuracy. Deep learning algorithms not only predict the future, but actively intervene in the future to shape it to suit their purposes. This is at odds with the past, in that historically critical social and political shifts in the future have to be negotiated with historical and institutional shifts in the past. In other words, the future is likely to be more like the past, in which the social and economic conditions of the present are shaped by the needs of the financial and legal sectors in the immediate and long term.The future is a site of massive transnational corporations and their legal domineering, globalised bureaucracies; it is also a site of accelerating ecological destruction. But beyond the everyday (how can we know what the future will be like?), there are fundamental questions about the nature of power and domination still pertaining to us – questions of sovereignty, of course. How can we collectively create, collectively become conscious of our own complicity in this destruction? How can we become conscious of our complicity in the future loss of planetary systems of ecology, species and ecosystems? How do we reclaim the means of planetary production from the machine?These questions and more are the subject of Werner Heine's fascinating and powerful new book The Fourth Industrial Revolution,4 which came out in 2017. It is the culmination of more than a century of theoretical work on the kind of social and political changes that are characteristic of a modern economy, but it is also a sudden and palpably visible reaction to the changing conditions of life on this planet. While not everyone involved was involved in the development of the technology, everyone</text>
    <text>subsequent iterative processing by machines is possible – and at some point inevitable – but progress is a process of increasing internal contradictions and internal creativity. The future looks like the past, in constant flux and change, but despite this constant shifting of the past, the future seems to present a logical extension of the past. The future is a scenario in which important decisions about the future are made on the basis of deep neural interfaces that predict and analyze the future with remarkable accuracy. Deep learning algorithms not only predict the future, but actively intervene in the future to shape it to suit their purposes. This is at odds with the past, in that historically critical social and political shifts in the future have to be negotiated with historical and institutional shifts in the past. In other words, the future is likely to be more like the past, in which the social and economic conditions of the present are shaped by the needs of the financial and legal sectors in the immediate and long term.The future is a site of massive transnational corporations and their legal domineering effects on the landscape of the future. These are likely to be influenced by the trends predicted by deep learning, and will be driven by the logic of legalism, not by the needs of the planet as a whole. And although this legalism may be driven by the needs of the capitalist court, its actualities will be shaped by the whims of unpredictable technological innovations. In other words, the legal order and legal regime that emerges out of the contradictions of the social and the economic may be questioned. This is an in-prong of what we might call ‘biennarchical capitalism’.In the past, questions of ecology and sustainability were controversial, but with the explosion of global shipping and the subsequent shift of many of these ships to coastlines, questions of ecological injustice shifted from the ocean to the shore – questions of resource exploitation, specifically. Today, we do not have the scientific or legal sophistication to question the sustainability of ecologies that routinely exploit resources and shift the ecological</text>
    <text>subsequent iterative processing by machines is possible – and at some point inevitable – but progress is a process of increasing internal contradictions and internal creativity. The future looks like the past, in constant flux and change, but despite this constant shifting of the past, the future seems to present a logical extension of the past. The future is a scenario in which important decisions about the future are made on the basis of deep neural interfaces that predict and analyze the future with remarkable accuracy. Deep learning algorithms not only predict the future, but actively intervene in the future to shape it to suit their purposes. This is at odds with the past, in that historically critical social and political shifts in the future have to be negotiated with historical and institutional shifts in the past. In other words, the future is likely to be more like the past, in which the social and economic conditions of the present are shaped by the needs of the financial and legal sectors in the immediate and long term.The future is a site of massive transnational corporations and their legal domineering influence, which aims to establish a universal, universal commonwealth. These corporations do not create wealth for themselves; rather, they strive to expand markets and capture resources in the service of their operations. As a consequence, the world as a global commons is distributed across many different kinds of actors – including many different kinds of actors with very different motivations and capacities – who all need to engage in certain kinds of negotiation with one another. To some extent, this means that various kinds of economic and legal coercion is required. To others, it means that different kinds of people are likely to be able to coexist in harmonious ways.I think that what is needed is a politics of the ambiguous, not just a politics of the finite, capable and subject, but a politics of the unknowable and the non-knowable, including a politics of the powerless and the non-human. I would like to think of ways of envisioning that which do not necessarily correspond to concrete, concrete forms but can</text>
    <text>subsequent iterative processing by machines is possible – and at some point inevitable – but progress is a process of increasing internal contradictions and internal creativity. The future looks like the past, in constant flux and change, but despite this constant shifting of the past, the future seems to present a logical extension of the past. The future is a scenario in which important decisions about the future are made on the basis of deep neural interfaces that predict and analyze the future with remarkable accuracy. Deep learning algorithms not only predict the future, but actively intervene in the future to shape it to suit their purposes. This is at odds with the past, in that historically critical social and political shifts in the future have to be negotiated with historical and institutional shifts in the past. In other words, the future is likely to be more like the past, in which the social and economic conditions of the present are shaped by the needs of the financial and legal sectors in the immediate and long term.The future is a site of massive transnational corporations and their legal domineering influence. These companies are likely to produce ever more autonomous and connected vehicles and services, augmenting human intelligence and the infrastructure of social interaction with ever more sophisticated data mining and crunching. These developments are likely to result in ever greater bureaucratic complexity, and potentially in the creation of nonstate actors capable of implementing these technologies and social interactions. The concerns of vulnerable and nonstate actors are likely to be exacerbated, and new threats will emerge with increasing frequency as a consequence of this growing interfaceality. These may include situations of civil unrest, but also attacks on critical infrastructures of information and communication infrastructure, and the use of disruptive technologies on critical infrastructures.It is no exaggeration to suggest that the scale of these new threats is ever changing. As a consequence, it is not clear that the capabilities to mitigate a threat are present at the global level, and hence it is easy to be miscalculated by people with a clear interest in profit at the expense of the common. It</text>
    <text>subsequent iterative processing by machines is possible – and at some point inevitable – but progress is a process of increasing internal contradictions and internal creativity. The future looks like the past, in constant flux and change, but despite this constant shifting of the past, the future seems to present a logical extension of the past. The future is a scenario in which important decisions about the future are made on the basis of deep neural interfaces that predict and analyze the future with remarkable accuracy. Deep learning algorithms not only predict the future, but actively intervene in the future to shape it to suit their purposes. This is at odds with the past, in that historically critical social and political shifts in the future have to be negotiated with historical and institutional shifts in the past. In other words, the future is likely to be more like the past, in which the social and economic conditions of the present are shaped by the needs of the financial and legal sectors in the immediate and long term.The future is a site of massive transnational corporations and their legal domineering influence over the social, economic and political landscape of the world. It is also a site of massive inversions and manipulations – outturns, which are the backswards of a financial system, not the fronts. In other words, the future may well be more like the past, in that critical shifts in the future are negotiated with historical and institutional shifts in the past. I have a fantasy in which the social and economic conditions of the present are shaped by the needs of big business, and the courts and regulatory agencies that enforce those needs. But this is a fantasy with a legal flavour. The social conditions of the future have to be negotiated with the social conjugation of big data and big data-enabled analytics.The social conjugation of big data and big data-enabled analytics is the inevitable outcome of an Information Society. But here is the twist: while big data and analytics enable the social conjugation, they also tend to make the social impossible. That is,</text>
    <text>subsequent iterative processing by machines is possible – and at some point inevitable – but progress is a process of increasing internal contradictions and internal creativity. The future looks like the past, in constant flux and change, but despite this constant shifting of the past, the future seems to present a logical extension of the past. The future is a scenario in which important decisions about the future are made on the basis of deep neural interfaces that predict and analyze the future with remarkable accuracy. Deep learning algorithms not only predict the future, but actively intervene in the future to shape it to suit their purposes. This is at odds with the past, in that historically critical social and political shifts in the future have to be negotiated with historical and institutional shifts in the past. In other words, the future is likely to be more like the past, in which the social and economic conditions of the present are shaped by the needs of the financial and legal sectors in the immediate and long term.The future is a site of massive transnational corporations and their legal domineering effects on the world. It is likely that a city will exhibit similar corporate tendencies in the future, but unlike the past, this will not necessarily be a good or a place to be a place of resistance or resistance. We do not yet know what the infrastructure of a future city will look like, but based on the decisions being made today on the basis of deep learning, cities will exhibit tendencies towards a dystopian future. The rapid growth of financial and legal infrastructures will be the dominant forms of large-scale interactive media and interactive computing, and cities will play a significant role in this transition. We do not yet know what kinds of displacements such a city will inevitably enact, but given the inefficiency with which data is gathered and the inefficiency with which it is used – both here and elsewhere in the world – it is probable that there will be displaced workers and communities of inhabitants whose lives will be shaped by the information technologies of the future. This is particularly true for those groups</text>
    <text>subsequent iterative processing by machines is possible – and at some point inevitable – but progress is a process of increasing internal contradictions and internal creativity. The future looks like the past, in constant flux and change, but despite this constant shifting of the past, the future seems to present a logical extension of the past. The future is a scenario in which important decisions about the future are made on the basis of deep neural interfaces that predict and analyze the future with remarkable accuracy. Deep learning algorithms not only predict the future, but actively intervene in the future to shape it to suit their purposes. This is at odds with the past, in that historically critical social and political shifts in the future have to be negotiated with historical and institutional shifts in the past. In other words, the future is likely to be more like the past, in which the social and economic conditions of the present are shaped by the needs of the financial and legal sectors in the immediate and long term.The future is a site of massive transnational corporations and their legal domineering effects. These companies are not just interested in the expansion of their legal rights; they also have motives that derive from the future considerations of manufacturing, distribution and profit. Thus while technological progress may account for some of the increase in income and wealth in the past few decades, much of the increase has come from outside the legal and economic systems that regulate how and where income is earned, distributed and privileged. This is particularly true for women and people of color, whose economic rights have often been impacted by the legal systems that emerged in the wake of the economic boom. This is especially true in the postindustrial cities that have been part of the US post-WWII coastal juggernaut, such as New York and Los Angeles, which are overwhelmingly populated by white people. The postindustrial coastal city is increasingly the site of multiracial and multi-lingual coastal settlements populated by white people who speak multiple languages and mix cultures. Languages are no longer seen as a source of economic power but are now valued</text>
    <text>subsequent iterative processing by machines is possible – and at some point inevitable – but progress is a process of increasing internal contradictions and internal creativity. The future looks like the past, in constant flux and change, but despite this constant shifting of the past, the future seems to present a logical extension of the past. The future is a scenario in which important decisions about the future are made on the basis of deep neural interfaces that predict and analyze the future with remarkable accuracy. Deep learning algorithms not only predict the future, but actively intervene in the future to shape it to suit their purposes. This is at odds with the past, in that historically critical social and political shifts in the future have to be negotiated with historical and institutional shifts in the past. In other words, the future is likely to be more like the past, in which the social and economic conditions of the present are shaped by the needs of the financial and legal sectors in the immediate and long term.The future is a site of massive transnational corporations and their legal domineering influence – which is to say, its possible that these very corporations will still rule the world, but the ways in which that will be negotiated and managed will be significantly different from the ways in which we know these companies exist.’So what does it mean to be ‘in the now’? To be ‘in the present’, one must have some knowledge of the past, and be able to identify patterns or ‘habits’ that can be used to predict the future. One can only speculate, but there are indications that in the technology-led future, the human element will play a significantly diminished role. There will still be many forms of social organization capable of influencing the course of history, but the social possibilities for such power will be limited. One can only imagine what the future might be like if the human element were left out of the equation entirely. What do you think the future might be like if the human element were removed? Tell us in</text>
    <text>subsequent iterative processing by machines is possible – and at some point inevitable – but progress is a process of increasing internal contradictions and internal creativity. The future looks like the past, in constant flux and change, but despite this constant shifting of the past, the future seems to present a logical extension of the past. The future is a scenario in which important decisions about the future are made on the basis of deep neural interfaces that predict and analyze the future with remarkable accuracy. Deep learning algorithms not only predict the future, but actively intervene in the future to shape it to suit their purposes. This is at odds with the past, in that historically critical social and political shifts in the future have to be negotiated with historical and institutional shifts in the past. In other words, the future is likely to be more like the past, in which the social and economic conditions of the present are shaped by the needs of the financial and legal sectors in the immediate and long term.The future is a site of massive transnational corporations and their legal domineering effects on the landscape of the future. The constant shifting of corporate jurisdictions and national laws that prescribe what kinds of buildings can be built and how much can be built – along with the shifting of physical locations as the need for cheap housing and open, dense urban environments morphs – shapes what kinds of buildings can be built as well as what kinds of jobs can be created. These calculations and social-technical revolutions, which seem to magically become more precise and precise with every step, create conditions in the city in which there are no jobs, where open, dense urban environments are endangered, and where social struggles are frequent and intense. In Seoul, for example, the number of vacant buildings grew by 10% between 2007 and 2012, coinciding with a rapid expansion of financial and research industries. In New York City, the number of vacant buildings nearly doubled between 2000 and 2010, coinciding with a rapid expansion of financial and research industries. And yet, despite these statistical doubling and threefold increases, the</text>
    <text>subsequent iterative processing by machines is possible – and at some point inevitable – but progress is a process of increasing internal contradictions and internal creativity. The future looks like the past, in constant flux and change, but despite this constant shifting of the past, the future seems to present a logical extension of the past. The future is a scenario in which important decisions about the future are made on the basis of deep neural interfaces that predict and analyze the future with remarkable accuracy. Deep learning algorithms not only predict the future, but actively intervene in the future to shape it to suit their purposes. This is at odds with the past, in that historically critical social and political shifts in the future have to be negotiated with historical and institutional shifts in the past. In other words, the future is likely to be more like the past, in which the social and economic conditions of the present are shaped by the needs of the financial and legal sectors in the immediate and long term.The future is a site of massive transnational corporations and their legal domineering, multinational bureaucracies, often operating in cooperation with nongovernmental organizations (NGOs), staffed with former workers: taxi drivers, warehouse workers, receptionists, lorries and parcel men. These are just some of the characters we encounter in the book, drawn from a fictional framework encompassing real-life stakeholders, stakeholders with a point of view, stakeholders with a history, and stakeholders with a language. The question we address is: how can we collectively create spaces and situations in which to be part of a future that is uncertain, ambiguous and in which creativity and imagination are at the core of possibilities?This is part of our Cultural Justice project, part of our wider 'We Are Culture' initiative, and part of our 'Futures of our World' series exploring the ways in which our culture is embedded in the planet. Cultural Justice: Taking the Place of ProductionBrent Bellamy is Artistic Director for the Biennale of Sydney, and has worked with many artists</text>
  </text>
</AIcurator>
